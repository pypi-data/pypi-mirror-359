{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2025 Microsoft Corporation.\n",
    "import sys\n",
    "\n",
    "sys.path.insert(1, \"../../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "from pydantic import SecretStr\n",
    "\n",
    "from benchmark_qed.autod.data_processor.embedding import TextEmbedder\n",
    "from benchmark_qed.autod.data_processor.text_splitting import TokenTextSplitter\n",
    "from benchmark_qed.autod.io.document import (\n",
    "    create_documents,\n",
    "    save_documents,\n",
    ")\n",
    "from benchmark_qed.autod.io.text_unit import create_text_units, save_text_units\n",
    "from benchmark_qed.config.llm_config import LLMConfig, LLMProvider\n",
    "from benchmark_qed.llm.factory import ModelFactory\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger(\"httpx\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoD\n",
    "\n",
    "AutoD provides utilities for sampling datasets to match a target specification, defined in terms of the breadth (number of topic clusters to sample from) and depth (the number of samples per cluster) of data units (e.g. documents). It also provides ability to generate dataset summaries using a map-reduce approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA_PATH = \"../../datasets/AP_news/raw_data\"\n",
    "OUTPUT_DATA_PATH = \"./output/AP_news/processed_data\"\n",
    "TEXT_COLUMN = \"body_nitf\"\n",
    "METADATA_COLUMNS = [\"headline\", \"firstcreated\"]\n",
    "JSON_ENCODING = \"utf-8-sig\"\n",
    "\n",
    "# tokenizer used for chunking documents into text units\n",
    "ENCODING_MODEL = \"o200k_base\"\n",
    "CHUNK_SIZE = 600\n",
    "CHUNK_OVERLAP = 100\n",
    "\n",
    "# llm/embedding settings\n",
    "API_KEY = SecretStr(os.getenv(\"OPENAI_API_KEY\", \"\"))\n",
    "EMBEDDING_MODEL = \"text-embedding-3-large\"\n",
    "LLM_MODEL = \"gpt-4.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load documents\n",
    "\n",
    "- Supports CSV, JSON, and TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = create_documents(\n",
    "    input_path=INPUT_DATA_PATH,\n",
    "    input_type=\"json\",\n",
    "    text_tag=TEXT_COLUMN,\n",
    "    metadata_tags=METADATA_COLUMNS,\n",
    "    encoding=JSON_ENCODING,\n",
    ")\n",
    "document_df = save_documents(documents, OUTPUT_DATA_PATH)\n",
    "print(f\"Document count: {len(document_df)}\")\n",
    "document_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create text units\n",
    "\n",
    "Chunk documents into text units of the specified chunk size and overlap and embed all text units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(\n",
    "    encoding_name=ENCODING_MODEL,\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    ")\n",
    "\n",
    "text_embedder = TextEmbedder(\n",
    "    ModelFactory.create_embedding_model(\n",
    "        LLMConfig(\n",
    "            model=EMBEDDING_MODEL,\n",
    "            api_key=API_KEY,\n",
    "            llm_provider=LLMProvider.OpenAIEmbedding,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "text_units = await create_text_units(\n",
    "    documents=documents,\n",
    "    metadata_tags=METADATA_COLUMNS,\n",
    "    text_splitter=text_splitter,\n",
    "    text_embedder=text_embedder,\n",
    "    embed_text=True,\n",
    ")\n",
    "text_unit_df = save_text_units(text_units, OUTPUT_DATA_PATH)\n",
    "print(f\"Text unit count: {len(text_unit_df)}\")\n",
    "text_unit_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample text units using a Kmeans-based sampler\n",
    "\n",
    "The sampling process consists of 3 steps:\n",
    "\n",
    "1. Input text units are first clustered into K-clusters using Kmeans\n",
    "2. Select a representative unit for each cluster\n",
    "3. For each representative, select N nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark_qed.autod.sampler.enums import ClusterRepresentativeSelectionType\n",
    "from benchmark_qed.autod.sampler.sampling.kmeans_sampler import KmeansTextSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLUSTERS = 50\n",
    "NUM_SAMPLES_PER_CLUSTER = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = KmeansTextSampler()\n",
    "sampled_text_units = sampler.sample(\n",
    "    text_units=text_units,\n",
    "    sample_size=None,\n",
    "    num_clusters=NUM_CLUSTERS,\n",
    "    num_samples_per_cluster=NUM_SAMPLES_PER_CLUSTER,\n",
    "    cluster_representative_selection_type=ClusterRepresentativeSelectionType.CENTROID,\n",
    ")\n",
    "print(f\"Sampled text unit count: {len(sampled_text_units)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality check: check number of clusters\n",
    "clusters = [\n",
    "    sampled_text_units[i : i + NUM_SAMPLES_PER_CLUSTER]\n",
    "    for i in range(0, len(sampled_text_units), NUM_SAMPLES_PER_CLUSTER)\n",
    "]\n",
    "print(f\"Cluster count: {len(clusters)}\")\n",
    "\n",
    "# print first cluster\n",
    "print(\"First cluster:\")\n",
    "for i, text_unit in enumerate(clusters[0]):\n",
    "    print(f\"Text {i}: {text_unit.text}\")\n",
    "    print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize sampled text units using map-reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "from benchmark_qed.autod.summarization.global_summarizer import GlobalSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust this based on your model. For example, some reasoning models do not support temperature settings\n",
    "LLM_PARAMS = {\"temperature\": 0.0, \"seed\": 42}\n",
    "\n",
    "llm = ModelFactory.create_chat_model(\n",
    "    model_config=LLMConfig(\n",
    "        model=LLM_MODEL,\n",
    "        api_key=API_KEY,\n",
    "        llm_provider=LLMProvider.OpenAIChat,\n",
    "        call_args=LLM_PARAMS,\n",
    "    )\n",
    ")\n",
    "token_encoder = tiktoken.get_encoding(ENCODING_MODEL)\n",
    "\n",
    "summarizer = GlobalSummarizer(\n",
    "    llm=llm,\n",
    "    token_encoder=token_encoder,\n",
    "    response_type=\"single paragraph\",\n",
    "    max_data_tokens=8000,\n",
    "    map_llm_params=LLM_PARAMS,\n",
    "    reduce_llm_params=LLM_PARAMS,\n",
    "    concurrent_coroutines=32,\n",
    ")\n",
    "\n",
    "summary_result = await summarizer.asummarize(\n",
    "    text_units=sampled_text_units,\n",
    ")\n",
    "print(f\"Summary: {summary_result.summary}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmark-qed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
