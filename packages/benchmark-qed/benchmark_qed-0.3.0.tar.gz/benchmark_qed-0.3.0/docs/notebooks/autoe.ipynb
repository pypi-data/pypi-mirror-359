{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0caa0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2025 Microsoft Corporation.\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import cast\n",
    "\n",
    "import pandas as pd\n",
    "from pydantic import SecretStr\n",
    "from rich import print as rich_print\n",
    "\n",
    "from benchmark_qed.autoe.pairwise_scores import analyze_criteria, get_pairwise_scores\n",
    "from benchmark_qed.autoe.reference_scores import (\n",
    "    get_reference_scores,\n",
    "    summarize_reference_scores,\n",
    ")\n",
    "from benchmark_qed.cli.utils import print_df\n",
    "from benchmark_qed.config.llm_config import (\n",
    "    LLMConfig,\n",
    "    LLMProvider,\n",
    ")\n",
    "from benchmark_qed.config.model.score import (\n",
    "    pairwise_scores_criteria,\n",
    "    reference_scores_criteria,\n",
    ")\n",
    "from benchmark_qed.llm.factory import ModelFactory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87209e6",
   "metadata": {},
   "source": [
    "# AutoE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9ea340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351b3cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173041a8",
   "metadata": {},
   "source": [
    "# Relative Comparisons of RAG methods\n",
    "\n",
    "The AutoE component automates relative comparisons of RAG methods using the LLM-as-a-judge approach. It presents an LLM with pairs of answers, along with the query and target metric, in a counterbalanced order. The model then judges whether the first answer wins, loses, or ties with the second. Aggregating these judgments across multiple queries and trials yields **win rates** for each method.\n",
    "\n",
    "In the example below, we compare Vector RAG (baseline) with two competing methods: [GraphRAG's Global Search](https://github.com/microsoft/graphrag) and [LazyGraphRAG](https://www.microsoft.com/en-us/research/blog/lazygraphrag-setting-a-new-standard-for-quality-and-cost/?msockid=0ae860de9eb16660047973969f0b679b). We use 100 synthetic questions (50 activity-local and 50 activity-global) generated from 1,397 AP News health-related articles using AutoQ. Each query is evaluated in 4 counterbalanced trials across four default metrics (comprehensiveness, diversity, empowerment, and relevance), using GPT-4.1 as the judge.\n",
    "\n",
    "We hypothesize that GraphRAG’s Global Search, which is optimized for global questions, will outperform Vector RAG on global queries but underperform on local ones. LazyGraphRAG, a hybrid method, is expected to perform well on both.\n",
    "\n",
    "Choosing the right LLM judge is **critical**: less capable models may introduce biases and yield unreliable results. A useful first step in validating a judge model is to run an A/A test — comparing a RAG method against itself. This should result in a ~0.5 win rate with no statistically significant differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6af0cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config LLM model to be used as judge\n",
    "llm_config = LLMConfig(\n",
    "    model=\"gpt-4.1\",\n",
    "    api_key=SecretStr(os.environ[\"OPENAI_API_KEY\"]),\n",
    "    llm_provider=LLMProvider.OpenAIChat,\n",
    "    concurrent_requests=32,\n",
    "    call_args={\"temperature\": 0.0, \"seed\": 42},\n",
    ")\n",
    "llm_client = ModelFactory.create_chat_model(llm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aa3068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config conditions for comparison\n",
    "base = \"vector_rag\"\n",
    "others = [\"lazygraphrag\", \"graphrag_global\"]\n",
    "question_sets = [\"activity_global\", \"activity_local\"]\n",
    "trials = 4  # number of trials to run for each combination of [query, base, other]. Trials must be an even number to support counterbalancing.\n",
    "alpha = 0.05  # significance level used for statistical tests\n",
    "\n",
    "input_dir = \"./example_answers\"\n",
    "output_dir = Path(\"./output/win_rates\")\n",
    "if not output_dir.exists():\n",
    "    output_dir.mkdir(parents=True)\n",
    "\n",
    "# load default criteria. You can also define your own criteria as a list Criteria objects\n",
    "criteria = pairwise_scores_criteria()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06848cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run pairwise comparisons for each question set and each pair of [base, other].\n",
    "all_results = []\n",
    "for question_set in question_sets:\n",
    "    for other in others:\n",
    "        rich_print(f\"Processing {base} vs {other} for question set: {question_set}\")\n",
    "        result = get_pairwise_scores(\n",
    "            llm_client=llm_client,\n",
    "            llm_config=llm_config,\n",
    "            base_name=base,\n",
    "            other_name=other,\n",
    "            base_answers=pd.read_json(f\"{input_dir}/{base}/{question_set}.json\"),\n",
    "            other_answers=pd.read_json(f\"{input_dir}/{other}/{question_set}.json\"),\n",
    "            criteria=criteria,\n",
    "            trials=trials,\n",
    "            include_score_id_in_prompt=True,\n",
    "            question_id_key=\"question_id\",\n",
    "        )\n",
    "        result[\"question_set\"] = question_set\n",
    "        all_results.append(result)\n",
    "\n",
    "        # save pairwise results for each question set and pair of [base, other]\n",
    "        result.to_csv(\n",
    "            output_dir / f\"{question_set}_{base}--{other}.csv\",\n",
    "            index=False,\n",
    "        )\n",
    "\n",
    "# save all pairwise results in a single file\n",
    "all_results_df = pd.concat(all_results, ignore_index=True)\n",
    "all_results_df.to_csv(output_dir / \"win_rates.csv\", index=False)\n",
    "\n",
    "# perform significance testing on the results\n",
    "significance_test_results = analyze_criteria(\n",
    "    all_results_df,\n",
    "    alpha=alpha,\n",
    ")\n",
    "significance_test_results.to_csv(output_dir / \"winrates_sig_tests.csv\", index=False)\n",
    "\n",
    "print_df(\n",
    "    cast(\n",
    "        pd.DataFrame,\n",
    "        significance_test_results[\n",
    "            [\n",
    "                \"question_set\",\n",
    "                \"criteria\",\n",
    "                \"base_name\",\n",
    "                \"other_name\",\n",
    "                \"base_mean\",\n",
    "                \"other_mean\",\n",
    "                \"formatted_corrected_p_value\",\n",
    "            ]\n",
    "        ],\n",
    "    ),\n",
    "    \"Win Rates Summary\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1d756e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rich_print(\"Model usage statistics:\")\n",
    "rich_print(llm_client.get_usage())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de73fc78",
   "metadata": {},
   "source": [
    "## Reference-based Scoring\n",
    "\n",
    "When reference answers (such as ground truth or \"gold standard\" responses) are available, AutoE can evaluate RAG-generated answers against these references using metrics like correctness, completeness, or other user-defined criteria on a customizable scoring scale.\n",
    "\n",
    "In the example below, we use the same 100 synthetic questions (50 activity-local and 50 activity-global) generated by AutoQ. Since AutoQ does not provide ground-truth answers, we use LazyGraphRAG as the reference method because it achieved the best win rates in the pairwise relative comparisons above. We then score the answers from Vector RAG against those from LazyGraphRAG using the default metrics (correctness and completeness) on a scale from 1 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c898ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config LLM model to be used as judge\n",
    "llm_config = LLMConfig(\n",
    "    model=\"gpt-4.1\",\n",
    "    api_key=SecretStr(os.environ[\"OPENAI_API_KEY\"]),\n",
    "    llm_provider=LLMProvider.OpenAIChat,\n",
    "    concurrent_requests=32,\n",
    "    call_args={\"temperature\": 0.0, \"seed\": 42},\n",
    ")\n",
    "llm_client = ModelFactory.create_chat_model(llm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3306b14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config conditions for comparison\n",
    "reference = \"lazygraphrag\"\n",
    "generated_rags = [\n",
    "    \"vector_rag\"\n",
    "]  # you can add more generated RAGs to compare against the reference\n",
    "question_sets = [\"activity_global\", \"activity_local\"]\n",
    "trials = 4  # number of trials must be an even number to support counterbalancing\n",
    "\n",
    "input_dir = \"./example_answers\"\n",
    "output_dir = Path(\"./output/reference_scores\")\n",
    "if not output_dir.exists():\n",
    "    output_dir.mkdir(parents=True)\n",
    "\n",
    "# load default criteria (correctness and completeness). You can also define your own criteria as a list Criteria objects\n",
    "criteria = reference_scores_criteria()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d77ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run comparisons for each question set and each pair of [generated, reference].\n",
    "all_results = []\n",
    "all_summaries = []\n",
    "for question_set in question_sets:\n",
    "    for generated in generated_rags:\n",
    "        rich_print(\n",
    "            f\"Comparing {generated} vs. {reference} for question set: {question_set}\"\n",
    "        )\n",
    "        result = get_reference_scores(\n",
    "            llm_client=llm_client,\n",
    "            llm_config=llm_config,\n",
    "            reference_answers=pd.read_json(\n",
    "                f\"{input_dir}/{reference}/{question_set}.json\"\n",
    "            ),\n",
    "            generated_answers=pd.read_json(\n",
    "                f\"{input_dir}/{generated}/{question_set}.json\"\n",
    "            ),\n",
    "            criteria=criteria,\n",
    "            trials=trials,\n",
    "            score_min=1,\n",
    "            score_max=10,\n",
    "            include_score_id_in_prompt=True,\n",
    "            question_id_key=\"question_id\",\n",
    "        )\n",
    "\n",
    "        all_results.append(result)\n",
    "        result.to_csv(\n",
    "            output_dir / f\"{question_set}_{reference}--{generated}.csv\",\n",
    "            index=False,\n",
    "        )\n",
    "        summary_df = summarize_reference_scores(result)\n",
    "        summary_df[\"question_set\"] = question_set\n",
    "        summary_df[\"reference\"] = reference\n",
    "        summary_df[\"generated\"] = generated\n",
    "        all_summaries.append(summary_df)\n",
    "\n",
    "# save all results into a single file\n",
    "all_results_df = pd.concat(all_results, ignore_index=True)\n",
    "all_results_df.to_csv(output_dir / \"reference_scores.csv\", index=False)\n",
    "\n",
    "all_summary_df = pd.concat(all_summaries, ignore_index=True)\n",
    "print_df(\n",
    "    all_summary_df[\n",
    "        [\"question_set\", \"criteria\", \"reference\", \"generated\", \"mean\", \"std\"]\n",
    "    ].reset_index(),\n",
    "    \"Reference Scores Summary\",\n",
    ")\n",
    "all_summary_df.to_csv(output_dir / \"reference_scores_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e395a8e",
   "metadata": {},
   "source": [
    "## Assertion-based Scoring\n",
    "\n",
    "Assertion-based scoring evaluates RAG-generated answers by checking whether they contain specific factual assertions or claims that should be present according to a reference or gold standard. This approach is especially useful for tasks where the presence or absence of key facts is more important than holistic correctness or completeness.\n",
    "\n",
    "In this example, we use the same synthetic questions and answers as before. We assume that a set of reference assertions has been extracted for each question (e.g., using an information extraction pipeline or manual annotation). The LLM judge is tasked with verifying whether each assertion is supported by the generated answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225c147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config LLM model to be used as judge\n",
    "llm_config = LLMConfig(\n",
    "    model=\"gpt-4.1\",\n",
    "    api_key=SecretStr(os.environ[\"OPENAI_API_KEY\"]),\n",
    "    llm_provider=LLMProvider.OpenAIChat,\n",
    "    concurrent_requests=100,\n",
    "    call_args={\"temperature\": 0.0, \"seed\": 42},\n",
    ")\n",
    "llm_client = ModelFactory.create_chat_model(llm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675cee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "assertions_file = \"activity_global_assertions.json\"\n",
    "generated_rag = \"vector_rag\"\n",
    "pass_threshold = 0.5\n",
    "trials = 4  # number of trials\n",
    "\n",
    "input_dir = \"./example_answers\"\n",
    "output_dir = Path(\"./output/assertion_scores\")\n",
    "if not output_dir.exists():\n",
    "    output_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207cbce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from benchmark_qed.autoe.assertion_scores import get_assertion_scores\n",
    "\n",
    "answers = pd.read_json(f\"{input_dir}/{generated_rag}/activity_global.json\")\n",
    "\n",
    "assertions = (\n",
    "    pd.read_json(f\"{input_dir}/{assertions_file}\")\n",
    "    .explode(\"assertions\")\n",
    "    .rename(columns={\"assertions\": \"assertion\"})\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "assertion_score = get_assertion_scores(\n",
    "    llm_client=llm_client,\n",
    "    llm_config=llm_config,\n",
    "    answers=answers,\n",
    "    assertions=assertions,\n",
    "    trials=4,\n",
    "    question_id_key=\"question_id\",\n",
    "    question_text_key=\"question_text\",\n",
    "    answer_text_key=\"answer\",\n",
    ")\n",
    "\n",
    "assertion_score.to_csv(output_dir / \"assertion_scores.csv\", index=False)\n",
    "\n",
    "summary_by_assertion = (\n",
    "    assertion_score.groupby([\"question\", \"assertion\"])\n",
    "    .agg(score=(\"score\", lambda x: int(x.mean() > 0.5)), scores=(\"score\", list))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "summary_by_question = (\n",
    "    summary_by_assertion.groupby([\"question\"])\n",
    "    .agg(\n",
    "        success=(\"score\", lambda x: (x == 1).sum()),\n",
    "        fail=(\"score\", lambda x: (x == 0).sum()),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "summary_by_assertion[\"score_mean\"] = summary_by_assertion[\"scores\"].apply(\n",
    "    lambda x: np.mean(x) if len(x) > 0 else 0.0\n",
    ")\n",
    "summary_by_assertion[\"score_std\"] = summary_by_assertion[\"scores\"].apply(\n",
    "    lambda x: np.std(x) if len(x) > 0 else 0.0\n",
    ")\n",
    "summary_by_assertion = summary_by_assertion.drop(columns=[\"scores\"])\n",
    "\n",
    "print_df(\n",
    "    summary_by_question,\n",
    "    \"Assertion Scores Summary by Question\",\n",
    ")\n",
    "\n",
    "failed_assertions: pd.DataFrame = cast(\n",
    "    pd.DataFrame, summary_by_assertion[summary_by_assertion[\"score\"] == 0]\n",
    ")\n",
    "\n",
    "failed_assertions = failed_assertions.drop(columns=[\"score\"])\n",
    "\n",
    "if len(failed_assertions) > 0:\n",
    "    print_df(\n",
    "        failed_assertions,\n",
    "        f\"[bold red]{failed_assertions.shape[0]} Failed Assertions[/bold red]\",\n",
    "    )\n",
    "    rich_print(\n",
    "        f\"[bold red]{failed_assertions.shape[0]} assertions failed. See {output_dir / 'assertion_scores.csv'} for details.[/bold red]\"\n",
    "    )\n",
    "else:\n",
    "    rich_print(\"[bold green]All assertions passed.[/bold green]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fde1f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmark-qed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
