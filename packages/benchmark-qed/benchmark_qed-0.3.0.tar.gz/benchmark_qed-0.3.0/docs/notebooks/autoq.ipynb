{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a1db8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2025 Microsoft Corporation.\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(1, \"../../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dacb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb39cd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from pydantic import SecretStr\n",
    "\n",
    "from benchmark_qed.autod.data_processor.embedding import TextEmbedder\n",
    "from benchmark_qed.autod.io.text_unit import load_text_units\n",
    "from benchmark_qed.autoq.io.activity import (\n",
    "    save_activity_context,\n",
    ")\n",
    "from benchmark_qed.autoq.io.question import (\n",
    "    load_questions,\n",
    "    save_questions,\n",
    ")\n",
    "from benchmark_qed.config.llm_config import LLMConfig, LLMProvider\n",
    "from benchmark_qed.llm.factory import ModelFactory\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "if logging.getLogger(\"httpx\") is not None:\n",
    "    logging.getLogger(\"httpx\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9270eb",
   "metadata": {},
   "source": [
    "# AutoQ\n",
    "![AutoQ](../images/AutoQ.png)\n",
    "\n",
    "AutoQ is an automated approach to question generation that supports the following typology of information-seeking questions:\n",
    "\n",
    "- *Question Scope*: the extent of the dataset that the question addresses\n",
    "    - *Local* questions targeting specific details of a text corpus\n",
    "    - *Global* questions targeting general aspects of a text corpus (e.g., themes, concerns, opportunities)\n",
    "- *Question Source*: the information used to generate local and global questions\n",
    "    - *Data-driven* questions based on text sampled from the overall corpus\n",
    "    - *Activity-driven* questions based on potential activities consistent with the data\n",
    "\n",
    "This typology gives four major question types, generated using LLM-based methods. The question generation process generally consists of 2 main steps:\n",
    "1. *Candidate Generation*: Use an LLM model to generate a large pool of candidate questions, typically oversampled (based on a specified oversample factor) to ensure sufficient variety for downstream selection.\n",
    "2. *Ranking and Selection*: The candidate questions are clustered, ranked, and filtered using various metrics to select the final set of target questions.\n",
    "\n",
    "Below is a more detailed description of the question generation method for each question class:\n",
    "\n",
    "1. *Data-driven local questions*\n",
    "    - Sample texts are extracted from the input text corpus, with target text regions selected\n",
    "    - Candidate local questions are generated for each target text region using a two-step (extract+expand) process\n",
    "    - Candidate questions are clustered and ranked using semantic similarity-based metrics to select a smaller subset of best questions.\n",
    "    - Relevant claims are extracted for each question based on the sources texts in the corresponding text region\n",
    "    - Any abstract categories (e.g., themes) reflected by the sample text are captured\n",
    "3. *Data-driven global questions*\n",
    "    - For each abstract category with 2+ local questions, generate a global question\n",
    "    - Relevant claims are extracted for each global question by aggregating relevant claims from the referenced local questions.\n",
    "    - Candidate questions are clustered and ranked using counts of extracted claims' references and input local questions to select a smaller subset of best questions.\n",
    "4. *Activity-driven local questions*\n",
    "    - A dataset summary is generated from the sample texts using AutoD\n",
    "    - A set of {persona, task, relevant entities} is generated based on the dataset summary and sample texts\n",
    "    - Candidate local questions are generated for each {persona, task, entities} combination\n",
    "    - Candidate questions are clustered and ranked using entity similarity metrics to select a smaller subset of best questions.\n",
    "5. *Activity-driven global questions*\n",
    "    - Generate candidate global questions for each {dataset, persona, task} combination\n",
    "    - Candidate global questions are clustered and ranked using a similarity-based metric to select a smaller subset of representative questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23749b8c",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b271ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA CONFIGS\n",
    "INPUT_DATA_PATH = \"../../datasets/AP_news/raw_data\"\n",
    "OUTPUT_DATA_PATH = \"../../output/AP_news/processed_data\"\n",
    "OUTPUT_QUESTIONS_PATH = \"../../output/AP_news/questions\"\n",
    "TEXT_COLUMN = \"body_nitf\"\n",
    "METADATA_COLUMNS = [\"headline\", \"firstcreated\"]\n",
    "FILE_ENCODING = \"utf-8-sig\"\n",
    "\n",
    "# tokenizer used for chunking documents into text units\n",
    "ENCODING_MODEL = \"o200k_base\"\n",
    "CHUNK_SIZE = 600\n",
    "CHUNK_OVERLAP = 100\n",
    "\n",
    "# DATA SAMPLING CONFIGS\n",
    "# These configs control the breadth and depth of the selected data sample.\n",
    "# Adjust these parameters based on your data size and the number of questions to be generated (e.g. try increasing number of clusters if you want to generate more diverse questions)\n",
    "# The final sample size will be NUM_CLUSTERS * NUM_SAMPLES_PER_CLUSTER\n",
    "NUM_CLUSTERS = 20\n",
    "NUM_SAMPLES_PER_CLUSTER = 10\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# GENERAL QUESTION GENERATION CONFIGS\n",
    "# Number of questions to generate for each question class. You can also specify a different number of questions for each class.\n",
    "NUM_QUESTIONS = 10\n",
    "# Factor by which to overgenerate candidate questions (you can specify a different factor for each question class). These candidate questions will be ranked and filtered using a question sampler to select the final questions.\n",
    "OVERSAMPLE_FACTOR = 2.0\n",
    "\n",
    "# CONFIGS SPECIFIC TO ACTIVITY QUESTIONS\n",
    "# these configs should be adjusted based on the number of questions to be generated. Try increasing these configs if you want to generate more questions.\n",
    "NUM_PERSONAS = 5\n",
    "NUM_TASKS_PER_PERSONA = 2\n",
    "NUM_ENTITIES_PER_TASK = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffbd099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL CONFIGS\n",
    "API_KEY = SecretStr(os.getenv(\"OPENAI_API_KEY\", \"\"))\n",
    "EMBEDDING_MODEL = \"text-embedding-3-large\"\n",
    "LLM_MODEL = \"gpt-4.1\"\n",
    "LLM_PARAMS = {\n",
    "    \"temperature\": 0.0,\n",
    "    \"seed\": 42,\n",
    "}  # adjust this based on your model. For example, some reasoning models do not support temperature settings\n",
    "CONCURRENT_REQUESTS = (\n",
    "    8  # Control for request concurrency. Adjust this based on your model capacity.\n",
    ")\n",
    "\n",
    "text_embedder = TextEmbedder(\n",
    "    ModelFactory.create_embedding_model(\n",
    "        LLMConfig(\n",
    "            model=EMBEDDING_MODEL,\n",
    "            api_key=API_KEY,\n",
    "            llm_provider=LLMProvider.OpenAIEmbedding,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "llm = ModelFactory.create_chat_model(\n",
    "    model_config=LLMConfig(\n",
    "        model=LLM_MODEL,\n",
    "        api_key=API_KEY,\n",
    "        llm_provider=LLMProvider.OpenAIChat,\n",
    "        call_args=LLM_PARAMS,\n",
    "    )\n",
    ")\n",
    "token_encoder = tiktoken.get_encoding(ENCODING_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd6e165",
   "metadata": {},
   "source": [
    "## Data Sampling\n",
    "\n",
    "In this step, we load documents from the input folders, chunk them into text units, and embed all text units. We then ample a subset of text units using the specified number of clusters and number of samples for each cluster. These clustered sample texts will be used to ground the question generation process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db1cfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark_qed.autod.sampler.sample_gen import acreate_clustered_sample\n",
    "\n",
    "clustered_sample = await acreate_clustered_sample(\n",
    "    input_path=INPUT_DATA_PATH,\n",
    "    output_path=OUTPUT_DATA_PATH,\n",
    "    text_embedder=text_embedder,\n",
    "    num_clusters=NUM_CLUSTERS,\n",
    "    num_samples_per_cluster=NUM_SAMPLES_PER_CLUSTER,\n",
    "    input_type=\"json\",\n",
    "    text_tag=TEXT_COLUMN,\n",
    "    metadata_tags=METADATA_COLUMNS,\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    file_encoding=FILE_ENCODING,\n",
    "    token_encoding=ENCODING_MODEL,\n",
    "    random_seed=RANDOM_SEED,\n",
    ")\n",
    "print(\n",
    "    f\"Sampled {len(clustered_sample.sample_texts)} samples from {len(clustered_sample.text_units)} text units in {len(clustered_sample.documents)} documents.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ee0b96",
   "metadata": {},
   "source": [
    "## Data-Local Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eb2cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark_qed.autoq.question_gen.data_questions.local_question_gen import (\n",
    "    DataLocalQuestionGen,\n",
    ")\n",
    "\n",
    "# load clustered text sample (result from the data sampling step)\n",
    "# If you have previously run the data sampling step, you can load the sample from disk instead of re-running the data sampling step as the below example.\n",
    "# Otherwise, you can use clustered_sample.sample_texts directly\n",
    "sample_texts_df = pd.read_parquet(f\"{OUTPUT_DATA_PATH}/sample_texts.parquet\")\n",
    "sample_texts = load_text_units(df=sample_texts_df)\n",
    "\n",
    "data_local_generator = DataLocalQuestionGen(\n",
    "    llm=llm,\n",
    "    text_embedder=text_embedder,\n",
    "    text_units=sample_texts,\n",
    "    concurrent_coroutines=CONCURRENT_REQUESTS,\n",
    "    random_seed=RANDOM_SEED,\n",
    ")\n",
    "\n",
    "data_local_question_results = await data_local_generator.agenerate(\n",
    "    num_questions=NUM_QUESTIONS,\n",
    "    oversample_factor=OVERSAMPLE_FACTOR,\n",
    ")\n",
    "\n",
    "# save both candidate questions and the final selected questions\n",
    "save_questions(\n",
    "    data_local_question_results.selected_questions,\n",
    "    f\"{OUTPUT_QUESTIONS_PATH}/data_local_questions/\",\n",
    "    \"selected_questions\",\n",
    ")\n",
    "save_questions(\n",
    "    data_local_question_results.selected_questions,\n",
    "    f\"{OUTPUT_QUESTIONS_PATH}/data_local_questions/\",\n",
    "    \"selected_questions_text\",\n",
    "    question_text_only=True,\n",
    ")\n",
    "save_questions(\n",
    "    data_local_question_results.candidate_questions,\n",
    "    f\"{OUTPUT_QUESTIONS_PATH}/data_local_questions/\",\n",
    "    \"candidate_questions\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eec03a",
   "metadata": {},
   "source": [
    "## Data Global Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72211a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark_qed.autoq.question_gen.data_questions.global_question_gen import (\n",
    "    DataGlobalQuestionGen,\n",
    ")\n",
    "\n",
    "# Load candidate questions (result from the data local question generation step)\n",
    "# Please note that we load all the candidate local questions (not just the selected ones) as that gives us a bigger pool of local questions to aggregate from.\n",
    "# If you have previously run the data local question generation step, you can load the candidate questions from disk instead of re-running the data local question generation step as the below example.\n",
    "# Otherwise, you can use data_local_question_results.candidate_questions directly\n",
    "local_questions = load_questions(\n",
    "    f\"{OUTPUT_QUESTIONS_PATH}/data_local_questions/candidate_questions.json\"\n",
    ")\n",
    "print(f\"Loaded {len(local_questions)} candidate local questions.\")\n",
    "\n",
    "data_global_generator = DataGlobalQuestionGen(\n",
    "    llm=llm,\n",
    "    text_embedder=text_embedder,\n",
    "    local_questions=local_questions,\n",
    "    concurrent_coroutines=CONCURRENT_REQUESTS,\n",
    "    random_seed=RANDOM_SEED,\n",
    ")\n",
    "\n",
    "data_global_question_results = await data_global_generator.agenerate(\n",
    "    num_questions=NUM_QUESTIONS,\n",
    "    oversample_factor=OVERSAMPLE_FACTOR,\n",
    ")\n",
    "\n",
    "# save both candidate questions and the final selected questions\n",
    "save_questions(\n",
    "    data_global_question_results.selected_questions,\n",
    "    f\"{OUTPUT_QUESTIONS_PATH}/data_global_questions/\",\n",
    "    \"selected_questions\",\n",
    ")\n",
    "save_questions(\n",
    "    data_global_question_results.selected_questions,\n",
    "    f\"{OUTPUT_QUESTIONS_PATH}/data_global_questions/\",\n",
    "    \"selected_questions_text\",\n",
    "    question_text_only=True,\n",
    ")\n",
    "save_questions(\n",
    "    data_global_question_results.candidate_questions,\n",
    "    f\"{OUTPUT_QUESTIONS_PATH}/data_global_questions/\",\n",
    "    \"candidate_questions\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb1549d",
   "metadata": {},
   "source": [
    "## Activity Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd3ab66",
   "metadata": {},
   "source": [
    "### Generate Activity Context\n",
    "\n",
    "Generate personas, their associated tasks, and relevant entities used to ground the activity-based question generation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5789553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark_qed.autoq.question_gen.activity_questions.context_gen.activity_context_gen import (\n",
    "    ActivityContextGen,\n",
    ")\n",
    "\n",
    "# load clustered text sample (result from the data sampling step)\n",
    "# If you have previously run the data sampling step, you can load the sample from disk instead of re-running the data sampling step as the below example.\n",
    "# Otherwise, you can use clustered_sample.sample_texts directly\n",
    "sample_texts_df = pd.read_parquet(f\"{OUTPUT_DATA_PATH}/sample_texts.parquet\")\n",
    "sample_texts = load_text_units(\n",
    "    df=sample_texts_df, attributes_cols=[\"is_representative\"]\n",
    ")\n",
    "\n",
    "activity_generator = ActivityContextGen(\n",
    "    llm=llm,\n",
    "    text_embedder=text_embedder,\n",
    "    token_encoder=token_encoder,\n",
    "    text_units=sample_texts,\n",
    "    concurrent_coroutines=CONCURRENT_REQUESTS,\n",
    ")\n",
    "\n",
    "activity_context = await activity_generator.agenerate(\n",
    "    num_personas=NUM_PERSONAS,\n",
    "    num_tasks=NUM_TASKS_PER_PERSONA,\n",
    "    num_entities_per_task=NUM_ENTITIES_PER_TASK,\n",
    "    oversample_factor=OVERSAMPLE_FACTOR,\n",
    "    use_representative_samples_only=True,  # if True, we will only use a subset of representative samples from the clustered texts to generate activity context (for efficiency). If False, we will use all the samples in the clustered texts.\n",
    ")\n",
    "\n",
    "save_activity_context(activity_context, f\"{OUTPUT_QUESTIONS_PATH}/context/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9671cf9f",
   "metadata": {},
   "source": [
    "### Generate Activity Local Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3269838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from benchmark_qed.autoq.data_model.activity import ActivityContext\n",
    "from benchmark_qed.autoq.question_gen.activity_questions.local_question_gen import (\n",
    "    ActivityLocalQuestionGen,\n",
    ")\n",
    "\n",
    "# load activity context (result from the activity context generation step)\n",
    "# If you have previously run the activity context generation step, you can load the context from disk instead of re-running the activity context generation step as the below example.\n",
    "activity_context = ActivityContext(\n",
    "    **json.loads(\n",
    "        Path(f\"{OUTPUT_QUESTIONS_PATH}/context/activity_context_full.json\").read_text()\n",
    "    )\n",
    ")\n",
    "print(f\"Loaded {len(activity_context.task_contexts)} tasks.\")\n",
    "\n",
    "activity_local_generator = ActivityLocalQuestionGen(\n",
    "    llm=llm,\n",
    "    text_embedder=text_embedder,\n",
    "    activity_context=activity_context,\n",
    "    concurrent_coroutines=CONCURRENT_REQUESTS,\n",
    "    random_seed=RANDOM_SEED,\n",
    ")\n",
    "\n",
    "activity_local_question_results = await activity_local_generator.agenerate(\n",
    "    num_questions=NUM_QUESTIONS,\n",
    "    oversample_factor=OVERSAMPLE_FACTOR,\n",
    ")\n",
    "\n",
    "# save both candidate questions and the final selected questions\n",
    "save_questions(\n",
    "    activity_local_question_results.selected_questions,\n",
    "    f\"{OUTPUT_QUESTIONS_PATH}/activity_local_questions/\",\n",
    "    \"selected_questions\",\n",
    ")\n",
    "save_questions(\n",
    "    activity_local_question_results.selected_questions,\n",
    "    f\"{OUTPUT_QUESTIONS_PATH}/activity_local_questions/\",\n",
    "    \"selected_questions_text\",\n",
    "    question_text_only=True,\n",
    ")\n",
    "save_questions(\n",
    "    activity_local_question_results.candidate_questions,\n",
    "    f\"{OUTPUT_QUESTIONS_PATH}/activity_local_questions/\",\n",
    "    \"candidate_questions\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17c8c15",
   "metadata": {},
   "source": [
    "### Generate Activity Global Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5b229e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark_qed.autoq.question_gen.activity_questions.global_question_gen import (\n",
    "    ActivityGlobalQuestionGen,\n",
    ")\n",
    "\n",
    "# load activity context (result from the activity context generation step)\n",
    "# If you have previously run the activity context generation step, you can load the context from disk instead of re-running the activity context generation step as the below example.\n",
    "activity_context = ActivityContext(\n",
    "    **json.loads(\n",
    "        Path(f\"{OUTPUT_QUESTIONS_PATH}/context/activity_context_full.json\").read_text()\n",
    "    )\n",
    ")\n",
    "print(f\"Loaded {len(activity_context.task_contexts)} tasks.\")\n",
    "\n",
    "activity_global_generator = ActivityGlobalQuestionGen(\n",
    "    llm=llm,\n",
    "    text_embedder=text_embedder,\n",
    "    activity_context=activity_context,\n",
    "    concurrent_coroutines=CONCURRENT_REQUESTS,\n",
    "    random_seed=RANDOM_SEED,\n",
    ")\n",
    "\n",
    "activity_global_question_results = await activity_global_generator.agenerate(\n",
    "    num_questions=NUM_QUESTIONS,\n",
    "    oversample_factor=OVERSAMPLE_FACTOR,\n",
    ")\n",
    "\n",
    "# save both candidate questions and the final selected questions\n",
    "save_questions(\n",
    "    activity_global_question_results.selected_questions,\n",
    "    f\"{OUTPUT_QUESTIONS_PATH}/activity_global_questions/\",\n",
    "    \"selected_questions\",\n",
    ")\n",
    "save_questions(\n",
    "    activity_global_question_results.selected_questions,\n",
    "    f\"{OUTPUT_QUESTIONS_PATH}/activity_global_questions/\",\n",
    "    \"selected_questions_text\",\n",
    "    question_text_only=True,\n",
    ")\n",
    "save_questions(\n",
    "    activity_global_question_results.candidate_questions,\n",
    "    f\"{OUTPUT_QUESTIONS_PATH}/activity_global_questions/\",\n",
    "    \"candidate_questions\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmark-qed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
