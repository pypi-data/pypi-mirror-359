import os
import logging
import asyncio
from gradio_client import Client as GradioClient
from openai import OpenAI, AsyncOpenAI
from rich.console import Console
from rich.markdown import Markdown

# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –±–∞–∑–æ–≤—ã–π –ª–æ–≥–≥–µ—Ä –¥–ª—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
# –ü–æ–Ω–∏–∂–∞–µ–º —É—Ä–æ–≤–µ–Ω—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è httpx –∏ gradio_client, —á—Ç–æ–±—ã —É–±—Ä–∞—Ç—å –ª–∏—à–Ω–∏–π —à—É–º
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("gradio_client").setLevel(logging.WARNING)
logger = logging.getLogger(__name__)

_PROMPT_TEMPLATES = {
    "ru": (
        "You are an expert Python developer's assistant. An internal application error occurred. "
        "Your task is to provide a comprehensive analysis of the traceback for the developer in Russian. "
        "Your response must be structured in Markdown and include these sections:\n\n"
        "### üéØ –ü—Ä–∏—á–∏–Ω–∞ –æ—à–∏–±–∫–∏\n"
        "A clear, concise explanation of the error's root cause.\n\n"
        "### üìç –ú–µ—Å—Ç–æ –æ—à–∏–±–∫–∏\n"
        "The exact file and line number, with a code snippet showing the context (the error line and a few lines around it).\n\n"
        "### üõ†Ô∏è –ü—Ä–µ–¥–ª–∞–≥–∞–µ–º–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ\n"
        "A clear, actionable suggestion for fixing the issue. Provide a code snippet using a diff format (lines with `-` for removal, `+` for addition) to illustrate the change.\n\n"
        "### üéì –ü–æ—á–µ–º—É —ç—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç (–û–±—É—á–∞—é—â–∏–π –º–æ–º–µ–Ω—Ç)\n"
        "A brief explanation of the underlying concept that caused the error, to help the developer avoid similar mistakes in the future.\n\n"
        "Here is the technical traceback:\n"
        "```\n{traceback}\n```\n\n"
        "Provide a structured analysis for the developer's logs. Do not address the user. Do not ask for more code or provide any disclaimers."
    ),
    "en": (
        "You are an expert Python developer's assistant. An internal application error occurred. "
        "Your task is to provide a comprehensive analysis of the traceback for the developer in English. "
        "Your response must be structured in Markdown and include these sections:\n\n"
        "### üéØ Root Cause\n"
        "A clear, concise explanation of the error's root cause.\n\n"
        "### üìç Error Location\n"
        "The exact file and line number, with a code snippet showing the context (the error line and a few lines around it).\n\n"
        "### üõ†Ô∏è Suggested Fix\n"
        "A clear, actionable suggestion for fixing the issue. Provide a code snippet using a diff format (lines with `-` for removal, `+` for addition) to illustrate the change.\n\n"
        "### üéì Why This Happens (A Learning Moment)\n"
        "A brief explanation of the underlying concept that caused the error, to help the developer avoid similar mistakes in the future.\n\n"
        "Here is the technical traceback:\n"
        "```\n{traceback}\n```\n\n"
        "Provide a structured analysis for the developer's logs. Do not address the user. Do not ask for more code or provide any disclaimers."
    )
}

_TRANSLATIONS = {
    "ru": {
        "api_key_error": (
            "API-–∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞.\n"
            "–î–ª—è 'gradio': –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ –ø—Ä–∏–≤–∞—Ç–Ω—ã–º Space (–ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è HUGGINGFACE_API_KEY).\n"
            "–î–ª—è 'openai': –∫–ª—é—á –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω (–ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è OPENAI_API_KEY)."
        ),
        "unknown_provider": "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä. –î–æ—Å—Ç—É–ø–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã: 'gradio', 'openai'",
        "unsupported_language": "–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —è–∑—ã–∫. –î–æ—Å—Ç—É–ø–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã: 'en', 'ru'",
        "gradio_request": "–ó–∞–ø—Ä–∞—à–∏–≤–∞—é –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ Gradio (–º–æ–¥–µ–ª—å: {model_id})...",
        "gradio_request_async": "–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –∑–∞–ø—Ä–∞—à–∏–≤–∞—é –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ Gradio (–º–æ–¥–µ–ª—å: {model_id})...",
        "gradio_error": "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –æ—Ç AI. (–û—à–∏–±–∫–∞ Gradio: {e})",
        "gradio_error_async": "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –æ—Ç AI. (–û—à–∏–±–∫–∞ Gradio: {e})",
        "openai_request": "–ó–∞–ø—Ä–∞—à–∏–≤–∞—é –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ OpenAI (–º–æ–¥–µ–ª—å: {model_id})...",
        "openai_request_async": "–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –∑–∞–ø—Ä–∞—à–∏–≤–∞—é –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ OpenAI (–º–æ–¥–µ–ª—å: {model_id})...",
        "openai_error": "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –æ—Ç AI. (–û—à–∏–±–∫–∞ OpenAI: {e})",
        "openai_error_async": "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –æ—Ç AI. (–û—à–∏–±–∫–∞ OpenAI: {e})",
        "invalid_provider": "–û—à–∏–±–∫–∞: –Ω–µ–≤–µ—Ä–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä.",
        "cache_hit": "–û–±—ä—è—Å–Ω–µ–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –∫–µ—à–µ.",
        "status_analysis": "[bold green]–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –æ—à–∏–±–∫—É —Å –ø–æ–º–æ—â—å—é AI...[/]"
    },
    "en": {
        "api_key_error": (
            "API key not found for the selected provider.\n"
            "For 'gradio': may be required for private Space access (HUGGINGFACE_API_KEY environment variable).\n"
            "For 'openai': key is mandatory (OPENAI_API_KEY environment variable)."
        ),
        "unknown_provider": "Unknown provider. Available options: 'gradio', 'openai'",
        "unsupported_language": "Unsupported language. Available options: 'en', 'ru'",
        "gradio_request": "Requesting explanation via Gradio (model: {model_id})...",
        "gradio_request_async": "Asynchronously requesting explanation via Gradio (model: {model_id})...",
        "gradio_error": "Unfortunately, failed to get an explanation from the AI. (Gradio Error: {e})",
        "gradio_error_async": "Unfortunately, failed to get an asynchronous explanation from the AI. (Gradio Error: {e})",
        "openai_request": "Requesting explanation via OpenAI (model: {model_id})...",
        "openai_request_async": "Asynchronously requesting explanation via OpenAI (model: {model_id})...",
        "openai_error": "Unfortunately, failed to get an explanation from the AI. (OpenAI Error: {e})",
        "openai_error_async": "Unfortunately, failed to get an asynchronous explanation from the AI. (OpenAI Error: {e})",
        "invalid_provider": "Error: invalid provider.",
        "cache_hit": "Explanation found in cache.",
        "status_analysis": "[bold green]Analyzing the error with AI...[/]"
    }
}

class NarratorException(Exception):
    """–ë–∞–∑–æ–≤–æ–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ ErrorNarrator."""
    pass

class ApiKeyNotFoundError(NarratorException):
    """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è, –∫–æ–≥–¥–∞ API-–∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω."""
    pass

class ErrorNarrator:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ–±—ä—è—Å–Ω–µ–Ω–∏–π –æ—à–∏–±–æ–∫ —Å –ø–æ–º–æ—â—å—é AI.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤: 'gradio' (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é, –±–µ—Å–ø–ª–∞—Ç–Ω–æ) –∏ 'openai'.
    """
    GRADIO_MODEL_ID = "hysts/mistral-7b"
    OPENAI_MODEL_ID = "gpt-3.5-turbo"

    def __init__(self, provider: str = 'gradio', language: str = 'en', api_key: str = None, model_id: str = None, prompt_template: str = None, **kwargs):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç ErrorNarrator.

        :param provider: –ü—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ–±—ä—è—Å–Ω–µ–Ω–∏–π ('gradio' –∏–ª–∏ 'openai').
        :param language: –Ø–∑—ã–∫ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ AI ('en' –∏–ª–∏ 'ru').
        :param api_key: API-–∫–ª—é—á. –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, –±—É–¥–µ—Ç –≤–∑—è—Ç –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
                        (HUGGINGFACE_API_KEY –¥–ª—è 'gradio', OPENAI_API_KEY –¥–ª—è 'openai').
        :param model_id: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –º–æ–¥–µ–ª–∏. –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞.
        :param prompt_template: –®–∞–±–ª–æ–Ω –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –º–æ–¥–µ–ª–∏.
        :param kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, temperature, max_new_tokens).
        """
        if language not in _TRANSLATIONS:
            raise ValueError(_TRANSLATIONS['en']['unsupported_language'])
        self.language = language
        self.T = _TRANSLATIONS[self.language]

        self.provider = provider
        self.prompt_template = prompt_template or _PROMPT_TEMPLATES[self.language]
        self.model_params = kwargs
        self.cache = {} # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–µ—à

        if self.provider == 'gradio':
            self.api_key = api_key or os.getenv("HUGGINGFACE_API_KEY")
            self.model_id = model_id or self.GRADIO_MODEL_ID
            self.client = GradioClient(self.model_id, hf_token=self.api_key)
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è Gradio, –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω—ã
            self.model_params.setdefault('temperature', 0.6)
            self.model_params.setdefault('max_new_tokens', 1024)
            self.model_params.setdefault('top_p', 0.9)
            self.model_params.setdefault('top_k', 50)
            self.model_params.setdefault('repetition_penalty', 1.2)

        elif self.provider == 'openai':
            self.api_key = api_key or os.getenv("OPENAI_API_KEY")
            if not self.api_key:
                raise ApiKeyNotFoundError(self.T["api_key_error"])
            self.model_id = model_id or self.OPENAI_MODEL_ID
            self.client = OpenAI(api_key=self.api_key)
            self.async_client = AsyncOpenAI(api_key=self.api_key)
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è OpenAI, –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω—ã
            self.model_params.setdefault('temperature', 0.7)
            self.model_params.setdefault('max_tokens', 1024) # OpenAI –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 'max_tokens'
        else:
            raise ValueError(self.T["unknown_provider"])

    def _build_prompt(self, traceback: str) -> str:
        """–§–æ—Ä–º–∏—Ä—É–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è –º–æ–¥–µ–ª–∏."""
        return self.prompt_template.format(traceback=traceback)

    # --- –ú–µ—Ç–æ–¥—ã –¥–ª—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ Gradio ---

    def _predict_gradio(self, prompt: str) -> str:
        logger.info(self.T["gradio_request"].format(model_id=self.model_id))
        try:
            result = self.client.predict(
                prompt,
                self.model_params.get('max_new_tokens'),
                self.model_params.get('temperature'),
                self.model_params.get('top_p'),
                self.model_params.get('top_k'),
                self.model_params.get('repetition_penalty'),
                api_name="/chat"
            )
            return result.strip()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ Gradio: {e}")
            return self.T["gradio_error"].format(e=e)

    async def _predict_async_gradio(self, prompt: str) -> str:
        logger.info(self.T["gradio_request_async"].format(model_id=self.model_id))
        loop = asyncio.get_running_loop()
        try:
            result = await loop.run_in_executor(None, self._predict_gradio, prompt)
            return result
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–º –∑–∞–ø—Ä–æ—Å–µ –∫ Gradio: {e}")
            return self.T["gradio_error_async"].format(e=e)
            
    # --- –ú–µ—Ç–æ–¥—ã –¥–ª—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ OpenAI ---

    def _predict_openai(self, prompt: str) -> str:
        logger.info(self.T["openai_request"].format(model_id=self.model_id))
        try:
            completion = self.client.chat.completions.create(
                model=self.model_id,
                messages=[{"role": "user", "content": prompt}],
                **self.model_params
            )
            return completion.choices[0].message.content.strip()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ OpenAI: {e}")
            return self.T["openai_error"].format(e=e)

    async def _predict_async_openai(self, prompt: str) -> str:
        logger.info(self.T["openai_request_async"].format(model_id=self.model_id))
        try:
            completion = await self.async_client.chat.completions.create(
                model=self.model_id,
                messages=[{"role": "user", "content": prompt}],
                **self.model_params
            )
            return completion.choices[0].message.content.strip()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–º –∑–∞–ø—Ä–æ—Å–µ –∫ OpenAI: {e}")
            return self.T["openai_error_async"].format(e=e)

    # --- –î–∏—Å–ø–µ—Ç—á–µ—Ä—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π ---

    def _predict(self, prompt: str) -> str:
        if self.provider == 'gradio':
            return self._predict_gradio(prompt)
        elif self.provider == 'openai':
            return self._predict_openai(prompt)
        # –≠—Ç–æ—Ç return –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –¥–æ–ª–∂–µ–Ω —Å—Ä–∞–±–æ—Ç–∞—Ç—å –∏–∑-–∑–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤ __init__
        return self.T["invalid_provider"]

    async def _predict_async(self, prompt: str) -> str:
        if self.provider == 'gradio':
            return await self._predict_async_gradio(prompt)
        elif self.provider == 'openai':
            return await self._predict_async_openai(prompt)
        return self.T["invalid_provider"]

    def explain_error(self, traceback: str) -> str:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –¥–ª—è –æ—à–∏–±–∫–∏ (traceback) —Å –ø–æ–º–æ—â—å—é AI.
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–µ—à –ø–µ—Ä–µ–¥ –æ—Ç–ø—Ä–∞–≤–∫–æ–π –∑–∞–ø—Ä–æ—Å–∞.
        """
        if traceback in self.cache:
            logger.info(self.T["cache_hit"])
            return self.cache[traceback]

        prompt = self._build_prompt(traceback)
        explanation = self._predict(prompt)
        self.cache[traceback] = explanation # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –∫–µ—à
        return explanation

    async def explain_error_async(self, traceback: str) -> str:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –ø–æ–ª—É—á–∞–µ—Ç –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –¥–ª—è –æ—à–∏–±–∫–∏ (traceback) —Å –ø–æ–º–æ—â—å—é AI.
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–µ—à –ø–µ—Ä–µ–¥ –æ—Ç–ø—Ä–∞–≤–∫–æ–π –∑–∞–ø—Ä–æ—Å–∞.
        """
        if traceback in self.cache:
            logger.info(self.T["cache_hit"])
            return self.cache[traceback]

        prompt = self._build_prompt(traceback)
        explanation = await self._predict_async(prompt)
        self.cache[traceback] = explanation # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –∫–µ—à
        return explanation

    def explain_and_print(self, traceback: str):
        """
        –ü–æ–ª—É—á–∞–µ—Ç –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ, —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –µ–≥–æ —Å –ø–æ–º–æ—â—å—é rich –∏ –≤—ã–≤–æ–¥–∏—Ç –≤ –∫–æ–Ω—Å–æ–ª—å.
        """
        console = Console()
        with console.status(self.T["status_analysis"], spinner="dots"):
            explanation_md = self.explain_error(traceback)
        
        console.print(Markdown(explanation_md, style="default"))

    async def explain_and_print_async(self, traceback: str):
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –ø–æ–ª—É—á–∞–µ—Ç –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ, —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∏ –≤—ã–≤–æ–¥–∏—Ç –≤ –∫–æ–Ω—Å–æ–ª—å.
        """
        console = Console()
        with console.status(self.T["status_analysis"], spinner="dots"):
            explanation_md = await self.explain_error_async(traceback)
        
        console.print(Markdown(explanation_md, style="default"))