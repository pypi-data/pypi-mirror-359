{
    "config": {
        "abort": {
            "already_configured": "Servi\u00e7o j\u00e1 configurado"
        },
        "error": {
            "cannot_connect": "A liga\u00e7\u00e3o falhou",
            "unknown": "Erro inesperado"
        },
        "step": {
            "user": {
                "data": {
                    "url": "URL"
                }
            }
        }
    },
    "config_subentries": {
        "conversation": {
            "abort": {
                "reconfigure_successful": "A reconfigura\u00e7\u00e3o foi bem sucedida"
            },
            "entry_type": "Agente de conversa\u00e7\u00e3o",
            "initiate_flow": {
                "reconfigure": "Reconfigurar o agente de conversa\u00e7\u00e3o",
                "user": "Adicionar agente de conversa\u00e7\u00e3o"
            },
            "step": {
                "set_options": {
                    "data": {
                        "keep_alive": "Manter vivo",
                        "llm_hass_api": "Controlar Home Assistant",
                        "max_history": "M\u00e1ximo de mensagens do hist\u00f3rico",
                        "name": "Nome",
                        "num_ctx": "Tamanho da janela de contexto",
                        "prompt": "Instru\u00e7\u00f5es",
                        "think": "Pensar antes de responder"
                    },
                    "data_description": {
                        "keep_alive": "Dura\u00e7\u00e3o em segundos para que Ollama mantenha o modelo em mem\u00f3ria. -1 = indefinido, 0 = nunca.",
                        "num_ctx": "N\u00famero m\u00e1ximo de tokens de texto que o modelo pode processar. Diminuir para reduzir a RAM do Ollama, ou aumentar para um grande n\u00famero de entidades expostas.",
                        "prompt": "Indicar como o LLM deve responder. Pode ser um modelo.",
                        "think": "Se ativado, o LLM pensar\u00e1 antes de responder. Isto pode melhorar a qualidade da resposta, mas pode aumentar a lat\u00eancia."
                    }
                }
            }
        }
    }
}