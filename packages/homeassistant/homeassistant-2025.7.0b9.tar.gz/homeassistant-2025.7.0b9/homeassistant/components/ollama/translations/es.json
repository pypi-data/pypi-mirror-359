{
    "config": {
        "abort": {
            "already_configured": "El servicio ya est\u00e1 configurado"
        },
        "error": {
            "cannot_connect": "No se pudo conectar",
            "unknown": "Error inesperado"
        },
        "step": {
            "user": {
                "data": {
                    "url": "URL"
                }
            }
        }
    },
    "config_subentries": {
        "conversation": {
            "abort": {
                "reconfigure_successful": "Se volvi\u00f3 a configurar correctamente"
            },
            "entry_type": "Agente de conversaci\u00f3n",
            "initiate_flow": {
                "reconfigure": "Volver a configurar el agente de conversaci\u00f3n",
                "user": "A\u00f1adir agente de conversaci\u00f3n"
            },
            "step": {
                "set_options": {
                    "data": {
                        "keep_alive": "Mantener vivo",
                        "llm_hass_api": "Controla Home Assistant",
                        "max_history": "M\u00e1ximo de mensajes del historial",
                        "name": "Nombre",
                        "num_ctx": "Tama\u00f1o de la ventana de contexto",
                        "prompt": "Instrucciones",
                        "think": "Pensar antes de responder"
                    },
                    "data_description": {
                        "keep_alive": "Duraci\u00f3n en segundos que tarda Ollama en mantener el modelo en la memoria. -1 = indefinido, 0 = nunca.",
                        "num_ctx": "N\u00famero m\u00e1ximo de tokens de texto que el modelo puede procesar. Disminuye el valor para reducir la RAM de Ollama o aum\u00e9ntalo para una gran cantidad de entidades expuestas.",
                        "prompt": "Indica c\u00f3mo debe responder el LLM. Puede ser una plantilla.",
                        "think": "Si est\u00e1 habilitado, el LLM pensar\u00e1 antes de responder. Esto puede mejorar la calidad de la respuesta, pero podr\u00eda aumentar la latencia."
                    }
                }
            }
        }
    }
}