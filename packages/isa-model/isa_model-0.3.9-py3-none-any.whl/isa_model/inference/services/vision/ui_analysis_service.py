"""
UI Analysis Stacked Service

A stacked service for analyzing UI screenshots using multiple AI models:
- Layer 1: Page Intelligence (GPT-4V)
- Layer 2: Element Detection (OmniParser/Florence-2/YOLO)  
- Layer 3: Classification (GPT-4V)
"""

from typing import Dict, Any, List, Optional, Union, BinaryIO
import json
from PIL import Image, ImageDraw, ImageFont

from .helpers.base_stacked_service import BaseStackedService, LayerConfig, LayerType, LayerResult

class UIAnalysisService(BaseStackedService):
    """
    Generic UI Analysis service using stacked AI models for comprehensive UI understanding.
    Can handle different page types: login, search, content extraction, navigation, etc.
    """
    
    def __init__(self, ai_factory, config=None):
        super().__init__(ai_factory, "ui_analysis")
        self.ai_factory = ai_factory
        self.config = config or {}
        self.task_type = self.config.get("task_type", "login")  # Default to login for backward compatibility
        self._setup_layers_by_task()
    
    def _setup_layers_by_task(self):
        """Setup layer configuration based on task type"""
        
        if self.task_type == "search":
            self._setup_search_layers()
        elif self.task_type == "content":
            self._setup_content_layers()
        elif self.task_type == "navigation":
            self._setup_navigation_layers()
        else:
            # Default to login/form analysis
            self._setup_default_layers()
    
    def _setup_default_layers(self):
        """Setup simplified two-layer architecture for UI analysis"""
        
        # Layer 1: OmniParser å…ƒç´ æ£€æµ‹
        self.add_layer(LayerConfig(
            name="ui_detection",
            layer_type=LayerType.DETECTION,
            service_type="vision",
            model_name="omniparser",
            parameters={
                "task": "ui_detection",
                "imgsz": 640,
                "box_threshold": 0.05,
                "iou_threshold": 0.1
            },
            depends_on=[],
            timeout=30.0,
            fallback_enabled=True
        ))
        
        # Layer 2: GPT-4.1-nano æ™ºèƒ½å†³ç­–
        self.add_layer(LayerConfig(
            name="action_planning",
            layer_type=LayerType.INTELLIGENCE,
            service_type="vision",
            model_name="default",
            parameters={
                "task": "action_planning",
                "max_tokens": 500
            },
            depends_on=["ui_detection"],
            timeout=15.0,
            fallback_enabled=False
        ))
    
    def _setup_search_layers(self):
        """Setup simplified two-layer architecture for search page analysis"""
        
        self.add_layer(LayerConfig(
            name="ui_detection",
            layer_type=LayerType.DETECTION,
            service_type="vision",
            model_name="omniparser",
            parameters={
                "task": "ui_detection",
                "imgsz": 640,
                "box_threshold": 0.05,
                "iou_threshold": 0.1
            },
            depends_on=[],
            timeout=30.0,
            fallback_enabled=True
        ))
        
        self.add_layer(LayerConfig(
            name="action_planning",
            layer_type=LayerType.INTELLIGENCE,
            service_type="vision",
            model_name="default",
            parameters={
                "task": "search_action_planning",
                "max_tokens": 500
            },
            depends_on=["ui_detection"],
            timeout=15.0,
            fallback_enabled=False
        ))
    
    def _setup_content_layers(self):
        """Setup simplified two-layer architecture for content extraction"""
        
        self.add_layer(LayerConfig(
            name="ui_detection",
            layer_type=LayerType.DETECTION,
            service_type="vision",
            model_name="omniparser",
            parameters={
                "task": "ui_detection",
                "imgsz": 640,
                "box_threshold": 0.05,
                "iou_threshold": 0.1
            },
            depends_on=[],
            timeout=30.0,
            fallback_enabled=True
        ))
        
        self.add_layer(LayerConfig(
            name="action_planning",
            layer_type=LayerType.INTELLIGENCE,
            service_type="vision",
            model_name="default",
            parameters={
                "task": "content_action_planning",
                "max_tokens": 500
            },
            depends_on=["ui_detection"],
            timeout=15.0,
            fallback_enabled=False
        ))
    
    def _setup_navigation_layers(self):
        """Setup simplified two-layer architecture for navigation analysis"""
        
        self.add_layer(LayerConfig(
            name="ui_detection",
            layer_type=LayerType.DETECTION,
            service_type="vision",
            model_name="omniparser",
            parameters={
                "task": "ui_detection",
                "imgsz": 640,
                "box_threshold": 0.05,
                "iou_threshold": 0.1
            },
            depends_on=[],
            timeout=30.0,
            fallback_enabled=True
        ))
        
        self.add_layer(LayerConfig(
            name="action_planning",
            layer_type=LayerType.INTELLIGENCE,
            service_type="vision",
            model_name="default",
            parameters={
                "task": "navigation_action_planning",
                "max_tokens": 500
            },
            depends_on=["ui_detection"],
            timeout=15.0,
            fallback_enabled=False
        ))
    
    def configure_detection_model(self, model_name: str, parameters: Dict[str, Any]):
        """Configure the detection model and parameters"""
        for layer in self.layers:
            if layer.name == "element_detection":
                layer.model_name = model_name
                layer.parameters.update(parameters)
                break
    
    def configure_intelligence_model(self, model_name: str, parameters: Dict[str, Any]):
        """Configure the page intelligence model"""
        for layer in self.layers:
            if layer.name == "page_intelligence":
                layer.model_name = model_name
                layer.parameters.update(parameters)
                break
    
    async def execute_layer_logic(self, layer: LayerConfig, service: Any, context: Dict[str, Any]) -> Any:
        """Execute specific logic for each layer type using unified invoke method"""
        
        task = layer.parameters.get("task")
        image_path = context["input"]["image_path"]
        
        if task == "ui_detection":
            return await self._invoke_ui_detection(service, image_path, layer.parameters)
        
        elif task in ["action_planning", "search_action_planning", "content_action_planning", "navigation_action_planning"]:
            ui_elements = context["results"]["ui_detection"].data
            return await self._invoke_action_planning(service, image_path, ui_elements, layer.parameters)
        
        else:
            raise ValueError(f"Unsupported task: {task}")
    
    # ==================== SIMPLIFIED TWO-LAYER METHODS ====================
    
    async def _invoke_ui_detection(self, service: Any, image_path: str, parameters: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æ‰§è¡ŒUIå…ƒç´ æ£€æµ‹ï¼ˆç¬¬ä¸€å±‚ï¼‰"""
        
        if hasattr(service, 'run_omniparser'):
            result = await service.run_omniparser(
                image=image_path,
                **{k: v for k, v in parameters.items() if k != "task"}
            )
            
            # è¿”å›žæ‰€æœ‰å…ƒç´ ï¼ˆåŒ…æ‹¬æ–‡æœ¬å’Œäº¤äº’å…ƒç´ ï¼‰
            elements = result.get("parsed_elements", [])
            
            # æ·»åŠ æ›´å¤šæœ‰ç”¨çš„ä¿¡æ¯
            for i, element in enumerate(elements):
                element['element_id'] = i
                element['element_index'] = i
                
                # è®¡ç®—ä¸­å¿ƒç‚¹åæ ‡
                bbox = element.get('bbox', [0, 0, 0, 0])
                if len(bbox) == 4:
                    # è½¬æ¢å½’ä¸€åŒ–åæ ‡åˆ°åƒç´ åæ ‡
                    img = Image.open(image_path)
                    img_width, img_height = img.size
                    
                    x1, y1, x2, y2 = bbox
                    pixel_x1 = int(x1 * img_width)
                    pixel_y1 = int(y1 * img_height)
                    pixel_x2 = int(x2 * img_width)
                    pixel_y2 = int(y2 * img_height)
                    
                    element['pixel_bbox'] = [pixel_x1, pixel_y1, pixel_x2, pixel_y2]
                    element['center'] = [
                        (pixel_x1 + pixel_x2) // 2,
                        (pixel_y1 + pixel_y2) // 2
                    ]
                    element['size'] = [pixel_x2 - pixel_x1, pixel_y2 - pixel_y1]
            
            return elements
        else:
            raise ValueError("OmniParser service not available")
    
    async def _invoke_action_planning(self, service: Any, image_path: str, ui_elements: List[Dict[str, Any]], parameters: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œè¡ŒåŠ¨è§„åˆ’ï¼ˆç¬¬äºŒå±‚ï¼‰"""
        
        task_type = parameters.get("task", "action_planning")
        
        # æž„å»ºå…ƒç´ æ‘˜è¦
        elements_summary = []
        interactive_elements = []
        
        for element in ui_elements:
            summary = {
                "id": element.get('element_id'),
                "type": element.get('type'),
                "center": element.get('center'),
                "content": element.get('content', ''),
                "interactivity": element.get('interactivity', False),
                "size": element.get('size')
            }
            elements_summary.append(summary)
            
            if element.get('interactivity', False):
                interactive_elements.append(summary)
        
        # æž„å»ºæ™ºèƒ½å†³ç­–æç¤ºè¯
        prompt = self._build_action_planning_prompt(
            task_type=task_type,
            elements_summary=elements_summary,
            interactive_elements=interactive_elements
        )
        
        # è°ƒç”¨GPTè¿›è¡Œå†³ç­–
        result = await service.invoke(
            image=image_path,
            prompt=prompt,
            task="analyze",
            max_tokens=parameters.get("max_tokens", 500)
        )
        
        # è§£æžå†³ç­–ç»“æžœ
        decision = self._parse_action_plan(result.get('text', ''))
        
        # å°†å†³ç­–ä¸Žå®žé™…å…ƒç´ åŒ¹é…
        action_plan = self._match_actions_to_elements(decision, ui_elements)
        
        return action_plan

    # ==================== UNIFIED INVOKE METHODS ====================
    
    async def _invoke_page_intelligence(self, service: Any, image_path: str, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """Invoke page intelligence analysis using unified interface"""
        
        task = parameters.get("task", "page_intelligence")
        prompt = self._get_intelligence_prompt(task)

        # Use unified invoke method
        result = await service.invoke(
            image=image_path,
            prompt=prompt,
            task="analyze",
            max_tokens=parameters.get("max_tokens", 500)
        )
        
        # Parse JSON response
        response_text = result['text'].strip()
        json_start = response_text.find('{')
        json_end = response_text.rfind('}') + 1
        
        if json_start >= 0 and json_end > json_start:
            json_text = response_text[json_start:json_end]
            try:
                intelligence_data = json.loads(json_text)
            except json.JSONDecodeError:
                intelligence_data = self._parse_intelligence_fallback(response_text)
        else:
            intelligence_data = self._parse_intelligence_fallback(response_text)
        
        return intelligence_data
    
    def _get_intelligence_prompt(self, task: str) -> str:
        """Get task-specific prompt for page intelligence"""
        
        if task == "search_page_intelligence":
            return '''Analyze this webpage screenshot to understand the search interface structure.

Identify:
1. Page type (search_engine, search_results, query_page, homepage)
2. Search elements (search box, search button, filters, suggestions)
3. Layout pattern (header_search, center_search, sidebar_search)
4. Language used in the interface
5. Additional features (voice search, image search, advanced options)
6. Visible text elements and labels

Return analysis as JSON with this exact structure:
{
  "page_type": "search_engine|search_results|query_page|homepage|other",
  "layout_pattern": "header_search|center_search|sidebar_search|embedded",
  "language": "en|zh|es|fr|de|other",
  "search_features": ["voice_search", "image_search", "advanced_options", "suggestions"],
  "complexity_score": 0.1-1.0,
  "visible_text_elements": ["Search", "Google Search", "I'm Feeling Lucky"],
  "search_area_estimate": {"x": 0, "y": 0, "width": 0, "height": 0},
  "confidence": 0.1-1.0,
  "analysis_notes": "brief description of what you observe"
}

Be precise and only include elements you can clearly see.'''
            
        elif task == "content_page_intelligence":
            return '''Analyze this webpage screenshot to understand the content structure.

Identify:
1. Page type (article, blog, news, documentation, product_page)
2. Content layout (single_column, multi_column, grid, sidebar)
3. Content elements (headings, paragraphs, images, videos, links)
4. Language used in the interface
5. Navigation elements (menu, breadcrumbs, pagination)
6. Visible text content and structure

Return analysis as JSON with this exact structure:
{
  "page_type": "article|blog|news|documentation|product_page|other",
  "layout_pattern": "single_column|multi_column|grid|sidebar",
  "language": "en|zh|es|fr|de|other",
  "content_features": ["headings", "paragraphs", "images", "videos", "links", "navigation"],
  "complexity_score": 0.1-1.0,
  "visible_text_elements": ["Title", "Content", "Read More"],
  "content_area_estimate": {"x": 0, "y": 0, "width": 0, "height": 0},
  "confidence": 0.1-1.0,
  "analysis_notes": "brief description of what you observe"
}

Be precise and only include elements you can clearly see.'''
            
        elif task == "navigation_page_intelligence":
            return '''Analyze this webpage screenshot to understand the navigation structure.

Identify:
1. Page type (homepage, category_page, landing_page, dashboard)
2. Navigation elements (menu, toolbar, sidebar, footer)
3. Layout pattern (horizontal_nav, vertical_nav, dropdown_nav, mega_menu)
4. Language used in the interface
5. Interactive elements (buttons, links, icons, search)
6. Visible navigation labels and structure

Return analysis as JSON with this exact structure:
{
  "page_type": "homepage|category_page|landing_page|dashboard|other",
  "layout_pattern": "horizontal_nav|vertical_nav|dropdown_nav|mega_menu",
  "language": "en|zh|es|fr|de|other",
  "navigation_features": ["main_menu", "sidebar", "footer", "breadcrumbs", "search"],
  "complexity_score": 0.1-1.0,
  "visible_text_elements": ["Home", "About", "Contact", "Products"],
  "navigation_area_estimate": {"x": 0, "y": 0, "width": 0, "height": 0},
  "confidence": 0.1-1.0,
  "analysis_notes": "brief description of what you observe"
}

Be precise and only include elements you can clearly see.'''
            
        else:
            # Default login/form analysis prompt
            return '''Analyze this webpage screenshot to understand the login interface structure.

Identify:
1. Page type (login, register, multi-step auth, SSO)
2. Layout pattern (vertical form, horizontal, modal, tabs)
3. Language used in the interface
4. Security features visible (CAPTCHA, 2FA indicators)
5. Form complexity level
6. Visible text elements that indicate field purposes

Return analysis as JSON with this exact structure:
{
  "page_type": "login|register|multi_step|sso|other",
  "layout_pattern": "vertical|horizontal|modal|tabs|embedded",
  "language": "en|zh|es|fr|de|other",
  "security_features": ["captcha", "recaptcha", "2fa_indicator", "security_questions"],
  "complexity_score": 0.1-1.0,
  "visible_text_elements": ["Login", "Password", "Sign In"],
  "form_area_estimate": {"x": 0, "y": 0, "width": 0, "height": 0},
  "confidence": 0.1-1.0,
  "analysis_notes": "brief description of what you observe"
}

Be precise and only include elements you can clearly see.'''
    
    async def execute_fallback(self, layer: LayerConfig, context: Dict[str, Any], error: str) -> Optional[Any]:
        """Execute fallback logic for failed layers"""
        
        if layer.layer_type == LayerType.INTELLIGENCE:
            # Return basic page intelligence
            return {
                "page_type": "login",
                "layout_pattern": "vertical",
                "language": "en",
                "security_features": [],
                "complexity_score": 0.5,
                "visible_text_elements": ["Login", "Password"],
                "form_area_estimate": {"x": 200, "y": 200, "width": 600, "height": 400},
                "confidence": 0.3,
                "analysis_notes": f"Fallback analysis due to error: {error}"
            }
        
        elif layer.layer_type == LayerType.DETECTION:
            # Create fallback elements based on typical form layout
            intelligence = context["results"]["page_intelligence"].data
            return self._create_fallback_elements(context["input"]["image_path"], intelligence)
        
        return None
    
    def generate_final_output(self, results: Dict[str, LayerResult]) -> Dict[str, Any]:
        """Generate final UI analysis output for simplified two-layer architecture"""
        
        # Extract data from the two layers
        ui_elements = results.get("ui_detection", {}).data or []
        action_plan = results.get("action_planning", {}).data or {}
        
        # åˆ†ç¦»äº¤äº’å’Œéžäº¤äº’å…ƒç´ 
        interactive_elements = [e for e in ui_elements if e.get('interactivity', False)]
        text_elements = [e for e in ui_elements if not e.get('interactivity', False)]
        
        return {
            "ui_elements": {
                "total_elements": len(ui_elements),
                "interactive_elements": interactive_elements,
                "text_elements": text_elements,
                "summary": {
                    "interactive_count": len(interactive_elements),
                    "text_count": len(text_elements)
                }
            },
            "action_plan": action_plan,
            "automation_ready": {
                "ready": action_plan.get("success_probability", 0) > 0.7,
                "confidence": action_plan.get("success_probability", 0),
                "steps_count": len(action_plan.get("action_plan", [])),
                "page_type": action_plan.get("page_analysis", {}).get("page_type", "unknown")
            },
            "execution_summary": {
                "can_automate": len(action_plan.get("action_plan", [])) > 0,
                "recommended_action": action_plan.get("action_plan", [{}])[0] if action_plan.get("action_plan") else None
            }
        }
    
    async def visualize_results(self, image_path: str, analysis_result: Dict[str, Any], 
                              output_path: str = "ui_analysis_result.png") -> str:
        """Generate visualization of the analysis results"""
        
        # Load original image
        img = Image.open(image_path)
        img_copy = img.copy()
        draw = ImageDraw.Draw(img_copy)
        
        try:
            font_large = ImageFont.truetype("arial.ttf", 24)
            font_small = ImageFont.truetype("arial.ttf", 16)
        except:
            font_large = None
            font_small = None
        
        colors = {
            'username_field': 'red',
            'password_field': 'blue', 
            'login_button': 'green',
            'register_button': 'orange',
            'submit_button': 'purple',
            'checkbox': 'yellow',
            'link': 'cyan',
            'other': 'gray'
        }
        
        final_output = analysis_result.get("final_output", {})
        classified_elements = final_output.get("classified_elements", [])
        
        # Draw each classified element
        for element in classified_elements:
            classification = element.get('classification', 'other')
            confidence = element.get('classification_confidence', 0)
            center = element.get('precise_center', element.get('center', [0, 0]))
            bbox = element.get('bbox', [0, 0, 100, 100])
            priority = element.get('interaction_priority', 5)
            
            color = colors.get(classification, 'gray')
            center_x, center_y = center
            x1, y1, x2, y2 = bbox
            
            # Draw bounding box
            draw.rectangle([x1, y1, x2, y2], outline=color, width=2)
            
            # Draw center crosshair
            crosshair_size = 35
            draw.line([center_x - crosshair_size, center_y, center_x + crosshair_size, center_y], 
                     fill=color, width=8)
            draw.line([center_x, center_y - crosshair_size, center_x, center_y + crosshair_size], 
                     fill=color, width=8)
            
            # Draw center circle
            circle_radius = 20
            draw.ellipse([center_x - circle_radius, center_y - circle_radius, 
                         center_x + circle_radius, center_y + circle_radius], 
                       outline=color, width=8)
            
            # Draw labels
            label = f"{classification.replace('_', ' ').title()}"
            detail = f"Conf: {confidence:.2f} | Pri: {priority}"
            
            if font_large:
                draw.text((x1, y1 - 50), label, fill=color, font=font_large)
                draw.text((x1, y1 - 25), detail, fill=color, font=font_small)
            else:
                draw.text((x1, y1 - 40), label, fill=color)
                draw.text((x1, y1 - 20), detail, fill=color)
        
        # Add header information
        intelligence = final_output.get("page_intelligence", {})
        summary = final_output.get("analysis_summary", {})
        
        header_text = f"UI Analysis: {summary.get('page_type', 'unknown')} | Elements: {summary.get('interactive_elements', 0)} | Confidence: {summary.get('overall_confidence', 0):.2f}"
        
        if font_large:
            draw.text((10, 10), "Stacked UI Analysis", fill='black', font=font_large)
            draw.text((10, 40), header_text, fill='black', font=font_small)
        else:
            draw.text((10, 10), "Stacked UI Analysis", fill='black')
            draw.text((10, 30), header_text, fill='black')
        
        # Save visualization
        img_copy.save(output_path)
        return output_path
    
    # Helper methods
    def _parse_intelligence_fallback(self, text: str) -> Dict[str, Any]:
        """Fallback parser for intelligence data"""
        return {
            "page_type": "login",
            "layout_pattern": "vertical",
            "language": "en",
            "security_features": [],
            "complexity_score": 0.6,
            "visible_text_elements": ["Username", "Password", "Login"],
            "form_area_estimate": {"x": 200, "y": 200, "width": 600, "height": 400},
            "confidence": 0.4,
            "analysis_notes": "Parsed from text analysis"
        }
    
    def _parse_classification_result(self, text: str) -> Dict[str, Any]:
        """Parse classification result from GPT response"""
        try:
            json_start = text.find('{')
            json_end = text.rfind('}') + 1
            
            if json_start >= 0 and json_end > json_start:
                json_text = text[json_start:json_end]
                return json.loads(json_text)
        except:
            pass
        
        # Fallback text parsing
        classification = 'other'
        confidence = 0.5
        
        text_lower = text.lower()
        if 'username' in text_lower or 'email' in text_lower:
            classification = 'username_field'
            confidence = 0.7
        elif 'password' in text_lower:
            classification = 'password_field'
            confidence = 0.7
        elif 'button' in text_lower and ('login' in text_lower or 'sign' in text_lower):
            classification = 'login_button'
            confidence = 0.7
        
        return {
            'classification': classification,
            'confidence': confidence,
            'reasoning': 'Parsed from text',
            'visual_evidence': ['text_analysis'],
            'interaction_priority': 5
        }
    
    def _build_action_planning_prompt(self, task_type: str, elements_summary: List[Dict], interactive_elements: List[Dict]) -> str:
        """æž„å»ºè¡ŒåŠ¨è§„åˆ’æç¤ºè¯"""
        
        # ä»»åŠ¡ç‰¹å®šçš„æŒ‡å¼•
        if "search" in task_type:
            instruction = """
ç›®æ ‡ï¼šå®Œæˆæœç´¢æ“ä½œ
éœ€è¦è¯†åˆ«ï¼š
1. æœç´¢è¾“å…¥æ¡†
2. æœç´¢æŒ‰é’®ï¼ˆå¯é€‰ï¼Œé€šå¸¸å¯ä»¥æŒ‰å›žè½¦ï¼‰
æ“ä½œé¡ºåºï¼šç‚¹å‡»æœç´¢æ¡† â†’ è¾“å…¥æœç´¢å†…å®¹ â†’ ç‚¹å‡»æœç´¢æŒ‰é’®æˆ–æŒ‰å›žè½¦
"""
        elif "content" in task_type:
            instruction = """
ç›®æ ‡ï¼šè¯†åˆ«å’Œæå–é¡µé¢å†…å®¹
éœ€è¦è¯†åˆ«ï¼š
1. ä¸»è¦å†…å®¹åŒºåŸŸ
2. æ ‡é¢˜å’Œæ­£æ–‡
3. ç›¸å…³é“¾æŽ¥
4. å¯¼èˆªå…ƒç´ 
"""
        elif "navigation" in task_type:
            instruction = """
ç›®æ ‡ï¼šè¯†åˆ«é¡µé¢å¯¼èˆªç»“æž„
éœ€è¦è¯†åˆ«ï¼š
1. ä¸»å¯¼èˆªèœå•
2. å­èœå•é¡¹
3. é¢åŒ…å±‘å¯¼èˆª
4. é¡µè„šé“¾æŽ¥
"""
        else:
            instruction = """
ç›®æ ‡ï¼šå®Œæˆç™»å½•æ“ä½œ
éœ€è¦è¯†åˆ«ï¼š
1. ç”¨æˆ·å/é‚®ç®±è¾“å…¥æ¡†
2. å¯†ç è¾“å…¥æ¡†  
3. ç™»å½•/æäº¤æŒ‰é’®
æ“ä½œé¡ºåºï¼šç‚¹å‡»ç”¨æˆ·åæ¡† â†’ è¾“å…¥ç”¨æˆ·å â†’ ç‚¹å‡»å¯†ç æ¡† â†’ è¾“å…¥å¯†ç  â†’ ç‚¹å‡»ç™»å½•æŒ‰é’®
"""
        
        # æž„å»ºå…ƒç´ åˆ—è¡¨
        elements_text = "å¯ç”¨çš„UIå…ƒç´ ï¼š\n"
        for i, elem in enumerate(elements_summary):
            interactivity_mark = "ðŸ”´" if elem['interactivity'] else "âšª"
            elements_text += f"{interactivity_mark} å…ƒç´ {elem['id']}: {elem['type']} - \"{elem['content'][:50]}\" - ä¸­å¿ƒç‚¹{elem['center']}\n"
        
        interactive_text = f"\näº¤äº’å…ƒç´ ï¼ˆå…±{len(interactive_elements)}ä¸ªï¼‰ï¼š\n"
        for elem in interactive_elements:
            interactive_text += f"ðŸ”´ å…ƒç´ {elem['id']}: \"{elem['content'][:30]}\" - ä¸­å¿ƒç‚¹{elem['center']}\n"
        
        return f"""ä½ æ˜¯ä¸€ä¸ªUIè‡ªåŠ¨åŒ–ä¸“å®¶ã€‚åŸºäºŽä»¥ä¸‹ä¿¡æ¯ï¼Œä¸ºç½‘é¡µæ“ä½œåˆ¶å®šç²¾ç¡®çš„è¡ŒåŠ¨è®¡åˆ’ã€‚

{instruction}

{elements_text}
{interactive_text}

è¯·åˆ†æžè¿™ä¸ªé¡µé¢å¹¶æä¾›æ“ä½œè®¡åˆ’ï¼š

1. ç¡®å®šé¡µé¢ç±»åž‹å’Œå½“å‰çŠ¶æ€
2. è¯†åˆ«å®Œæˆç›®æ ‡æ‰€éœ€çš„å…³é”®å…ƒç´ 
3. æä¾›ç²¾ç¡®çš„æ“ä½œæ­¥éª¤ï¼ˆåŒ…æ‹¬ç‚¹å‡»åæ ‡ï¼‰

è¿”å›žJSONæ ¼å¼ï¼š
{{
  "page_analysis": {{
    "page_type": "ç™»å½•é¡µé¢|æœç´¢é¡µé¢|å†…å®¹é¡µé¢|å¯¼èˆªé¡µé¢",
    "confidence": 0.1-1.0,
    "key_elements_found": ["element_type1", "element_type2"]
  }},
  "action_plan": [
    {{
      "step": 1,
      "action": "click|type|scroll",
      "target_element_id": å…ƒç´ ID,
      "target_coordinates": [x, y],
      "description": "æ“ä½œæè¿°",
      "input_text": "è¦è¾“å…¥çš„æ–‡æœ¬ï¼ˆå¦‚æžœæ˜¯typeæ“ä½œï¼‰"
    }}
  ],
  "success_probability": 0.1-1.0,
  "notes": "é¢å¤–è¯´æ˜Ž"
}}

åªåŸºäºŽå®žé™…çœ‹åˆ°çš„å…ƒç´ åˆ¶å®šè®¡åˆ’ï¼Œç¡®ä¿åæ ‡å‡†ç¡®ã€‚"""
    
    def _parse_action_plan(self, text: str) -> Dict[str, Any]:
        """è§£æžè¡ŒåŠ¨è®¡åˆ’"""
        try:
            # å°è¯•è§£æžJSON
            json_start = text.find('{')
            json_end = text.rfind('}') + 1
            
            if json_start >= 0 and json_end > json_start:
                json_text = text[json_start:json_end]
                return json.loads(json_text)
        except:
            pass
        
        # å¤±è´¥æ—¶è¿”å›žåŸºç¡€è®¡åˆ’
        return {
            "page_analysis": {
                "page_type": f"{self.task_type}é¡µé¢",
                "confidence": 0.5,
                "key_elements_found": []
            },
            "action_plan": [],
            "success_probability": 0.3,
            "notes": "è§£æžå¤±è´¥ï¼Œä½¿ç”¨fallbackè®¡åˆ’"
        }
    
    def _match_actions_to_elements(self, decision: Dict[str, Any], ui_elements: List[Dict[str, Any]]) -> Dict[str, Any]:
        """å°†å†³ç­–ä¸Žå®žé™…UIå…ƒç´ åŒ¹é…"""
        
        action_plan = decision.get("action_plan", [])
        
        # ä¸ºæ¯ä¸ªè¡ŒåŠ¨æ­¥éª¤åŒ¹é…å®žé™…å…ƒç´ 
        for step in action_plan:
            target_id = step.get("target_element_id")
            
            if target_id is not None:
                # æ‰¾åˆ°å¯¹åº”çš„å…ƒç´ 
                target_element = None
                for element in ui_elements:
                    if element.get('element_id') == target_id:
                        target_element = element
                        break
                
                if target_element:
                    # ä½¿ç”¨å®žé™…å…ƒç´ çš„åæ ‡
                    step["actual_coordinates"] = target_element.get("center")
                    step["actual_bbox"] = target_element.get("pixel_bbox")
                    step["element_content"] = target_element.get("content")
                    step["element_type"] = target_element.get("type")
                    step["element_size"] = target_element.get("size")
        
        return {
            "page_analysis": decision.get("page_analysis", {}),
            "action_plan": action_plan,
            "success_probability": decision.get("success_probability", 0.5),
            "notes": decision.get("notes", ""),
            "total_steps": len(action_plan),
            "interactive_elements_available": len([e for e in ui_elements if e.get('interactivity')])
        }
    
    def _create_fallback_elements(self, image_path: str, intelligence: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Create fallback elements based on typical layouts"""
        img = Image.open(image_path)
        img_width, img_height = img.size
        
        layout = intelligence.get('layout_pattern', 'vertical')
        
        if layout == 'vertical':
            form_center_x = img_width // 2
            form_start_y = img_height // 3
            
            return [
                {
                    'id': 'fallback_username',
                    'bbox': [form_center_x - 150, form_start_y, form_center_x + 150, form_start_y + 40],
                    'center': [form_center_x, form_start_y + 20],
                    'size': [300, 40],
                    'confidence': 0.6,
                    'type': 'input'
                },
                {
                    'id': 'fallback_password',
                    'bbox': [form_center_x - 150, form_start_y + 70, form_center_x + 150, form_start_y + 110],
                    'center': [form_center_x, form_start_y + 90],
                    'size': [300, 40],
                    'confidence': 0.6,
                    'type': 'input'
                },
                {
                    'id': 'fallback_button',
                    'bbox': [form_center_x - 75, form_start_y + 140, form_center_x + 75, form_start_y + 180],
                    'center': [form_center_x, form_start_y + 160],
                    'size': [150, 40],
                    'confidence': 0.5,
                    'type': 'button'
                }
            ]
        else:
            return [
                {
                    'id': 'fallback_form',
                    'bbox': [img_width//4, img_height//3, 3*img_width//4, 2*img_height//3],
                    'center': [img_width//2, img_height//2],
                    'size': [img_width//2, img_height//3],
                    'confidence': 0.4,
                    'type': 'form'
                }
            ]