import json
import logging
from pathlib import Path
from typing import Optional, Union

import mlflow
import numpy as np
import onnx

from .base_mlflow_client import BaseMLflowClient

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class OnnxClient(BaseMLflowClient):
    def __init__(self):
        super().__init__()

    def upload(
        self,
        model: Union[onnx.ModelProto, str, bytes, Path],
    ) -> Optional[str]:
        """
        ONNX ëª¨ë¸ì„ MLflowì— ì—…ë¡œë“œí•˜ê³  í•„ìš”í•œ ê²½ìš° RabbitMQì— ì•Œë¦¼ì„ ë³´ëƒ…ë‹ˆë‹¤.

        Args:
            model: ONNX ëª¨ë¸ ë˜ëŠ” ëª¨ë¸ ê²½ë¡œ

        Returns:
            Optional[str]: í”„ë¡œë•ì…˜ ëª¨ë“œê°€ ì•„ë‹Œ ê²½ìš° ëª¨ë¸ ê²½ë¡œ ë°˜í™˜

        Raises:
            Exception: ëª¨ë¸ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ

        """
        try:
            logger.info("ONNX ëª¨ë¸ ìœ íš¨ì„± ê²€ì‚¬ ì‹œì‘")

            # íŒŒì¼ ê²½ë¡œì¸ ê²½ìš° ëª¨ë¸ ë¡œë“œ
            if isinstance(model, (str, Path)):
                logger.debug(f"ëª¨ë¸ íŒŒì¼ ë¡œë“œ ì¤‘: {model}")
                model_proto = onnx.load(str(model))
            elif isinstance(model, onnx.ModelProto):
                model_proto = model
            else:
                raise ValueError(
                    "model must be either a file path or an ONNX ModelProto"
                )

            # ONNX ëª¨ë¸ ìœ íš¨ì„± ê²€ì‚¬
            onnx.checker.check_model(model_proto)
            logger.info("âœ… ONNX ëª¨ë¸ ìœ íš¨ì„± ê²€ì‚¬ ì™„ë£Œ")

            # í…ì„œ ì •ë³´ ë¡œê¹…
            self._log_tensor(model_proto)

            # MLflow 3.11.1: input_exampleì„ ì‚¬ìš©í•˜ì—¬ ìë™ signature ì¶”ë¡ 
            input_example = self._get_input_example(model_proto)

            # MLflowì— ëª¨ë¸ ë¡œê¹…
            path = self._log_model(model=model_proto, input_example=input_example)

            # í”„ë¡œë•ì…˜ ëª¨ë“œê°€ ì•„ë‹Œ ê²½ìš° RabbitMQ ê±´ë„ˆëœ€
            if not self.is_production:
                logger.info("ê°œë°œ ëª¨ë“œ: RabbitMQ ë©”ì‹œì§€ ì „ì†¡ ê±´ë„ˆëœ€")
                return path

            # RabbitMQì— ëª¨ë¸ ì—…ë¡œë“œ ì•Œë¦¼ ë°œí–‰
            self._publish_to_rabbitmq(path)
            logger.info("ğŸš€ ONNX ëª¨ë¸ ì—…ë¡œë“œ ë° RabbitMQ ë°œí–‰ ì™„ë£Œ")

            return path

        except Exception as e:
            logger.error(f"ONNX ëª¨ë¸ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e!s}", exc_info=True)
            raise

    def _get_input_example(
        self, onnx_model: onnx.ModelProto
    ) -> Union[np.ndarray, dict[str, np.ndarray]]:
        """
        ONNX ëª¨ë¸ì˜ ì…ë ¥ ìŠ¤í‚¤ë§ˆë¥¼ ë¶„ì„í•˜ì—¬ ì…ë ¥ ì˜ˆì œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        ë‹¤ì¤‘ ì…ë ¥ì„ ì§€ì›í•©ë‹ˆë‹¤.

        Args:
            onnx_model: ONNX ëª¨ë¸

        Returns:
            Union[np.ndarray, dict[str, np.ndarray]]: ë‹¨ì¼ ì…ë ¥ì¸ ê²½ìš° ë°°ì—´, ë‹¤ì¤‘ ì…ë ¥ì¸ ê²½ìš° ë”•ì…”ë„ˆë¦¬

        Raises:
            ValueError: ì…ë ¥ í…ì„œê°€ ì—†ëŠ” ê²½ìš°
            Exception: ì…ë ¥ ì˜ˆì œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ

        """
        try:
            input_tensors = onnx_model.graph.input

            if not input_tensors:
                raise ValueError("ONNX ëª¨ë¸ì— ì…ë ¥ í…ì„œê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            # ë‹¨ì¼ ì…ë ¥ì¸ ê²½ìš°
            if len(input_tensors) == 1:
                input_tensor = input_tensors[0]
                input_example = self._create_tensor_example(input_tensor)
                logger.debug(
                    f"ë‹¨ì¼ ì…ë ¥ ì˜ˆì œ ìƒì„±: í˜•íƒœ={input_example.shape}, íƒ€ì…={input_example.dtype}"
                )
                return input_example

            # ë‹¤ì¤‘ ì…ë ¥ì¸ ê²½ìš°
            else:
                input_examples = {}
                for input_tensor in input_tensors:
                    input_name = input_tensor.name
                    input_example = self._create_tensor_example(input_tensor)
                    input_examples[input_name] = input_example
                    logger.debug(
                        f"ë‹¤ì¤‘ ì…ë ¥ ì˜ˆì œ ìƒì„±: {input_name}, í˜•íƒœ={input_example.shape}, íƒ€ì…={input_example.dtype}"
                    )

                logger.info(f"ë‹¤ì¤‘ ì…ë ¥ ì˜ˆì œ ìƒì„± ì™„ë£Œ: {len(input_examples)}ê°œ ì…ë ¥")
                return input_examples

        except Exception as e:
            logger.error(f"ì…ë ¥ ì˜ˆì œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e!s}", exc_info=True)
            raise

    def _create_tensor_example(self, input_tensor) -> np.ndarray:
        """
        ê°œë³„ í…ì„œì— ëŒ€í•œ ì˜ˆì œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            input_tensor: ONNX ì…ë ¥ í…ì„œ

        Returns:
            np.ndarray: ì˜ˆì œ ë°ì´í„°

        Raises:
            ValueError: ì§€ì›ë˜ì§€ ì•ŠëŠ” ë°ì´í„° íƒ€ì…

        """
        # ë°ì´í„° íƒ€ì… ë° í˜•íƒœ ì¶”ì¶œ
        dtype = self.get_triton_compatible_type(input_tensor.type.tensor_type)
        shape = [
            dim.dim_value if dim.dim_value > 0 else 1  # ë™ì  ì°¨ì›ì€ 1ë¡œ ì„¤ì •
            for dim in input_tensor.type.tensor_type.shape.dim
        ]

        # ë¹ˆ shape ì²˜ë¦¬
        if not shape:
            shape = [1]

        # NumPy ë°ì´í„° íƒ€ì… ë³€í™˜
        numpy_dtype = self._get_numpy_dtype(dtype)

        # íš¨ìœ¨ì ì¸ ì˜ˆì œ ë°ì´í„° ìƒì„± (ëœë¤ ëŒ€ì‹  zeros ì‚¬ìš©ìœ¼ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ)
        if numpy_dtype in [np.bool_, np.uint8, np.int32, np.int64]:
            # ì •ìˆ˜í˜•/ë¶ˆë¦°í˜•ì€ zeros
            input_example = np.zeros(shape, dtype=numpy_dtype)
        else:
            # ì‹¤ìˆ˜í˜•ì€ ì‘ì€ ëœë¤ ê°’ (ì¼ë¶€ ëª¨ë¸ì—ì„œ zero ì…ë ¥ ì‹œ ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŒ)
            input_example = np.random.rand(*shape).astype(numpy_dtype) * 0.1

        return input_example

    def _process_tensors(self, tensors) -> dict[str, np.ndarray]:
        """
        í…ì„œ ì •ë³´ë¥¼ ì²˜ë¦¬í•˜ì—¬ ìŠ¤í‚¤ë§ˆ íŒŒë¼ë¯¸í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            tensors: í…ì„œ ë¦¬ìŠ¤íŠ¸

        Returns:
            dict[str, np.ndarray]: í…ì„œ ì´ë¦„ì„ í‚¤ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬

        """
        schema_params = {}
        for tensor in tensors:
            name = tensor.name
            dtype = self.get_triton_compatible_type(tensor.type.tensor_type)
            shape = [
                dim.dim_value if dim.dim_value > 0 else 1  # ë™ì  ì°¨ì›ì€ 1ë¡œ ì„¤ì •
                for dim in tensor.type.tensor_type.shape.dim
            ]

            numpy_dtype = self._get_numpy_dtype(dtype)
            schema_params[name] = np.ones(shape, dtype=numpy_dtype)
            logger.debug(f"í…ì„œ ì²˜ë¦¬: {name}, í˜•íƒœ: {shape}, íƒ€ì…: {dtype}")

        return schema_params

    def _publish_to_rabbitmq(self, path: str) -> None:
        """
        RabbitMQì— ëª¨ë¸ ì—…ë¡œë“œ ë©”ì‹œì§€ë¥¼ ë°œí–‰í•©ë‹ˆë‹¤.

        Args:
            path: ì—…ë¡œë“œëœ ëª¨ë¸ ê²½ë¡œ

        Raises:
            Exception: RabbitMQ ë©”ì‹œì§€ ë°œí–‰ ì‹¤íŒ¨

        """
        channel = None
        try:
            channel = self.get_connection().channel()

            message = json.dumps(
                {"train_id": self.train_id, "full_path": path}, ensure_ascii=False
            )

            channel.basic_publish(
                exchange=self._uploadModelExchange,
                routing_key=self._uploadModelExchange,
                body=message,
            )
            logger.info(f"RabbitMQì— ëª¨ë¸ ì—…ë¡œë“œ ë©”ì‹œì§€ ë°œí–‰ ì™„ë£Œ: {message}")

        except Exception as e:
            logger.error(f"RabbitMQ ë©”ì‹œì§€ ë°œí–‰ ì‹¤íŒ¨: {e!s}", exc_info=True)
            raise
        finally:
            if channel:
                channel.close()

    def _log_model(
        self,
        model: onnx.ModelProto,
        input_example: Union[np.ndarray, dict[str, np.ndarray]],
    ) -> str:
        """
        MLflowì— ONNX ëª¨ë¸ì„ ë¡œê¹…í•©ë‹ˆë‹¤.

        Args:
            model: ONNX ëª¨ë¸
            input_example: ìë™ signature ì¶”ë¡ ì„ ìœ„í•œ ì…ë ¥ ì˜ˆì œ

        Returns:
            str: ë¡œê¹…ëœ ëª¨ë¸ì˜ ì „ì²´ ê²½ë¡œ

        Raises:
            Exception: ëª¨ë¸ ë¡œê¹… ì¤‘ ì˜¤ë¥˜ ë°œìƒ

        """
        try:
            # MLflow 3.11.1: input_exampleì„ ì‚¬ìš©í•˜ì—¬ ìë™ signature ì¶”ë¡ 
            model_info = mlflow.onnx.log_model(
                onnx_model=model,
                artifact_path=self.model_name,
                input_example=input_example,  # ìë™ signature ì¶”ë¡ 
                # MLflow 3.11.1 ì¶”ê°€ ì˜µì…˜ë“¤
                registered_model_name=None,  # í•„ìš”ì‹œ ë“±ë¡ëœ ëª¨ë¸ëª… ì§€ì • ê°€ëŠ¥
                await_registration_for=None,  # ë“±ë¡ ëŒ€ê¸° ì‹œê°„
                metadata={
                    "framework": "onnx",
                    "source": "pytorch_trace",
                },  # ë©”íƒ€ë°ì´í„° ì¶”ê°€
                # ONNX ëª¨ë¸ íŠ¹í™” ì˜µì…˜
                save_as_external_data=True,  # í° ëª¨ë¸ì˜ ê²½ìš° ì™¸ë¶€ ë°ì´í„°ë¡œ ì €ì¥
            )

            logger.info(f"MLflowì— ONNX ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_info.model_uri}")
            logger.debug(f"ëª¨ë¸ ì •ë³´: {model_info}")

            return model_info.model_uri

        except Exception as e:
            logger.error(f"ONNX ëª¨ë¸ ë¡œê¹… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e!s}", exc_info=True)
            raise

    def _log_tensor(self, onnx_model: onnx.ModelProto) -> None:
        """
        ONNX ëª¨ë¸ì˜ ì…ë ¥ ë° ì¶œë ¥ í…ì„œ ì •ë³´ë¥¼ ë¡œê·¸ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.

        Args:
            onnx_model: ONNX ëª¨ë¸

        """
        logger.info("=== ONNX ëª¨ë¸ í…ì„œ ì •ë³´ ===")

        # ì…ë ¥ í…ì„œ ì •ë³´
        logger.info(f"ì…ë ¥ í…ì„œ ê°œìˆ˜: {len(onnx_model.graph.input)}")
        for i, input_tensor in enumerate(onnx_model.graph.input):
            input_name = input_tensor.name
            input_type = self.get_triton_compatible_type(input_tensor.type.tensor_type)
            input_shape = [
                dim.dim_value for dim in input_tensor.type.tensor_type.shape.dim
            ]

            logger.info(
                f"ì…ë ¥ {i + 1}: ì´ë¦„={input_name}, íƒ€ì…={input_type}, í˜•íƒœ={input_shape}"
            )

        # ì¶œë ¥ í…ì„œ ì •ë³´
        logger.info(f"ì¶œë ¥ í…ì„œ ê°œìˆ˜: {len(onnx_model.graph.output)}")
        for i, output_tensor in enumerate(onnx_model.graph.output):
            output_name = output_tensor.name
            output_type = self.get_triton_compatible_type(
                output_tensor.type.tensor_type
            )
            output_shape = [
                dim.dim_value for dim in output_tensor.type.tensor_type.shape.dim
            ]

            logger.info(
                f"ì¶œë ¥ {i + 1}: ì´ë¦„={output_name}, íƒ€ì…={output_type}, í˜•íƒœ={output_shape}"
            )

    def _get_numpy_dtype(self, triton_type: str) -> np.dtype:
        """
        Triton ë°ì´í„° íƒ€ì…ì„ NumPy ë°ì´í„° íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

        Args:
            triton_type: Triton í˜¸í™˜ ë°ì´í„° íƒ€ì… ë¬¸ìì—´

        Returns:
            np.dtype: NumPy ë°ì´í„° íƒ€ì…

        Raises:
            ValueError: ì§€ì›ë˜ì§€ ì•ŠëŠ” ë°ì´í„° íƒ€ì…

        """
        mapping = {
            "TYPE_BOOL": np.bool_,
            "TYPE_UINT8": np.uint8,
            "TYPE_UINT16": np.uint16,
            "TYPE_UINT32": np.uint32,
            "TYPE_UINT64": np.uint64,
            "TYPE_INT8": np.int8,
            "TYPE_INT16": np.int16,
            "TYPE_INT32": np.int32,
            "TYPE_INT64": np.int64,
            "TYPE_FP16": np.float16,
            "TYPE_FP32": np.float32,
            "TYPE_FP64": np.float64,
            "TYPE_STRING": np.str_,
            # í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ê¸°ì¡´ ë§¤í•‘
            "BOOL": np.bool_,
            "UINT8": np.uint8,
            "UINT16": np.uint16,
            "INT32": np.int32,
            "INT64": np.int64,
            "FP16": np.float16,
            "FP32": np.float32,
            "FP64": np.float64,
        }

        result = mapping.get(triton_type)
        if result is None:
            logger.warning(
                f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ë°ì´í„° íƒ€ì…: {triton_type}, FP32ë¡œ ê¸°ë³¸ ì„¤ì •"
            )
            return np.dtype(np.float32)

        return np.dtype(result)
