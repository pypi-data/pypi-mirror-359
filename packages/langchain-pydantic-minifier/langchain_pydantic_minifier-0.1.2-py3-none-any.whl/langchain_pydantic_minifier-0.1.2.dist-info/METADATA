Metadata-Version: 2.3
Name: langchain-pydantic-minifier
Version: 0.1.2
Summary: automatically reduces token usage by replacing field names with short identifiers (e.g., a, b, c, â€¦), while preserving full descriptions and reversibility.
License: MIT
Author: JulienStitelet
Author-email: julien.stitelet@careerbuilder.com
Requires-Python: >=3.9,<4.0
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Requires-Dist: langchain (>=0.3.26,<0.4.0)
Requires-Dist: langchain-core (>=0.3.67,<0.4.0)
Project-URL: Bug Tracker, https://github.com/julienstitelet/langchain-pydantic-minifier/issues
Description-Content-Type: text/markdown

# ðŸš€ Add MinifiedPydanticOutputParser to Optimize Token Usage in LLM Outputs

This PR introduces a new class: MinifiedPydanticOutputParser, a drop-in replacement for PydanticOutputParser that automatically reduces token usage by replacing field names with short identifiers (e.g., a, b, c, â€¦), while preserving full descriptions and reversibility.
## âœ¨ What It Does
* Transforms a given Pydantic schema by replacing verbose field names with shorter aliases.
* Retains all Field(..., description=...) information â€” essential for prompt construction and LLM understanding.
* Accepts minified JSON outputs from the LLM and reconstructs the original schema transparently.
* Supports nested models and list fields recursively.
* Compatible with strict=True mode used with with_structured_output.

## âœ… Benefits
* Reduces prompt and completion token count, leading to faster LLM response times and lower inference costs.
* Maintains clarity in the LLM's understanding of the field semantics thanks to preserved descriptions.
* No code change required downstream â€” consumers receive the original schema post-parsing.

## ðŸ“‰ Performance Impact
In personal benchmarks, this optimization led to a ~30% reduction in LLM response time, due to:
* Fewer tokens needing generation
* Reduced I/O and parsing overhead

## ðŸ’¸ This also translates to lower API costs, especially in high-throughput or large-output scenarios.

## ðŸ§ª Example

Given:
```python
class User(BaseModel):
    first_name: str = Field(..., description="The user's first name")
    last_name: str = Field(..., description="The user's last name")
```
The model is transformed into:
```python
class MinifiedUser(BaseModel):
    a: str = Field(..., description="The user's first name")
    b: str = Field(..., description="The user's last name")
```

Then seamlessly restored to the original User class after parsing the LLM output.
