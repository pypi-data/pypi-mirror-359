# ðŸš€ Add MinifiedPydanticOutputParser to Optimize Token Usage in LLM Outputs

This PR introduces a new class: MinifiedPydanticOutputParser, a drop-in replacement for PydanticOutputParser that automatically reduces token usage by replacing field names with short identifiers (e.g., a, b, c, â€¦), while preserving full descriptions and reversibility.
## âœ¨ What It Does
* Transforms a given Pydantic schema by replacing verbose field names with shorter aliases.
* Retains all Field(..., description=...) information â€” essential for prompt construction and LLM understanding.
* Accepts minified JSON outputs from the LLM and reconstructs the original schema transparently.
* Supports nested models and list fields recursively.
* Compatible with strict=True mode used with with_structured_output.

## âœ… Benefits
* Reduces prompt and completion token count, leading to faster LLM response times and lower inference costs.
* Maintains clarity in the LLM's understanding of the field semantics thanks to preserved descriptions.
* No code change required downstream â€” consumers receive the original schema post-parsing.

## ðŸ“‰ Performance Impact
In personal benchmarks, this optimization led to a ~30% reduction in LLM response time, due to:
* Fewer tokens needing generation
* Reduced I/O and parsing overhead

## ðŸ’¸ This also translates to lower API costs, especially in high-throughput or large-output scenarios.

## Examples

* [See Gemini example](examples/gemini.md)
* [see OpenAI OutputParser example](examples/openai_output_parser.md)
* [see OpenAI structured_output strict example](examples/openai_strict.md)


## ðŸ§ª What it does

Given:
```python
class User(BaseModel):
    first_name: str = Field(..., description="The user's first name")
    last_name: str = Field(..., description="The user's last name")
```
The model is transformed into:
```python
class MinifiedUser(BaseModel):
    a: str = Field(..., description="The user's first name")
    b: str = Field(..., description="The user's last name")
```

Then seamlessly restored to the original User class after parsing the LLM output.