{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Registry Client Example\n",
    "\n",
    "This notebook demonstrates how to use the `LocalRegistryClient` to access model information from LangGate's model registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint as pp\n",
    "\n",
    "# Optional environment variables\n",
    "# os.environ[\"LANGGATE_CONFIG\"] = \"path/to/langgate_config.yaml\"\n",
    "# os.environ[\"LANGGATE_MODELS\"] = \"path/to/langgate_models.json\"\n",
    "# os.environ[\"LOG_LEVEL\"] = \"debug\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Registry Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-06-17 23:25:50\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mcreating_model_registry_singleton\u001b[0m\n",
      "\u001b[2m2025-06-17 23:25:50\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mloaded_model_data             \u001b[0m \u001b[36mmodel_count\u001b[0m=\u001b[35m62\u001b[0m \u001b[36mmodels_data_path\u001b[0m=\u001b[35m/Users/someuser/langgate/packages/registry/src/langgate/registry/data/default_models.json\u001b[0m\n",
      "\u001b[2m2025-06-17 23:25:50\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mloaded_config                 \u001b[0m \u001b[36mconfig_path\u001b[0m=\u001b[35m/Users/someuser/langgate/examples/langgate_config.yaml\u001b[0m\n",
      "\u001b[2m2025-06-17 23:25:50\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1minitialized_model_registry_singleton\u001b[0m \u001b[36mconfig_path\u001b[0m=\u001b[35m/Users/someuser/langgate/examples/langgate_config.yaml\u001b[0m \u001b[36menv_file_exists\u001b[0m=\u001b[35mFalse\u001b[0m \u001b[36menv_file_path\u001b[0m=\u001b[35m/Users/someuser/langgate/examples/.env\u001b[0m \u001b[36mmodels_data_path\u001b[0m=\u001b[35m/Users/someuser/langgate/packages/registry/src/langgate/registry/data/default_models.json\u001b[0m\n",
      "\u001b[2m2025-06-17 23:25:50\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1minitialized_base_local_registry_client\u001b[0m\n",
      "\u001b[2m2025-06-17 23:25:50\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1minitialized_local_registry_client_singleton\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langgate.registry import LocalRegistryClient\n",
    "\n",
    "# Initialize the registry client\n",
    "# LocalRegistryClient is a singleton\n",
    "client = LocalRegistryClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Available Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-06-17 23:25:57\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mrefreshing_model_cache        \u001b[0m\n",
      "\u001b[2m2025-06-17 23:25:57\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mrefreshed_model_cache         \u001b[0m \u001b[36mmodel_count\u001b[0m=\u001b[35m5\u001b[0m\n",
      "Available models: 5\n",
      "- openai/gpt-4.1: GPT-4.1\n",
      "- openai/o3: o3\n",
      "- openai/o3-high: o3-high\n",
      "- anthropic/claude-sonnet-4: Claude-4 Sonnet\n",
      "- anthropic/claude-sonnet-4-reasoning: Claude-4 Sonnet R\n"
     ]
    }
   ],
   "source": [
    "# List available models\n",
    "models = await client.list_models()\n",
    "print(f\"Available models: {len(models)}\")\n",
    "for model in models[:5]:\n",
    "    print(f\"- {model.id}: {model.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Detailed Model Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: GPT-4.1\n",
      "Provider: OpenAI\n",
      "\n",
      "Model metadata:\n",
      "{'capabilities': {'supports_parallel_tool_calls': True,\n",
      "                  'supports_prompt_caching': True,\n",
      "                  'supports_response_schema': True,\n",
      "                  'supports_system_messages': True,\n",
      "                  'supports_tool_choice': True,\n",
      "                  'supports_tools': True,\n",
      "                  'supports_vision': True},\n",
      " 'context_window': {'max_input_tokens': 1047576, 'max_output_tokens': 32768},\n",
      " 'costs': {'cache_read_input_token_cost': Decimal('5E-7'),\n",
      "           'input_cost_per_token': Decimal('0.000002'),\n",
      "           'output_cost_per_token': Decimal('0.000008')},\n",
      " 'description': \"GPT-4.1 is the latest iteration of OpenAI's flagship model \"\n",
      "                'with improved capabilities across all domains.',\n",
      " 'id': 'openai/gpt-4.1',\n",
      " 'name': 'GPT-4.1',\n",
      " 'provider': {'id': 'openai', 'name': 'OpenAI'},\n",
      " 'provider_id': 'openai',\n",
      " 'updated_dt': datetime.datetime(2025, 6, 17, 22, 25, 50, 54626, tzinfo=datetime.timezone.utc)}\n"
     ]
    }
   ],
   "source": [
    "# Get a specific model ID from the available models\n",
    "if models:\n",
    "    model_id = models[0].id\n",
    "    model_info = await client.get_model_info(model_id)\n",
    "\n",
    "    print(f\"\\nModel: {model_info.name}\")\n",
    "    print(f\"Provider: {model_info.provider.name}\")\n",
    "\n",
    "    print(\"\\nModel metadata:\")\n",
    "    pp(model_info.model_dump(exclude_none=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI models: 3\n",
      "- openai/gpt-4.1: GPT-4.1\n",
      "- openai/o3: o3\n",
      "- openai/o3-high: o3-high\n"
     ]
    }
   ],
   "source": [
    "# Filter models by provider\n",
    "openai_models = [model for model in models if model.provider.id == \"openai\"]\n",
    "print(f\"OpenAI models: {len(openai_models)}\")\n",
    "for model in openai_models[:3]:\n",
    "    print(f\"- {model.id}: {model.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Models with vision capabilities: 5\n",
      "- openai/gpt-4.1: GPT-4.1\n",
      "- openai/o3: o3\n",
      "- openai/o3-high: o3-high\n",
      "- anthropic/claude-sonnet-4: Claude-4 Sonnet\n",
      "- anthropic/claude-sonnet-4-reasoning: Claude-4 Sonnet R\n"
     ]
    }
   ],
   "source": [
    "# Filter models by capability\n",
    "vision_models = [model for model in models if model.capabilities.supports_vision]\n",
    "print(f\"\\nModels with vision capabilities: {len(vision_models)}\")\n",
    "for model in vision_models[:5]:\n",
    "    print(f\"- {model.id}: {model.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Model Costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost information for GPT-4.1:\n",
      "{'cache_read_input_token_cost': Decimal('5E-7'),\n",
      " 'input_cost_per_token': Decimal('0.000002'),\n",
      " 'output_cost_per_token': Decimal('0.000008')}\n"
     ]
    }
   ],
   "source": [
    "# Get cost information for a model\n",
    "if models:\n",
    "    model_id = models[0].id\n",
    "    model_info = await client.get_model_info(model_id)\n",
    "\n",
    "    print(f\"Cost information for {model_info.name}:\")\n",
    "    pp(model_info.costs.model_dump(exclude_none=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
