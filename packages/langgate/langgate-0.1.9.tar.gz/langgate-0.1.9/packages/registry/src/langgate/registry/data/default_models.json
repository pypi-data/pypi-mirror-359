{
  "eleutheria/vllm/el-1": {
    "name": "El-1",
    "mode": "chat",
    "service_provider": "eleutheria/vllm",
    "model_provider": "eleutheria",
    "model_provider_name": "Eleutheria",
    "context": {
      "max_input_tokens": 4096,
      "max_output_tokens": 4096
    },
    "costs": {
      "input_cost_per_token": "0",
      "output_cost_per_token": "0"
    },
    "description": "Eleutheria's flagship language model, trained to respond without bias or censorship.",
    "source": "https://eleutheria.ai/pricing"
  },
  "openai/gpt-4o": {
    "name": "GPT-4o",
    "mode": "chat",
    "service_provider": "openai",
    "model_provider": "openai",
    "model_provider_name": "OpenAI",
    "capabilities": {
      "supports_tools": true,
      "supports_parallel_tool_calls": true,
      "supports_vision": true,
      "supports_prompt_caching": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 128000,
      "max_output_tokens": 16384
    },
    "costs": {
      "input_cost_per_token": "0.0000025",
      "output_cost_per_token": "0.00001",
      "input_cost_per_token_batches": "0.00000125",
      "output_cost_per_token_batches": "0.000005",
      "cache_read_input_token_cost": "0.00000125",
      "input_cost_per_image": "0.003613"
    },
    "description": "The GPT-4o (omni) model from OpenAI builds upon the GPT-4 series with improved performance and multimodal capabilities. GPT-4o is great for most tasks.",
    "_last_updated": "2025-07-01T16:47:00.873641+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "openai/gpt-4o"
  },
  "openai/gpt-4o-mini": {
    "name": "GPT-4o mini",
    "mode": "chat",
    "service_provider": "openai",
    "model_provider": "openai",
    "model_provider_name": "OpenAI",
    "capabilities": {
      "supports_tools": true,
      "supports_parallel_tool_calls": true,
      "supports_vision": true,
      "supports_prompt_caching": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 128000,
      "max_output_tokens": 16384
    },
    "costs": {
      "input_cost_per_token": "1.5E-7",
      "output_cost_per_token": "6E-7",
      "input_cost_per_token_batches": "7.5E-8",
      "output_cost_per_token_batches": "3E-7",
      "cache_read_input_token_cost": "7.5E-8",
      "input_cost_per_image": "0.000217"
    },
    "description": "A smaller version of GPT-4o optimized for efficiency.",
    "_last_updated": "2025-07-01T16:47:00.874047+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "openai/gpt-4o-mini"
  },
  "openai/o1": {
    "name": "o1",
    "mode": "chat",
    "service_provider": "openai",
    "model_provider": "openai",
    "model_provider_name": "OpenAI",
    "capabilities": {
      "supports_tools": true,
      "supports_parallel_tool_calls": true,
      "supports_vision": true,
      "supports_prompt_caching": true,
      "supports_reasoning": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 200000,
      "max_output_tokens": 100000
    },
    "costs": {
      "input_cost_per_token": "0.000015",
      "output_cost_per_token": "0.00006",
      "cache_read_input_token_cost": "0.0000075",
      "input_cost_per_image": "0.021675"
    },
    "description": "o1 is the first generation reasoning model from OpenAI. o1 spends additional time thinking (generating a chain of thought) before generating an answer, which makes it better for complex reasoning tasks, particularly in science, mathematics and coding.",
    "_last_updated": "2025-07-01T16:47:00.874359+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "openai/o1"
  },
  "openai/o3": {
    "name": "o3",
    "mode": "chat",
    "service_provider": "openai",
    "model_provider": "openai",
    "model_provider_name": "OpenAI",
    "capabilities": {
      "supports_tools": true,
      "supports_parallel_tool_calls": false,
      "supports_vision": true,
      "supports_prompt_caching": true,
      "supports_reasoning": true,
      "supports_response_schema": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 200000,
      "max_output_tokens": 100000
    },
    "costs": {
      "input_cost_per_token": "0.000002",
      "output_cost_per_token": "0.000008",
      "cache_read_input_token_cost": "5E-7",
      "reasoning_cost_per_token": "0",
      "input_cost_per_image": "0.00153"
    },
    "description": "OpenAI's o3 is their most powerful reasoning model, setting new state-of-the-art benchmarks in coding, math, science, and visual perception. It excels at complex queries requiring multi-faceted analysis, with particular strength in analyzing images, charts, and graphics.",
    "_last_updated": "2025-07-01T16:47:00.874586+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "openai/o3"
  },
  "openai/o3-mini": {
    "name": "o3-mini",
    "mode": "chat",
    "service_provider": "openai",
    "model_provider": "openai",
    "model_provider_name": "OpenAI",
    "capabilities": {
      "supports_tools": true,
      "supports_parallel_tool_calls": false,
      "supports_prompt_caching": true,
      "supports_response_schema": true,
      "supports_tool_choice": true,
      "supports_vision": false,
      "supports_reasoning": true
    },
    "context": {
      "max_input_tokens": 200000,
      "max_output_tokens": 100000
    },
    "costs": {
      "input_cost_per_token": "0.0000011",
      "output_cost_per_token": "0.0000044",
      "cache_read_input_token_cost": "5.5E-7"
    },
    "description": "o3-mini is a cost-efficient reasoning model from OpenAI. o3-mini excels at STEM, especially math and coding. As an alternative to o1-pro, it has a lower cost and lower latency.",
    "_last_updated": "2025-07-01T16:47:00.874946+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "openai/o3-mini"
  },
  "openai/o4-mini": {
    "name": "o4-mini",
    "mode": "chat",
    "service_provider": "openai",
    "model_provider": "openai",
    "model_provider_name": "OpenAI",
    "capabilities": {
      "supports_tools": true,
      "supports_parallel_tool_calls": true,
      "supports_vision": true,
      "supports_prompt_caching": true,
      "supports_reasoning": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 200000,
      "max_output_tokens": 100000
    },
    "costs": {
      "input_cost_per_token": "0.0000011",
      "output_cost_per_token": "0.0000044",
      "cache_read_input_token_cost": "2.75E-7",
      "input_cost_per_image": "0.0008415"
    },
    "description": "o4-mini is a new reasoning model from OpenAI that balances performance and cost.",
    "openrouter_model_id": "openai/o4-mini",
    "_last_updated": "2025-07-01T16:47:00.875185+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "openai/o4-mini"
  },
  "openai/o3-pro": {
    "name": "o3 Pro",
    "mode": "chat",
    "service_provider": "openai",
    "model_provider": "openai",
    "model_provider_name": "OpenAI",
    "capabilities": {
      "supports_tools": true,
      "supports_parallel_tool_calls": false,
      "supports_vision": true,
      "supports_prompt_caching": true,
      "supports_reasoning": true,
      "supports_response_schema": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 200000,
      "max_output_tokens": 100000
    },
    "costs": {
      "input_cost_per_token": "0.00002",
      "output_cost_per_token": "0.00008",
      "reasoning_cost_per_token": "0",
      "input_cost_per_image": "0.0153"
    },
    "description": "OpenAI's o-series models are trained with reinforcement learning to think before they answer and perform complex reasoning. The o3 Pro model uses more compute than o3, spending longer thinking through a problem.",
    "_last_updated": "2025-07-01T16:47:00.875389+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "openai/o3-pro"
  },
  "openai/gpt-4.1": {
    "name": "GPT-4.1",
    "mode": "chat",
    "service_provider": "openai",
    "model_provider": "openai",
    "model_provider_name": "OpenAI",
    "capabilities": {
      "supports_tools": true,
      "supports_parallel_tool_calls": true,
      "supports_vision": true,
      "supports_prompt_caching": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 1047576,
      "max_output_tokens": 32768
    },
    "costs": {
      "input_cost_per_token": "0.000002",
      "output_cost_per_token": "0.000008",
      "cache_read_input_token_cost": "5E-7"
    },
    "description": "GPT-4.1 is the latest iteration of OpenAI's flagship model with improved capabilities across all domains.",
    "openrouter_model_id": "openai/gpt-4.1",
    "_last_updated": "2025-07-01T16:47:00.875592+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "openai/gpt-4.1"
  },
  "openai/gpt-4.1-mini": {
    "name": "GPT-4.1 mini",
    "mode": "chat",
    "service_provider": "openai",
    "model_provider": "openai",
    "model_provider_name": "OpenAI",
    "capabilities": {
      "supports_tools": true,
      "supports_parallel_tool_calls": true,
      "supports_vision": true,
      "supports_prompt_caching": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 1047576,
      "max_output_tokens": 32768
    },
    "costs": {
      "input_cost_per_token": "4E-7",
      "output_cost_per_token": "0.0000016",
      "cache_read_input_token_cost": "1E-7"
    },
    "description": "GPT-4.1 mini is a cost-effective version of GPT-4.1 optimized for efficiency.",
    "openrouter_model_id": "openai/gpt-4.1-mini",
    "_last_updated": "2025-07-01T16:47:00.875823+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "openai/gpt-4.1-mini"
  },
  "openai/gpt-4.1-nano": {
    "name": "GPT-4.1 nano",
    "mode": "chat",
    "service_provider": "openai",
    "model_provider": "openai",
    "model_provider_name": "OpenAI",
    "capabilities": {
      "supports_tools": true,
      "supports_parallel_tool_calls": true,
      "supports_vision": true,
      "supports_prompt_caching": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 1047576,
      "max_output_tokens": 32768
    },
    "costs": {
      "input_cost_per_token": "1E-7",
      "output_cost_per_token": "4E-7",
      "cache_read_input_token_cost": "2.5E-8"
    },
    "description": "GPT-4.1 nano is the smallest and most cost-effective version of GPT-4.1.",
    "openrouter_model_id": "openai/gpt-4.1-nano",
    "_last_updated": "2025-07-01T16:47:00.876023+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "openai/gpt-4.1-nano"
  },
  "openai/chatgpt-4o-latest": {
    "name": "ChatGPT-4o",
    "mode": "chat",
    "service_provider": "openai",
    "model_provider": "openai",
    "model_provider_name": "OpenAI",
    "capabilities": {
      "supports_tools": true,
      "supports_parallel_tool_calls": true,
      "supports_vision": true,
      "supports_prompt_caching": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 128000,
      "max_output_tokens": 16384
    },
    "costs": {
      "input_cost_per_token": "0.000005",
      "output_cost_per_token": "0.000015",
      "input_cost_per_image": "0.007225"
    },
    "description": "ChatGPT-4o is the most recent version of the conversational GPT-4o model used in ChatGPT.",
    "openrouter_model_id": "openai/chatgpt-4o-latest",
    "_last_updated": "2025-07-01T16:47:00.876208+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "openai/chatgpt-4o-latest"
  },
  "anthropic/claude-3-7-sonnet-latest": {
    "name": "Claude-3.7 Sonnet",
    "mode": "chat",
    "service_provider": "anthropic",
    "model_provider": "anthropic",
    "model_provider_name": "Anthropic",
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_prompt_caching": true,
      "supports_reasoning": true,
      "supports_response_schema": true,
      "supports_assistant_prefill": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 200000,
      "max_output_tokens": 64000
    },
    "costs": {
      "input_cost_per_token": "0.000003",
      "output_cost_per_token": "0.000015",
      "cache_read_input_token_cost": "3E-7",
      "cache_creation_input_token_cost": "0.00000375",
      "input_cost_per_image": "0.0048"
    },
    "description": "Claude 3.7 Sonnet is Anthropic's hybrid reasoning model, featuring new 'extended thinking' capabilities. It excels at complex coding, STEM tasks, and multimodal data analysis with large context windows.",
    "openrouter_model_id": "anthropic/claude-3.7-sonnet",
    "_last_updated": "2025-07-01T16:47:00.876401+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "anthropic/claude-3.7-sonnet"
  },
  "anthropic/claude-3-5-sonnet-latest": {
    "name": "Claude-3.5 Sonnet",
    "mode": "chat",
    "service_provider": "anthropic",
    "model_provider": "anthropic",
    "model_provider_name": "Anthropic",
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_prompt_caching": true,
      "supports_response_schema": true,
      "supports_assistant_prefill": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 200000,
      "max_output_tokens": 8192
    },
    "costs": {
      "input_cost_per_token": "0.000003",
      "output_cost_per_token": "0.000015",
      "cache_read_input_token_cost": "3E-7",
      "cache_creation_input_token_cost": "0.00000375",
      "input_cost_per_image": "0.0048"
    },
    "description": "Claude 3.5 Sonnet strikes the ideal balance between intelligence and speed, particularly for enterprise workloads. It delivers strong performance at a lower cost compared to its peers, and is engineered for high endurance in large-scale AI deployments.",
    "deprecation_date": "2025-06-01",
    "tool_use_system_prompt_tokens": 159,
    "openrouter_model_id": "anthropic/claude-3.5-sonnet",
    "_last_updated": "2025-07-01T16:47:00.876601+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "anthropic/claude-3.5-sonnet"
  },
  "anthropic/claude-3-5-sonnet-20240620": {
    "name": "Claude-3.5 Sonnet",
    "mode": "chat",
    "service_provider": "anthropic",
    "model_provider": "anthropic",
    "model_provider_name": "Anthropic",
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_prompt_caching": true,
      "supports_response_schema": true,
      "supports_assistant_prefill": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 200000,
      "max_output_tokens": 8192
    },
    "costs": {
      "input_cost_per_token": "0.000003",
      "output_cost_per_token": "0.000015",
      "cache_read_input_token_cost": "3E-7",
      "cache_creation_input_token_cost": "0.00000375",
      "input_cost_per_image": "0.0048"
    },
    "description": "Claude 3.5 Sonnet strikes the ideal balance between intelligence and speed, particularly for enterprise workloads. It delivers strong performance at a lower cost compared to its peers, and is engineered for high endurance in large-scale AI deployments.",
    "deprecation_date": "2025-06-01",
    "tool_use_system_prompt_tokens": 159,
    "openrouter_model_id": "anthropic/claude-3.5-sonnet-20240620",
    "_last_updated": "2025-07-01T16:47:00.876791+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "anthropic/claude-3.5-sonnet-20240620"
  },
  "anthropic/claude-3-5-haiku-latest": {
    "name": "Claude 3.5 Haiku",
    "mode": "chat",
    "service_provider": "anthropic",
    "model_provider": "anthropic",
    "model_provider_name": "Anthropic",
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_prompt_caching": true,
      "supports_response_schema": true,
      "supports_assistant_prefill": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 200000,
      "max_output_tokens": 8192
    },
    "costs": {
      "input_cost_per_token": "8E-7",
      "output_cost_per_token": "0.000004",
      "cache_read_input_token_cost": "8E-8",
      "cache_creation_input_token_cost": "0.00000125",
      "input_cost_per_image": "0.0004"
    },
    "description": "Claude 3 Haiku is Anthropic's fastest model yet, designed for enterprise workloads which often involve longer prompts. Haiku quickly analyzes large volumes of documents, such as quarterly filings, contracts, or legal cases, for half the cost of other models in its performance tier.",
    "deprecation_date": "2025-10-01",
    "tool_use_system_prompt_tokens": 264,
    "openrouter_model_id": "anthropic/claude-3.5-haiku",
    "_last_updated": "2025-07-01T16:47:00.876983+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "anthropic/claude-3.5-haiku"
  },
  "anthropic/claude-3-5-haiku-20241022": {
    "name": "Claude 3.5 Haiku",
    "mode": "chat",
    "service_provider": "anthropic",
    "model_provider": "anthropic",
    "model_provider_name": "Anthropic",
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_prompt_caching": true,
      "supports_response_schema": true,
      "supports_assistant_prefill": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 200000,
      "max_output_tokens": 8192
    },
    "costs": {
      "input_cost_per_token": "8E-7",
      "output_cost_per_token": "0.000004",
      "cache_read_input_token_cost": "8E-8",
      "cache_creation_input_token_cost": "0.00000125",
      "input_cost_per_image": "0.0004"
    },
    "description": "Claude 3 Haiku is Anthropic's fastest model yet, designed for enterprise workloads which often involve longer prompts. Haiku quickly analyzes large volumes of documents, such as quarterly filings, contracts, or legal cases, for half the cost of other models in its performance tier.",
    "deprecation_date": "2025-10-01",
    "tool_use_system_prompt_tokens": 264,
    "openrouter_model_id": "anthropic/claude-3.5-haiku-20241022",
    "_last_updated": "2025-07-01T16:47:00.877155+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "anthropic/claude-3.5-haiku-20241022"
  },
  "anthropic/claude-3-opus-latest": {
    "name": "Claude 3 Opus",
    "mode": "chat",
    "service_provider": "anthropic",
    "model_provider": "anthropic",
    "model_provider_name": "Anthropic",
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_prompt_caching": true,
      "supports_response_schema": true,
      "supports_assistant_prefill": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 200000,
      "max_output_tokens": 4096
    },
    "costs": {
      "input_cost_per_token": "0.000015",
      "output_cost_per_token": "0.000075",
      "cache_read_input_token_cost": "0.0000015",
      "cache_creation_input_token_cost": "0.00001875",
      "input_cost_per_image": "0.024"
    },
    "description": "Claude 3 Opus is Anthropic's most intelligent model, with best-in-market performance on highly complex tasks. It navigates open-ended prompts and sight-unseen scenarios with remarkable fluency and human-like understanding, showcasing the outer limits of generative AI.",
    "deprecation_date": "2025-03-01",
    "tool_use_system_prompt_tokens": 395,
    "openrouter_model_id": "anthropic/claude-3-opus",
    "_last_updated": "2025-07-01T16:47:00.877345+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "anthropic/claude-3-opus"
  },
  "anthropic/claude-sonnet-4-0": {
    "name": "Claude-4 Sonnet",
    "mode": "chat",
    "service_provider": "anthropic",
    "model_provider": "anthropic",
    "model_provider_name": "Anthropic",
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_prompt_caching": true,
      "supports_reasoning": true,
      "supports_response_schema": true,
      "supports_assistant_prefill": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 200000,
      "max_output_tokens": 64000
    },
    "costs": {
      "input_cost_per_token": "0.000003",
      "output_cost_per_token": "0.000015",
      "cache_read_input_token_cost": "3E-7",
      "cache_creation_input_token_cost": "0.00000375",
      "input_cost_per_image": "0.0048"
    },
    "description": "Claude 4 Sonnet is Anthropic's latest flagship model offering enhanced reasoning capabilities and improved performance across all domains.",
    "openrouter_model_id": "anthropic/claude-sonnet-4",
    "_last_updated": "2025-07-01T16:47:00.877539+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "anthropic/claude-sonnet-4"
  },
  "anthropic/claude-opus-4-0": {
    "name": "Claude-4 Opus",
    "mode": "chat",
    "service_provider": "anthropic",
    "model_provider": "anthropic",
    "model_provider_name": "Anthropic",
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_prompt_caching": true,
      "supports_reasoning": true,
      "supports_response_schema": true,
      "supports_assistant_prefill": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 200000,
      "max_output_tokens": 32000
    },
    "costs": {
      "input_cost_per_token": "0.000015",
      "output_cost_per_token": "0.000075",
      "cache_read_input_token_cost": "0.0000015",
      "cache_creation_input_token_cost": "0.00001875",
      "input_cost_per_image": "0.024"
    },
    "description": "Claude-4 Opus is Anthropic's most advanced model, featuring superior reasoning, analysis, and complex task capabilities.",
    "openrouter_model_id": "anthropic/claude-opus-4",
    "_last_updated": "2025-07-01T16:47:00.877727+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "anthropic/claude-opus-4"
  },
  "fireworks_ai/accounts/fireworks/models/deepseek-r1": {
    "name": "DeepSeek-R1",
    "mode": "chat",
    "service_provider": "fireworks_ai",
    "model_provider": "deepseek",
    "model_provider_name": "DeepSeek",
    "capabilities": {
      "supports_reasoning": true
    },
    "context": {
      "max_input_tokens": 128000,
      "max_output_tokens": 8192
    },
    "costs": {
      "input_cost_per_token": "0.000003",
      "output_cost_per_token": "0.000008"
    },
    "description": "R1 is a groundbreaking open source reasoning model developed by DeepSeek. R1 uses Chain of Thought (CoT) deductions to spend more time working through solutions to improve accuracy. Before providing a final answer, it generates detailed reasoning steps, allowing users to examine and leverage the model's thought process. This self-hosted version of R1 is deployed on US servers.",
    "source": "https://fireworks.ai/pricing"
  },
  "fireworks_ai/accounts/fireworks/models/deepseek-v3": {
    "name": "DeepSeek-V3",
    "mode": "chat",
    "service_provider": "fireworks_ai",
    "model_provider": "deepseek",
    "model_provider_name": "DeepSeek",
    "capabilities": {
      "supports_response_schema": true,
      "supports_tools": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 131072,
      "max_output_tokens": 8192
    },
    "costs": {
      "input_cost_per_token": "0.0000009",
      "output_cost_per_token": "0.0000009"
    },
    "description": "DeepSeek-V3 is an open-source 671B parameter Mixture-of-Experts (MoE) model that builds upon Meta's LLaMA, rivalling or exceeding similar closed source models across numerous benchmarks. This self-hosted version of V3 is deployed on US servers.",
    "source": "https://fireworks.ai/pricing"
  },
  "fireworks_ai/accounts/fireworks/models/deepseek-chat-v3-0324": {
    "name": "DeepSeek Chat V3 0324",
    "mode": "chat",
    "service_provider": "fireworks_ai",
    "model_provider": "deepseek",
    "model_provider_name": "DeepSeek",
    "capabilities": {
      "supports_vision": true,
      "supports_response_schema": true,
      "supports_tools": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 163840,
      "max_output_tokens": 8192
    },
    "costs": {
      "input_cost_per_token": "0.0000009",
      "output_cost_per_token": "0.0000009"
    },
    "description": "DeepSeek Chat V3 0324 is an open-source 671B parameter Mixture-of-Experts (MoE) model that builds upon Meta's LLaMA, rivalling or exceeding similar closed source models across numerous benchmarks. This self-hosted version of V3 is deployed on US servers.",
    "source": "https://fireworks.ai/pricing#serverless-pricing"
  },
  "fireworks_ai/accounts/fireworks/models/deepseek-r1-0528": {
    "name": "DeepSeek R1 0528",
    "mode": "chat",
    "service_provider": "fireworks_ai",
    "model_provider": "deepseek",
    "model_provider_name": "DeepSeek",
    "capabilities": {
      "supports_response_schema": true,
      "supports_tools": true,
      "supports_tool_choice": true,
      "supports_reasoning": true
    },
    "context": {
      "max_input_tokens": 163840,
      "max_output_tokens": 8192
    },
    "costs": {
      "input_cost_per_token": "0.000003",
      "output_cost_per_token": "0.000008"
    },
    "description": "The updated DeepSeek-R1-0528 model delivers major improvements in reasoning, inference, and accuracy through enhanced post-training optimization and greater computational resources. This version performs at a level approaching models like o3 and Gemini 2.5 Pro, with notable gains in complex tasks such as math and programming. In the AIME 2025 benchmark, accuracy jumped from 70% to 87.5%, supported by deeper reasoning (23K vs. 12K tokens per question). The update also reduces hallucinations, improves function calling, and enhances the coding experience.",
    "source": "https://fireworks.ai/pricing#serverless-pricing"
  },
  "fireworks_ai/accounts/fireworks/models/deepseek-r1-basic": {
    "name": "DeepSeek R1 Econ",
    "mode": "chat",
    "service_provider": "fireworks_ai",
    "model_provider": "deepseek",
    "model_provider_name": "DeepSeek",
    "capabilities": {
      "supports_response_schema": true,
      "supports_tool_choice": true,
      "supports_reasoning": true
    },
    "context": {
      "max_input_tokens": 163840,
      "max_output_tokens": 8192
    },
    "costs": {
      "input_cost_per_token": "0.00000055",
      "output_cost_per_token": "0.00000219"
    },
    "description": "DeepSeek R1 Econ (Economy) is a cost-optimized deployment of DeepSeek-R1. Compared to the standard DeepSeek R1, this R1 (Economy) version provides lower per-token prices with slower speeds. Both models are identical, so there are no quality differences.",
    "source": "https://fireworks.ai/pricing#serverless-pricing"
  },
  "fireworks_ai/accounts/fireworks/models/llama-v3p3-70b-instruct": {
    "name": "Llama 3.3 70B",
    "mode": "chat",
    "service_provider": "fireworks_ai",
    "model_provider": "meta",
    "model_provider_name": "Meta",
    "capabilities": {
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 128000,
      "max_output_tokens": 2048
    },
    "costs": {
      "input_cost_per_token": "9E-7",
      "output_cost_per_token": "9E-7"
    },
    "description": "Llama 3.3 70B by Meta is an instruction tuned text only model, optimized for multilingual dialogue use cases. It improves upon Llama 3.1 70B with advances in tool calling, multilingual text support, math and coding. The model achieves stellar results in reasoning, math and instructions, providing similar performance as 3.1 405B but with significant speed and cost improvements.",
    "source": "https://fireworks.ai/pricing"
  },
  "fireworks_ai/accounts/fireworks/models/llama-v3p1-405b-instruct": {
    "name": "Llama 3.1 405B",
    "mode": "chat",
    "service_provider": "fireworks_ai",
    "model_provider": "meta",
    "model_provider_name": "Meta",
    "capabilities": {
      "supports_tools": true,
      "supports_response_schema": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 128000,
      "max_output_tokens": 128000
    },
    "costs": {
      "input_cost_per_token": "0.000003",
      "output_cost_per_token": "0.000003"
    },
    "description": "Llama is a 405 billion parameter open source model by Meta, fine-tuned for instruction following purposes. It excels at multilingual dialogue use cases. As the largest of the Llama 3.1 instruction tuned text only models, the 405B version is the most capable from the Llama 3.1 family. This model is served in FP8 closely matching reference implementation.",
    "source": "https://fireworks.ai/pricing"
  },
  "fireworks_ai/accounts/fireworks/models/llama-v3p1-8b-instruct": {
    "name": "Llama 3.1 8B",
    "mode": "chat",
    "service_provider": "fireworks_ai",
    "model_provider": "meta",
    "model_provider_name": "Meta",
    "capabilities": {
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 128000,
      "max_output_tokens": 16384
    },
    "costs": {
      "input_cost_per_token": "2E-7",
      "output_cost_per_token": "2E-7"
    },
    "description": "The Meta Llama 3.1 collection of multilingual models are pretrained and instruction tuned generative models in 8B, 70B and 405B sizes, optimized for multilingual dialogue use cases. The 8B model is the smallest of the family and most cost effective for basic instruction following tasks.",
    "source": "https://fireworks.ai/pricing"
  },
  "fireworks_ai/accounts/fireworks/models/llama4-maverick-instruct-basic": {
    "name": "Llama 4 Maverick",
    "mode": "chat",
    "service_provider": "fireworks_ai",
    "model_provider": "meta",
    "model_provider_name": "Meta",
    "capabilities": {
      "supports_vision": true,
      "supports_tools": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 1000000,
      "max_output_tokens": 16384
    },
    "costs": {
      "input_cost_per_token": "0.00000022",
      "output_cost_per_token": "0.00000088"
    },
    "description": "The Llama 4 collection of models are natively multimodal AI models. Llama 4 Maverick is a general purpose LLM containing 17 billion active parameters, 128 experts, and 400 billion total parameters, offering high quality at a lower price compared to Llama 3.3 70B.",
    "source": "https://fireworks.ai/pricing#serverless-pricing"
  },
  "fireworks_ai/accounts/fireworks/models/llama4-scout-instruct-basic": {
    "name": "Llama 4 Scout",
    "mode": "chat",
    "service_provider": "fireworks_ai",
    "model_provider": "meta",
    "model_provider_name": "Meta",
    "capabilities": {
      "supports_vision": true,
      "supports_tools": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 1048576,
      "max_output_tokens": 16384
    },
    "costs": {
      "input_cost_per_token": "0.00000015",
      "output_cost_per_token": "0.0000006"
    },
    "description": "The Llama 4 collection of models are natively multimodal AI models. Llama 4 Scout is smaller than Maverick, with 17B active parameters, 16 experts and 109B total parameters. Its low cost, large context window, and multimodal understanding make it ideal for processing large amounts of data, such as documents, code, and images.",
    "source": "https://fireworks.ai/pricing#serverless-pricing"
  },
  "fireworks_ai/accounts/fireworks/models/qwen2p5-72b-instruct": {
    "name": "Qwen 2.5 72B",
    "mode": "chat",
    "service_provider": "fireworks_ai",
    "model_provider": "alibaba",
    "model_provider_name": "Alibaba",
    "capabilities": {
      "supports_tools": true,
      "supports_response_schema": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 32768,
      "max_output_tokens": 32768
    },
    "costs": {
      "input_cost_per_token": "9E-7",
      "output_cost_per_token": "9E-7"
    },
    "description": "Qwen2.5 72B is Alibaba's largest open-source language model, trained on 18 trillion tokens. It achieves strong performance across knowledge tasks (85+ on MMLU), coding (85+ on HumanEval), and mathematics (80+ on MATH). The model supports 29+ languages, handles up to 128K context tokens, can generate 8K tokens, and shows improved capabilities in instruction following, long-text generation, and structured data handling. It competes with larger models like Llama-3.1-405B and DeepSeek-V2.5.",
    "source": "https://fireworks.ai/pricing"
  },
  "fireworks_ai/accounts/fireworks/models/qwen2p5-coder-32b-instruct": {
    "name": "Qwen 2.5 Coder 32B",
    "mode": "chat",
    "service_provider": "fireworks_ai",
    "model_provider": "alibaba",
    "model_provider_name": "Alibaba",
    "capabilities": {
      "supports_tools": false
    },
    "context": {
      "max_input_tokens": 32768,
      "max_output_tokens": 4096
    },
    "costs": {
      "input_cost_per_token": "9E-7",
      "output_cost_per_token": "9E-7"
    },
    "description": "Qwen2.5 Coder is Alibaba's latest series of models trained specifically for coding tasks. The 32B model is the largest of the series. It achieves strong performance across coding tasks, matching the coding capabilities of GPT-4o. It also possesses good general and mathematical skills.",
    "source": "https://fireworks.ai/pricing"
  },
  "fireworks_ai/accounts/fireworks/models/qwen3-235b-a22b": {
    "name": "Qwen 3 235B A22B",
    "mode": "chat",
    "service_provider": "fireworks_ai",
    "model_provider": "alibaba",
    "model_provider_name": "Alibaba",
    "capabilities": {
      "supports_tools": true,
      "supports_response_schema": true,
      "supports_tool_choice": true,
      "supports_reasoning": true
    },
    "context": {
      "max_input_tokens": 131072,
      "max_output_tokens": 8192
    },
    "costs": {
      "input_cost_per_token": "0.00000022",
      "output_cost_per_token": "0.00000088"
    },
    "description": "Qwen3 is the latest evolution in the Qwen LLM series, featuring both dense and MoE models with major advancements in reasoning, agent capabilities, multilingual support, and instruction following. It uniquely allows seamless switching between \"thinking\" (for complex logic, math, coding) and \"non-thinking\" modes (for fast, general dialogue), delivering strong performance across tasks. The flagship model, Qwen3-235B-A22B, has 235B parameters (22B active), 94 layers, and a native context length of 32K, extendable to 131K with YaRN.",
    "source": "https://fireworks.ai/pricing#serverless-pricing"
  },
  "fireworks_ai/accounts/fireworks/models/qwen3-30b-a3b": {
    "name": "Qwen 3 30B A3B",
    "mode": "chat",
    "service_provider": "fireworks_ai",
    "model_provider": "alibaba",
    "model_provider_name": "Alibaba",
    "capabilities": {
      "supports_tools": true,
      "supports_response_schema": true,
      "supports_tool_choice": true,
      "supports_reasoning": true
    },
    "context": {
      "max_input_tokens": 40000,
      "max_output_tokens": 8192
    },
    "costs": {
      "input_cost_per_token": "0.00000015",
      "output_cost_per_token": "0.0000006"
    },
    "description": "Qwen3 is the latest evolution in the Qwen LLM series, featuring both dense and MoE models with major advancements in reasoning, agent capabilities, multilingual support, and instruction following. It uniquely allows seamless switching between \"thinking\" (for complex logic, math, coding) and \"non-thinking\" modes (for fast, general dialogue), delivering strong performance across tasks.",
    "source": "https://fireworks.ai/pricing#serverless-pricing"
  },
  "gemini/gemini-2.0-flash": {
    "name": "Gemini 2.0 Flash",
    "mode": "chat",
    "service_provider": "gemini",
    "model_provider": "google",
    "model_provider_name": "Google",
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_audio_output": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 1048576,
      "max_output_tokens": 8192,
      "max_audio_length_hours": "8.4",
      "max_audio_per_prompt": 1,
      "max_images_per_prompt": 3000,
      "max_pdf_size_mb": 30,
      "max_video_length": 1,
      "max_videos_per_prompt": 10
    },
    "costs": {
      "input_cost_per_token": "1E-7",
      "output_cost_per_token": "4E-7",
      "cache_read_input_token_cost": "2.5E-8",
      "input_cost_per_audio_token": "7E-7",
      "input_cost_per_image": "0.0000258"
    },
    "description": "Gemini 2.0 Flash is a versatile multimodal model with a 1 million token context window, designed for diverse tasks.",
    "rpm": 10000,
    "tpm": 10000000,
    "openrouter_model_id": "google/gemini-2.0-flash-001",
    "_last_updated": "2025-07-01T16:47:00.880380+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "google/gemini-2.0-flash-001"
  },
  "gemini/gemini-2.0-flash-lite": {
    "name": "Gemini 2.0 Flash-Lite",
    "mode": "chat",
    "service_provider": "gemini",
    "model_provider": "google",
    "model_provider_name": "Google",
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_audio_output": false,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 1048576,
      "max_output_tokens": 8192,
      "max_audio_length_hours": "8.4",
      "max_audio_per_prompt": 1,
      "max_images_per_prompt": 3000,
      "max_pdf_size_mb": 30,
      "max_video_length": 1,
      "max_videos_per_prompt": 10
    },
    "costs": {
      "input_cost_per_token": "7.5E-8",
      "output_cost_per_token": "3E-7",
      "input_cost_per_audio_token": "7.5E-8"
    },
    "description": "Gemini 2.0 Flash-Lite is a cost-effective model optimized for low-latency and high-volume tasks. Flash-Lite's 1 million token context window makes it suitable for processing large amounts of data.",
    "rpm": 60000,
    "tpm": 10000000,
    "openrouter_model_id": "google/gemini-2.0-flash-lite-001",
    "_last_updated": "2025-07-01T16:47:00.880582+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "google/gemini-2.0-flash-lite-001"
  },
  "gemini/gemini-1.5-pro": {
    "name": "Gemini 1.5 Pro",
    "mode": "chat",
    "service_provider": "gemini",
    "model_provider": "google",
    "model_provider_name": "Google",
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 2000000,
      "max_output_tokens": 8192
    },
    "costs": {
      "input_cost_per_token": "0.00000125",
      "output_cost_per_token": "0.000005",
      "input_cost_per_token_above_128k_tokens": "0.000007",
      "output_cost_per_token_above_128k_tokens": "0.000021",
      "input_cost_per_image": "0.0006575"
    },
    "description": "Gemini 1.5 Pro is Google's flagship model with a 2 million token context window, capable of seamlessly analyzing, classifying, and summarizing large volumes of content for complex analytical tasks.",
    "rpm": 1000,
    "tpm": 4000000,
    "openrouter_model_id": "google/gemini-pro-1.5",
    "_last_updated": "2025-07-01T16:47:00.880765+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "google/gemini-pro-1.5"
  },
  "gemini/gemini-1.5-flash": {
    "name": "Gemini 1.5 Flash",
    "mode": "chat",
    "service_provider": "gemini",
    "model_provider": "google",
    "model_provider_name": "Google",
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 1000000,
      "max_output_tokens": 8192,
      "max_audio_length_hours": "8.4",
      "max_audio_per_prompt": 1,
      "max_images_per_prompt": 3000,
      "max_pdf_size_mb": 30,
      "max_video_length": 1,
      "max_videos_per_prompt": 10
    },
    "costs": {
      "input_cost_per_token": "7.5E-8",
      "output_cost_per_token": "3E-7",
      "cache_read_input_token_cost": "1.875E-8",
      "input_cost_per_token_above_128k_tokens": "1.5E-7",
      "output_cost_per_token_above_128k_tokens": "6E-7",
      "input_cost_per_image": "0.00004"
    },
    "description": "Gemini 1.5 Flash offers balanced performance for diverse tasks with a 1 million token context window.",
    "rpm": 2000,
    "tpm": 4000000,
    "openrouter_model_id": "google/gemini-flash-1.5",
    "_last_updated": "2025-07-01T16:47:00.880961+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "google/gemini-flash-1.5"
  },
  "gemini/gemini-1.5-flash-8b": {
    "name": "Gemini 1.5 Flash 8B",
    "mode": "chat",
    "service_provider": "gemini",
    "model_provider": "google",
    "model_provider_name": "Google",
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_prompt_caching": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 1000000,
      "max_output_tokens": 8192,
      "max_audio_length_hours": "8.4",
      "max_audio_per_prompt": 1,
      "max_images_per_prompt": 3000,
      "max_pdf_size_mb": 30,
      "max_video_length": 1,
      "max_videos_per_prompt": 10
    },
    "costs": {
      "input_cost_per_token": "3.75E-8",
      "output_cost_per_token": "1.5E-7",
      "cache_read_input_token_cost": "1E-8",
      "input_cost_per_token_above_128k_tokens": "0",
      "output_cost_per_token_above_128k_tokens": "0"
    },
    "description": "Gemini 1.5 Flash 8B is designed for high-volume, lower-intelligence tasks with a 1 million token context window.",
    "rpm": 4000,
    "tpm": 4000000,
    "openrouter_model_id": "google/gemini-flash-1.5-8b",
    "_last_updated": "2025-07-01T16:47:00.881165+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "google/gemini-flash-1.5-8b"
  },
  "gemini/gemini-2.5-pro": {
    "name": "Gemini 2.5 Pro",
    "mode": "chat",
    "service_provider": "gemini",
    "model_provider": "google",
    "model_provider_name": "Google",
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_reasoning": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 1048576,
      "max_output_tokens": 65536
    },
    "costs": {
      "input_cost_per_token": "0.00000125",
      "output_cost_per_token": "0.00001",
      "cache_read_input_token_cost": "3.1E-7",
      "input_cost_per_image": "0.00516"
    },
    "description": "Gemini 2.5 Pro is Google's state-of-the-art thinking model, capable of reasoning over complex problems in code, math, and STEM, as well as analyzing large datasets, codebases, and documents using long context.",
    "openrouter_model_id": "google/gemini-2.5-pro",
    "_last_updated": "2025-07-01T16:47:00.881327+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "google/gemini-2.5-pro"
  },
  "gemini/gemini-2.5-pro-preview-06-05": {
    "name": "Gemini 2.5 Pro 0605",
    "mode": "chat",
    "service_provider": "gemini",
    "model_provider": "google",
    "model_provider_name": "Google",
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_reasoning": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 1048576,
      "max_output_tokens": 65535
    },
    "costs": {
      "input_cost_per_token": "0.00000125",
      "output_cost_per_token": "0.00001",
      "cache_read_input_token_cost": "3.1E-7",
      "input_cost_per_image": "0.00516"
    },
    "description": "Gemini 2.5 Pro is Google's state-of-the-art thinking model, capable of reasoning over complex problems in code, math, and STEM, as well as analyzing large datasets, codebases, and documents using long context.",
    "openrouter_model_id": "google/gemini-2.5-pro-preview-05-06",
    "_last_updated": "2025-07-01T16:47:00.881526+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "google/gemini-2.5-pro-preview-05-06"
  },
  "gemini/gemini-2.5-flash": {
    "name": "Gemini 2.5 Flash",
    "mode": "chat",
    "service_provider": "gemini",
    "model_provider": "google",
    "model_provider_name": "Google",
    "modalities": {
      "input": [
        "text",
        "image",
        "audio",
        "video"
      ],
      "output": [
        "text"
      ]
    },
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_reasoning": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 1048576,
      "max_output_tokens": 65535
    },
    "costs": {
      "input_cost_per_token": "3E-7",
      "output_cost_per_token": "0.0000025",
      "cache_read_input_token_cost": "7.5E-8",
      "input_cost_per_image": "0.001238"
    },
    "description": "Gemini 2.5 Flash is a versatile multimodal model with a 1 million token context window, designed for diverse tasks.",
    "openrouter_model_id": "google/gemini-2.5-flash",
    "_last_updated": "2025-07-01T16:47:00.881700+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "google/gemini-2.5-flash"
  },
  "gemini/gemini-2.5-flash-lite-preview-06-17": {
    "name": "Gemini 2.5 Flash Lite",
    "mode": "chat",
    "service_provider": "gemini",
    "model_provider": "google",
    "model_provider_name": "Google",
    "modalities": {
      "input": [
        "file",
        "text",
        "image"
      ],
      "output": [
        "text"
      ]
    },
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_reasoning": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 1048576,
      "max_output_tokens": 65535
    },
    "costs": {
      "input_cost_per_token": "1E-7",
      "output_cost_per_token": "4E-7"
    },
    "description": "Gemini 2.5 Flash-Lite is a lightweight reasoning model in the Gemini 2.5 family, optimized for ultra-low latency and cost efficiency. It offers improved throughput, faster token generation, and better performance compared to earlier Flash models.",
    "openrouter_model_id": "google/gemini-2.5-flash-lite-preview-06-17",
    "_last_updated": "2025-07-01T16:47:00.881890+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "google/gemini-2.5-flash-lite-preview-06-17"
  },
  "gemini/gemma-3-27b-it": {
    "name": "Gemma 3 27B",
    "mode": "chat",
    "service_provider": "google",
    "model_provider": "google",
    "model_provider_name": "Google",
    "capabilities": {
      "supports_tools": false,
      "supports_vision": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 32768,
      "max_output_tokens": 32768
    },
    "costs": {
      "input_cost_per_token": "9E-8",
      "output_cost_per_token": "1.7E-7",
      "input_cost_per_image": "0.0000256"
    },
    "description": "Gemma 3 is an open-source model family from Google that supports multimodal inputs with a 128k token context window and comes in four sizes (1B, 4B, 12B, and 27B parameters). It excels at math, reasoning, coding, and multilingual tasks across 140+ languages.",
    "openrouter_model_id": "google/gemma-3-27b-it",
    "_last_updated": "2025-07-01T16:47:00.882080+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "google/gemma-3-27b-it"
  },
  "openrouter/google/gemma-3-27b-it:free": {
    "name": "Gemma 3 27B",
    "mode": "chat",
    "service_provider": "google",
    "model_provider": "google",
    "model_provider_name": "Google",
    "capabilities": {
      "supports_tools": false,
      "supports_vision": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 32768,
      "max_output_tokens": 32768
    },
    "costs": {
      "input_cost_per_token": "9E-8",
      "output_cost_per_token": "1.7E-7",
      "input_cost_per_image": "0.0000256"
    },
    "description": "Gemma 3 is an open-source model family from Google that supports multimodal inputs with a 128k token context window and comes in four sizes (1B, 4B, 12B, and 27B parameters). It excels at math, reasoning, coding, and multilingual tasks across 140+ languages.",
    "openrouter_model_id": "google/gemma-3-27b-it",
    "_last_updated": "2025-07-01T16:47:00.882226+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "google/gemma-3-27b-it"
  },
  "xai/grok-2-latest": {
    "name": "Grok 2",
    "mode": "chat",
    "service_provider": "xai",
    "model_provider": "xai",
    "model_provider_name": "xAI",
    "capabilities": {
      "supports_tools": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 131072,
      "max_output_tokens": 131072
    },
    "costs": {
      "input_cost_per_token": "0.000002",
      "output_cost_per_token": "0.00001"
    },
    "description": "Grok-2 is the latest API-accessible release from xAI, supporting structured outputs. Grok-2 has yet to be open sourced and the architecture remains unknown. Grok 1 was a 314 billion parameter Mixture-of-Experts model. Grok models are known to have a sense of humour and xAI CEO Elon Musk says Grok is intended to be \"a maximum truth-seeking AI that tries to understand the nature of the universe.\"",
    "openrouter_model_id": "x-ai/grok-2-1212",
    "_last_updated": "2025-07-01T16:47:00.882494+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "x-ai/grok-2-1212"
  },
  "xai/grok-2-vision-latest": {
    "name": "Grok 2 Vision",
    "mode": "chat",
    "service_provider": "xai",
    "model_provider": "xai",
    "model_provider_name": "xAI",
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 32768,
      "max_output_tokens": 32768
    },
    "costs": {
      "input_cost_per_token": "0.000002",
      "output_cost_per_token": "0.00001",
      "input_cost_per_image": "0.0036"
    },
    "description": "Grok-2 Vision is the latest API-accessible multi-modal release from xAI. Grok-2 Vision can process a wide variety of visual information, including documents, diagrams, charts, screenshots, and photographs.",
    "openrouter_model_id": "x-ai/grok-2-vision-1212",
    "_last_updated": "2025-07-01T16:47:00.882662+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "x-ai/grok-2-vision-1212"
  },
  "xai/grok-3-latest": {
    "name": "Grok 3",
    "mode": "chat",
    "service_provider": "xai",
    "model_provider": "xai",
    "model_provider_name": "xAI",
    "capabilities": {
      "supports_tools": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 131072,
      "max_output_tokens": 1000000
    },
    "costs": {
      "input_cost_per_token": "0.000003",
      "output_cost_per_token": "0.000015",
      "cache_read_input_token_cost": "7.5E-7",
      "cached_prompt_text_token_price": "0.0000075"
    },
    "description": "Grok-3 is the latest model from xAI. Grok models are known to have a sense of humour and xAI CEO Elon Musk says Grok is intended to be \"a maximum truth-seeking AI that tries to understand the nature of the universe.\"",
    "openrouter_model_id": "x-ai/grok-3-beta",
    "_last_updated": "2025-07-01T16:47:00.883191+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "x-ai/grok-3-beta"
  },
  "xai/grok-3-fast": {
    "name": "Grok 3 Fast",
    "mode": "chat",
    "service_provider": "xai",
    "model_provider": "xai",
    "model_provider_name": "xAI",
    "capabilities": {
      "supports_tools": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 1000000,
      "max_output_tokens": 1000000
    },
    "costs": {
      "input_cost_per_token": "0.00005",
      "output_cost_per_token": "0.00025",
      "cached_prompt_text_token_price": "0.0000125"
    },
    "description": "Grok 3 Fast is a faster variant of Grok-3 from xAI, optimized for speed while maintaining high performance.",
    "source": "https://docs.x.ai/docs/models"
  },
  "xai/grok-3-mini": {
    "name": "Grok 3 Mini",
    "mode": "chat",
    "service_provider": "xai",
    "model_provider": "xai",
    "model_provider_name": "xAI",
    "capabilities": {
      "supports_tools": true,
      "supports_tool_choice": true,
      "supports_reasoning": true
    },
    "context": {
      "max_input_tokens": 131072,
      "max_output_tokens": 1000000
    },
    "costs": {
      "input_cost_per_token": "3E-7",
      "output_cost_per_token": "5E-7",
      "cache_read_input_token_cost": "7.5E-8",
      "cached_prompt_text_token_price": "0.00000075"
    },
    "description": "Grok 3 Mini is a A lightweight model from xAI that thinks before responding. Fast, smart, and great for logic-based tasks that do not require deep domain knowledge.",
    "openrouter_model_id": "x-ai/grok-3-mini-beta",
    "_last_updated": "2025-07-01T16:47:00.884168+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "x-ai/grok-3-mini-beta"
  },
  "xai/grok-3-mini-fast": {
    "name": "Grok 3 Mini Fast",
    "mode": "chat",
    "service_provider": "xai",
    "model_provider": "xai",
    "model_provider_name": "xAI",
    "capabilities": {
      "supports_tools": true,
      "supports_tool_choice": true,
      "supports_reasoning": true
    },
    "context": {
      "max_input_tokens": 1000000,
      "max_output_tokens": 1000000
    },
    "costs": {
      "input_cost_per_token": "0.000006",
      "output_cost_per_token": "0.00004",
      "cached_prompt_text_token_price": "0.0000015"
    },
    "description": "Grok 3 Mini Fast is a speed-omtimized deployment of Grok 3 Mini. It responds faster in exchange for a higher per-token cost. Both models are identical, so there are no quality differences.",
    "source": "https://docs.x.ai/docs/models"
  },
  "minimax/MiniMax-Text-01": {
    "name": "MiniMax-01",
    "mode": "chat",
    "service_provider": "minimax",
    "model_provider": "minimax",
    "model_provider_name": "MiniMax",
    "capabilities": {
      "supports_vision": true,
      "supports_system_messages": true
    },
    "context": {
      "max_input_tokens": 1000192,
      "max_output_tokens": 1000192
    },
    "costs": {
      "input_cost_per_token": "2E-7",
      "output_cost_per_token": "0.0000011"
    },
    "openrouter_model_id": "minimax/minimax-01",
    "description": "MiniMax-01 combines MiniMax-Text-01 for text generation and MiniMax-VL-01 for image understanding that can handle a context of over a million tokens. It is a 456 billion parameter Mixture-of-Experts (MoE) model with 45.9 billion params active at inference, combining Lightning and Softmax Attention. The image model adopts the \u201cViT-MLP-LLM\u201d framework, trained on the text model.",
    "_last_updated": "2025-07-01T16:47:00.884550+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "minimax/minimax-01"
  },
  "openrouter/minimax/minimax-01": {
    "name": "MiniMax-01",
    "mode": "chat",
    "service_provider": "minimax",
    "model_provider": "minimax",
    "model_provider_name": "MiniMax",
    "capabilities": {
      "supports_vision": true,
      "supports_system_messages": true
    },
    "context": {
      "max_input_tokens": 1000192,
      "max_output_tokens": 1000192
    },
    "costs": {
      "input_cost_per_token": "2E-7",
      "output_cost_per_token": "0.0000011"
    },
    "openrouter_model_id": "minimax/minimax-01",
    "description": "MiniMax-01 combines MiniMax-Text-01 for text generation and MiniMax-VL-01 for image understanding that can handle a context of over a million tokens. It is a 456 billion parameter Mixture-of-Experts (MoE) model with 45.9 billion params active at inference, combining Lightning and Softmax Attention. The image model adopts the \u201cViT-MLP-LLM\u201d framework, trained on the text model.",
    "_last_updated": "2025-07-01T16:47:00.884798+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "minimax/minimax-01"
  },
  "minimax/MiniMax-M1": {
    "name": "MiniMax-M1",
    "mode": "chat",
    "service_provider": "minimax",
    "model_provider": "minimax",
    "model_provider_name": "MiniMax",
    "capabilities": {
      "supports_tools": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 1000000,
      "max_output_tokens": 40000
    },
    "costs": {
      "input_cost_per_token": "3E-7",
      "output_cost_per_token": "0.00000165"
    },
    "openrouter_model_id": "minimax/minimax-m1",
    "description": "MiniMax-M1 is an open-weight, large-scale reasoning model with extended context, designed for high-efficiency reasoning on complex, multi-step tasks. It leverages a 456 billion parameter hybrid Mixture-of-Experts (MoE) architecture with 45.9B params active per token, paired with a custom \"lightning attention\" mechanism, allowing it to process up to 1 million tokens while maintaining competitive FLOP efficiency. Trained via a custom reinforcement learning pipeline (CISPO), M1 is competitive against other open models like DeepSeek R1 and Qwen3-235B.",
    "_last_updated": "2025-07-01T16:47:00.885078+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "minimax/minimax-m1"
  },
  "openrouter/minimax/minimax-m1": {
    "name": "MiniMax-M1",
    "mode": "chat",
    "service_provider": "minimax",
    "model_provider": "minimax",
    "model_provider_name": "MiniMax",
    "capabilities": {
      "supports_response_schema": true,
      "supports_system_messages": true
    },
    "context": {
      "max_input_tokens": 1000000,
      "max_output_tokens": 40000
    },
    "costs": {
      "input_cost_per_token": "3E-7",
      "output_cost_per_token": "0.00000165"
    },
    "openrouter_model_id": "minimax/minimax-m1",
    "description": "MiniMax-M1 is an open-weight, large-scale reasoning model with extended context, designed for high-efficiency reasoning on complex, multi-step tasks. It leverages a 456 billion parameter hybrid Mixture-of-Experts (MoE) architecture with 45.9B params active per token, paired with a custom \"lightning attention\" mechanism, allowing it to process up to 1 million tokens while maintaining competitive FLOP efficiency. Trained via a custom reinforcement learning pipeline (CISPO), M1 is competitive against other open models like DeepSeek R1 and Qwen3-235B.",
    "_last_updated": "2025-07-01T16:47:00.885301+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "minimax/minimax-m1"
  },
  "mistralai/magistral-medium-latest": {
    "name": "Magistral Medium",
    "mode": "chat",
    "service_provider": "mistralai",
    "model_provider": "mistralai",
    "model_provider_name": "Mistral AI",
    "capabilities": {
      "supports_tools": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true,
      "supports_reasoning": true
    },
    "context": {
      "max_input_tokens": 40960,
      "max_output_tokens": 40000
    },
    "costs": {
      "input_cost_per_token": "0.000002",
      "output_cost_per_token": "0.000005"
    },
    "description": "Magistral is the first reasoning model series by Mistral AI, released in June 2025. Magistral is designed to think before responding, excelling in domain-specific, transparent, and multilingual reasoning. Magistral Small is a 24B parameter open-source model while Magistral Medium is a more powerful, enterprise version.",
    "_last_updated": "2025-07-01T16:47:00.885747+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "mistralai/magistral-medium-2506"
  },
  "mistralai/magistral-small-latest": {
    "name": "Magistral Small",
    "mode": "chat",
    "service_provider": "mistralai",
    "model_provider": "mistralai",
    "model_provider_name": "Mistral AI",
    "capabilities": {
      "supports_tools": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true,
      "supports_reasoning": true
    },
    "context": {
      "max_input_tokens": 40000,
      "max_output_tokens": 40000
    },
    "costs": {
      "input_cost_per_token": "5E-7",
      "output_cost_per_token": "0.0000015"
    },
    "description": "Magistral is the first reasoning model series by Mistral AI, released in June 2025. Magistral Small is a 24B parameter model based on Mistral-Small-3.1, enhanced through supervised fine-tuning on traces from Magistral Medium and reinforcement learning. It is optimized for reasoning with support for over 20 languages.",
    "_last_updated": "2025-07-01T16:47:00.886199+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "mistralai/magistral-small-2506"
  },
  "mistralai/mistral-large-latest": {
    "name": "Mistral Large Latest",
    "mode": "chat",
    "service_provider": "mistralai",
    "model_provider": "mistralai",
    "model_provider_name": "Mistral AI",
    "capabilities": {
      "supports_tools": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 131072,
      "max_output_tokens": 8192
    },
    "costs": {
      "input_cost_per_token": "0.000002",
      "output_cost_per_token": "0.000006"
    },
    "description": "Mistral Large is Mistral AI's flagship model designed for complex problem solving and advanced tasks.",
    "openrouter_model_id": "mistralai/mistral-large-latest",
    "_last_updated": "2025-07-01T16:47:00.886713+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "mistralai/mistral-large-2411"
  },
  "mistralai/mistral-medium-latest": {
    "name": "Mistral Medium Latest",
    "mode": "chat",
    "service_provider": "mistralai",
    "model_provider": "mistralai",
    "model_provider_name": "Mistral AI",
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 131072,
      "max_output_tokens": 8192
    },
    "costs": {
      "input_cost_per_token": "4E-7",
      "output_cost_per_token": "0.000002"
    },
    "description": "Mistral Medium offers balanced performance for general-purpose tasks.",
    "openrouter_model_id": "mistralai/mistral-medium-latest",
    "_last_updated": "2025-07-01T16:47:00.887139+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "mistralai/mistral-medium-3"
  },
  "mistralai/pixtral-large-latest": {
    "name": "Pixtral Large Latest",
    "mode": "chat",
    "service_provider": "mistralai",
    "model_provider": "mistralai",
    "model_provider_name": "Mistral AI",
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 131072,
      "max_output_tokens": 8192
    },
    "costs": {
      "input_cost_per_token": "0.000002",
      "output_cost_per_token": "0.000006",
      "input_cost_per_image": "0.002888"
    },
    "description": "Pixtral Large is Mistral AI's multimodal model with vision capabilities for processing images and text.",
    "openrouter_model_id": "mistralai/pixtral-large-latest",
    "_last_updated": "2025-07-01T16:47:00.887511+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "mistralai/pixtral-large-2411"
  },
  "mistralai/mistral-small-latest": {
    "name": "Mistral Small Latest",
    "mode": "chat",
    "service_provider": "mistralai",
    "model_provider": "mistralai",
    "model_provider_name": "Mistral AI",
    "capabilities": {
      "supports_tools": true,
      "supports_vision": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 96000,
      "max_output_tokens": 96000
    },
    "costs": {
      "input_cost_per_token": "0.0000002",
      "output_cost_per_token": "0.0000006"
    },
    "description": "Mistral Small is a cost-effective model optimized for simple tasks and high throughput.",
    "openrouter_model_id": "mistralai/mistral-small-latest",
    "_last_updated": "2025-07-01T16:47:00.888055+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "mistralai/mistral-small-3.2-24b-instruct:free"
  },
  "mistralai/codestral-latest": {
    "name": "Codestral Latest",
    "mode": "chat",
    "service_provider": "mistralai",
    "model_provider": "mistralai",
    "model_provider_name": "Mistral AI",
    "capabilities": {
      "supports_tools": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 262144,
      "max_output_tokens": 8192
    },
    "costs": {
      "input_cost_per_token": "3E-7",
      "output_cost_per_token": "9E-7"
    },
    "description": "Codestral is Mistral AI's specialized model for code generation and programming tasks.",
    "openrouter_model_id": "mistralai/codestral-latest",
    "_last_updated": "2025-07-01T16:47:00.888474+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "mistralai/codestral-2501"
  },
  "mistralai/open-mixtral-8x22b": {
    "name": "Mixtral 8x22B",
    "mode": "chat",
    "service_provider": "mistralai",
    "model_provider": "mistralai",
    "model_provider_name": "Mistral AI",
    "capabilities": {
      "supports_tools": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 65536,
      "max_output_tokens": 8192
    },
    "costs": {
      "input_cost_per_token": "0.0000002",
      "output_cost_per_token": "0.0000006"
    },
    "description": "Open Mixtral 8x22B is a mixture-of-experts model with 22B active parameters out of 141B total.",
    "source": "https://mistral.ai/pricing#api-pricing"
  },
  "mistralai/open-mistral-nemo": {
    "name": "Mistral Nemo",
    "mode": "chat",
    "service_provider": "mistralai",
    "model_provider": "mistralai",
    "model_provider_name": "Mistral AI",
    "capabilities": {
      "supports_tools": true,
      "supports_response_schema": true,
      "supports_system_messages": true,
      "supports_tool_choice": true
    },
    "context": {
      "max_input_tokens": 131072,
      "max_output_tokens": 131072
    },
    "costs": {
      "input_cost_per_token": "1E-8",
      "output_cost_per_token": "1.1E-8"
    },
    "description": "Mistral Nemo is a 12B model with state-of-the-art reasoning, world knowledge, and coding performance.",
    "openrouter_model_id": "mistralai/mistral-nemo",
    "_last_updated": "2025-07-01T16:47:00.889029+00:00",
    "_data_source": "openrouter",
    "_last_updated_from_id": "mistralai/mistral-nemo"
  }
}