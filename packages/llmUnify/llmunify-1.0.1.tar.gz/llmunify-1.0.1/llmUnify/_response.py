from typing import Optional

from pydantic import BaseModel


class LlmResponse(BaseModel):
    """
    Represents the standardized response from an LLM provider.

    This model unifies the structure of responses from multiple LLM providers,
    enabling a consistent interface for accessing generated content and metadata
    regardless of the provider used.

    Attributes:
        generated_text (Optional[str]): The text generated by the model.
        model (Optional[str]): The name of the model used to generate the text.
        generated_token_count (Optional[int]): The number of tokens generated in the response.
        input_token_count (Optional[int]): The number of tokens in the input prompt.

        aws (Optional[AwsResponse]): Provider-specific response for AWS Bedrock.
        azure (Optional[AzureResponse]): Provider-specific response for Azure Foundry.
        google (Optional[GoogleResponse]): Provider-specific response for Google Vertex AI
        ollama (Optional[OllamaResponse]): Provider-specific response for Ollama.
        watsonx (Optional[WatsonxResponse]): Provider-specific response for IBM WatsonX.
    """

    generated_text: Optional[str] = None
    model: Optional[str] = None
    generated_token_count: Optional[int] = None
    input_token_count: Optional[int] = None

    aws: Optional["AwsResponse"] = None
    azure: Optional["AzureResponse"] = None
    google: Optional["GoogleResponse"] = None
    ollama: Optional["OllamaResponse"] = None
    watsonx: Optional["WatsonxResponse"] = None


from .providers.aws._response import AwsResponse  # noqa: E402
from .providers.azure._response import AzureResponse  # noqa: E402
from .providers.google._response import GoogleResponse  # noqa: E402
from .providers.ollama._response import OllamaResponse  # noqa: E402
from .providers.watsonx._response import WatsonxResponse  # noqa: E402

LlmResponse.model_rebuild()
