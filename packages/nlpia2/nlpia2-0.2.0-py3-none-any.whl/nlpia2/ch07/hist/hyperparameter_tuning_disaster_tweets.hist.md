>>> %run main_ch07.py
>>> ls -hal
>>> pwd
'/home/hobs/code/tangibleai/nlpia2/src/nlpia2/ch07/cnn'
>>> %run model_ch07.py
>>> who
>>> %run train_ch07.py
>>> who
>>> learning_df[['train_accuracies', 'validation_accuracies']].plot(linewidth=2)
>>> hyperparams
>>> hyperparms
{'use_glove': True,
 'expand_glove_vocab': True,
 'seq_len': 40,
 'vocab_size': 2000,
 'embedding_size': 50,
 'out_channels': 50,
 'num_stopwords': 0,
 'kernel_lengths': [1, 2, 3, 4, 5, 6],
 'strides': [1, 1, 1, 1, 1, 1],
 'batch_size': 24,
 'learning_rate': 0.002,
 'dropout': 0,
 'num_epochs': 400,
 'y_train': [0,
  0,
  1,
  1,
  0,
  1,
  0,
  1,
  1,
  1,
  0,
  0,
  1,
  0,
  1,
  0,
  1,
  0,
  1,
  0,
  0,
  0,
  1,
  1,
  0,
  1,
  0,
  1,
  0,
  0,
  1,
  0,
  0,
  0,
  0,
  0,
  0,
  0,
  0,
  1,
  1,
  0,
  1,
  0,
  0,
  1,
  1,
  0,
  1,
  0,
  1,
  0,
  1,
  1,
  1,
  0,
  1,
  0,
  1,
  1,
  0,
  1,
  0,
  0,
  1,
  1,
  0,
  0,
  0,
  0,
  1,
  1,
  0,
  0,
  1,
  0,
  0,
  1,
  1,
  0,
  0,
  0,
  0,
  0,
  0,
  1,
  1,
  1,
  0,
  0,
  0,
  1,
  1,
  1,
  0,
  0,
  0,
  1,
  1,
  0,
  1,
  1,
  1,
  1,
  1,
  1,
  0,
  0,
  0,
  1,
  0,
  0,
  0,
  0,
  0,
  0,
  1,
  0,
  0,
  0,
  0,
  0,
  1,
  0,
  1,
  1,
  0,
  0,
  1,
  1,
  0,
  0,
  1,
  0,
  0,
  1,
  0,
  1,
  0,
  0,
  1,
  1,
  0,
  0,
  0,
  1,
  0,
  1,
  1,
  0,
  1,
  0,
  0,
  1,
  0,
  0,
  1,
  1,
  1,
  0,
  0,
  0,
  1,
  0,
  0,
  1,
  0,
  1,
  0,
  0,
  1,
  1,
  1,
  1,
  0,
  1,
  1,
  0,
  1,
  1,
  1,
  1,
  0,
  1,
  0,
  0,
  0,
  0,
  0,
  0,
  0,
  0,
  0,
  0,
  0,
  1,
  1,
  1,
  0,
  0,
  1,
  0,
  0,
  0,
  0,
  0,
  0,
  0,
  1,
  1,
  0,
  0,
  0,
  1,
  1,
  1,
  1,
  1,
  0,
  0,
  0,
  1,
  0,
  0,
  0,
  0,
  0,
  0,
  1,
  1,
  0,
  1,
  0,
  1,
  0,
  0,
  0,
  1,
  0,
  1,
  0,
  0,
  0,
  1,
  0,
  0,
  0,
  0,
  0,
  1,
  0,
  0,
  1,
  1,
  1,
  0,
  0,
  0,
  0,
  1,
  1,
  1,
  1,
  1,
  1,
  0,
  1,
  1,
  1,
  0,
  1,
  1,
  1,
  1,
  1,
  0,
  0,
  0,
  0,
  0,
  0,
  0,
  1,
  0,
  0,
  0,
  1,
  1,
  1,
  0,
  0,
  0,
  0,
  0,
  1,
  1,
  0,
  0,
  0,
  0,
  0,
  0,
  1,
  1,
  1,
  0,
  0,
  0,
  0,
  1,
  1,
  0,
  0,
  1,
  0,
  0,
  1,
  1,
  0,
  0,
  0,
  1,
  0,
  1,
  0,
  0,
  1,
  0,
  0,
  1,
  1,
  0,
  0,
  0,
  0,
  1,
  1,
  0,
  0,
  0,
  0,
  0,
  0,
  0,
  0,
  1,
  0,
  1,
  0,
  0,
  0,
  0,
  1,
  0,
  1,
  1,
  1,
  0,
  0,
  0,
  1,
  0,
  0,
  0,
  1,
  1,
  1,
  0,
  0,
  1,
  1,
  0,
  1,
  0,
  1,
  1,
  1,
  0,
  0,
  0,
  1,
  0,
  0,
  0,
  1,
  0,
  1,
  0,
  1,
  1,
  1,
  0,
  1,
  1,
  0,
  0,
  1,
  1,
  1,
  0,
  0,
  0,
  1,
  1,
  0,
  0,
  0,
  0,
  0,
  0,
  0,
  0,
  0,
  1,
  1,
  0,
  0,
  1,
  1,
  1,
  1,
  0,
  0,
  0,
  0,
  1,
  1,
  0,
  0,
  0,
  0,
  0,
  1,
  0,
  0,
  0,
  0,
  1,
  1,
  0,
  1,
  0,
  0,
  1,
  0,
  0,
  1,
  1,
  0,
  0,
  1,
  1,
  0,
  1,
  0,
  0,
  1,
  1,
  0,
  0,
  0,
  0,
  0,
  1,
  0,
  1,
  1,
  0,
  1,
  0,
  1,
  0,
  1,
  1,
  1,
  0,
  1,
  0,
  0,
  0,
  0,
  0,
  0,
  0,
  1,
  0,
  0,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  0,
  1,
  1,
  1,
  1,
  0,
  1,
  0,
  0,
  0,
  1,
  0,
  0,
  0,
  1,
  1,
  1,
  0,
  1,
  1,
  1,
  1,
  0,
  1,
  0,
  0,
  1,
  0,
  0,
  1,
  1,
  1,
  1,
  0,
  0,
  1,
  1,
  1,
  0,
  0,
  1,
  0,
  0,
  0,
  0,
  0,
  1,
  0,
  0,
  0,
  1,
  1,
  0,
  0,
  0,
  0,
  0,
  0,
  0,
  1,
  1,
  0,
  0,
  1,
  1,
  0,
  0,
  0,
  0,
  1,
  0,
  0,
  0,
  1,
  0,
  1,
  0,
  0,
  0,
  1,
  0,
  0,
  1,
  0,
  0,
  0,
  1,
  0,
  1,
  1,
  0,
  0,
  0,
  0,
  1,
  1,
  1,
  1,
  1,
  0,
  1,
  0,
  0,
  1,
  0,
  0,
  1,
  0,
  1,
  0,
  1,
  1,
  1,
  1,
  0,
  1,
  0,
  0,
  0,
  0,
  1,
  1,
  0,
  0,
  1,
  0,
  0,
  0,
  0,
  0,
  1,
  0,
  0,
  0,
  0,
  0,
  0,
  1,
  0,
  0,
  1,
  1,
  1,
  1,
  0,
  1,
  0,
  1,
  1,
  1,
  1,
  0,
  0,
  1,
  0,
  0,
  1,
  0,
  1,
  1,
  0,
  1,
  0,
  0,
  1,
  1,
  1,
  1,
  0,
  1,
  0,
  0,
  0,
  0,
  0,
  0,
  0,
  1,
  0,
  1,
  0,
  0,
  1,
  0,
  0,
  0,
  0,
  0,
  1,
  1,
  1,
  1,
  0,
  0,
  0,
  1,
  1,
  0,
  0,
  1,
  0,
  1,
  1,
  0,
  0,
  1,
  0,
  0,
  0,
  1,
  1,
  0,
  0,
  0,
  0,
  0,
  1,
  1,
  0,
  0,
  1,
  0,
  0,
  0,
  1,
  1,
  0,
  0,
  0,
  1,
  0,
  1,
  1,
  1,
  1,
  0,
  1,
  0,
  1,
  1,
  0,
  0,
  0,
  1,
  0,
  1,
  0,
  1,
  0,
  1,
  0,
  0,
  1,
  1,
  0,
  0,
  1,
  0,
  1,
  0,
  0,
  1,
  0,
  0,
  1,
  0,
  0,
  1,
  1,
  1,
  1,
  1,
  1,
  0,
  0,
  0,
  0,
  0,
  1,
  1,
  1,
  1,
  1,
  0,
  1,
  1,
  0,
  0,
  0,
  1,
  1,
  1,
  0,
  1,
  1,
  1,
  0,
  1,
  0,
  1,
  1,
  1,
  1,
  0,
  1,
  0,
  1,
  0,
  0,
  0,
  0,
  0,
  1,
  0,
  1,
  0,
  0,
  1,
  1,
  1,
  0,
  1,
  0,
  0,
  0,
  0,
  1,
  0,
  1,
  1,
  0,
  0,
  0,
  0,
  1,
  0,
  0,
  1,
  0,
  1,
  1,
  0,
  0,
  0,
  0,
  1,
  1,
  1,
  1,
  0,
  1,
  0,
  1,
  0,
  0,
  0,
  1,
  0,
  0,
  1,
  1,
  0,
  0,
  1,
  1,
  0,
  0,
  0,
  0,
  1,
  1,
  1,
  0,
  0,
  1,
  0,
  0,
  0,
  0,
  1,
  0,
  0,
  1,
  0,
  0,
  0,
  1,
  0,
  1,
  1,
  1,
  0,
  1,
  1,
  1,
  1,
  0,
  1,
  0,
  0,
  0,
  0,
  0,
  1,
  0,
  0,
  0,
  0,
  0,
  0,
  0,
  0,
  0,
  1,
  0,
  0,
  0,
  0,
  1,
  0,
  0,
  0,
  1,
  1,
  1,
  0,
  1,
  0,
  1,
  1,
  1,
  0,
  1,
  1,
  1,
  1,
  0,
  0,
  1,
  1,
  1,
  0,
  1,
  0,
  1,
  1,
  1,
  0,
  1,
  1,
  0,
  0,
  1,
  0,
  1,
  1,
  0,
  1,
  1,
  1,
  0,
  0,
  1,
  1,
  0,
  0,
  0,
  0,
  1,
  0,
  0,
  1,
  0,
  0,
  1,
  0,
  0,
  1,
  0,
  1,
  0,
  1,
  1,
  1,
  1,
  0,
  0,
  1,
  0,
  0,
  1,
  0,
  0,
  0,
  0,
  0,
  0,
  0,
  0,
  0,
  1,
  0,
  0,
  0,
  0,
  1,
  ...],
 'y_test': [1,
  0,
  1,
  0,
  0,
  0,
  1,
  0,
  0,
  1,
  1,
  0,
  1,
  1,
  1,
  0,
  0,
  0,
  1,
  0,
  0,
  0,
  0,
  0,
  1,
  0,
  0,
  1,
  1,
  1,
  0,
  0,
  1,
  1,
  0,
  1,
  0,
  0,
  1,
  0,
  1,
  1,
  0,
  1,
  0,
  0,
  1,
  1,
  0,
  0,
  1,
  0,
  0,
  1,
  0,
  1,
  0,
  1,
  0,
  0,
  0,
  0,
  0,
  1,
  1,
  0,
  0,
  0,
  0,
  1,
  0,
  0,
  0,
  1,
  0,
  1,
  0,
  0,
  1,
  0,
  1,
  0,
  1,
  0,
  0,
  0,
  0,
  0,
  0,
  0,
  0,
  1,
  1,
  0,
  0,
  0,
  1,
  1,
  0,
  1,
  0,
  1,
  0,
  1,
  1,
  0,
  1,
  0,
  0,
  1,
  1,
  1,
  0,
  0,
  0,
  1,
  0,
  1,
  0,
  0,
  1,
  1,
  0,
  0,
  0,
  0,
  1,
  1,
  1,
  0,
  1,
  0,
  0,
  0,
  0,
  0,
  1,
  0,
  0,
  0,
  0,
  1,
  0,
  0,
  1,
  1,
  1,
  0,
  0,
  0,
  1,
  1,
  1,
  1,
  0,
  0,
  0,
  0,
  1,
  0,
  0,
  1,
  0,
  0,
  1,
  0,
  1,
  1,
  0,
  0,
  0,
  0,
  1,
  0,
  0,
  0,
  0,
  0,
  1,
  0,
  1,
  1,
  0,
  1,
  0,
  0,
  1,
  1,
  0,
  1,
  0,
  0,
  1,
  1,
  1,
  0,
  0,
  1,
  1,
  0,
  0,
  1,
  0,
  1,
  1,
  0,
  1,
  0,
  1,
  1,
  0,
  0,
  1,
  0,
  1,
  0,
  0,
  0,
  0,
  0,
  0,
  0,
  1,
  0,
  1,
  0,
  1,
  1,
  1,
  1,
  1,
  0,
  0,
  1,
  1,
  0,
  0,
  1,
  0,
  0,
  0,
  1,
  1,
  0,
  1,
  1,
  0,
  1,
  1,
  1,
  1,
  0,
  1,
  1,
  1,
  1,
  0,
  0,
  1,
  0,
  0,
  1,
  0,
  1,
  1,
  1,
  1,
  1,
  0,
  1,
  1,
  1,
  0,
  1,
  0,
  0,
  1,
  1,
  1,
  0,
  1,
  0,
  1,
  0,
  1,
  0,
  1,
  1,
  0,
  0,
  0,
  0,
  1,
  0,
  0,
  1,
  1,
  0,
  0,
  0,
  0,
  0,
  0,
  0,
  0,
  0,
  1,
  0,
  0,
  1,
  1,
  1,
  0,
  1,
  1,
  0,
  0,
  1,
  1,
  0,
  0,
  0,
  0,
  1,
  0,
  1,
  0,
  1,
  1,
  1,
  1,
  1,
  1,
  0,
  0,
  1,
  0,
  0,
  0,
  1,
  0,
  0,
  0,
  1,
  0,
  1,
  0,
  0,
  0,
  1,
  1,
  0,
  0,
  0,
  0,
  0,
  0,
  0,
  0,
  0,
  0,
  1,
  0,
  0,
  1,
  1,
  0,
  0,
  0,
  0,
  1,
  1,
  0,
  0,
  0,
  0,
  1,
  0,
  0,
  0,
  1,
  1,
  1,
  0,
  0,
  0,
  1,
  1,
  0,
  0,
  0,
  0,
  1,
  1,
  1,
  0,
  0,
  0,
  0,
  0,
  0,
  0,
  1,
  0,
  0,
  1,
  0,
  0,
  0,
  1,
  0,
  0,
  1,
  0,
  0,
  0,
  0,
  1,
  0,
  0,
  1,
  1,
  0,
  0,
  0,
  0,
  1,
  0,
  1,
  0,
  1,
  1,
  1,
  0,
  0,
  1,
  1,
  1,
  0,
  1,
  1,
  0,
  0,
  1,
  1,
  0,
  0,
  1,
  0,
  0,
  1,
  0,
  0,
  1,
  1,
  0,
  0,
  1,
  0,
  0,
  0,
  0,
  0,
  1,
  0,
  0,
  1,
  0,
  0,
  1,
  0,
  0,
  0,
  0,
  1,
  1,
  1,
  0,
  0,
  1,
  0,
  0,
  0,
  0,
  0,
  0,
  1,
  1,
  0,
  0,
  1,
  0,
  1,
  0,
  0,
  0,
  1,
  0,
  1,
  0,
  0,
  1,
  0,
  1,
  1,
  0,
  0,
  1,
  0,
  1,
  0,
  0,
  1,
  1,
  0,
  1,
  1,
  1,
  0,
  0,
  0,
  1,
  0,
  0,
  1,
  0,
  1,
  0,
  0,
  0,
  0,
  0,
  0,
  0,
  1,
  1,
  0,
  1,
  0,
  0,
  0,
  0,
  0,
  1,
  0,
  0,
  1,
  0,
  1,
  1,
  0,
  0,
  0,
  0,
  0,
  0,
  1,
  1,
  1,
  1,
  0,
  0,
  1,
  0,
  1,
  0,
  1,
  1,
  1,
  1,
  1,
  0,
  1,
  0,
  0,
  0,
  0,
  0,
  0,
  1,
  0,
  1,
  1,
  0,
  0,
  1,
  0,
  0,
  1,
  0,
  0,
  0,
  0,
  0,
  0,
  0,
  0,
  1,
  0,
  1,
  0,
  0,
  1,
  1,
  0,
  0,
  1,
  1,
  0,
  0,
  1,
  0,
  1,
  0,
  0,
  0,
  1,
  0,
  0,
  0,
  1,
  0,
  1,
  1,
  0,
  0,
  1,
  1,
  0,
  1,
  1,
  0,
  1,
  0,
  0,
  1,
  0,
  1,
  0,
  0,
  1,
  1,
  0,
  0,
  0,
  1,
  1,
  1,
  0,
  1,
  1,
  0,
  0,
  1,
  0,
  1,
  0,
  0,
  1,
  1,
  1,
  0,
  0,
  1,
  1,
  0,
  1,
  1,
  1,
  0,
  0,
  0,
  1,
  1,
  0,
  0,
  1,
  0,
  0,
  0,
  0,
  1,
  1,
  0,
  1,
  1,
  0,
  1,
  0,
  1,
  0,
  1,
  1,
  0,
  0,
  0,
  0,
  0,
  1,
  0,
  0,
  1,
  0,
  1,
  0,
  0,
  0,
  1,
  0,
  1,
  0,
  0,
  1,
  1,
  0,
  0,
  0,
  0,
  0,
  1,
  0,
  1,
  1,
  0,
  0,
  0,
  0,
  1,
  1,
  0,
  0,
  1,
  1,
  0,
  0,
  0,
  1,
  0,
  1,
  0,
  1,
  0,
  0,
  1,
  0,
  0,
  0,
  0,
  0,
  1,
  1,
  0,
  1,
  1,
  1,
  0,
  0,
  1,
  0,
  0,
  1,
  1],
 'learning_curve': [[0.6259374022483826,
   0.5891110786746461,
   0.6194225721784777],
  [0.572498619556427, 0.6585899868632317, 0.6286089238845144],
  [0.53183913230896, 0.6793168880455408, 0.6509186351706037],
  [0.49689456820487976, 0.6898263027295285, 0.6640419947506562],
  [0.4692693054676056, 0.7023792147131805, 0.6640419947506562],
  [0.44030293822288513, 0.7102612757261714, 0.6666666666666666],
  [0.41839009523391724, 0.7168296599036637, 0.6627296587926509],
  [0.3946485221385956, 0.7254415413808203, 0.6653543307086615],
  [0.36861589550971985, 0.7327397460224785, 0.6627296587926509],
  [0.34860774874687195, 0.7381404174573055, 0.6692913385826772],
  [0.3334750831127167, 0.7422274120566341, 0.6797900262467191],
  [0.31342318654060364, 0.7471901912129616, 0.6758530183727034],
  [0.3014084994792938, 0.7486498321412932, 0.6824146981627297],
  [0.28968751430511475, 0.7524448985549554, 0.6850393700787402],
  [0.2789236307144165, 0.7553641804116188, 0.6863517060367454],
  [0.26269322633743286, 0.7587213545467815, 0.6902887139107612],
  [0.2505471110343933, 0.7616406364034447, 0.6902887139107612],
  [0.23823508620262146, 0.7638300977959422, 0.6863517060367454],
  [0.22794361412525177, 0.7679170923952707, 0.6837270341207349],
  [0.22079095244407654, 0.7709823383447671, 0.6876640419947506],
  [0.21458177268505096, 0.7728798715515983, 0.6863517060367454],
  [0.20860357582569122, 0.7763830097795942, 0.6850393700787402],
  [0.20090197026729584, 0.777988614800759, 0.6889763779527559],
  [0.19494499266147614, 0.7807619325645891, 0.6876640419947506],
  [0.1914031058549881, 0.7844110348854182, 0.6850393700787402],
  [0.18804816901683807, 0.7863085680922494, 0.6889763779527559],
  [0.18425463140010834, 0.787768209020581, 0.6916010498687664],
  [0.17979001998901367, 0.7893738140417458, 0.6902887139107612],
  [0.17644211649894714, 0.7920011677127426, 0.6889763779527559],
  [0.1716540902853012, 0.7947744854765728, 0.6850393700787402],
  [0.16693919897079468, 0.7957962341264049, 0.6929133858267716],
  [0.16285091638565063, 0.7981316596117355, 0.6889763779527559],
  [0.15844137966632843, 0.7995913005400671, 0.6916010498687664],
  [0.15444864332675934, 0.8013428696540651, 0.6942257217847769],
  [0.15088999271392822, 0.8029484746752299, 0.6916010498687664],
  [0.14783570170402527, 0.8047000437892279, 0.6902887139107612],
  [0.14531686902046204, 0.8080572179243906, 0.6863517060367454],
  [0.14415951073169708, 0.8070354692745585, 0.6902887139107612],
  [0.14097829163074493, 0.8082031820172237, 0.6902887139107612],
  [0.14019550383090973, 0.8086410742957233, 0.6902887139107612],
  [0.13907593488693237, 0.8090789665742227, 0.6889763779527559],
  [0.13634692132472992, 0.8084951102028901, 0.6889763779527559],
  [0.13360236585140228, 0.811998248430886, 0.6863517060367454],
  [0.13133986294269562, 0.8127280688950518, 0.6889763779527559],
  [0.12807287275791168, 0.8140417457305503, 0.6889763779527559],
  [0.1264640837907791, 0.813749817544884, 0.6889763779527559],
  [0.1227492019534111, 0.8141877098233834, 0.6863517060367454],
  [0.11908814311027527, 0.8141877098233834, 0.6850393700787402],
  [0.11597103625535965, 0.8143336739162166, 0.6876640419947506],
  [0.1140332892537117, 0.816523135308714, 0.6876640419947506],
  [0.11218272894620895, 0.8178368121442126, 0.6863517060367454],
  [0.111467145383358, 0.8178368121442126, 0.6850393700787402],
  [0.1112506091594696, 0.8184206685155452, 0.6850393700787402],
  [0.11217039078474045, 0.8185666326083784, 0.6811023622047244],
  [0.11314628273248672, 0.8207560940008758, 0.6837270341207349],
  [0.11165676265954971, 0.8210480221865422, 0.6889763779527559],
  [0.1117490604519844, 0.8225076631148738, 0.6863517060367454],
  [0.11151406913995743, 0.8230915194862064, 0.6850393700787402],
  [0.11094939708709717, 0.8244051963217048, 0.6850393700787402],
  [0.11058790236711502, 0.8239673040432054, 0.6824146981627297],
  [0.10955417901277542, 0.8242592322288717, 0.6824146981627297],
  [0.11022614687681198, 0.8260108013428696, 0.6824146981627297],
  [0.11118431389331818, 0.8287841191066998, 0.6837270341207349],
  [0.11229486763477325, 0.8306816523135309, 0.6837270341207349],
  [0.11335647851228714, 0.8322872573346957, 0.6771653543307087],
  [0.11361518502235413, 0.8338928623558605, 0.6797900262467191],
  [0.11557462811470032, 0.8341847905415268, 0.6797900262467191],
  [0.11500710994005203, 0.8343307546343599, 0.6824146981627297],
  [0.11584677547216415, 0.8346226828200263, 0.6811023622047244],
  [0.11663386225700378, 0.8347686469128595, 0.6824146981627297],
  [0.11613094806671143, 0.8362282878411911, 0.6850393700787402],
  [0.11703653633594513, 0.8376879287695227, 0.6850393700787402],
  [0.11800866574048996, 0.838855641512188, 0.6863517060367454],
  [0.11805939674377441, 0.8394394978835207, 0.6889763779527559],
  [0.11940591037273407, 0.8401693183476865, 0.6902887139107612],
  [0.11960262805223465, 0.8404612465333527, 0.6916010498687664],
  [0.11915998160839081, 0.8406072106261859, 0.6902887139107612],
  [0.11931496113538742, 0.8411910669975186, 0.6929133858267716],
  [0.12117541581392288, 0.8416289592760181, 0.6916010498687664],
  [0.12069189548492432, 0.8430886002043497, 0.6902887139107612],
  [0.12134433537721634, 0.8427966720186834, 0.6863517060367454],
  [0.12273436039686203, 0.8444022770398482, 0.6889763779527559],
  [0.12293139100074768, 0.8446942052255145, 0.6889763779527559],
  [0.1248202845454216, 0.845132097504014, 0.6902887139107612],
  [0.1258859783411026, 0.8458619179681798, 0.6916010498687664],
  [0.12555809319019318, 0.847759451175011, 0.6902887139107612],
  [0.12636514008045197, 0.8480513793606773, 0.6889763779527559],
  [0.12449076771736145, 0.8483433075463436, 0.6902887139107612],
  [0.12507480382919312, 0.8492190921033426, 0.6955380577427821],
  [0.12564979493618011, 0.8502408407531747, 0.6968503937007874],
  [0.12622347474098206, 0.8503868048460079, 0.6955380577427821],
  [0.12684820592403412, 0.8500948766603416, 0.6981627296587927],
  [0.12647314369678497, 0.8500948766603416, 0.6981627296587927],
  [0.12695150077342987, 0.8521383739600058, 0.6968503937007874],
  [0.1268935352563858, 0.8521383739600058, 0.6968503937007874],
  [0.1258523315191269, 0.852284338052839, 0.6968503937007874],
  [0.1248580738902092, 0.852284338052839, 0.6981627296587927],
  [0.12372999638319016, 0.8528681944241716, 0.6968503937007874],
  [0.12391958385705948, 0.8534520507955043, 0.6955380577427821],
  [0.12302741408348083, 0.8541818712596702, 0.6968503937007874],
  [0.12275329232215881, 0.8550576558166691, 0.6955380577427821],
  [0.12391042709350586, 0.855933440373668, 0.6955380577427821],
  [0.12350936233997345, 0.8560794044665012, 0.6902887139107612],
  [0.1244114562869072, 0.8562253685593344, 0.6902887139107612],
  [0.12433343380689621, 0.8553495840023354, 0.6929133858267716],
  [0.12551288306713104, 0.856809224930667, 0.6916010498687664],
  [0.1276174932718277, 0.8569551890235002, 0.6929133858267716],
  [0.128916934132576, 0.8563713326521676, 0.6929133858267716],
  [0.12893995642662048, 0.856809224930667, 0.6955380577427821],
  [0.1295662522315979, 0.856809224930667, 0.6968503937007874],
  [0.13293081521987915, 0.8563713326521676, 0.6955380577427821],
  [0.134719118475914, 0.8571011531163334, 0.6942257217847769],
  [0.13639169931411743, 0.8579769376733324, 0.6955380577427821],
  [0.1399882286787033, 0.858560794044665, 0.6929133858267716],
  [0.14085753262043, 0.8591446504159976, 0.6902887139107612],
  [0.14182049036026, 0.8604583272514962, 0.6863517060367454],
  [0.14303551614284515, 0.8598744708801634, 0.6863517060367454],
  [0.14421547949314117, 0.860312363158663, 0.6824146981627297],
  [0.14475460350513458, 0.860312363158663, 0.6824146981627297],
  [0.14548085629940033, 0.8608962195299956, 0.6850393700787402],
  [0.14546428620815277, 0.8614800759013282, 0.6850393700787402],
  [0.1452685296535492, 0.8613341118084951, 0.6837270341207349],
  [0.14852215349674225, 0.8616260399941614, 0.6837270341207349],
  [0.1491166204214096, 0.862209896365494, 0.6837270341207349],
  [0.14965397119522095, 0.8623558604583272, 0.6824146981627297],
  [0.1503164917230606, 0.862209896365494, 0.6837270341207349],
  [0.15142513811588287, 0.8623558604583272, 0.6837270341207349],
  [0.15288692712783813, 0.862209896365494, 0.6850393700787402],
  [0.15314558148384094, 0.8619179681798278, 0.6837270341207349],
  [0.15331223607063293, 0.8620639322726609, 0.6863517060367454],
  [0.15308968722820282, 0.8627937527368268, 0.6837270341207349],
  [0.1521802842617035, 0.8641074295723252, 0.6850393700787402],
  [0.1516047865152359, 0.8643993577579915, 0.6876640419947506],
  [0.15273408591747284, 0.8645453218508247, 0.6902887139107612],
  [0.15106187760829926, 0.86571303459349, 0.6902887139107612],
  [0.1505802720785141, 0.8658589986863232, 0.6902887139107612],
  [0.15085400640964508, 0.8668807473361553, 0.6876640419947506],
  [0.15060485899448395, 0.8673186396146548, 0.6916010498687664],
  [0.14833666384220123, 0.8679024959859875, 0.6929133858267716],
  [0.14715121686458588, 0.8684863523573201, 0.6916010498687664],
  [0.14609958231449127, 0.8689242446358196, 0.6942257217847769],
  [0.14491525292396545, 0.8690702087286527, 0.6902887139107612],
  [0.14532387256622314, 0.8696540650999854, 0.6889763779527559],
  [0.144317165017128, 0.8693621369143191, 0.6876640419947506],
  [0.1436614990234375, 0.8695081010071523, 0.6863517060367454],
  [0.1414957344532013, 0.8695081010071523, 0.6876640419947506],
  [0.1396719515323639, 0.8700919573784849, 0.6863517060367454],
  [0.138599693775177, 0.8703838855641512, 0.6863517060367454],
  [0.1388465315103531, 0.8711137060283171, 0.6850393700787402],
  [0.13711129128932953, 0.8714056342139833, 0.6850393700787402],
  [0.13788489997386932, 0.8711137060283171, 0.6850393700787402],
  [0.13621674478054047, 0.871989490585316, 0.6824146981627297],
  [0.13497048616409302, 0.8727193110494819, 0.6837270341207349],
  [0.13484768569469452, 0.8724273828638155, 0.6850393700787402],
  [0.13548220694065094, 0.8718435264924829, 0.6837270341207349],
  [0.1327059268951416, 0.872865275142315, 0.678477690288714],
  [0.13338936865329742, 0.8733031674208145, 0.678477690288714],
  [0.1329735964536667, 0.8735950956064809, 0.678477690288714],
  [0.13225704431533813, 0.8734491315136477, 0.6771653543307087],
  [0.13001951575279236, 0.8737410596993139, 0.6758530183727034],
  [0.12895803153514862, 0.8741789519778135, 0.6758530183727034],
  [0.12749136984348297, 0.8754926288133119, 0.6758530183727034],
  [0.12600469589233398, 0.8757845569989783, 0.6758530183727034],
  [0.12360285967588425, 0.8768063056488103, 0.6758530183727034],
  [0.12228386849164963, 0.8768063056488103, 0.6745406824146981],
  [0.12193263322114944, 0.8768063056488103, 0.6745406824146981],
  [0.12174955010414124, 0.8769522697416435, 0.678477690288714],
  [0.11901557445526123, 0.8778280542986425, 0.6771653543307087],
  [0.11828911304473877, 0.8779740183914757, 0.6771653543307087],
  [0.11885488778352737, 0.8785578747628083, 0.6771653543307087],
  [0.11778559535741806, 0.8788498029484747, 0.678477690288714],
  [0.11658801883459091, 0.879141731134141, 0.6797900262467191],
  [0.11548890918493271, 0.8792876952269741, 0.6797900262467191],
  [0.11704497784376144, 0.8798715515983068, 0.678477690288714],
  [0.11611378192901611, 0.8806013720624727, 0.678477690288714],
  [0.11733183264732361, 0.8810392643409721, 0.6797900262467191],
  [0.11761214584112167, 0.8813311925266385, 0.6797900262467191],
  [0.1185489371418953, 0.8807473361553058, 0.6811023622047244],
  [0.1199507936835289, 0.8807473361553058, 0.6797900262467191],
  [0.11990635842084885, 0.8803094438768063, 0.678477690288714],
  [0.11955814808607101, 0.8808933002481389, 0.678477690288714],
  [0.12049592286348343, 0.8798715515983068, 0.678477690288714],
  [0.12180326133966446, 0.8804554079696395, 0.678477690288714],
  [0.12243335694074631, 0.8813311925266385, 0.6797900262467191],
  [0.12315106391906738, 0.8814771566194716, 0.6797900262467191],
  [0.12270169705152512, 0.8816231207123048, 0.678477690288714],
  [0.12235722690820694, 0.8823529411764706, 0.6824146981627297],
  [0.12402277439832687, 0.8826448693621369, 0.6824146981627297],
  [0.12300503253936768, 0.8824989052693037, 0.6824146981627297],
  [0.12303822487592697, 0.8830827616406364, 0.6837270341207349],
  [0.1235603615641594, 0.8827908334549701, 0.6837270341207349],
  [0.1233774945139885, 0.8824989052693037, 0.6824146981627297],
  [0.12410643696784973, 0.8822069770836374, 0.6837270341207349],
  [0.1239432618021965, 0.8835206539191359, 0.6824146981627297],
  [0.12322299927473068, 0.8836666180119691, 0.6837270341207349],
  [0.12208916991949081, 0.8836666180119691, 0.6824146981627297],
  [0.12331083416938782, 0.8843964384761349, 0.6811023622047244],
  [0.12283509224653244, 0.8829367975478032, 0.6824146981627297],
  [0.12238173931837082, 0.8833746898263027, 0.6797900262467191],
  [0.12163782864809036, 0.8835206539191359, 0.678477690288714],
  [0.12174157798290253, 0.8832287257334696, 0.678477690288714],
  [0.12234080582857132, 0.8830827616406364, 0.6771653543307087],
  [0.12341048568487167, 0.8832287257334696, 0.6797900262467191],
  [0.12354560196399689, 0.8838125821048022, 0.6771653543307087],
  [0.12224540114402771, 0.8843964384761349, 0.6758530183727034],
  [0.12297835946083069, 0.8851262589403007, 0.6758530183727034],
  [0.12222766131162643, 0.8842504743833017, 0.6771653543307087],
  [0.12149050831794739, 0.8842504743833017, 0.6771653543307087],
  [0.11971525847911835, 0.884542402568968, 0.6771653543307087],
  [0.11685986816883087, 0.8836666180119691, 0.6771653543307087],
  [0.116354800760746, 0.8839585461976354, 0.6745406824146981],
  [0.11518728733062744, 0.8842504743833017, 0.6732283464566929],
  [0.11208456009626389, 0.8846883666618012, 0.6745406824146981],
  [0.11124517768621445, 0.8848343307546344, 0.6745406824146981],
  [0.10939962416887283, 0.8846883666618012, 0.6732283464566929],
  [0.11193754523992538, 0.8864399357757992, 0.6745406824146981],
  [0.11157060414552689, 0.8870237921471318, 0.6745406824146981],
  [0.1131632998585701, 0.886293971682966, 0.6758530183727034],
  [0.11355380713939667, 0.8878995767041308, 0.6732283464566929],
  [0.1150955930352211, 0.888045540796964, 0.6732283464566929],
  [0.11473088711500168, 0.8883374689826302, 0.6706036745406824],
  [0.11451518535614014, 0.8886293971682966, 0.6692913385826772],
  [0.1164586991071701, 0.8878995767041308, 0.6692913385826772],
  [0.1161404401063919, 0.888921325353963, 0.6679790026246719],
  [0.1162874698638916, 0.8887753612611298, 0.6666666666666666],
  [0.11801162362098694, 0.888045540796964, 0.6706036745406824],
  [0.1173032745718956, 0.8884834330754634, 0.6666666666666666],
  [0.11605603247880936, 0.8884834330754634, 0.6666666666666666],
  [0.11531450599431992, 0.8887753612611298, 0.6679790026246719],
  [0.11506204307079315, 0.8878995767041308, 0.6706036745406824],
  [0.11631068587303162, 0.8883374689826302, 0.6692913385826772],
  [0.11430792510509491, 0.8887753612611298, 0.6706036745406824],
  [0.114576555788517, 0.8893592176324624, 0.6706036745406824],
  [0.11349448561668396, 0.889797109910962, 0.6706036745406824],
  [0.11339213699102402, 0.8892132535396292, 0.6706036745406824],
  [0.11392004042863846, 0.889797109910962, 0.6706036745406824],
  [0.11314418166875839, 0.8903809662822946, 0.6706036745406824],
  [0.11197909712791443, 0.8900890380966282, 0.6732283464566929],
  [0.11079270392656326, 0.890818858560794, 0.6732283464566929],
  [0.11128787696361542, 0.8915486790249598, 0.6758530183727034],
  [0.11163058876991272, 0.8921325353962926, 0.6758530183727034],
  [0.11112847179174423, 0.892570427674792, 0.6745406824146981],
  [0.1109260842204094, 0.8928623558604584, 0.6745406824146981],
  [0.11126334220170975, 0.8921325353962926, 0.6732283464566929],
  [0.11080717295408249, 0.8921325353962926, 0.6745406824146981],
  [0.10975110530853271, 0.8927163917676252, 0.6758530183727034],
  [0.10931424796581268, 0.892570427674792, 0.6758530183727034],
  [0.10899872332811356, 0.892570427674792, 0.6732283464566929],
  [0.11041024327278137, 0.8921325353962926, 0.6745406824146981],
  [0.10952839255332947, 0.8928623558604584, 0.6758530183727034],
  [0.1090397834777832, 0.8937381404174574, 0.6745406824146981],
  [0.10792262852191925, 0.8935921763246242, 0.6732283464566929],
  [0.1074131652712822, 0.8940300686031236, 0.6745406824146981],
  [0.10697662830352783, 0.8938841045102904, 0.6745406824146981],
  [0.10648062080144882, 0.8938841045102904, 0.6745406824146981],
  [0.1050146296620369, 0.8941760326959568, 0.678477690288714],
  [0.1035919189453125, 0.8940300686031236, 0.6719160104986877],
  [0.1013340950012207, 0.8938841045102904, 0.6706036745406824],
  [0.10345327854156494, 0.8940300686031236, 0.6745406824146981],
  [0.1029980480670929, 0.8947598890672894, 0.6706036745406824],
  [0.10302376747131348, 0.89432199678879, 0.6732283464566929],
  [0.10120727866888046, 0.8944679608816232, 0.6666666666666666],
  [0.10132627189159393, 0.895197781345789, 0.6719160104986877],
  [0.10033635795116425, 0.8947598890672894, 0.6679790026246719],
  [0.10035263746976852, 0.8944679608816232, 0.6692913385826772],
  [0.09886478632688522, 0.8956356736242884, 0.6692913385826772],
  [0.09833907335996628, 0.8949058531601226, 0.6679790026246719],
  [0.0979771539568901, 0.8947598890672894, 0.6706036745406824],
  [0.09781228750944138, 0.895197781345789, 0.6719160104986877],
  [0.09852898865938187, 0.895197781345789, 0.6692913385826772],
  [0.09778757393360138, 0.8950518172529558, 0.6719160104986877],
  [0.09776855260133743, 0.8954897095314552, 0.6719160104986877],
  [0.09840738028287888, 0.8957816377171216, 0.6719160104986877],
  [0.09878213703632355, 0.895197781345789, 0.6719160104986877],
  [0.09856272488832474, 0.8959276018099548, 0.6732283464566929],
  [0.09924983978271484, 0.8956356736242884, 0.6745406824146981],
  [0.09942004829645157, 0.8957816377171216, 0.6732283464566929],
  [0.09896726161241531, 0.8963654940884542, 0.6732283464566929],
  [0.09980059415102005, 0.8969493504597869, 0.6732283464566929],
  [0.09911493957042694, 0.8965114581812874, 0.6719160104986877],
  [0.09914491325616837, 0.8973872427382864, 0.6732283464566929],
  [0.09841728210449219, 0.8968033863669538, 0.6679790026246719],
  [0.09744410216808319, 0.8968033863669538, 0.6679790026246719],
  [0.09809164702892303, 0.89709531455262, 0.6706036745406824],
  [0.09714630246162415, 0.8966574222741206, 0.6679790026246719],
  [0.09828286617994308, 0.8965114581812874, 0.6706036745406824],
  [0.0969001054763794, 0.8972412786454532, 0.6706036745406824],
  [0.0967048704624176, 0.89709531455262, 0.6706036745406824],
  [0.09493348747491837, 0.8968033863669538, 0.6679790026246719],
  [0.09512541443109512, 0.8973872427382864, 0.6692913385826772],
  [0.09386732429265976, 0.8973872427382864, 0.6692913385826772],
  [0.09192614257335663, 0.8975332068311196, 0.6679790026246719],
  [0.09119489043951035, 0.8972412786454532, 0.6679790026246719],
  [0.09122523665428162, 0.8973872427382864, 0.6692913385826772],
  [0.09272227436304092, 0.897971099109619, 0.6640419947506562],
  [0.09176881611347198, 0.8978251350167858, 0.6666666666666666],
  [0.09264300018548965, 0.8982630272952854, 0.6666666666666666],
  [0.09273745864629745, 0.897971099109619, 0.6653543307086615],
  [0.09373707324266434, 0.8982630272952854, 0.6653543307086615],
  [0.09328442066907883, 0.898846883666618, 0.6666666666666666],
  [0.09071975946426392, 0.898846883666618, 0.6679790026246719],
  [0.0921832025051117, 0.8994307400379506, 0.6666666666666666],
  [0.09181783348321915, 0.8992847759451175, 0.6653543307086615],
  [0.09052600711584091, 0.8995767041307838, 0.6666666666666666],
  [0.09013950079679489, 0.8994307400379506, 0.6640419947506562],
  [0.08959482610225677, 0.8992847759451175, 0.6640419947506562],
  [0.08851291984319687, 0.8995767041307838, 0.6640419947506562],
  [0.08968604356050491, 0.8992847759451175, 0.6640419947506562],
  [0.08716054260730743, 0.9001605605021165, 0.6653543307086615],
  [0.08819270133972168, 0.9007444168734491, 0.6627296587926509],
  [0.08678565174341202, 0.9003065245949496, 0.6627296587926509],
  [0.08613330870866776, 0.9001605605021165, 0.6627296587926509],
  [0.08561703562736511, 0.9004524886877828, 0.6614173228346457],
  [0.0852675586938858, 0.9004524886877828, 0.6627296587926509],
  [0.08444416522979736, 0.9003065245949496, 0.6614173228346457],
  [0.08591742068529129, 0.9004524886877828, 0.6614173228346457],
  [0.08555956184864044, 0.899722668223617, 0.6587926509186351],
  [0.08629468083381653, 0.9000145964092833, 0.6587926509186351],
  [0.08680441975593567, 0.9004524886877828, 0.6587926509186351],
  [0.08642014861106873, 0.900598452780616, 0.6587926509186351],
  [0.08516581356525421, 0.9004524886877828, 0.65748031496063],
  [0.08604884147644043, 0.9011823091519486, 0.65748031496063],
  [0.08523999899625778, 0.9013282732447818, 0.65748031496063],
  [0.08461377024650574, 0.9010363450591155, 0.6587926509186351],
  [0.08524414151906967, 0.9008903809662823, 0.6601049868766404],
  [0.08456365764141083, 0.9008903809662823, 0.6601049868766404],
  [0.08465293049812317, 0.900598452780616, 0.6601049868766404],
  [0.08404279500246048, 0.9003065245949496, 0.6614173228346457],
  [0.08416647464036942, 0.8998686323164502, 0.6601049868766404],
  [0.0843980684876442, 0.8998686323164502, 0.6601049868766404],
  [0.08409326523542404, 0.900598452780616, 0.6601049868766404],
  [0.08384498953819275, 0.9001605605021165, 0.6601049868766404],
  [0.08359826356172562, 0.9003065245949496, 0.6614173228346457],
  [0.08309374749660492, 0.9001605605021165, 0.6614173228346457],
  [0.08232683688402176, 0.9003065245949496, 0.6614173228346457],
  [0.08336617052555084, 0.9003065245949496, 0.6614173228346457],
  [0.08284062147140503, 0.9004524886877828, 0.6614173228346457],
  [0.08271432667970657, 0.9010363450591155, 0.6601049868766404],
  [0.08307965099811554, 0.900598452780616, 0.6601049868766404],
  [0.08208606392145157, 0.9011823091519486, 0.6614173228346457],
  [0.08228060603141785, 0.9001605605021165, 0.6601049868766404],
  [0.08290530741214752, 0.9016202014304481, 0.6601049868766404],
  [0.08188912272453308, 0.9011823091519486, 0.6614173228346457],
  [0.0812857449054718, 0.9011823091519486, 0.6601049868766404],
  [0.08226113766431808, 0.9010363450591155, 0.6614173228346457],
  [0.08257483690977097, 0.9013282732447818, 0.6614173228346457],
  [0.08237696439027786, 0.9014742373376149, 0.65748031496063],
  [0.08251427114009857, 0.9022040578017808, 0.6601049868766404],
  [0.08082962036132812, 0.9020580937089476, 0.6601049868766404],
  [0.08225329220294952, 0.9026419500802803, 0.6601049868766404],
  [0.08305620402097702, 0.9023500218946139, 0.6587926509186351],
  [0.08289766311645508, 0.9030798423587797, 0.6587926509186351],
  [0.0825447142124176, 0.9030798423587797, 0.6601049868766404],
  [0.08437757194042206, 0.9026419500802803, 0.6587926509186351],
  [0.08287899941205978, 0.9029338782659466, 0.6614173228346457],
  [0.08300655335187912, 0.9029338782659466, 0.6587926509186351],
  [0.08237507194280624, 0.9026419500802803, 0.6601049868766404],
  [0.08091677725315094, 0.9027879141731134, 0.6601049868766404],
  [0.08251866698265076, 0.9023500218946139, 0.6614173228346457],
  [0.08086046576499939, 0.9022040578017808, 0.6601049868766404],
  [0.0804816409945488, 0.9023500218946139, 0.6614173228346457],
  [0.080595962703228, 0.9026419500802803, 0.6627296587926509],
  [0.0789780467748642, 0.9019121296161144, 0.65748031496063],
  [0.07925847172737122, 0.9023500218946139, 0.6587926509186351],
  [0.07942099124193192, 0.9023500218946139, 0.6614173228346457],
  [0.07868018001317978, 0.9027879141731134, 0.6601049868766404],
  [0.07944988459348679, 0.9024959859874471, 0.6614173228346457],
  [0.07935013622045517, 0.9023500218946139, 0.6601049868766404],
  [0.07987784594297409, 0.9029338782659466, 0.6601049868766404],
  [0.0800325945019722, 0.9027879141731134, 0.6587926509186351],
  [0.07949370890855789, 0.9024959859874471, 0.6614173228346457],
  [0.07907908409833908, 0.9022040578017808, 0.6587926509186351],
  [0.07910068333148956, 0.9026419500802803, 0.6601049868766404],
  [0.07822238653898239, 0.9023500218946139, 0.6587926509186351],
  [0.0785532221198082, 0.9022040578017808, 0.6601049868766404],
  [0.07831349223852158, 0.9026419500802803, 0.6601049868766404],
  [0.07798413932323456, 0.9026419500802803, 0.6587926509186351],
  [0.07787753641605377, 0.9026419500802803, 0.6601049868766404],
  [0.07811322063207626, 0.9029338782659466, 0.6601049868766404],
  [0.07766449451446533, 0.9023500218946139, 0.6601049868766404],
  [0.07741981744766235, 0.9020580937089476, 0.6601049868766404],
  [0.07719100266695023, 0.9022040578017808, 0.6587926509186351],
  [0.07808368653059006, 0.9019121296161144, 0.6587926509186351],
  [0.07707874476909637, 0.9022040578017808, 0.6601049868766404],
  [0.07656962424516678, 0.9027879141731134, 0.6601049868766404],
  [0.07600928097963333, 0.9024959859874471, 0.6601049868766404],
  [0.0748334676027298, 0.9030798423587797, 0.6601049868766404],
  [0.07469306141138077, 0.9035177346372792, 0.6601049868766404],
  [0.07480860501527786, 0.9036636987301124, 0.6601049868766404],
  [0.07533594965934753, 0.9035177346372792, 0.6614173228346457],
  [0.07554034888744354, 0.9036636987301124, 0.6614173228346457],
  [0.07561344653367996, 0.9033717705444461, 0.6614173228346457],
  [0.07490844279527664, 0.9038096628229455, 0.6614173228346457],
  [0.07412657141685486, 0.9043935191942782, 0.6627296587926509],
  [0.0751730352640152, 0.9036636987301124, 0.6614173228346457],
  [0.07516764104366302, 0.9039556269157787, 0.6627296587926509],
  [0.075544074177742, 0.9038096628229455, 0.6614173228346457],
  [0.07597342878580093, 0.9038096628229455, 0.6601049868766404],
  [0.07564716041088104, 0.9029338782659466, 0.6614173228346457],
  [0.07657432556152344, 0.9033717705444461, 0.6614173228346457]],
 'loss': 0.07657432556152344,
 'train_accuracy': 0.9033717705444461,
 'test_accuracy': 0.6614173228346457}
>>> hyyperparms['learning_curve']
>>> hyperparms['learning_curve']
[[0.6259374022483826, 0.5891110786746461, 0.6194225721784777],
 [0.572498619556427, 0.6585899868632317, 0.6286089238845144],
 [0.53183913230896, 0.6793168880455408, 0.6509186351706037],
 [0.49689456820487976, 0.6898263027295285, 0.6640419947506562],
 [0.4692693054676056, 0.7023792147131805, 0.6640419947506562],
 [0.44030293822288513, 0.7102612757261714, 0.6666666666666666],
 [0.41839009523391724, 0.7168296599036637, 0.6627296587926509],
 [0.3946485221385956, 0.7254415413808203, 0.6653543307086615],
 [0.36861589550971985, 0.7327397460224785, 0.6627296587926509],
 [0.34860774874687195, 0.7381404174573055, 0.6692913385826772],
 [0.3334750831127167, 0.7422274120566341, 0.6797900262467191],
 [0.31342318654060364, 0.7471901912129616, 0.6758530183727034],
 [0.3014084994792938, 0.7486498321412932, 0.6824146981627297],
 [0.28968751430511475, 0.7524448985549554, 0.6850393700787402],
 [0.2789236307144165, 0.7553641804116188, 0.6863517060367454],
 [0.26269322633743286, 0.7587213545467815, 0.6902887139107612],
 [0.2505471110343933, 0.7616406364034447, 0.6902887139107612],
 [0.23823508620262146, 0.7638300977959422, 0.6863517060367454],
 [0.22794361412525177, 0.7679170923952707, 0.6837270341207349],
 [0.22079095244407654, 0.7709823383447671, 0.6876640419947506],
 [0.21458177268505096, 0.7728798715515983, 0.6863517060367454],
 [0.20860357582569122, 0.7763830097795942, 0.6850393700787402],
 [0.20090197026729584, 0.777988614800759, 0.6889763779527559],
 [0.19494499266147614, 0.7807619325645891, 0.6876640419947506],
 [0.1914031058549881, 0.7844110348854182, 0.6850393700787402],
 [0.18804816901683807, 0.7863085680922494, 0.6889763779527559],
 [0.18425463140010834, 0.787768209020581, 0.6916010498687664],
 [0.17979001998901367, 0.7893738140417458, 0.6902887139107612],
 [0.17644211649894714, 0.7920011677127426, 0.6889763779527559],
 [0.1716540902853012, 0.7947744854765728, 0.6850393700787402],
 [0.16693919897079468, 0.7957962341264049, 0.6929133858267716],
 [0.16285091638565063, 0.7981316596117355, 0.6889763779527559],
 [0.15844137966632843, 0.7995913005400671, 0.6916010498687664],
 [0.15444864332675934, 0.8013428696540651, 0.6942257217847769],
 [0.15088999271392822, 0.8029484746752299, 0.6916010498687664],
 [0.14783570170402527, 0.8047000437892279, 0.6902887139107612],
 [0.14531686902046204, 0.8080572179243906, 0.6863517060367454],
 [0.14415951073169708, 0.8070354692745585, 0.6902887139107612],
 [0.14097829163074493, 0.8082031820172237, 0.6902887139107612],
 [0.14019550383090973, 0.8086410742957233, 0.6902887139107612],
 [0.13907593488693237, 0.8090789665742227, 0.6889763779527559],
 [0.13634692132472992, 0.8084951102028901, 0.6889763779527559],
 [0.13360236585140228, 0.811998248430886, 0.6863517060367454],
 [0.13133986294269562, 0.8127280688950518, 0.6889763779527559],
 [0.12807287275791168, 0.8140417457305503, 0.6889763779527559],
 [0.1264640837907791, 0.813749817544884, 0.6889763779527559],
 [0.1227492019534111, 0.8141877098233834, 0.6863517060367454],
 [0.11908814311027527, 0.8141877098233834, 0.6850393700787402],
 [0.11597103625535965, 0.8143336739162166, 0.6876640419947506],
 [0.1140332892537117, 0.816523135308714, 0.6876640419947506],
 [0.11218272894620895, 0.8178368121442126, 0.6863517060367454],
 [0.111467145383358, 0.8178368121442126, 0.6850393700787402],
 [0.1112506091594696, 0.8184206685155452, 0.6850393700787402],
 [0.11217039078474045, 0.8185666326083784, 0.6811023622047244],
 [0.11314628273248672, 0.8207560940008758, 0.6837270341207349],
 [0.11165676265954971, 0.8210480221865422, 0.6889763779527559],
 [0.1117490604519844, 0.8225076631148738, 0.6863517060367454],
 [0.11151406913995743, 0.8230915194862064, 0.6850393700787402],
 [0.11094939708709717, 0.8244051963217048, 0.6850393700787402],
 [0.11058790236711502, 0.8239673040432054, 0.6824146981627297],
 [0.10955417901277542, 0.8242592322288717, 0.6824146981627297],
 [0.11022614687681198, 0.8260108013428696, 0.6824146981627297],
 [0.11118431389331818, 0.8287841191066998, 0.6837270341207349],
 [0.11229486763477325, 0.8306816523135309, 0.6837270341207349],
 [0.11335647851228714, 0.8322872573346957, 0.6771653543307087],
 [0.11361518502235413, 0.8338928623558605, 0.6797900262467191],
 [0.11557462811470032, 0.8341847905415268, 0.6797900262467191],
 [0.11500710994005203, 0.8343307546343599, 0.6824146981627297],
 [0.11584677547216415, 0.8346226828200263, 0.6811023622047244],
 [0.11663386225700378, 0.8347686469128595, 0.6824146981627297],
 [0.11613094806671143, 0.8362282878411911, 0.6850393700787402],
 [0.11703653633594513, 0.8376879287695227, 0.6850393700787402],
 [0.11800866574048996, 0.838855641512188, 0.6863517060367454],
 [0.11805939674377441, 0.8394394978835207, 0.6889763779527559],
 [0.11940591037273407, 0.8401693183476865, 0.6902887139107612],
 [0.11960262805223465, 0.8404612465333527, 0.6916010498687664],
 [0.11915998160839081, 0.8406072106261859, 0.6902887139107612],
 [0.11931496113538742, 0.8411910669975186, 0.6929133858267716],
 [0.12117541581392288, 0.8416289592760181, 0.6916010498687664],
 [0.12069189548492432, 0.8430886002043497, 0.6902887139107612],
 [0.12134433537721634, 0.8427966720186834, 0.6863517060367454],
 [0.12273436039686203, 0.8444022770398482, 0.6889763779527559],
 [0.12293139100074768, 0.8446942052255145, 0.6889763779527559],
 [0.1248202845454216, 0.845132097504014, 0.6902887139107612],
 [0.1258859783411026, 0.8458619179681798, 0.6916010498687664],
 [0.12555809319019318, 0.847759451175011, 0.6902887139107612],
 [0.12636514008045197, 0.8480513793606773, 0.6889763779527559],
 [0.12449076771736145, 0.8483433075463436, 0.6902887139107612],
 [0.12507480382919312, 0.8492190921033426, 0.6955380577427821],
 [0.12564979493618011, 0.8502408407531747, 0.6968503937007874],
 [0.12622347474098206, 0.8503868048460079, 0.6955380577427821],
 [0.12684820592403412, 0.8500948766603416, 0.6981627296587927],
 [0.12647314369678497, 0.8500948766603416, 0.6981627296587927],
 [0.12695150077342987, 0.8521383739600058, 0.6968503937007874],
 [0.1268935352563858, 0.8521383739600058, 0.6968503937007874],
 [0.1258523315191269, 0.852284338052839, 0.6968503937007874],
 [0.1248580738902092, 0.852284338052839, 0.6981627296587927],
 [0.12372999638319016, 0.8528681944241716, 0.6968503937007874],
 [0.12391958385705948, 0.8534520507955043, 0.6955380577427821],
 [0.12302741408348083, 0.8541818712596702, 0.6968503937007874],
 [0.12275329232215881, 0.8550576558166691, 0.6955380577427821],
 [0.12391042709350586, 0.855933440373668, 0.6955380577427821],
 [0.12350936233997345, 0.8560794044665012, 0.6902887139107612],
 [0.1244114562869072, 0.8562253685593344, 0.6902887139107612],
 [0.12433343380689621, 0.8553495840023354, 0.6929133858267716],
 [0.12551288306713104, 0.856809224930667, 0.6916010498687664],
 [0.1276174932718277, 0.8569551890235002, 0.6929133858267716],
 [0.128916934132576, 0.8563713326521676, 0.6929133858267716],
 [0.12893995642662048, 0.856809224930667, 0.6955380577427821],
 [0.1295662522315979, 0.856809224930667, 0.6968503937007874],
 [0.13293081521987915, 0.8563713326521676, 0.6955380577427821],
 [0.134719118475914, 0.8571011531163334, 0.6942257217847769],
 [0.13639169931411743, 0.8579769376733324, 0.6955380577427821],
 [0.1399882286787033, 0.858560794044665, 0.6929133858267716],
 [0.14085753262043, 0.8591446504159976, 0.6902887139107612],
 [0.14182049036026, 0.8604583272514962, 0.6863517060367454],
 [0.14303551614284515, 0.8598744708801634, 0.6863517060367454],
 [0.14421547949314117, 0.860312363158663, 0.6824146981627297],
 [0.14475460350513458, 0.860312363158663, 0.6824146981627297],
 [0.14548085629940033, 0.8608962195299956, 0.6850393700787402],
 [0.14546428620815277, 0.8614800759013282, 0.6850393700787402],
 [0.1452685296535492, 0.8613341118084951, 0.6837270341207349],
 [0.14852215349674225, 0.8616260399941614, 0.6837270341207349],
 [0.1491166204214096, 0.862209896365494, 0.6837270341207349],
 [0.14965397119522095, 0.8623558604583272, 0.6824146981627297],
 [0.1503164917230606, 0.862209896365494, 0.6837270341207349],
 [0.15142513811588287, 0.8623558604583272, 0.6837270341207349],
 [0.15288692712783813, 0.862209896365494, 0.6850393700787402],
 [0.15314558148384094, 0.8619179681798278, 0.6837270341207349],
 [0.15331223607063293, 0.8620639322726609, 0.6863517060367454],
 [0.15308968722820282, 0.8627937527368268, 0.6837270341207349],
 [0.1521802842617035, 0.8641074295723252, 0.6850393700787402],
 [0.1516047865152359, 0.8643993577579915, 0.6876640419947506],
 [0.15273408591747284, 0.8645453218508247, 0.6902887139107612],
 [0.15106187760829926, 0.86571303459349, 0.6902887139107612],
 [0.1505802720785141, 0.8658589986863232, 0.6902887139107612],
 [0.15085400640964508, 0.8668807473361553, 0.6876640419947506],
 [0.15060485899448395, 0.8673186396146548, 0.6916010498687664],
 [0.14833666384220123, 0.8679024959859875, 0.6929133858267716],
 [0.14715121686458588, 0.8684863523573201, 0.6916010498687664],
 [0.14609958231449127, 0.8689242446358196, 0.6942257217847769],
 [0.14491525292396545, 0.8690702087286527, 0.6902887139107612],
 [0.14532387256622314, 0.8696540650999854, 0.6889763779527559],
 [0.144317165017128, 0.8693621369143191, 0.6876640419947506],
 [0.1436614990234375, 0.8695081010071523, 0.6863517060367454],
 [0.1414957344532013, 0.8695081010071523, 0.6876640419947506],
 [0.1396719515323639, 0.8700919573784849, 0.6863517060367454],
 [0.138599693775177, 0.8703838855641512, 0.6863517060367454],
 [0.1388465315103531, 0.8711137060283171, 0.6850393700787402],
 [0.13711129128932953, 0.8714056342139833, 0.6850393700787402],
 [0.13788489997386932, 0.8711137060283171, 0.6850393700787402],
 [0.13621674478054047, 0.871989490585316, 0.6824146981627297],
 [0.13497048616409302, 0.8727193110494819, 0.6837270341207349],
 [0.13484768569469452, 0.8724273828638155, 0.6850393700787402],
 [0.13548220694065094, 0.8718435264924829, 0.6837270341207349],
 [0.1327059268951416, 0.872865275142315, 0.678477690288714],
 [0.13338936865329742, 0.8733031674208145, 0.678477690288714],
 [0.1329735964536667, 0.8735950956064809, 0.678477690288714],
 [0.13225704431533813, 0.8734491315136477, 0.6771653543307087],
 [0.13001951575279236, 0.8737410596993139, 0.6758530183727034],
 [0.12895803153514862, 0.8741789519778135, 0.6758530183727034],
 [0.12749136984348297, 0.8754926288133119, 0.6758530183727034],
 [0.12600469589233398, 0.8757845569989783, 0.6758530183727034],
 [0.12360285967588425, 0.8768063056488103, 0.6758530183727034],
 [0.12228386849164963, 0.8768063056488103, 0.6745406824146981],
 [0.12193263322114944, 0.8768063056488103, 0.6745406824146981],
 [0.12174955010414124, 0.8769522697416435, 0.678477690288714],
 [0.11901557445526123, 0.8778280542986425, 0.6771653543307087],
 [0.11828911304473877, 0.8779740183914757, 0.6771653543307087],
 [0.11885488778352737, 0.8785578747628083, 0.6771653543307087],
 [0.11778559535741806, 0.8788498029484747, 0.678477690288714],
 [0.11658801883459091, 0.879141731134141, 0.6797900262467191],
 [0.11548890918493271, 0.8792876952269741, 0.6797900262467191],
 [0.11704497784376144, 0.8798715515983068, 0.678477690288714],
 [0.11611378192901611, 0.8806013720624727, 0.678477690288714],
 [0.11733183264732361, 0.8810392643409721, 0.6797900262467191],
 [0.11761214584112167, 0.8813311925266385, 0.6797900262467191],
 [0.1185489371418953, 0.8807473361553058, 0.6811023622047244],
 [0.1199507936835289, 0.8807473361553058, 0.6797900262467191],
 [0.11990635842084885, 0.8803094438768063, 0.678477690288714],
 [0.11955814808607101, 0.8808933002481389, 0.678477690288714],
 [0.12049592286348343, 0.8798715515983068, 0.678477690288714],
 [0.12180326133966446, 0.8804554079696395, 0.678477690288714],
 [0.12243335694074631, 0.8813311925266385, 0.6797900262467191],
 [0.12315106391906738, 0.8814771566194716, 0.6797900262467191],
 [0.12270169705152512, 0.8816231207123048, 0.678477690288714],
 [0.12235722690820694, 0.8823529411764706, 0.6824146981627297],
 [0.12402277439832687, 0.8826448693621369, 0.6824146981627297],
 [0.12300503253936768, 0.8824989052693037, 0.6824146981627297],
 [0.12303822487592697, 0.8830827616406364, 0.6837270341207349],
 [0.1235603615641594, 0.8827908334549701, 0.6837270341207349],
 [0.1233774945139885, 0.8824989052693037, 0.6824146981627297],
 [0.12410643696784973, 0.8822069770836374, 0.6837270341207349],
 [0.1239432618021965, 0.8835206539191359, 0.6824146981627297],
 [0.12322299927473068, 0.8836666180119691, 0.6837270341207349],
 [0.12208916991949081, 0.8836666180119691, 0.6824146981627297],
 [0.12331083416938782, 0.8843964384761349, 0.6811023622047244],
 [0.12283509224653244, 0.8829367975478032, 0.6824146981627297],
 [0.12238173931837082, 0.8833746898263027, 0.6797900262467191],
 [0.12163782864809036, 0.8835206539191359, 0.678477690288714],
 [0.12174157798290253, 0.8832287257334696, 0.678477690288714],
 [0.12234080582857132, 0.8830827616406364, 0.6771653543307087],
 [0.12341048568487167, 0.8832287257334696, 0.6797900262467191],
 [0.12354560196399689, 0.8838125821048022, 0.6771653543307087],
 [0.12224540114402771, 0.8843964384761349, 0.6758530183727034],
 [0.12297835946083069, 0.8851262589403007, 0.6758530183727034],
 [0.12222766131162643, 0.8842504743833017, 0.6771653543307087],
 [0.12149050831794739, 0.8842504743833017, 0.6771653543307087],
 [0.11971525847911835, 0.884542402568968, 0.6771653543307087],
 [0.11685986816883087, 0.8836666180119691, 0.6771653543307087],
 [0.116354800760746, 0.8839585461976354, 0.6745406824146981],
 [0.11518728733062744, 0.8842504743833017, 0.6732283464566929],
 [0.11208456009626389, 0.8846883666618012, 0.6745406824146981],
 [0.11124517768621445, 0.8848343307546344, 0.6745406824146981],
 [0.10939962416887283, 0.8846883666618012, 0.6732283464566929],
 [0.11193754523992538, 0.8864399357757992, 0.6745406824146981],
 [0.11157060414552689, 0.8870237921471318, 0.6745406824146981],
 [0.1131632998585701, 0.886293971682966, 0.6758530183727034],
 [0.11355380713939667, 0.8878995767041308, 0.6732283464566929],
 [0.1150955930352211, 0.888045540796964, 0.6732283464566929],
 [0.11473088711500168, 0.8883374689826302, 0.6706036745406824],
 [0.11451518535614014, 0.8886293971682966, 0.6692913385826772],
 [0.1164586991071701, 0.8878995767041308, 0.6692913385826772],
 [0.1161404401063919, 0.888921325353963, 0.6679790026246719],
 [0.1162874698638916, 0.8887753612611298, 0.6666666666666666],
 [0.11801162362098694, 0.888045540796964, 0.6706036745406824],
 [0.1173032745718956, 0.8884834330754634, 0.6666666666666666],
 [0.11605603247880936, 0.8884834330754634, 0.6666666666666666],
 [0.11531450599431992, 0.8887753612611298, 0.6679790026246719],
 [0.11506204307079315, 0.8878995767041308, 0.6706036745406824],
 [0.11631068587303162, 0.8883374689826302, 0.6692913385826772],
 [0.11430792510509491, 0.8887753612611298, 0.6706036745406824],
 [0.114576555788517, 0.8893592176324624, 0.6706036745406824],
 [0.11349448561668396, 0.889797109910962, 0.6706036745406824],
 [0.11339213699102402, 0.8892132535396292, 0.6706036745406824],
 [0.11392004042863846, 0.889797109910962, 0.6706036745406824],
 [0.11314418166875839, 0.8903809662822946, 0.6706036745406824],
 [0.11197909712791443, 0.8900890380966282, 0.6732283464566929],
 [0.11079270392656326, 0.890818858560794, 0.6732283464566929],
 [0.11128787696361542, 0.8915486790249598, 0.6758530183727034],
 [0.11163058876991272, 0.8921325353962926, 0.6758530183727034],
 [0.11112847179174423, 0.892570427674792, 0.6745406824146981],
 [0.1109260842204094, 0.8928623558604584, 0.6745406824146981],
 [0.11126334220170975, 0.8921325353962926, 0.6732283464566929],
 [0.11080717295408249, 0.8921325353962926, 0.6745406824146981],
 [0.10975110530853271, 0.8927163917676252, 0.6758530183727034],
 [0.10931424796581268, 0.892570427674792, 0.6758530183727034],
 [0.10899872332811356, 0.892570427674792, 0.6732283464566929],
 [0.11041024327278137, 0.8921325353962926, 0.6745406824146981],
 [0.10952839255332947, 0.8928623558604584, 0.6758530183727034],
 [0.1090397834777832, 0.8937381404174574, 0.6745406824146981],
 [0.10792262852191925, 0.8935921763246242, 0.6732283464566929],
 [0.1074131652712822, 0.8940300686031236, 0.6745406824146981],
 [0.10697662830352783, 0.8938841045102904, 0.6745406824146981],
 [0.10648062080144882, 0.8938841045102904, 0.6745406824146981],
 [0.1050146296620369, 0.8941760326959568, 0.678477690288714],
 [0.1035919189453125, 0.8940300686031236, 0.6719160104986877],
 [0.1013340950012207, 0.8938841045102904, 0.6706036745406824],
 [0.10345327854156494, 0.8940300686031236, 0.6745406824146981],
 [0.1029980480670929, 0.8947598890672894, 0.6706036745406824],
 [0.10302376747131348, 0.89432199678879, 0.6732283464566929],
 [0.10120727866888046, 0.8944679608816232, 0.6666666666666666],
 [0.10132627189159393, 0.895197781345789, 0.6719160104986877],
 [0.10033635795116425, 0.8947598890672894, 0.6679790026246719],
 [0.10035263746976852, 0.8944679608816232, 0.6692913385826772],
 [0.09886478632688522, 0.8956356736242884, 0.6692913385826772],
 [0.09833907335996628, 0.8949058531601226, 0.6679790026246719],
 [0.0979771539568901, 0.8947598890672894, 0.6706036745406824],
 [0.09781228750944138, 0.895197781345789, 0.6719160104986877],
 [0.09852898865938187, 0.895197781345789, 0.6692913385826772],
 [0.09778757393360138, 0.8950518172529558, 0.6719160104986877],
 [0.09776855260133743, 0.8954897095314552, 0.6719160104986877],
 [0.09840738028287888, 0.8957816377171216, 0.6719160104986877],
 [0.09878213703632355, 0.895197781345789, 0.6719160104986877],
 [0.09856272488832474, 0.8959276018099548, 0.6732283464566929],
 [0.09924983978271484, 0.8956356736242884, 0.6745406824146981],
 [0.09942004829645157, 0.8957816377171216, 0.6732283464566929],
 [0.09896726161241531, 0.8963654940884542, 0.6732283464566929],
 [0.09980059415102005, 0.8969493504597869, 0.6732283464566929],
 [0.09911493957042694, 0.8965114581812874, 0.6719160104986877],
 [0.09914491325616837, 0.8973872427382864, 0.6732283464566929],
 [0.09841728210449219, 0.8968033863669538, 0.6679790026246719],
 [0.09744410216808319, 0.8968033863669538, 0.6679790026246719],
 [0.09809164702892303, 0.89709531455262, 0.6706036745406824],
 [0.09714630246162415, 0.8966574222741206, 0.6679790026246719],
 [0.09828286617994308, 0.8965114581812874, 0.6706036745406824],
 [0.0969001054763794, 0.8972412786454532, 0.6706036745406824],
 [0.0967048704624176, 0.89709531455262, 0.6706036745406824],
 [0.09493348747491837, 0.8968033863669538, 0.6679790026246719],
 [0.09512541443109512, 0.8973872427382864, 0.6692913385826772],
 [0.09386732429265976, 0.8973872427382864, 0.6692913385826772],
 [0.09192614257335663, 0.8975332068311196, 0.6679790026246719],
 [0.09119489043951035, 0.8972412786454532, 0.6679790026246719],
 [0.09122523665428162, 0.8973872427382864, 0.6692913385826772],
 [0.09272227436304092, 0.897971099109619, 0.6640419947506562],
 [0.09176881611347198, 0.8978251350167858, 0.6666666666666666],
 [0.09264300018548965, 0.8982630272952854, 0.6666666666666666],
 [0.09273745864629745, 0.897971099109619, 0.6653543307086615],
 [0.09373707324266434, 0.8982630272952854, 0.6653543307086615],
 [0.09328442066907883, 0.898846883666618, 0.6666666666666666],
 [0.09071975946426392, 0.898846883666618, 0.6679790026246719],
 [0.0921832025051117, 0.8994307400379506, 0.6666666666666666],
 [0.09181783348321915, 0.8992847759451175, 0.6653543307086615],
 [0.09052600711584091, 0.8995767041307838, 0.6666666666666666],
 [0.09013950079679489, 0.8994307400379506, 0.6640419947506562],
 [0.08959482610225677, 0.8992847759451175, 0.6640419947506562],
 [0.08851291984319687, 0.8995767041307838, 0.6640419947506562],
 [0.08968604356050491, 0.8992847759451175, 0.6640419947506562],
 [0.08716054260730743, 0.9001605605021165, 0.6653543307086615],
 [0.08819270133972168, 0.9007444168734491, 0.6627296587926509],
 [0.08678565174341202, 0.9003065245949496, 0.6627296587926509],
 [0.08613330870866776, 0.9001605605021165, 0.6627296587926509],
 [0.08561703562736511, 0.9004524886877828, 0.6614173228346457],
 [0.0852675586938858, 0.9004524886877828, 0.6627296587926509],
 [0.08444416522979736, 0.9003065245949496, 0.6614173228346457],
 [0.08591742068529129, 0.9004524886877828, 0.6614173228346457],
 [0.08555956184864044, 0.899722668223617, 0.6587926509186351],
 [0.08629468083381653, 0.9000145964092833, 0.6587926509186351],
 [0.08680441975593567, 0.9004524886877828, 0.6587926509186351],
 [0.08642014861106873, 0.900598452780616, 0.6587926509186351],
 [0.08516581356525421, 0.9004524886877828, 0.65748031496063],
 [0.08604884147644043, 0.9011823091519486, 0.65748031496063],
 [0.08523999899625778, 0.9013282732447818, 0.65748031496063],
 [0.08461377024650574, 0.9010363450591155, 0.6587926509186351],
 [0.08524414151906967, 0.9008903809662823, 0.6601049868766404],
 [0.08456365764141083, 0.9008903809662823, 0.6601049868766404],
 [0.08465293049812317, 0.900598452780616, 0.6601049868766404],
 [0.08404279500246048, 0.9003065245949496, 0.6614173228346457],
 [0.08416647464036942, 0.8998686323164502, 0.6601049868766404],
 [0.0843980684876442, 0.8998686323164502, 0.6601049868766404],
 [0.08409326523542404, 0.900598452780616, 0.6601049868766404],
 [0.08384498953819275, 0.9001605605021165, 0.6601049868766404],
 [0.08359826356172562, 0.9003065245949496, 0.6614173228346457],
 [0.08309374749660492, 0.9001605605021165, 0.6614173228346457],
 [0.08232683688402176, 0.9003065245949496, 0.6614173228346457],
 [0.08336617052555084, 0.9003065245949496, 0.6614173228346457],
 [0.08284062147140503, 0.9004524886877828, 0.6614173228346457],
 [0.08271432667970657, 0.9010363450591155, 0.6601049868766404],
 [0.08307965099811554, 0.900598452780616, 0.6601049868766404],
 [0.08208606392145157, 0.9011823091519486, 0.6614173228346457],
 [0.08228060603141785, 0.9001605605021165, 0.6601049868766404],
 [0.08290530741214752, 0.9016202014304481, 0.6601049868766404],
 [0.08188912272453308, 0.9011823091519486, 0.6614173228346457],
 [0.0812857449054718, 0.9011823091519486, 0.6601049868766404],
 [0.08226113766431808, 0.9010363450591155, 0.6614173228346457],
 [0.08257483690977097, 0.9013282732447818, 0.6614173228346457],
 [0.08237696439027786, 0.9014742373376149, 0.65748031496063],
 [0.08251427114009857, 0.9022040578017808, 0.6601049868766404],
 [0.08082962036132812, 0.9020580937089476, 0.6601049868766404],
 [0.08225329220294952, 0.9026419500802803, 0.6601049868766404],
 [0.08305620402097702, 0.9023500218946139, 0.6587926509186351],
 [0.08289766311645508, 0.9030798423587797, 0.6587926509186351],
 [0.0825447142124176, 0.9030798423587797, 0.6601049868766404],
 [0.08437757194042206, 0.9026419500802803, 0.6587926509186351],
 [0.08287899941205978, 0.9029338782659466, 0.6614173228346457],
 [0.08300655335187912, 0.9029338782659466, 0.6587926509186351],
 [0.08237507194280624, 0.9026419500802803, 0.6601049868766404],
 [0.08091677725315094, 0.9027879141731134, 0.6601049868766404],
 [0.08251866698265076, 0.9023500218946139, 0.6614173228346457],
 [0.08086046576499939, 0.9022040578017808, 0.6601049868766404],
 [0.0804816409945488, 0.9023500218946139, 0.6614173228346457],
 [0.080595962703228, 0.9026419500802803, 0.6627296587926509],
 [0.0789780467748642, 0.9019121296161144, 0.65748031496063],
 [0.07925847172737122, 0.9023500218946139, 0.6587926509186351],
 [0.07942099124193192, 0.9023500218946139, 0.6614173228346457],
 [0.07868018001317978, 0.9027879141731134, 0.6601049868766404],
 [0.07944988459348679, 0.9024959859874471, 0.6614173228346457],
 [0.07935013622045517, 0.9023500218946139, 0.6601049868766404],
 [0.07987784594297409, 0.9029338782659466, 0.6601049868766404],
 [0.0800325945019722, 0.9027879141731134, 0.6587926509186351],
 [0.07949370890855789, 0.9024959859874471, 0.6614173228346457],
 [0.07907908409833908, 0.9022040578017808, 0.6587926509186351],
 [0.07910068333148956, 0.9026419500802803, 0.6601049868766404],
 [0.07822238653898239, 0.9023500218946139, 0.6587926509186351],
 [0.0785532221198082, 0.9022040578017808, 0.6601049868766404],
 [0.07831349223852158, 0.9026419500802803, 0.6601049868766404],
 [0.07798413932323456, 0.9026419500802803, 0.6587926509186351],
 [0.07787753641605377, 0.9026419500802803, 0.6601049868766404],
 [0.07811322063207626, 0.9029338782659466, 0.6601049868766404],
 [0.07766449451446533, 0.9023500218946139, 0.6601049868766404],
 [0.07741981744766235, 0.9020580937089476, 0.6601049868766404],
 [0.07719100266695023, 0.9022040578017808, 0.6587926509186351],
 [0.07808368653059006, 0.9019121296161144, 0.6587926509186351],
 [0.07707874476909637, 0.9022040578017808, 0.6601049868766404],
 [0.07656962424516678, 0.9027879141731134, 0.6601049868766404],
 [0.07600928097963333, 0.9024959859874471, 0.6601049868766404],
 [0.0748334676027298, 0.9030798423587797, 0.6601049868766404],
 [0.07469306141138077, 0.9035177346372792, 0.6601049868766404],
 [0.07480860501527786, 0.9036636987301124, 0.6601049868766404],
 [0.07533594965934753, 0.9035177346372792, 0.6614173228346457],
 [0.07554034888744354, 0.9036636987301124, 0.6614173228346457],
 [0.07561344653367996, 0.9033717705444461, 0.6614173228346457],
 [0.07490844279527664, 0.9038096628229455, 0.6614173228346457],
 [0.07412657141685486, 0.9043935191942782, 0.6627296587926509],
 [0.0751730352640152, 0.9036636987301124, 0.6614173228346457],
 [0.07516764104366302, 0.9039556269157787, 0.6627296587926509],
 [0.075544074177742, 0.9038096628229455, 0.6614173228346457],
 [0.07597342878580093, 0.9038096628229455, 0.6601049868766404],
 [0.07564716041088104, 0.9029338782659466, 0.6614173228346457],
 [0.07657432556152344, 0.9033717705444461, 0.6614173228346457]]
>>> lc = pd.DataFrame(hyperparms['learning_curve'])
>>> lc.plot()
<AxesSubplot:>
>>> plt.show()
>>> from matplotlib import pyplot as plt
>>> plt.show()
>>> lc.columns = 'training_loss training_accuracy test_accuracy'.split()
>>> lc[['train_accuracies', 'validation_accuracies']].plot(linewidth=2, grid='on')
>>> lc[['train_accuracy', 'test_accuracy']].plot(linewidth=2, grid='on')
>>> lc[['training_accuracy', 'test_accuracy']].plot(linewidth=2, grid='on')
<AxesSubplot:>
>>> plt.show()
>>> lc[['training_accuracy', 'test_accuracy']].plot(linewidth=2, grid='on', xlabel='epochs')
<AxesSubplot:xlabel='epochs'>
>>>  {'seq_len': 40,
...  'vocab_size': 2000,
...  'embedding_size': 50,
...  'out_channels': 50,
...  'num_stopwords': 0,
...  'kernel_lengths': [1, 2, 3, 4, 5, 6],
...  'strides': [1, 1, 1, 1, 1, 1],
...  'batch_size': 24,
...  'learning_rate': 0.002,
...  'dropout': 0,
...  'num_epochs': 400,
... }
...
{'seq_len': 40,
 'vocab_size': 2000,
 'embedding_size': 50,
 'out_channels': 50,
 'num_stopwords': 0,
 'kernel_lengths': [1, 2, 3, 4, 5, 6],
 'strides': [1, 1, 1, 1, 1, 1],
 'batch_size': 24,
 'learning_rate': 0.002,
 'dropout': 0,
 'num_epochs': 400}
>>> main_hyperparams = _
>>> globals().update(main_hyperparams)
>>> title = f'seq_len={seq_len} vocab_size={vocab_size} embedding_size={embedding_size} kernel_lengths={kernel_lengths}'
>>> lc[['training_accuracy', 'test_accuracy']].plot(linewidth=2, grid='on', xlabel='epochs', title=title)
<AxesSubplot:title={'center':'seq_len=40 vocab_size=2000 embedding_size=50 kernel_lengths=[1, 2, 3, 4, 5, 6]'}, xlabel='epochs'>
>>> plt.show()
>>> more train_ch07.py
>>> hyperp
{'use_glove': True,
 'expand_glove_vocab': True,
 'seq_len': 40,
 'vocab_size': 2000,
 'embedding_size': 50,
 'out_channels': 50,
 'num_stopwords': 0,
 'kernel_lengths': [1, 2, 3, 4, 5, 6],
 'strides': [1, 1, 1, 1, 1, 1],
 'batch_size': 24,
 'learning_rate': 0.002,
 'dropout': 0,
 'num_epochs': 400}
>>> hyperp['num_epochs'] = 50
>>> hyperp['dropout'] = .2
>>> hyperp['kernel_lengths'] = [2, 3, 4, 5]
>>> hyperp['strides'] = [1, 1, 1, 1]
>>> hyperp['learning_rate'] = .0015
>>> ls *.json
>>> main()
{'pipeline': <__main__.Pipeline at 0x7ff455e89d00>,
 'hyperp': {'use_glove': True,
  'expand_glove_vocab': True,
  'seq_len': 40,
  'vocab_size': 2000,
  'embedding_size': 50,
  'out_channels': 50,
  'num_stopwords': 0,
  'kernel_lengths': [2, 3, 4, 5],
  'strides': [1, 1, 1, 1],
  'batch_size': 24,
  'learning_rate': 0.0015,
  'dropout': 0.2,
  'num_epochs': 50,
  'y_train': [0,
   0,
   1,
   0,
   1,
   0,
   1,
   0,
   1,
   0,
   0,
   1,
   0,
   1,
   0,
   0,
   0,
   1,
   0,
   1,
   1,
   0,
   0,
   0,
   1,
   0,
   0,
   1,
   0,
   1,
   0,
   1,
   0,
   0,
   1,
   0,
   0,
   1,
   1,
   0,
   1,
   0,
   1,
   0,
   1,
   0,
   1,
   0,
   0,
   1,
   0,
   1,
   0,
   0,
   1,
   0,
   1,
   0,
   0,
   0,
   1,
   0,
   1,
   0,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   0,
   1,
   1,
   0,
   0,
   0,
   1,
   1,
   1,
   0,
   0,
   0,
   0,
   1,
   0,
   1,
   0,
   1,
   0,
   1,
   0,
   0,
   1,
   0,
   0,
   0,
   0,
   1,
   1,
   0,
   1,
   1,
   0,
   1,
   1,
   1,
   1,
   1,
   0,
   1,
   0,
   0,
   1,
   1,
   0,
   0,
   0,
   0,
   0,
   0,
   0,
   1,
   0,
   0,
   1,
   1,
   1,
   0,
   1,
   0,
   1,
   0,
   0,
   1,
   1,
   0,
   0,
   0,
   1,
   1,
   0,
   0,
   0,
   0,
   1,
   1,
   0,
   0,
   1,
   0,
   1,
   1,
   1,
   0,
   0,
   0,
   0,
   1,
   0,
   1,
   0,
   0,
   1,
   0,
   0,
   1,
   1,
   1,
   0,
   0,
   0,
   0,
   1,
   0,
   0,
   1,
   0,
   0,
   0,
   1,
   0,
   1,
   0,
   1,
   0,
   0,
   0,
   0,
   0,
   1,
   0,
   0,
   1,
   0,
   1,
   1,
   0,
   0,
   0,
   0,
   1,
   1,
   1,
   1,
   1,
   0,
   1,
   0,
   1,
   0,
   1,
   1,
   1,
   0,
   1,
   0,
   0,
   1,
   0,
   0,
   1,
   0,
   0,
   0,
   0,
   0,
   0,
   1,
   0,
   1,
   0,
   0,
   0,
   1,
   0,
   1,
   1,
   1,
   1,
   0,
   0,
   1,
   0,
   1,
   0,
   0,
   1,
   1,
   0,
   0,
   0,
   0,
   0,
   0,
   0,
   1,
   1,
   1,
   1,
   0,
   1,
   1,
   1,
   1,
   0,
   1,
   0,
   1,
   1,
   1,
   0,
   1,
   1,
   1,
   1,
   0,
   0,
   0,
   0,
   0,
   0,
   0,
   1,
   1,
   0,
   1,
   1,
   1,
   0,
   0,
   1,
   1,
   0,
   1,
   1,
   1,
   1,
   1,
   0,
   0,
   1,
   1,
   0,
   0,
   1,
   0,
   1,
   1,
   0,
   0,
   0,
   0,
   1,
   0,
   0,
   0,
   0,
   0,
   0,
   1,
   1,
   1,
   0,
   0,
   0,
   1,
   1,
   1,
   1,
   1,
   1,
   0,
   0,
   1,
   0,
   0,
   0,
   0,
   0,
   1,
   1,
   0,
   0,
   1,
   0,
   1,
   0,
   1,
   1,
   1,
   1,
   0,
   0,
   0,
   0,
   1,
   0,
   1,
   0,
   0,
   0,
   1,
   1,
   0,
   1,
   1,
   1,
   1,
   0,
   0,
   0,
   0,
   1,
   0,
   0,
   1,
   0,
   1,
   1,
   0,
   0,
   0,
   0,
   0,
   0,
   1,
   1,
   1,
   0,
   0,
   1,
   0,
   0,
   1,
   1,
   1,
   1,
   0,
   1,
   0,
   0,
   0,
   0,
   0,
   0,
   0,
   1,
   1,
   0,
   0,
   1,
   0,
   0,
   0,
   0,
   0,
   1,
   0,
   0,
   1,
   1,
   1,
   0,
   1,
   0,
   1,
   0,
   1,
   1,
   0,
   0,
   1,
   0,
   0,
   0,
   0,
   1,
   0,
   0,
   1,
   1,
   1,
   0,
   0,
   0,
   0,
   0,
   0,
   1,
   0,
   1,
   0,
   0,
   0,
   0,
   0,
   0,
   1,
   0,
   0,
   1,
   0,
   0,
   0,
   0,
   0,
   1,
   1,
   0,
   0,
   0,
   0,
   0,
   0,
   0,
   1,
   0,
   1,
   1,
   1,
   1,
   1,
   1,
   0,
   0,
   0,
   0,
   1,
   0,
   1,
   0,
   0,
   1,
   0,
   0,
   0,
   0,
   0,
   0,
   1,
   0,
   1,
   0,
   0,
   1,
   1,
   0,
   0,
   1,
   0,
   0,
   0,
   1,
   0,
   0,
   0,
   0,
   0,
   0,
   0,
   0,
   1,
   1,
   1,
   1,
   0,
   0,
   0,
   0,
   1,
   0,
   0,
   1,
   0,
   1,
   0,
   1,
   0,
   0,
   1,
   0,
   1,
   0,
   1,
   1,
   1,
   0,
   1,
   0,
   0,
   0,
   1,
   0,
   0,
   0,
   0,
   1,
   1,
   0,
   0,
   0,
   0,
   1,
   0,
   1,
   1,
   0,
   1,
   0,
   1,
   0,
   0,
   1,
   0,
   0,
   0,
   0,
   0,
   0,
   1,
   1,
   1,
   0,
   0,
   1,
   1,
   1,
   1,
   0,
   0,
   0,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   0,
   1,
   1,
   0,
   0,
   0,
   0,
   1,
   1,
   0,
   0,
   0,
   1,
   0,
   0,
   0,
   0,
   0,
   1,
   0,
   1,
   0,
   0,
   1,
   1,
   1,
   0,
   1,
   1,
   1,
   1,
   0,
   1,
   1,
   1,
   0,
   0,
   0,
   1,
   0,
   0,
   0,
   0,
   1,
   0,
   1,
   1,
   0,
   0,
   0,
   0,
   1,
   0,
   1,
   0,
   0,
   1,
   0,
   0,
   1,
   0,
   0,
   0,
   1,
   1,
   1,
   0,
   0,
   1,
   0,
   0,
   1,
   0,
   1,
   1,
   1,
   0,
   0,
   1,
   1,
   1,
   1,
   0,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   0,
   0,
   0,
   1,
   0,
   1,
   0,
   0,
   1,
   0,
   1,
   1,
   1,
   0,
   0,
   0,
   0,
   0,
   1,
   1,
   0,
   0,
   0,
   0,
   0,
   0,
   0,
   0,
   0,
   1,
   0,
   1,
   0,
   0,
   0,
   0,
   1,
   0,
   1,
   1,
   0,
   0,
   0,
   1,
   0,
   0,
   1,
   1,
   0,
   0,
   0,
   0,
   1,
   0,
   1,
   1,
   1,
   0,
   1,
   1,
   1,
   0,
   0,
   0,
   0,
   0,
   1,
   0,
   1,
   0,
   0,
   0,
   1,
   1,
   1,
   1,
   0,
   1,
   0,
   1,
   0,
   0,
   0,
   0,
   0,
   0,
   1,
   1,
   1,
   0,
   0,
   0,
   0,
   1,
   0,
   1,
   0,
   1,
   0,
   1,
   1,
   1,
   0,
   0,
   0,
   1,
   0,
   1,
   1,
   0,
   0,
   0,
   1,
   0,
   1,
   1,
   0,
   0,
   0,
   1,
   0,
   1,
   0,
   1,
   0,
   1,
   0,
   1,
   1,
   1,
   0,
   1,
   0,
   1,
   0,
   1,
   1,
   0,
   0,
   1,
   1,
   0,
   0,
   0,
   0,
   1,
   0,
   1,
   0,
   0,
   1,
   0,
   1,
   0,
   0,
   0,
   0,
   0,
   0,
   1,
   0,
   1,
   1,
   0,
   0,
   0,
   0,
   0,
   1,
   1,
   0,
   0,
   0,
   1,
   0,
   0,
   0,
   1,
   0,
   0,
   0,
   1,
   0,
   0,
   0,
   0,
   1,
   1,
   0,
   1,
   0,
   0,
   1,
   0,
   1,
   1,
   0,
   0,
   0,
   0,
   0,
   0,
   0,
   1,
   0,
   0,
   1,
   1,
   1,
   1,
   1,
   1,
   0,
   0,
   1,
   0,
   0,
   0,
   0,
   0,
   0,
   1,
   0,
   0,
   0,
   0,
   0,
   0,
   0,
   1,
   0,
   1,
   1,
   0,
   1,
   0,
   0,
   1,
   0,
   1,
   0,
   1,
   0,
   0,
   0,
   0,
   1,
   0,
   0,
   0,
   0,
   1,
   1,
   1,
   1,
   1,
   1,
   0,
   0,
   0,
   0,
   0,
   0,
   0,
   0,
   1,
   1,
   1,
   1,
   0,
   0,
   1,
   1,
   0,
   1,
   1,
   0,
   1,
   1,
   0,
   1,
   0,
   0,
   1,
   0,
   0,
   0,
   1,
   0,
   1,
   0,
   1,
   1,
   1,
   1,
   0,
   1,
   1,
   0,
   1,
   0,
   1,
   0,
   1,
   0,
   0,
   0,
   0,
   1,
   ...],
  'y_test': [0,
   0,
   1,
   0,
   1,
   0,
   0,
   0,
   1,
   0,
   0,
   0,
   0,
   0,
   1,
   0,
   0,
   1,
   0,
   0,
   1,
   1,
   0,
   1,
   0,
   1,
   1,
   1,
   0,
   1,
   0,
   1,
   0,
   1,
   0,
   0,
   0,
   0,
   1,
   1,
   1,
   1,
   1,
   1,
   0,
   0,
   0,
   0,
   1,
   0,
   0,
   0,
   0,
   0,
   0,
   1,
   1,
   1,
   0,
   1,
   1,
   0,
   0,
   1,
   1,
   1,
   1,
   1,
   0,
   1,
   0,
   1,
   1,
   1,
   1,
   0,
   1,
   1,
   1,
   0,
   1,
   0,
   0,
   0,
   0,
   1,
   0,
   1,
   1,
   1,
   0,
   1,
   0,
   0,
   0,
   0,
   0,
   0,
   1,
   0,
   1,
   0,
   0,
   0,
   0,
   0,
   0,
   0,
   1,
   1,
   0,
   1,
   0,
   1,
   1,
   0,
   0,
   0,
   0,
   1,
   0,
   0,
   1,
   0,
   1,
   0,
   0,
   1,
   0,
   1,
   1,
   1,
   1,
   1,
   1,
   0,
   1,
   1,
   0,
   0,
   0,
   1,
   1,
   0,
   0,
   1,
   1,
   0,
   0,
   1,
   1,
   0,
   1,
   1,
   0,
   1,
   1,
   0,
   1,
   0,
   1,
   0,
   1,
   1,
   0,
   0,
   0,
   0,
   1,
   0,
   0,
   0,
   0,
   0,
   1,
   0,
   1,
   1,
   1,
   0,
   1,
   0,
   1,
   0,
   0,
   0,
   1,
   0,
   1,
   1,
   0,
   0,
   0,
   0,
   0,
   0,
   1,
   0,
   0,
   0,
   0,
   1,
   0,
   1,
   0,
   1,
   1,
   0,
   0,
   0,
   0,
   0,
   1,
   0,
   0,
   1,
   0,
   1,
   1,
   0,
   1,
   1,
   0,
   1,
   0,
   0,
   1,
   0,
   0,
   0,
   1,
   0,
   0,
   0,
   0,
   0,
   1,
   1,
   1,
   0,
   0,
   0,
   0,
   1,
   1,
   1,
   0,
   1,
   1,
   0,
   0,
   1,
   0,
   1,
   1,
   0,
   0,
   1,
   1,
   0,
   1,
   1,
   0,
   0,
   0,
   0,
   1,
   1,
   0,
   0,
   0,
   0,
   1,
   0,
   1,
   1,
   0,
   0,
   1,
   0,
   1,
   0,
   1,
   0,
   0,
   1,
   0,
   1,
   0,
   0,
   1,
   0,
   0,
   1,
   0,
   1,
   1,
   0,
   0,
   1,
   0,
   0,
   0,
   1,
   1,
   0,
   1,
   1,
   0,
   0,
   1,
   0,
   1,
   0,
   1,
   1,
   0,
   0,
   1,
   1,
   1,
   1,
   0,
   1,
   1,
   1,
   0,
   0,
   0,
   0,
   1,
   0,
   1,
   1,
   1,
   0,
   1,
   0,
   1,
   1,
   0,
   1,
   1,
   0,
   0,
   0,
   1,
   0,
   0,
   0,
   1,
   1,
   0,
   0,
   1,
   0,
   0,
   1,
   1,
   0,
   1,
   1,
   1,
   1,
   0,
   0,
   0,
   0,
   0,
   0,
   1,
   0,
   1,
   1,
   1,
   0,
   0,
   1,
   1,
   0,
   0,
   0,
   1,
   0,
   1,
   0,
   0,
   0,
   1,
   0,
   1,
   1,
   0,
   0,
   0,
   1,
   1,
   1,
   0,
   1,
   0,
   0,
   1,
   0,
   0,
   0,
   1,
   0,
   1,
   0,
   0,
   1,
   1,
   0,
   0,
   1,
   1,
   0,
   0,
   1,
   0,
   1,
   1,
   0,
   0,
   1,
   1,
   1,
   0,
   0,
   0,
   0,
   0,
   0,
   1,
   0,
   0,
   1,
   1,
   0,
   1,
   1,
   1,
   0,
   0,
   0,
   1,
   1,
   0,
   0,
   0,
   0,
   1,
   0,
   0,
   0,
   1,
   1,
   0,
   1,
   0,
   0,
   1,
   0,
   1,
   0,
   1,
   1,
   1,
   0,
   1,
   0,
   1,
   0,
   1,
   0,
   0,
   0,
   1,
   1,
   0,
   0,
   0,
   1,
   0,
   1,
   1,
   0,
   1,
   0,
   1,
   0,
   0,
   0,
   0,
   0,
   1,
   0,
   0,
   1,
   1,
   1,
   0,
   0,
   0,
   0,
   1,
   0,
   0,
   0,
   1,
   1,
   0,
   0,
   0,
   0,
   0,
   1,
   0,
   0,
   1,
   1,
   1,
   1,
   1,
   1,
   0,
   0,
   0,
   0,
   0,
   1,
   1,
   0,
   1,
   0,
   0,
   0,
   0,
   0,
   0,
   0,
   1,
   1,
   1,
   1,
   1,
   0,
   1,
   0,
   0,
   0,
   1,
   1,
   0,
   0,
   1,
   1,
   1,
   0,
   0,
   1,
   1,
   1,
   0,
   0,
   0,
   0,
   1,
   0,
   1,
   0,
   0,
   0,
   1,
   0,
   0,
   0,
   1,
   0,
   0,
   0,
   0,
   0,
   1,
   0,
   1,
   0,
   1,
   0,
   0,
   1,
   1,
   0,
   0,
   0,
   0,
   0,
   0,
   0,
   1,
   0,
   0,
   0,
   0,
   0,
   1,
   0,
   1,
   0,
   0,
   0,
   0,
   0,
   0,
   0,
   1,
   1,
   0,
   0,
   1,
   0,
   0,
   0,
   0,
   1,
   1,
   0,
   0,
   1,
   1,
   1,
   0,
   0,
   0,
   0,
   1,
   1,
   0,
   1,
   1,
   0,
   1,
   0,
   1,
   1,
   0,
   1,
   0,
   0,
   1,
   0,
   1,
   1,
   0,
   1,
   1,
   0,
   1,
   0,
   0,
   1,
   1,
   0,
   1,
   0,
   1,
   1,
   0,
   1,
   0,
   0,
   0,
   0,
   0,
   0,
   0,
   0,
   0,
   1,
   1,
   0,
   1,
   0,
   1,
   0,
   0,
   0,
   1,
   0,
   1,
   0,
   1,
   0,
   1,
   0,
   1,
   1,
   1,
   1,
   1,
   1,
   1,
   0,
   1,
   0,
   1,
   1,
   1,
   0,
   1,
   1,
   0,
   1,
   0,
   0,
   0,
   1,
   0,
   0,
   1,
   1,
   0,
   0,
   1,
   1,
   1,
   1,
   0,
   1,
   1,
   0,
   1,
   0,
   0,
   0,
   0,
   1,
   0,
   0,
   0,
   1,
   0,
   0,
   0,
   0,
   0,
   0,
   0,
   0,
   1,
   0,
   0,
   0,
   0,
   1,
   1,
   1,
   1,
   1,
   0,
   1],
  'learning_curve': [[0.6168196201324463,
    0.5781637717121588,
    0.562992125984252],
   [0.6062582731246948, 0.6266238505327689, 0.5918635170603674],
   [0.5932988524436951, 0.6568384177492337, 0.6299212598425197],
   [0.5708605051040649, 0.6746460370748796, 0.6548556430446194],
   [0.5520890355110168, 0.6864691285943658, 0.6706036745406824],
   [0.5329661965370178, 0.6933294409575245, 0.6824146981627297],
   [0.5152199268341064, 0.7007736096920157, 0.6916010498687664],
   [0.5002414584159851, 0.7085097066121734, 0.6955380577427821],
   [0.4899635314941406, 0.7136184498613342, 0.7086614173228346],
   [0.4819333255290985, 0.7193110494818274, 0.7099737532808399],
   [0.47489359974861145, 0.725149613195154, 0.7099737532808399],
   [0.46576157212257385, 0.7305502846299811, 0.7112860892388452],
   [0.459126740694046, 0.7337614946723107, 0.7125984251968503],
   [0.4525965750217438, 0.7390162020143045, 0.7125984251968503],
   [0.44497328996658325, 0.7419354838709677, 0.7099737532808399],
   [0.43659162521362305, 0.7445628375419647, 0.7099737532808399],
   [0.43227699398994446, 0.7468982630272953, 0.7139107611548556],
   [0.4304068684577942, 0.751131221719457, 0.7139107611548556],
   [0.42722776532173157, 0.7544883958546198, 0.7165354330708661],
   [0.42299577593803406, 0.7555101445044519, 0.7178477690288714],
   [0.4197345972061157, 0.7595971391037805, 0.7204724409448819],
   [0.41762229800224304, 0.76003503138228, 0.7204724409448819],
   [0.41408777236938477, 0.7628083491461101, 0.7217847769028871],
   [0.41080740094184875, 0.7651437746314407, 0.7191601049868767],
   [0.4065096080303192, 0.7657276310027733, 0.7178477690288714],
   [0.40253350138664246, 0.7658735950956065, 0.7204724409448819],
   [0.4005129039287567, 0.7689388410451029, 0.7217847769028871],
   [0.39972227811813354, 0.7696686615092687, 0.7178477690288714],
   [0.3988867998123169, 0.7731717997372647, 0.7152230971128609],
   [0.39898577332496643, 0.7755072252225952, 0.7152230971128609],
   [0.39970162510871887, 0.7763830097795942, 0.7165354330708661],
   [0.3994083106517792, 0.7787184352649248, 0.7191601049868767],
   [0.398284912109375, 0.7803240402860896, 0.7178477690288714],
   [0.397635817527771, 0.781491753028755, 0.7152230971128609],
   [0.3943805396556854, 0.7829513939570866, 0.7152230971128609],
   [0.3948005735874176, 0.7839731426069187, 0.7152230971128609],
   [0.3924834132194519, 0.7845569989782514, 0.7152230971128609],
   [0.3918274939060211, 0.7864545321850824, 0.7139107611548556],
   [0.39000681042671204, 0.7882061012990804, 0.7152230971128609],
   [0.38897770643234253, 0.7908334549700774, 0.7139107611548556],
   [0.3868725597858429, 0.7915632754342432, 0.7191601049868767],
   [0.38701239228248596, 0.7946285213837396, 0.7178477690288714],
   [0.38528767228126526, 0.7960881623120712, 0.7125984251968503],
   [0.3845404386520386, 0.7982776237045687, 0.7112860892388452],
   [0.38347452878952026, 0.7998832287257335, 0.7139107611548556],
   [0.38417378067970276, 0.7998832287257335, 0.7125984251968503],
   [0.3840097486972809, 0.8019267260253977, 0.7099737532808399],
   [0.38278916478157043, 0.8028025105823967, 0.7099737532808399],
   [0.3842552900314331, 0.8028025105823967, 0.7125984251968503],
   [0.38484591245651245, 0.8038242592322289, 0.7152230971128609]],
  'loss': 0.38484591245651245,
  'train_accuracy': 0.8038242592322289,
  'test_accuracy': 0.7152230971128609}}
>>> hyperp
{'use_glove': True,
 'expand_glove_vocab': True,
 'seq_len': 40,
 'vocab_size': 2000,
 'embedding_size': 50,
 'out_channels': 50,
 'num_stopwords': 0,
 'kernel_lengths': [2, 3, 4, 5],
 'strides': [1, 1, 1, 1],
 'batch_size': 24,
 'learning_rate': 0.0015,
 'dropout': 0.2,
 'num_epochs': 50}
>>> globals().update(hyperp)
>>> ls *.json
>>> title = f'seq_len={seq_len} vocab_size={vocab_size} embedding_size={embedding_size} kernel_lengths={kernel_lengths}'
>>> lc2 = pd.DataFrame(hyperparms['learning_curve'])
>>> lc2.columns = 'training_loss training_accuracy test_accuracy'.split()
>>> lc2[['training_accuracy', 'test_accuracy']].plot(linewidth=2, grid='on', xlabel='epochs', title=title)
<AxesSubplot:title={'center':'seq_len=40 vocab_size=2000 embedding_size=50 kernel_lengths=[2, 3, 4, 5]'}, xlabel='epochs'>
>>> plt.show()
>>> hyperparams['learning_curve']
>>> hyperparms['learning_curve']
[[0.6259374022483826, 0.5891110786746461, 0.6194225721784777],
 [0.572498619556427, 0.6585899868632317, 0.6286089238845144],
 [0.53183913230896, 0.6793168880455408, 0.6509186351706037],
 [0.49689456820487976, 0.6898263027295285, 0.6640419947506562],
 [0.4692693054676056, 0.7023792147131805, 0.6640419947506562],
 [0.44030293822288513, 0.7102612757261714, 0.6666666666666666],
 [0.41839009523391724, 0.7168296599036637, 0.6627296587926509],
 [0.3946485221385956, 0.7254415413808203, 0.6653543307086615],
 [0.36861589550971985, 0.7327397460224785, 0.6627296587926509],
 [0.34860774874687195, 0.7381404174573055, 0.6692913385826772],
 [0.3334750831127167, 0.7422274120566341, 0.6797900262467191],
 [0.31342318654060364, 0.7471901912129616, 0.6758530183727034],
 [0.3014084994792938, 0.7486498321412932, 0.6824146981627297],
 [0.28968751430511475, 0.7524448985549554, 0.6850393700787402],
 [0.2789236307144165, 0.7553641804116188, 0.6863517060367454],
 [0.26269322633743286, 0.7587213545467815, 0.6902887139107612],
 [0.2505471110343933, 0.7616406364034447, 0.6902887139107612],
 [0.23823508620262146, 0.7638300977959422, 0.6863517060367454],
 [0.22794361412525177, 0.7679170923952707, 0.6837270341207349],
 [0.22079095244407654, 0.7709823383447671, 0.6876640419947506],
 [0.21458177268505096, 0.7728798715515983, 0.6863517060367454],
 [0.20860357582569122, 0.7763830097795942, 0.6850393700787402],
 [0.20090197026729584, 0.777988614800759, 0.6889763779527559],
 [0.19494499266147614, 0.7807619325645891, 0.6876640419947506],
 [0.1914031058549881, 0.7844110348854182, 0.6850393700787402],
 [0.18804816901683807, 0.7863085680922494, 0.6889763779527559],
 [0.18425463140010834, 0.787768209020581, 0.6916010498687664],
 [0.17979001998901367, 0.7893738140417458, 0.6902887139107612],
 [0.17644211649894714, 0.7920011677127426, 0.6889763779527559],
 [0.1716540902853012, 0.7947744854765728, 0.6850393700787402],
 [0.16693919897079468, 0.7957962341264049, 0.6929133858267716],
 [0.16285091638565063, 0.7981316596117355, 0.6889763779527559],
 [0.15844137966632843, 0.7995913005400671, 0.6916010498687664],
 [0.15444864332675934, 0.8013428696540651, 0.6942257217847769],
 [0.15088999271392822, 0.8029484746752299, 0.6916010498687664],
 [0.14783570170402527, 0.8047000437892279, 0.6902887139107612],
 [0.14531686902046204, 0.8080572179243906, 0.6863517060367454],
 [0.14415951073169708, 0.8070354692745585, 0.6902887139107612],
 [0.14097829163074493, 0.8082031820172237, 0.6902887139107612],
 [0.14019550383090973, 0.8086410742957233, 0.6902887139107612],
 [0.13907593488693237, 0.8090789665742227, 0.6889763779527559],
 [0.13634692132472992, 0.8084951102028901, 0.6889763779527559],
 [0.13360236585140228, 0.811998248430886, 0.6863517060367454],
 [0.13133986294269562, 0.8127280688950518, 0.6889763779527559],
 [0.12807287275791168, 0.8140417457305503, 0.6889763779527559],
 [0.1264640837907791, 0.813749817544884, 0.6889763779527559],
 [0.1227492019534111, 0.8141877098233834, 0.6863517060367454],
 [0.11908814311027527, 0.8141877098233834, 0.6850393700787402],
 [0.11597103625535965, 0.8143336739162166, 0.6876640419947506],
 [0.1140332892537117, 0.816523135308714, 0.6876640419947506],
 [0.11218272894620895, 0.8178368121442126, 0.6863517060367454],
 [0.111467145383358, 0.8178368121442126, 0.6850393700787402],
 [0.1112506091594696, 0.8184206685155452, 0.6850393700787402],
 [0.11217039078474045, 0.8185666326083784, 0.6811023622047244],
 [0.11314628273248672, 0.8207560940008758, 0.6837270341207349],
 [0.11165676265954971, 0.8210480221865422, 0.6889763779527559],
 [0.1117490604519844, 0.8225076631148738, 0.6863517060367454],
 [0.11151406913995743, 0.8230915194862064, 0.6850393700787402],
 [0.11094939708709717, 0.8244051963217048, 0.6850393700787402],
 [0.11058790236711502, 0.8239673040432054, 0.6824146981627297],
 [0.10955417901277542, 0.8242592322288717, 0.6824146981627297],
 [0.11022614687681198, 0.8260108013428696, 0.6824146981627297],
 [0.11118431389331818, 0.8287841191066998, 0.6837270341207349],
 [0.11229486763477325, 0.8306816523135309, 0.6837270341207349],
 [0.11335647851228714, 0.8322872573346957, 0.6771653543307087],
 [0.11361518502235413, 0.8338928623558605, 0.6797900262467191],
 [0.11557462811470032, 0.8341847905415268, 0.6797900262467191],
 [0.11500710994005203, 0.8343307546343599, 0.6824146981627297],
 [0.11584677547216415, 0.8346226828200263, 0.6811023622047244],
 [0.11663386225700378, 0.8347686469128595, 0.6824146981627297],
 [0.11613094806671143, 0.8362282878411911, 0.6850393700787402],
 [0.11703653633594513, 0.8376879287695227, 0.6850393700787402],
 [0.11800866574048996, 0.838855641512188, 0.6863517060367454],
 [0.11805939674377441, 0.8394394978835207, 0.6889763779527559],
 [0.11940591037273407, 0.8401693183476865, 0.6902887139107612],
 [0.11960262805223465, 0.8404612465333527, 0.6916010498687664],
 [0.11915998160839081, 0.8406072106261859, 0.6902887139107612],
 [0.11931496113538742, 0.8411910669975186, 0.6929133858267716],
 [0.12117541581392288, 0.8416289592760181, 0.6916010498687664],
 [0.12069189548492432, 0.8430886002043497, 0.6902887139107612],
 [0.12134433537721634, 0.8427966720186834, 0.6863517060367454],
 [0.12273436039686203, 0.8444022770398482, 0.6889763779527559],
 [0.12293139100074768, 0.8446942052255145, 0.6889763779527559],
 [0.1248202845454216, 0.845132097504014, 0.6902887139107612],
 [0.1258859783411026, 0.8458619179681798, 0.6916010498687664],
 [0.12555809319019318, 0.847759451175011, 0.6902887139107612],
 [0.12636514008045197, 0.8480513793606773, 0.6889763779527559],
 [0.12449076771736145, 0.8483433075463436, 0.6902887139107612],
 [0.12507480382919312, 0.8492190921033426, 0.6955380577427821],
 [0.12564979493618011, 0.8502408407531747, 0.6968503937007874],
 [0.12622347474098206, 0.8503868048460079, 0.6955380577427821],
 [0.12684820592403412, 0.8500948766603416, 0.6981627296587927],
 [0.12647314369678497, 0.8500948766603416, 0.6981627296587927],
 [0.12695150077342987, 0.8521383739600058, 0.6968503937007874],
 [0.1268935352563858, 0.8521383739600058, 0.6968503937007874],
 [0.1258523315191269, 0.852284338052839, 0.6968503937007874],
 [0.1248580738902092, 0.852284338052839, 0.6981627296587927],
 [0.12372999638319016, 0.8528681944241716, 0.6968503937007874],
 [0.12391958385705948, 0.8534520507955043, 0.6955380577427821],
 [0.12302741408348083, 0.8541818712596702, 0.6968503937007874],
 [0.12275329232215881, 0.8550576558166691, 0.6955380577427821],
 [0.12391042709350586, 0.855933440373668, 0.6955380577427821],
 [0.12350936233997345, 0.8560794044665012, 0.6902887139107612],
 [0.1244114562869072, 0.8562253685593344, 0.6902887139107612],
 [0.12433343380689621, 0.8553495840023354, 0.6929133858267716],
 [0.12551288306713104, 0.856809224930667, 0.6916010498687664],
 [0.1276174932718277, 0.8569551890235002, 0.6929133858267716],
 [0.128916934132576, 0.8563713326521676, 0.6929133858267716],
 [0.12893995642662048, 0.856809224930667, 0.6955380577427821],
 [0.1295662522315979, 0.856809224930667, 0.6968503937007874],
 [0.13293081521987915, 0.8563713326521676, 0.6955380577427821],
 [0.134719118475914, 0.8571011531163334, 0.6942257217847769],
 [0.13639169931411743, 0.8579769376733324, 0.6955380577427821],
 [0.1399882286787033, 0.858560794044665, 0.6929133858267716],
 [0.14085753262043, 0.8591446504159976, 0.6902887139107612],
 [0.14182049036026, 0.8604583272514962, 0.6863517060367454],
 [0.14303551614284515, 0.8598744708801634, 0.6863517060367454],
 [0.14421547949314117, 0.860312363158663, 0.6824146981627297],
 [0.14475460350513458, 0.860312363158663, 0.6824146981627297],
 [0.14548085629940033, 0.8608962195299956, 0.6850393700787402],
 [0.14546428620815277, 0.8614800759013282, 0.6850393700787402],
 [0.1452685296535492, 0.8613341118084951, 0.6837270341207349],
 [0.14852215349674225, 0.8616260399941614, 0.6837270341207349],
 [0.1491166204214096, 0.862209896365494, 0.6837270341207349],
 [0.14965397119522095, 0.8623558604583272, 0.6824146981627297],
 [0.1503164917230606, 0.862209896365494, 0.6837270341207349],
 [0.15142513811588287, 0.8623558604583272, 0.6837270341207349],
 [0.15288692712783813, 0.862209896365494, 0.6850393700787402],
 [0.15314558148384094, 0.8619179681798278, 0.6837270341207349],
 [0.15331223607063293, 0.8620639322726609, 0.6863517060367454],
 [0.15308968722820282, 0.8627937527368268, 0.6837270341207349],
 [0.1521802842617035, 0.8641074295723252, 0.6850393700787402],
 [0.1516047865152359, 0.8643993577579915, 0.6876640419947506],
 [0.15273408591747284, 0.8645453218508247, 0.6902887139107612],
 [0.15106187760829926, 0.86571303459349, 0.6902887139107612],
 [0.1505802720785141, 0.8658589986863232, 0.6902887139107612],
 [0.15085400640964508, 0.8668807473361553, 0.6876640419947506],
 [0.15060485899448395, 0.8673186396146548, 0.6916010498687664],
 [0.14833666384220123, 0.8679024959859875, 0.6929133858267716],
 [0.14715121686458588, 0.8684863523573201, 0.6916010498687664],
 [0.14609958231449127, 0.8689242446358196, 0.6942257217847769],
 [0.14491525292396545, 0.8690702087286527, 0.6902887139107612],
 [0.14532387256622314, 0.8696540650999854, 0.6889763779527559],
 [0.144317165017128, 0.8693621369143191, 0.6876640419947506],
 [0.1436614990234375, 0.8695081010071523, 0.6863517060367454],
 [0.1414957344532013, 0.8695081010071523, 0.6876640419947506],
 [0.1396719515323639, 0.8700919573784849, 0.6863517060367454],
 [0.138599693775177, 0.8703838855641512, 0.6863517060367454],
 [0.1388465315103531, 0.8711137060283171, 0.6850393700787402],
 [0.13711129128932953, 0.8714056342139833, 0.6850393700787402],
 [0.13788489997386932, 0.8711137060283171, 0.6850393700787402],
 [0.13621674478054047, 0.871989490585316, 0.6824146981627297],
 [0.13497048616409302, 0.8727193110494819, 0.6837270341207349],
 [0.13484768569469452, 0.8724273828638155, 0.6850393700787402],
 [0.13548220694065094, 0.8718435264924829, 0.6837270341207349],
 [0.1327059268951416, 0.872865275142315, 0.678477690288714],
 [0.13338936865329742, 0.8733031674208145, 0.678477690288714],
 [0.1329735964536667, 0.8735950956064809, 0.678477690288714],
 [0.13225704431533813, 0.8734491315136477, 0.6771653543307087],
 [0.13001951575279236, 0.8737410596993139, 0.6758530183727034],
 [0.12895803153514862, 0.8741789519778135, 0.6758530183727034],
 [0.12749136984348297, 0.8754926288133119, 0.6758530183727034],
 [0.12600469589233398, 0.8757845569989783, 0.6758530183727034],
 [0.12360285967588425, 0.8768063056488103, 0.6758530183727034],
 [0.12228386849164963, 0.8768063056488103, 0.6745406824146981],
 [0.12193263322114944, 0.8768063056488103, 0.6745406824146981],
 [0.12174955010414124, 0.8769522697416435, 0.678477690288714],
 [0.11901557445526123, 0.8778280542986425, 0.6771653543307087],
 [0.11828911304473877, 0.8779740183914757, 0.6771653543307087],
 [0.11885488778352737, 0.8785578747628083, 0.6771653543307087],
 [0.11778559535741806, 0.8788498029484747, 0.678477690288714],
 [0.11658801883459091, 0.879141731134141, 0.6797900262467191],
 [0.11548890918493271, 0.8792876952269741, 0.6797900262467191],
 [0.11704497784376144, 0.8798715515983068, 0.678477690288714],
 [0.11611378192901611, 0.8806013720624727, 0.678477690288714],
 [0.11733183264732361, 0.8810392643409721, 0.6797900262467191],
 [0.11761214584112167, 0.8813311925266385, 0.6797900262467191],
 [0.1185489371418953, 0.8807473361553058, 0.6811023622047244],
 [0.1199507936835289, 0.8807473361553058, 0.6797900262467191],
 [0.11990635842084885, 0.8803094438768063, 0.678477690288714],
 [0.11955814808607101, 0.8808933002481389, 0.678477690288714],
 [0.12049592286348343, 0.8798715515983068, 0.678477690288714],
 [0.12180326133966446, 0.8804554079696395, 0.678477690288714],
 [0.12243335694074631, 0.8813311925266385, 0.6797900262467191],
 [0.12315106391906738, 0.8814771566194716, 0.6797900262467191],
 [0.12270169705152512, 0.8816231207123048, 0.678477690288714],
 [0.12235722690820694, 0.8823529411764706, 0.6824146981627297],
 [0.12402277439832687, 0.8826448693621369, 0.6824146981627297],
 [0.12300503253936768, 0.8824989052693037, 0.6824146981627297],
 [0.12303822487592697, 0.8830827616406364, 0.6837270341207349],
 [0.1235603615641594, 0.8827908334549701, 0.6837270341207349],
 [0.1233774945139885, 0.8824989052693037, 0.6824146981627297],
 [0.12410643696784973, 0.8822069770836374, 0.6837270341207349],
 [0.1239432618021965, 0.8835206539191359, 0.6824146981627297],
 [0.12322299927473068, 0.8836666180119691, 0.6837270341207349],
 [0.12208916991949081, 0.8836666180119691, 0.6824146981627297],
 [0.12331083416938782, 0.8843964384761349, 0.6811023622047244],
 [0.12283509224653244, 0.8829367975478032, 0.6824146981627297],
 [0.12238173931837082, 0.8833746898263027, 0.6797900262467191],
 [0.12163782864809036, 0.8835206539191359, 0.678477690288714],
 [0.12174157798290253, 0.8832287257334696, 0.678477690288714],
 [0.12234080582857132, 0.8830827616406364, 0.6771653543307087],
 [0.12341048568487167, 0.8832287257334696, 0.6797900262467191],
 [0.12354560196399689, 0.8838125821048022, 0.6771653543307087],
 [0.12224540114402771, 0.8843964384761349, 0.6758530183727034],
 [0.12297835946083069, 0.8851262589403007, 0.6758530183727034],
 [0.12222766131162643, 0.8842504743833017, 0.6771653543307087],
 [0.12149050831794739, 0.8842504743833017, 0.6771653543307087],
 [0.11971525847911835, 0.884542402568968, 0.6771653543307087],
 [0.11685986816883087, 0.8836666180119691, 0.6771653543307087],
 [0.116354800760746, 0.8839585461976354, 0.6745406824146981],
 [0.11518728733062744, 0.8842504743833017, 0.6732283464566929],
 [0.11208456009626389, 0.8846883666618012, 0.6745406824146981],
 [0.11124517768621445, 0.8848343307546344, 0.6745406824146981],
 [0.10939962416887283, 0.8846883666618012, 0.6732283464566929],
 [0.11193754523992538, 0.8864399357757992, 0.6745406824146981],
 [0.11157060414552689, 0.8870237921471318, 0.6745406824146981],
 [0.1131632998585701, 0.886293971682966, 0.6758530183727034],
 [0.11355380713939667, 0.8878995767041308, 0.6732283464566929],
 [0.1150955930352211, 0.888045540796964, 0.6732283464566929],
 [0.11473088711500168, 0.8883374689826302, 0.6706036745406824],
 [0.11451518535614014, 0.8886293971682966, 0.6692913385826772],
 [0.1164586991071701, 0.8878995767041308, 0.6692913385826772],
 [0.1161404401063919, 0.888921325353963, 0.6679790026246719],
 [0.1162874698638916, 0.8887753612611298, 0.6666666666666666],
 [0.11801162362098694, 0.888045540796964, 0.6706036745406824],
 [0.1173032745718956, 0.8884834330754634, 0.6666666666666666],
 [0.11605603247880936, 0.8884834330754634, 0.6666666666666666],
 [0.11531450599431992, 0.8887753612611298, 0.6679790026246719],
 [0.11506204307079315, 0.8878995767041308, 0.6706036745406824],
 [0.11631068587303162, 0.8883374689826302, 0.6692913385826772],
 [0.11430792510509491, 0.8887753612611298, 0.6706036745406824],
 [0.114576555788517, 0.8893592176324624, 0.6706036745406824],
 [0.11349448561668396, 0.889797109910962, 0.6706036745406824],
 [0.11339213699102402, 0.8892132535396292, 0.6706036745406824],
 [0.11392004042863846, 0.889797109910962, 0.6706036745406824],
 [0.11314418166875839, 0.8903809662822946, 0.6706036745406824],
 [0.11197909712791443, 0.8900890380966282, 0.6732283464566929],
 [0.11079270392656326, 0.890818858560794, 0.6732283464566929],
 [0.11128787696361542, 0.8915486790249598, 0.6758530183727034],
 [0.11163058876991272, 0.8921325353962926, 0.6758530183727034],
 [0.11112847179174423, 0.892570427674792, 0.6745406824146981],
 [0.1109260842204094, 0.8928623558604584, 0.6745406824146981],
 [0.11126334220170975, 0.8921325353962926, 0.6732283464566929],
 [0.11080717295408249, 0.8921325353962926, 0.6745406824146981],
 [0.10975110530853271, 0.8927163917676252, 0.6758530183727034],
 [0.10931424796581268, 0.892570427674792, 0.6758530183727034],
 [0.10899872332811356, 0.892570427674792, 0.6732283464566929],
 [0.11041024327278137, 0.8921325353962926, 0.6745406824146981],
 [0.10952839255332947, 0.8928623558604584, 0.6758530183727034],
 [0.1090397834777832, 0.8937381404174574, 0.6745406824146981],
 [0.10792262852191925, 0.8935921763246242, 0.6732283464566929],
 [0.1074131652712822, 0.8940300686031236, 0.6745406824146981],
 [0.10697662830352783, 0.8938841045102904, 0.6745406824146981],
 [0.10648062080144882, 0.8938841045102904, 0.6745406824146981],
 [0.1050146296620369, 0.8941760326959568, 0.678477690288714],
 [0.1035919189453125, 0.8940300686031236, 0.6719160104986877],
 [0.1013340950012207, 0.8938841045102904, 0.6706036745406824],
 [0.10345327854156494, 0.8940300686031236, 0.6745406824146981],
 [0.1029980480670929, 0.8947598890672894, 0.6706036745406824],
 [0.10302376747131348, 0.89432199678879, 0.6732283464566929],
 [0.10120727866888046, 0.8944679608816232, 0.6666666666666666],
 [0.10132627189159393, 0.895197781345789, 0.6719160104986877],
 [0.10033635795116425, 0.8947598890672894, 0.6679790026246719],
 [0.10035263746976852, 0.8944679608816232, 0.6692913385826772],
 [0.09886478632688522, 0.8956356736242884, 0.6692913385826772],
 [0.09833907335996628, 0.8949058531601226, 0.6679790026246719],
 [0.0979771539568901, 0.8947598890672894, 0.6706036745406824],
 [0.09781228750944138, 0.895197781345789, 0.6719160104986877],
 [0.09852898865938187, 0.895197781345789, 0.6692913385826772],
 [0.09778757393360138, 0.8950518172529558, 0.6719160104986877],
 [0.09776855260133743, 0.8954897095314552, 0.6719160104986877],
 [0.09840738028287888, 0.8957816377171216, 0.6719160104986877],
 [0.09878213703632355, 0.895197781345789, 0.6719160104986877],
 [0.09856272488832474, 0.8959276018099548, 0.6732283464566929],
 [0.09924983978271484, 0.8956356736242884, 0.6745406824146981],
 [0.09942004829645157, 0.8957816377171216, 0.6732283464566929],
 [0.09896726161241531, 0.8963654940884542, 0.6732283464566929],
 [0.09980059415102005, 0.8969493504597869, 0.6732283464566929],
 [0.09911493957042694, 0.8965114581812874, 0.6719160104986877],
 [0.09914491325616837, 0.8973872427382864, 0.6732283464566929],
 [0.09841728210449219, 0.8968033863669538, 0.6679790026246719],
 [0.09744410216808319, 0.8968033863669538, 0.6679790026246719],
 [0.09809164702892303, 0.89709531455262, 0.6706036745406824],
 [0.09714630246162415, 0.8966574222741206, 0.6679790026246719],
 [0.09828286617994308, 0.8965114581812874, 0.6706036745406824],
 [0.0969001054763794, 0.8972412786454532, 0.6706036745406824],
 [0.0967048704624176, 0.89709531455262, 0.6706036745406824],
 [0.09493348747491837, 0.8968033863669538, 0.6679790026246719],
 [0.09512541443109512, 0.8973872427382864, 0.6692913385826772],
 [0.09386732429265976, 0.8973872427382864, 0.6692913385826772],
 [0.09192614257335663, 0.8975332068311196, 0.6679790026246719],
 [0.09119489043951035, 0.8972412786454532, 0.6679790026246719],
 [0.09122523665428162, 0.8973872427382864, 0.6692913385826772],
 [0.09272227436304092, 0.897971099109619, 0.6640419947506562],
 [0.09176881611347198, 0.8978251350167858, 0.6666666666666666],
 [0.09264300018548965, 0.8982630272952854, 0.6666666666666666],
 [0.09273745864629745, 0.897971099109619, 0.6653543307086615],
 [0.09373707324266434, 0.8982630272952854, 0.6653543307086615],
 [0.09328442066907883, 0.898846883666618, 0.6666666666666666],
 [0.09071975946426392, 0.898846883666618, 0.6679790026246719],
 [0.0921832025051117, 0.8994307400379506, 0.6666666666666666],
 [0.09181783348321915, 0.8992847759451175, 0.6653543307086615],
 [0.09052600711584091, 0.8995767041307838, 0.6666666666666666],
 [0.09013950079679489, 0.8994307400379506, 0.6640419947506562],
 [0.08959482610225677, 0.8992847759451175, 0.6640419947506562],
 [0.08851291984319687, 0.8995767041307838, 0.6640419947506562],
 [0.08968604356050491, 0.8992847759451175, 0.6640419947506562],
 [0.08716054260730743, 0.9001605605021165, 0.6653543307086615],
 [0.08819270133972168, 0.9007444168734491, 0.6627296587926509],
 [0.08678565174341202, 0.9003065245949496, 0.6627296587926509],
 [0.08613330870866776, 0.9001605605021165, 0.6627296587926509],
 [0.08561703562736511, 0.9004524886877828, 0.6614173228346457],
 [0.0852675586938858, 0.9004524886877828, 0.6627296587926509],
 [0.08444416522979736, 0.9003065245949496, 0.6614173228346457],
 [0.08591742068529129, 0.9004524886877828, 0.6614173228346457],
 [0.08555956184864044, 0.899722668223617, 0.6587926509186351],
 [0.08629468083381653, 0.9000145964092833, 0.6587926509186351],
 [0.08680441975593567, 0.9004524886877828, 0.6587926509186351],
 [0.08642014861106873, 0.900598452780616, 0.6587926509186351],
 [0.08516581356525421, 0.9004524886877828, 0.65748031496063],
 [0.08604884147644043, 0.9011823091519486, 0.65748031496063],
 [0.08523999899625778, 0.9013282732447818, 0.65748031496063],
 [0.08461377024650574, 0.9010363450591155, 0.6587926509186351],
 [0.08524414151906967, 0.9008903809662823, 0.6601049868766404],
 [0.08456365764141083, 0.9008903809662823, 0.6601049868766404],
 [0.08465293049812317, 0.900598452780616, 0.6601049868766404],
 [0.08404279500246048, 0.9003065245949496, 0.6614173228346457],
 [0.08416647464036942, 0.8998686323164502, 0.6601049868766404],
 [0.0843980684876442, 0.8998686323164502, 0.6601049868766404],
 [0.08409326523542404, 0.900598452780616, 0.6601049868766404],
 [0.08384498953819275, 0.9001605605021165, 0.6601049868766404],
 [0.08359826356172562, 0.9003065245949496, 0.6614173228346457],
 [0.08309374749660492, 0.9001605605021165, 0.6614173228346457],
 [0.08232683688402176, 0.9003065245949496, 0.6614173228346457],
 [0.08336617052555084, 0.9003065245949496, 0.6614173228346457],
 [0.08284062147140503, 0.9004524886877828, 0.6614173228346457],
 [0.08271432667970657, 0.9010363450591155, 0.6601049868766404],
 [0.08307965099811554, 0.900598452780616, 0.6601049868766404],
 [0.08208606392145157, 0.9011823091519486, 0.6614173228346457],
 [0.08228060603141785, 0.9001605605021165, 0.6601049868766404],
 [0.08290530741214752, 0.9016202014304481, 0.6601049868766404],
 [0.08188912272453308, 0.9011823091519486, 0.6614173228346457],
 [0.0812857449054718, 0.9011823091519486, 0.6601049868766404],
 [0.08226113766431808, 0.9010363450591155, 0.6614173228346457],
 [0.08257483690977097, 0.9013282732447818, 0.6614173228346457],
 [0.08237696439027786, 0.9014742373376149, 0.65748031496063],
 [0.08251427114009857, 0.9022040578017808, 0.6601049868766404],
 [0.08082962036132812, 0.9020580937089476, 0.6601049868766404],
 [0.08225329220294952, 0.9026419500802803, 0.6601049868766404],
 [0.08305620402097702, 0.9023500218946139, 0.6587926509186351],
 [0.08289766311645508, 0.9030798423587797, 0.6587926509186351],
 [0.0825447142124176, 0.9030798423587797, 0.6601049868766404],
 [0.08437757194042206, 0.9026419500802803, 0.6587926509186351],
 [0.08287899941205978, 0.9029338782659466, 0.6614173228346457],
 [0.08300655335187912, 0.9029338782659466, 0.6587926509186351],
 [0.08237507194280624, 0.9026419500802803, 0.6601049868766404],
 [0.08091677725315094, 0.9027879141731134, 0.6601049868766404],
 [0.08251866698265076, 0.9023500218946139, 0.6614173228346457],
 [0.08086046576499939, 0.9022040578017808, 0.6601049868766404],
 [0.0804816409945488, 0.9023500218946139, 0.6614173228346457],
 [0.080595962703228, 0.9026419500802803, 0.6627296587926509],
 [0.0789780467748642, 0.9019121296161144, 0.65748031496063],
 [0.07925847172737122, 0.9023500218946139, 0.6587926509186351],
 [0.07942099124193192, 0.9023500218946139, 0.6614173228346457],
 [0.07868018001317978, 0.9027879141731134, 0.6601049868766404],
 [0.07944988459348679, 0.9024959859874471, 0.6614173228346457],
 [0.07935013622045517, 0.9023500218946139, 0.6601049868766404],
 [0.07987784594297409, 0.9029338782659466, 0.6601049868766404],
 [0.0800325945019722, 0.9027879141731134, 0.6587926509186351],
 [0.07949370890855789, 0.9024959859874471, 0.6614173228346457],
 [0.07907908409833908, 0.9022040578017808, 0.6587926509186351],
 [0.07910068333148956, 0.9026419500802803, 0.6601049868766404],
 [0.07822238653898239, 0.9023500218946139, 0.6587926509186351],
 [0.0785532221198082, 0.9022040578017808, 0.6601049868766404],
 [0.07831349223852158, 0.9026419500802803, 0.6601049868766404],
 [0.07798413932323456, 0.9026419500802803, 0.6587926509186351],
 [0.07787753641605377, 0.9026419500802803, 0.6601049868766404],
 [0.07811322063207626, 0.9029338782659466, 0.6601049868766404],
 [0.07766449451446533, 0.9023500218946139, 0.6601049868766404],
 [0.07741981744766235, 0.9020580937089476, 0.6601049868766404],
 [0.07719100266695023, 0.9022040578017808, 0.6587926509186351],
 [0.07808368653059006, 0.9019121296161144, 0.6587926509186351],
 [0.07707874476909637, 0.9022040578017808, 0.6601049868766404],
 [0.07656962424516678, 0.9027879141731134, 0.6601049868766404],
 [0.07600928097963333, 0.9024959859874471, 0.6601049868766404],
 [0.0748334676027298, 0.9030798423587797, 0.6601049868766404],
 [0.07469306141138077, 0.9035177346372792, 0.6601049868766404],
 [0.07480860501527786, 0.9036636987301124, 0.6601049868766404],
 [0.07533594965934753, 0.9035177346372792, 0.6614173228346457],
 [0.07554034888744354, 0.9036636987301124, 0.6614173228346457],
 [0.07561344653367996, 0.9033717705444461, 0.6614173228346457],
 [0.07490844279527664, 0.9038096628229455, 0.6614173228346457],
 [0.07412657141685486, 0.9043935191942782, 0.6627296587926509],
 [0.0751730352640152, 0.9036636987301124, 0.6614173228346457],
 [0.07516764104366302, 0.9039556269157787, 0.6627296587926509],
 [0.075544074177742, 0.9038096628229455, 0.6614173228346457],
 [0.07597342878580093, 0.9038096628229455, 0.6601049868766404],
 [0.07564716041088104, 0.9029338782659466, 0.6614173228346457],
 [0.07657432556152344, 0.9033717705444461, 0.6614173228346457]]
>>> more train_ch07.py
>>> more train_ch07.py
>>> hyperp['epochs'] = 25
>>> hyperp['learning_rate'] = .001
>>> hyperp['strides'] = [1, 1, 1, 1, 1, 1]
>>> hyperp['kernel_lengths'] = [1, 2, 3, 4, 5, 6]
>>> pipeline_hyperp_with_dropout = main()
>>> lc3 = pd.DataFrame(pipeline_hyperp_with_dropout['learning_curve'], columns=['training set', 'test set'])
>>> lc3 = pd.DataFrame(pipeline_hyperp_with_dropout['learning curve'], columns=['training set', 'test set'])
>>> lc3 = pd.DataFrame(pipeline_hyperp_with_dropout['hyperp']['learning_curve'], columns=['training set', 'test set'])
>>> lc3 = pd.DataFrame(pipeline_hyperp_with_dropout['hyperp']['learning_curve'], columns=['loss', 'training set', 'test set'])
>>> lc3.index.name = 'epoch'
>>> # lc3[['training set', 'test set']].plot(linewidth=2, grid='on', ylable='accuracy', xlabel='epochs', title=title)
>>> title = 'seq_len={seq_len} vocab_size={vocab_size} embedding_size={embedding_size} kernel_lengths={kernel_lengths}'.format(**pipeline_hyperp_with_dropout['hyperp'])
>>> title
'seq_len=40 vocab_size=2000 embedding_size=50 kernel_lengths=[1, 2, 3, 4, 5, 6]'
>>> lc3[['training set', 'test set']].plot(linewidth=2, grid='on', ylable='accuracy', xlabel='epochs', title=title)
>>> lc3[['training set', 'test set']].plot(linewidth=2, grid='on', ylabel='accuracy', xlabel='epochs', title=title)
<AxesSubplot:title={'center':'seq_len=40 vocab_size=2000 embedding_size=50 kernel_lengths=[1, 2, 3, 4, 5, 6]'}, xlabel='epochs', ylabel='accuracy'>
>>> plt.show()
>>> mv /home/hobs/overfit_learning_curve.png /home/hobs/code/tangibleai/nlpia-manuscript/manuscript/images/ch07/
>>> mv /home/hobs/underfit_learning_curve.png /home/hobs/code/tangibleai/nlpia-manuscript/manuscript/images/ch07/
>>> ls
>>> from pathlib import Path
>>> ls data
>>> !find /home/hobs/code/tangibleai/nlpia-manuscript/ -name 'learning*.png'
>>> !find /home/hobs/code/tangibleai/ -name 'learning*.png'
>>> !find /home/hobs/ -name 'learning*.png'
>>> ls data
>>> ls data/hyperparam-tuning/
>>> !mv disaster_tweets_cnn_pipeline_17*.json data/hyperparam-tuning/
>>> experiments = []
... for f in Path('data/hyperparam-tuning/').glob('*.json'):
...     with f.open() as fin:
...         experiments.append(json.load(fin))
...
>>> len(experiments)
24
>>> ex = experiments
>>> ex.keys()
>>> ex[0].keys()
dict_keys(['seq_len', 'usecols', 'tokenizer', 'embeddings', 'kernel_lengths', 'strides', 'conv_output_size', 'in_channels', 'planes', 'out_channels', 'groups', 'epochs', 'batch_size', 'learning_rate', 'test_size', 'dropout_portion', 'num_stopwords', 'case_sensitive', 'split_random_state', 'numpy_random_state', 'torch_random_state', 're_sub', 'vocab_size', 'embedding_size', 'learning_curve', 'loss', 'train_accuracy', 'test_accuracy'])
>>> pd.DataFrame([{k:e[k] for k in 'training_accuracy test_accuracy'.split()} for e in ex])
>>> df_exp = pd.DataFrame([{k:e[k] for k in 'train_accuracy test_accuracy'.split()} for e in ex])
>>> df_exp
    train_accuracy  test_accuracy
0         0.939133       0.783465
1         0.939863       0.725722
2         0.903372       0.661417
3         0.664866       0.628609
4         0.872719       0.790026
5         0.847176       0.753281
6         0.865275       0.770341
7         0.878850       0.649606
8         0.872573       0.746719
9         0.940739       0.729659
10        0.936652       0.761155
11        0.836520       0.761155
12        0.870968       0.758530
13        0.803824       0.715223
14        0.872719       0.790026
15        0.846008       0.665354
16        0.841921       0.728346
17        0.874325       0.750656
18        0.589403       0.594488
19        0.768501       0.751969
20        0.791855       0.709974
21        0.586484       0.590551
22        0.585170       0.601050
23        0.579040       0.545932
>>> df_exp.sort_values('test_accuracy')
    train_accuracy  test_accuracy
23        0.579040       0.545932
21        0.586484       0.590551
18        0.589403       0.594488
22        0.585170       0.601050
3         0.664866       0.628609
7         0.878850       0.649606
2         0.903372       0.661417
15        0.846008       0.665354
20        0.791855       0.709974
13        0.803824       0.715223
1         0.939863       0.725722
16        0.841921       0.728346
9         0.940739       0.729659
8         0.872573       0.746719
17        0.874325       0.750656
19        0.768501       0.751969
5         0.847176       0.753281
12        0.870968       0.758530
11        0.836520       0.761155
10        0.936652       0.761155
6         0.865275       0.770341
0         0.939133       0.783465
4         0.872719       0.790026
14        0.872719       0.790026
>>> ex[0].keys()
dict_keys(['seq_len', 'usecols', 'tokenizer', 'embeddings', 'kernel_lengths', 'strides', 'conv_output_size', 'in_channels', 'planes', 'out_channels', 'groups', 'epochs', 'batch_size', 'learning_rate', 'test_size', 'dropout_portion', 'num_stopwords', 'case_sensitive', 'split_random_state', 'numpy_random_state', 'torch_random_state', 're_sub', 'vocab_size', 'embedding_size', 'learning_curve', 'loss', 'train_accuracy', 'test_accuracy'])
>>> df_exp = pd.DataFrame([{k:e[k] for k in important_hyperparams.split()} for e in ex])
>>> important_hyperparams = 'kernel_lengths vocab_size dropout train_accuracy test_accuracy'.split()
>>> df_exp = pd.DataFrame([{k:e[k] for k in important_hyperparams.split()} for e in ex])
>>> important_hyperparams = 'kernel_lengths vocab_size dropout train_accuracy test_accuracy'.split()
>>> df_exp = pd.DataFrame([{k:e[k] for k in important_hyperparams} for e in ex])
>>> important_hyperparams = 'kernel_lengths vocab_size dropout_portion train_accuracy test_accuracy'.split()
>>> df_exp = pd.DataFrame([{k:e[k] for k in important_hyperparams} for e in ex])
>>> df_exp = pd.DataFrame([{k:e.get(k, None) for k in important_hyperparams} for e in ex])
>>> df_exp.sort_values('test_accuracy')
        kernel_lengths  vocab_size  dropout_portion  train_accuracy  test_accuracy
23                 [2]        2000              NaN        0.579040       0.545932
21                 [2]        2000              NaN        0.586484       0.590551
18                 [2]        2000              NaN        0.589403       0.594488
22                 [2]        2000              NaN        0.585170       0.601050
3                  [2]        2000              NaN        0.664866       0.628609
7         [2, 3, 4, 5]        2000              0.2        0.878850       0.649606
2   [1, 2, 3, 4, 5, 6]        2000              NaN        0.903372       0.661417
15        [2, 3, 4, 5]        2000              0.2        0.846008       0.665354
20  [1, 2, 3, 4, 5, 6]        2000              NaN        0.791855       0.709974
13        [2, 3, 4, 5]        2000              NaN        0.803824       0.715223
1         [2, 3, 4, 5]        2000              0.2        0.939863       0.725722
16        [2, 3, 4, 5]        2000              0.2        0.841921       0.728346
9         [2, 3, 4, 5]        2000              0.2        0.940739       0.729659
8         [2, 3, 4, 5]        2000              0.2        0.872573       0.746719
17        [2, 3, 4, 5]        2000              0.2        0.874325       0.750656
19  [1, 2, 3, 4, 5, 6]        2000              NaN        0.768501       0.751969
5                  [2]        2000              0.2        0.847176       0.753281
12        [2, 3, 4, 5]        2000              0.2        0.870968       0.758530
11        [2, 3, 4, 5]        2000              0.2        0.836520       0.761155
10        [2, 3, 4, 5]        2000              0.2        0.936652       0.761155
6         [2, 3, 4, 5]        2000              0.2        0.865275       0.770341
0         [2, 3, 4, 5]        2000              0.2        0.939133       0.783465
4         [2, 3, 4, 5]        2000              0.2        0.872719       0.790026
14        [2, 3, 4, 5]        2000              0.2        0.872719       0.790026
>>> ex[0].keys()
dict_keys(['seq_len', 'usecols', 'tokenizer', 'embeddings', 'kernel_lengths', 'strides', 'conv_output_size', 'in_channels', 'planes', 'out_channels', 'groups', 'epochs', 'batch_size', 'learning_rate', 'test_size', 'dropout_portion', 'num_stopwords', 'case_sensitive', 'split_random_state', 'numpy_random_state', 'torch_random_state', 're_sub', 'vocab_size', 'embedding_size', 'learning_curve', 'loss', 'train_accuracy', 'test_accuracy'])
>>> important_hyperparams = 'kernel_lengths learning_rate seq_len vocab_size dropout_portion train_accuracy test_accuracy'.split()
>>> df_exp.sort_values('test_accuracy')
        kernel_lengths  vocab_size  dropout_portion  train_accuracy  test_accuracy
23                 [2]        2000              NaN        0.579040       0.545932
21                 [2]        2000              NaN        0.586484       0.590551
18                 [2]        2000              NaN        0.589403       0.594488
22                 [2]        2000              NaN        0.585170       0.601050
3                  [2]        2000              NaN        0.664866       0.628609
7         [2, 3, 4, 5]        2000              0.2        0.878850       0.649606
2   [1, 2, 3, 4, 5, 6]        2000              NaN        0.903372       0.661417
15        [2, 3, 4, 5]        2000              0.2        0.846008       0.665354
20  [1, 2, 3, 4, 5, 6]        2000              NaN        0.791855       0.709974
13        [2, 3, 4, 5]        2000              NaN        0.803824       0.715223
1         [2, 3, 4, 5]        2000              0.2        0.939863       0.725722
16        [2, 3, 4, 5]        2000              0.2        0.841921       0.728346
9         [2, 3, 4, 5]        2000              0.2        0.940739       0.729659
8         [2, 3, 4, 5]        2000              0.2        0.872573       0.746719
17        [2, 3, 4, 5]        2000              0.2        0.874325       0.750656
19  [1, 2, 3, 4, 5, 6]        2000              NaN        0.768501       0.751969
5                  [2]        2000              0.2        0.847176       0.753281
12        [2, 3, 4, 5]        2000              0.2        0.870968       0.758530
11        [2, 3, 4, 5]        2000              0.2        0.836520       0.761155
10        [2, 3, 4, 5]        2000              0.2        0.936652       0.761155
6         [2, 3, 4, 5]        2000              0.2        0.865275       0.770341
0         [2, 3, 4, 5]        2000              0.2        0.939133       0.783465
4         [2, 3, 4, 5]        2000              0.2        0.872719       0.790026
14        [2, 3, 4, 5]        2000              0.2        0.872719       0.790026
>>> df_exp = pd.DataFrame([{k:e.get(k, None) for k in important_hyperparams} for e in ex])
>>> df_exp.sort_values('test_accuracy')
        kernel_lengths  learning_rate  seq_len  vocab_size  dropout_portion  train_accuracy  test_accuracy
23                 [2]         0.0010       32        2000              NaN        0.579040       0.545932
21                 [2]         0.0010       32        2000              NaN        0.586484       0.590551
18                 [2]         0.0010       32        2000              NaN        0.589403       0.594488
22                 [2]         0.0010       32        2000              NaN        0.585170       0.601050
3                  [2]         0.0010       32        2000              NaN        0.664866       0.628609
7         [2, 3, 4, 5]         0.0010       32        2000              0.2        0.878850       0.649606
2   [1, 2, 3, 4, 5, 6]         0.0020       40        2000              NaN        0.903372       0.661417
15        [2, 3, 4, 5]         0.0010       32        2000              0.2        0.846008       0.665354
20  [1, 2, 3, 4, 5, 6]         0.0010       40        2000              NaN        0.791855       0.709974
13        [2, 3, 4, 5]         0.0015       40        2000              NaN        0.803824       0.715223
1         [2, 3, 4, 5]         0.0010       32        2000              0.2        0.939863       0.725722
16        [2, 3, 4, 5]         0.0010       32        2000              0.2        0.841921       0.728346
9         [2, 3, 4, 5]         0.0010       32        2000              0.2        0.940739       0.729659
8         [2, 3, 4, 5]         0.0010       32        2000              0.2        0.872573       0.746719
17        [2, 3, 4, 5]         0.0010       32        2000              0.2        0.874325       0.750656
19  [1, 2, 3, 4, 5, 6]         0.0010       40        2000              NaN        0.768501       0.751969
5                  [2]         0.0010       32        2000              0.2        0.847176       0.753281
12        [2, 3, 4, 5]         0.0010       32        2000              0.2        0.870968       0.758530
11        [2, 3, 4, 5]         0.0010       32        2000              0.2        0.836520       0.761155
10        [2, 3, 4, 5]         0.0010       32        2000              0.2        0.936652       0.761155
6         [2, 3, 4, 5]         0.0010       32        2000              0.2        0.865275       0.770341
0         [2, 3, 4, 5]         0.0010       32        2000              0.2        0.939133       0.783465
4         [2, 3, 4, 5]         0.0010       32        2000              0.2        0.872719       0.790026
14        [2, 3, 4, 5]         0.0010       32        2000              0.2        0.872719       0.790026
>>> ex[0].keys()
dict_keys(['seq_len', 'usecols', 'tokenizer', 'embeddings', 'kernel_lengths', 'strides', 'conv_output_size', 'in_channels', 'planes', 'out_channels', 'groups', 'epochs', 'batch_size', 'learning_rate', 'test_size', 'dropout_portion', 'num_stopwords', 'case_sensitive', 'split_random_state', 'numpy_random_state', 'torch_random_state', 're_sub', 'vocab_size', 'embedding_size', 'learning_curve', 'loss', 'train_accuracy', 'test_accuracy'])
>>> important_hyperparams = 'kernel_lengths epochs learning_rate seq_len vocab_size dropout_portion train_accuracy test_accuracy'.split()
>>> df_exp = pd.DataFrame([{k:e.get(k, None) for k in important_hyperparams} for e in ex])
>>> df_exp.sort_values('test_accuracy')
        kernel_lengths  epochs  learning_rate  seq_len  vocab_size  dropout_portion  train_accuracy  test_accuracy
23                 [2]     NaN         0.0010       32        2000              NaN        0.579040       0.545932
21                 [2]     NaN         0.0010       32        2000              NaN        0.586484       0.590551
18                 [2]     NaN         0.0010       32        2000              NaN        0.589403       0.594488
22                 [2]     NaN         0.0010       32        2000              NaN        0.585170       0.601050
3                  [2]     NaN         0.0010       32        2000              NaN        0.664866       0.628609
7         [2, 3, 4, 5]    10.0         0.0010       32        2000              0.2        0.878850       0.649606
2   [1, 2, 3, 4, 5, 6]     NaN         0.0020       40        2000              NaN        0.903372       0.661417
15        [2, 3, 4, 5]    10.0         0.0010       32        2000              0.2        0.846008       0.665354
20  [1, 2, 3, 4, 5, 6]    25.0         0.0010       40        2000              NaN        0.791855       0.709974
13        [2, 3, 4, 5]     NaN         0.0015       40        2000              NaN        0.803824       0.715223
1         [2, 3, 4, 5]    20.0         0.0010       32        2000              0.2        0.939863       0.725722
16        [2, 3, 4, 5]    10.0         0.0010       32        2000              0.2        0.841921       0.728346
9         [2, 3, 4, 5]    20.0         0.0010       32        2000              0.2        0.940739       0.729659
8         [2, 3, 4, 5]    10.0         0.0010       32        2000              0.2        0.872573       0.746719
17        [2, 3, 4, 5]    10.0         0.0010       32        2000              0.2        0.874325       0.750656
19  [1, 2, 3, 4, 5, 6]     NaN         0.0010       40        2000              NaN        0.768501       0.751969
5                  [2]    10.0         0.0010       32        2000              0.2        0.847176       0.753281
12        [2, 3, 4, 5]    10.0         0.0010       32        2000              0.2        0.870968       0.758530
11        [2, 3, 4, 5]    10.0         0.0010       32        2000              0.2        0.836520       0.761155
10        [2, 3, 4, 5]    20.0         0.0010       32        2000              0.2        0.936652       0.761155
6         [2, 3, 4, 5]    10.0         0.0010       32        2000              0.2        0.865275       0.770341
0         [2, 3, 4, 5]    20.0         0.0010       32        2000              0.2        0.939133       0.783465
4         [2, 3, 4, 5]    10.0         0.0010       32        2000              0.2        0.872719       0.790026
14        [2, 3, 4, 5]    10.0         0.0010       32        2000              0.2        0.872719       0.790026
>>> ls *.json
>>> ls data/hyperparam-tuning/
>>> experiments = []
... for f in Path('data/hyperparam-tuning/').glob('*.json'):
...     with f.open() as fin:
...         d = json.laod(fin)
...         d['filename'] = f.name
...         experiments.append(d)
...
>>> experiments = []
... for f in Path('data/hyperparam-tuning/').glob('*.json'):
...     with f.open() as fin:
...         d = json.load(fin)
...         d['filename'] = f.name
...         experiments.append(d)
...
>>> ex = experiments
>>> ex[0].keys()
dict_keys(['seq_len', 'usecols', 'tokenizer', 'embeddings', 'kernel_lengths', 'strides', 'conv_output_size', 'in_channels', 'planes', 'out_channels', 'groups', 'epochs', 'batch_size', 'learning_rate', 'test_size', 'dropout_portion', 'num_stopwords', 'case_sensitive', 'split_random_state', 'numpy_random_state', 'torch_random_state', 're_sub', 'vocab_size', 'embedding_size', 'learning_curve', 'loss', 'train_accuracy', 'test_accuracy', 'filename'])
>>> important_hyperparams = 'split_random_state torch_random_state filename case_sensitive num_stopwords kernel_lengths epochs learning_rate seq_len vocab_size dropout_portion train_accuracy test_accuracy'.split()
>>> df_exp = pd.DataFrame([{k:e.get(k, None) for k in important_hyperparams} for e in ex])
>>> df_exp.sort_values('test_accuracy')
    split_random_state  torch_random_state                                  filename  ... dropout_portion  train_accuracy test_accuracy
23                 NaN                 NaN   disaster_tweets_cnn_pipeline_26110.json  ...             NaN        0.579040      0.545932
21                 NaN                 NaN   disaster_tweets_cnn_pipeline_26113.json  ...             NaN        0.586484      0.590551
18                 NaN                 NaN   disaster_tweets_cnn_pipeline_26105.json  ...             NaN        0.589403      0.594488
22                 NaN                 NaN   disaster_tweets_cnn_pipeline_26104.json  ...             NaN        0.585170      0.601050
3                  NaN                 NaN   disaster_tweets_cnn_pipeline_26119.json  ...             NaN        0.664866      0.628609
7            1460820.0            475689.0   disaster_tweets_cnn_pipeline_24348.json  ...             0.2        0.878850      0.649606
2                  NaN                 NaN  disaster_tweets_cnn_pipeline_172823.json  ...             NaN        0.903372      0.661417
15           1459833.0            656168.0   disaster_tweets_cnn_pipeline_24331.json  ...             0.2        0.846008      0.665354
20                 NaN                 NaN  disaster_tweets_cnn_pipeline_173147.json  ...             NaN        0.791855      0.709974
13                 NaN                 NaN  disaster_tweets_cnn_pipeline_173132.json  ...             NaN        0.803824      0.715223
1            1461049.0            841794.0   disaster_tweets_cnn_pipeline_24352.json  ...             0.2        0.939863      0.725722
16           1460013.0             48167.0   disaster_tweets_cnn_pipeline_24334.json  ...             0.2        0.841921      0.728346
9            1461348.0            945794.0   disaster_tweets_cnn_pipeline_24358.json  ...             0.2        0.940739      0.729659
8            1461523.0            968824.0   disaster_tweets_cnn_pipeline_24359.json  ...             0.2        0.872573      0.746719
17           1562131.0            186757.0   disaster_tweets_cnn_pipeline_26036.json  ...             0.2        0.874325      0.750656
19                 NaN                 NaN   disaster_tweets_cnn_pipeline_32808.json  ...             NaN        0.768501      0.751969
5            1566490.0            202925.0   disaster_tweets_cnn_pipeline_26108.json  ...             0.2        0.847176      0.753281
12           1460223.0            417554.0   disaster_tweets_cnn_pipeline_24338.json  ...             0.2        0.870968      0.758530
11           1460331.0            155048.0   disaster_tweets_cnn_pipeline_24339.json  ...             0.2        0.836520      0.761155
10           1461995.0            433994.0   disaster_tweets_cnn_pipeline_24368.json  ...             0.2        0.936652      0.761155
6            1461626.0            667307.0   disaster_tweets_cnn_pipeline_24361.json  ...             0.2        0.865275      0.770341
0            1460940.0            433994.0   disaster_tweets_cnn_pipeline_24365.json  ...             0.2        0.939133      0.783465
4            1460940.0            433994.0   disaster_tweets_cnn_pipeline_24350.json  ...             0.2        0.872719      0.790026
14           1460940.0            433994.0   disaster_tweets_cnn_pipeline_24363.json  ...             0.2        0.872719      0.790026

[24 rows x 13 columns]
>>> pd.options.display.float_format = '{:05.4f}'.format
>>> pd.options.display.max_columns = 20
>>> df_exp.sort_values('test_accuracy')
    split_random_state  torch_random_state  \
23               00nan               00nan   
21               00nan               00nan   
18               00nan               00nan   
22               00nan               00nan   
3                00nan               00nan   
7         1460820.0000         475689.0000   
2                00nan               00nan   
15        1459833.0000         656168.0000   
20               00nan               00nan   
13               00nan               00nan   
1         1461049.0000         841794.0000   
16        1460013.0000          48167.0000   
9         1461348.0000         945794.0000   
8         1461523.0000         968824.0000   
17        1562131.0000         186757.0000   
19               00nan               00nan   
5         1566490.0000         202925.0000   
12        1460223.0000         417554.0000   
11        1460331.0000         155048.0000   
10        1461995.0000         433994.0000   
6         1461626.0000         667307.0000   
0         1460940.0000         433994.0000   
4         1460940.0000         433994.0000   
14        1460940.0000         433994.0000   

                                    filename case_sensitive  num_stopwords  \
23   disaster_tweets_cnn_pipeline_26110.json           None              0   
21   disaster_tweets_cnn_pipeline_26113.json           None              0   
18   disaster_tweets_cnn_pipeline_26105.json           None              0   
22   disaster_tweets_cnn_pipeline_26104.json           None              0   
3    disaster_tweets_cnn_pipeline_26119.json           None              0   
7    disaster_tweets_cnn_pipeline_24348.json           True              0   
2   disaster_tweets_cnn_pipeline_172823.json           None              0   
15   disaster_tweets_cnn_pipeline_24331.json           True              0   
20  disaster_tweets_cnn_pipeline_173147.json           None              0   
13  disaster_tweets_cnn_pipeline_173132.json           None              0   
1    disaster_tweets_cnn_pipeline_24352.json           True              0   
16   disaster_tweets_cnn_pipeline_24334.json           True              0   
9    disaster_tweets_cnn_pipeline_24358.json           True              0   
8    disaster_tweets_cnn_pipeline_24359.json           True              0   
17   disaster_tweets_cnn_pipeline_26036.json           True              0   
19   disaster_tweets_cnn_pipeline_32808.json           None              0   
5    disaster_tweets_cnn_pipeline_26108.json           True              0   
12   disaster_tweets_cnn_pipeline_24338.json           True              0   
11   disaster_tweets_cnn_pipeline_24339.json           True              0   
10   disaster_tweets_cnn_pipeline_24368.json           True              0   
6    disaster_tweets_cnn_pipeline_24361.json           True              0   
0    disaster_tweets_cnn_pipeline_24365.json           True              0   
4    disaster_tweets_cnn_pipeline_24350.json           True              0   
14   disaster_tweets_cnn_pipeline_24363.json           True              0   

        kernel_lengths  epochs  learning_rate  seq_len  vocab_size  \
23                 [2]   00nan         0.0010       32        2000   
21                 [2]   00nan         0.0010       32        2000   
18                 [2]   00nan         0.0010       32        2000   
22                 [2]   00nan         0.0010       32        2000   
3                  [2]   00nan         0.0010       32        2000   
7         [2, 3, 4, 5] 10.0000         0.0010       32        2000   
2   [1, 2, 3, 4, 5, 6]   00nan         0.0020       40        2000   
15        [2, 3, 4, 5] 10.0000         0.0010       32        2000   
20  [1, 2, 3, 4, 5, 6] 25.0000         0.0010       40        2000   
13        [2, 3, 4, 5]   00nan         0.0015       40        2000   
1         [2, 3, 4, 5] 20.0000         0.0010       32        2000   
16        [2, 3, 4, 5] 10.0000         0.0010       32        2000   
9         [2, 3, 4, 5] 20.0000         0.0010       32        2000   
8         [2, 3, 4, 5] 10.0000         0.0010       32        2000   
17        [2, 3, 4, 5] 10.0000         0.0010       32        2000   
19  [1, 2, 3, 4, 5, 6]   00nan         0.0010       40        2000   
5                  [2] 10.0000         0.0010       32        2000   
12        [2, 3, 4, 5] 10.0000         0.0010       32        2000   
11        [2, 3, 4, 5] 10.0000         0.0010       32        2000   
10        [2, 3, 4, 5] 20.0000         0.0010       32        2000   
6         [2, 3, 4, 5] 10.0000         0.0010       32        2000   
0         [2, 3, 4, 5] 20.0000         0.0010       32        2000   
4         [2, 3, 4, 5] 10.0000         0.0010       32        2000   
14        [2, 3, 4, 5] 10.0000         0.0010       32        2000   

    dropout_portion  train_accuracy  test_accuracy  
23            00nan          0.5790         0.5459  
21            00nan          0.5865         0.5906  
18            00nan          0.5894         0.5945  
22            00nan          0.5852         0.6010  
3             00nan          0.6649         0.6286  
7            0.2000          0.8788         0.6496  
2             00nan          0.9034         0.6614  
15           0.2000          0.8460         0.6654  
20            00nan          0.7919         0.7100  
13            00nan          0.8038         0.7152  
1            0.2000          0.9399         0.7257  
16           0.2000          0.8419         0.7283  
9            0.2000          0.9407         0.7297  
8            0.2000          0.8726         0.7467  
17           0.2000          0.8743         0.7507  
19            00nan          0.7685         0.7520  
5            0.2000          0.8472         0.7533  
12           0.2000          0.8710         0.7585  
11           0.2000          0.8365         0.7612  
10           0.2000          0.9367         0.7612  
6            0.2000          0.8653         0.7703  
0            0.2000          0.9391         0.7835  
4            0.2000          0.8727         0.7900  
14           0.2000          0.8727         0.7900  
>>> pd.options.display.float_format = '{: 5.4f}'.format
>>> df_exp.sort_values('test_accuracy')
    split_random_state  torch_random_state  \
23                 nan                 nan   
21                 nan                 nan   
18                 nan                 nan   
22                 nan                 nan   
3                  nan                 nan   
7         1460820.0000         475689.0000   
2                  nan                 nan   
15        1459833.0000         656168.0000   
20                 nan                 nan   
13                 nan                 nan   
1         1461049.0000         841794.0000   
16        1460013.0000          48167.0000   
9         1461348.0000         945794.0000   
8         1461523.0000         968824.0000   
17        1562131.0000         186757.0000   
19                 nan                 nan   
5         1566490.0000         202925.0000   
12        1460223.0000         417554.0000   
11        1460331.0000         155048.0000   
10        1461995.0000         433994.0000   
6         1461626.0000         667307.0000   
0         1460940.0000         433994.0000   
4         1460940.0000         433994.0000   
14        1460940.0000         433994.0000   

                                    filename case_sensitive  num_stopwords  \
23   disaster_tweets_cnn_pipeline_26110.json           None              0   
21   disaster_tweets_cnn_pipeline_26113.json           None              0   
18   disaster_tweets_cnn_pipeline_26105.json           None              0   
22   disaster_tweets_cnn_pipeline_26104.json           None              0   
3    disaster_tweets_cnn_pipeline_26119.json           None              0   
7    disaster_tweets_cnn_pipeline_24348.json           True              0   
2   disaster_tweets_cnn_pipeline_172823.json           None              0   
15   disaster_tweets_cnn_pipeline_24331.json           True              0   
20  disaster_tweets_cnn_pipeline_173147.json           None              0   
13  disaster_tweets_cnn_pipeline_173132.json           None              0   
1    disaster_tweets_cnn_pipeline_24352.json           True              0   
16   disaster_tweets_cnn_pipeline_24334.json           True              0   
9    disaster_tweets_cnn_pipeline_24358.json           True              0   
8    disaster_tweets_cnn_pipeline_24359.json           True              0   
17   disaster_tweets_cnn_pipeline_26036.json           True              0   
19   disaster_tweets_cnn_pipeline_32808.json           None              0   
5    disaster_tweets_cnn_pipeline_26108.json           True              0   
12   disaster_tweets_cnn_pipeline_24338.json           True              0   
11   disaster_tweets_cnn_pipeline_24339.json           True              0   
10   disaster_tweets_cnn_pipeline_24368.json           True              0   
6    disaster_tweets_cnn_pipeline_24361.json           True              0   
0    disaster_tweets_cnn_pipeline_24365.json           True              0   
4    disaster_tweets_cnn_pipeline_24350.json           True              0   
14   disaster_tweets_cnn_pipeline_24363.json           True              0   

        kernel_lengths   epochs  learning_rate  seq_len  vocab_size  \
23                 [2]      nan         0.0010       32        2000   
21                 [2]      nan         0.0010       32        2000   
18                 [2]      nan         0.0010       32        2000   
22                 [2]      nan         0.0010       32        2000   
3                  [2]      nan         0.0010       32        2000   
7         [2, 3, 4, 5]  10.0000         0.0010       32        2000   
2   [1, 2, 3, 4, 5, 6]      nan         0.0020       40        2000   
15        [2, 3, 4, 5]  10.0000         0.0010       32        2000   
20  [1, 2, 3, 4, 5, 6]  25.0000         0.0010       40        2000   
13        [2, 3, 4, 5]      nan         0.0015       40        2000   
1         [2, 3, 4, 5]  20.0000         0.0010       32        2000   
16        [2, 3, 4, 5]  10.0000         0.0010       32        2000   
9         [2, 3, 4, 5]  20.0000         0.0010       32        2000   
8         [2, 3, 4, 5]  10.0000         0.0010       32        2000   
17        [2, 3, 4, 5]  10.0000         0.0010       32        2000   
19  [1, 2, 3, 4, 5, 6]      nan         0.0010       40        2000   
5                  [2]  10.0000         0.0010       32        2000   
12        [2, 3, 4, 5]  10.0000         0.0010       32        2000   
11        [2, 3, 4, 5]  10.0000         0.0010       32        2000   
10        [2, 3, 4, 5]  20.0000         0.0010       32        2000   
6         [2, 3, 4, 5]  10.0000         0.0010       32        2000   
0         [2, 3, 4, 5]  20.0000         0.0010       32        2000   
4         [2, 3, 4, 5]  10.0000         0.0010       32        2000   
14        [2, 3, 4, 5]  10.0000         0.0010       32        2000   

    dropout_portion  train_accuracy  test_accuracy  
23              nan          0.5790         0.5459  
21              nan          0.5865         0.5906  
18              nan          0.5894         0.5945  
22              nan          0.5852         0.6010  
3               nan          0.6649         0.6286  
7            0.2000          0.8788         0.6496  
2               nan          0.9034         0.6614  
15           0.2000          0.8460         0.6654  
20              nan          0.7919         0.7100  
13              nan          0.8038         0.7152  
1            0.2000          0.9399         0.7257  
16           0.2000          0.8419         0.7283  
9            0.2000          0.9407         0.7297  
8            0.2000          0.8726         0.7467  
17           0.2000          0.8743         0.7507  
19              nan          0.7685         0.7520  
5            0.2000          0.8472         0.7533  
12           0.2000          0.8710         0.7585  
11           0.2000          0.8365         0.7612  
10           0.2000          0.9367         0.7612  
6            0.2000          0.8653         0.7703  
0            0.2000          0.9391         0.7835  
4            0.2000          0.8727         0.7900  
14           0.2000          0.8727         0.7900  
>>> df_exp.fillna(-1)
    split_random_state  torch_random_state  \
0         1460940.0000         433994.0000   
1         1461049.0000         841794.0000   
2              -1.0000             -1.0000   
3              -1.0000             -1.0000   
4         1460940.0000         433994.0000   
5         1566490.0000         202925.0000   
6         1461626.0000         667307.0000   
7         1460820.0000         475689.0000   
8         1461523.0000         968824.0000   
9         1461348.0000         945794.0000   
10        1461995.0000         433994.0000   
11        1460331.0000         155048.0000   
12        1460223.0000         417554.0000   
13             -1.0000             -1.0000   
14        1460940.0000         433994.0000   
15        1459833.0000         656168.0000   
16        1460013.0000          48167.0000   
17        1562131.0000         186757.0000   
18             -1.0000             -1.0000   
19             -1.0000             -1.0000   
20             -1.0000             -1.0000   
21             -1.0000             -1.0000   
22             -1.0000             -1.0000   
23             -1.0000             -1.0000   

                                    filename case_sensitive  num_stopwords  \
0    disaster_tweets_cnn_pipeline_24365.json           True              0   
1    disaster_tweets_cnn_pipeline_24352.json           True              0   
2   disaster_tweets_cnn_pipeline_172823.json             -1              0   
3    disaster_tweets_cnn_pipeline_26119.json             -1              0   
4    disaster_tweets_cnn_pipeline_24350.json           True              0   
5    disaster_tweets_cnn_pipeline_26108.json           True              0   
6    disaster_tweets_cnn_pipeline_24361.json           True              0   
7    disaster_tweets_cnn_pipeline_24348.json           True              0   
8    disaster_tweets_cnn_pipeline_24359.json           True              0   
9    disaster_tweets_cnn_pipeline_24358.json           True              0   
10   disaster_tweets_cnn_pipeline_24368.json           True              0   
11   disaster_tweets_cnn_pipeline_24339.json           True              0   
12   disaster_tweets_cnn_pipeline_24338.json           True              0   
13  disaster_tweets_cnn_pipeline_173132.json             -1              0   
14   disaster_tweets_cnn_pipeline_24363.json           True              0   
15   disaster_tweets_cnn_pipeline_24331.json           True              0   
16   disaster_tweets_cnn_pipeline_24334.json           True              0   
17   disaster_tweets_cnn_pipeline_26036.json           True              0   
18   disaster_tweets_cnn_pipeline_26105.json             -1              0   
19   disaster_tweets_cnn_pipeline_32808.json             -1              0   
20  disaster_tweets_cnn_pipeline_173147.json             -1              0   
21   disaster_tweets_cnn_pipeline_26113.json             -1              0   
22   disaster_tweets_cnn_pipeline_26104.json             -1              0   
23   disaster_tweets_cnn_pipeline_26110.json             -1              0   

        kernel_lengths   epochs  learning_rate  seq_len  vocab_size  \
0         [2, 3, 4, 5]  20.0000         0.0010       32        2000   
1         [2, 3, 4, 5]  20.0000         0.0010       32        2000   
2   [1, 2, 3, 4, 5, 6]  -1.0000         0.0020       40        2000   
3                  [2]  -1.0000         0.0010       32        2000   
4         [2, 3, 4, 5]  10.0000         0.0010       32        2000   
5                  [2]  10.0000         0.0010       32        2000   
6         [2, 3, 4, 5]  10.0000         0.0010       32        2000   
7         [2, 3, 4, 5]  10.0000         0.0010       32        2000   
8         [2, 3, 4, 5]  10.0000         0.0010       32        2000   
9         [2, 3, 4, 5]  20.0000         0.0010       32        2000   
10        [2, 3, 4, 5]  20.0000         0.0010       32        2000   
11        [2, 3, 4, 5]  10.0000         0.0010       32        2000   
12        [2, 3, 4, 5]  10.0000         0.0010       32        2000   
13        [2, 3, 4, 5]  -1.0000         0.0015       40        2000   
14        [2, 3, 4, 5]  10.0000         0.0010       32        2000   
15        [2, 3, 4, 5]  10.0000         0.0010       32        2000   
16        [2, 3, 4, 5]  10.0000         0.0010       32        2000   
17        [2, 3, 4, 5]  10.0000         0.0010       32        2000   
18                 [2]  -1.0000         0.0010       32        2000   
19  [1, 2, 3, 4, 5, 6]  -1.0000         0.0010       40        2000   
20  [1, 2, 3, 4, 5, 6]  25.0000         0.0010       40        2000   
21                 [2]  -1.0000         0.0010       32        2000   
22                 [2]  -1.0000         0.0010       32        2000   
23                 [2]  -1.0000         0.0010       32        2000   

    dropout_portion  train_accuracy  test_accuracy  
0            0.2000          0.9391         0.7835  
1            0.2000          0.9399         0.7257  
2           -1.0000          0.9034         0.6614  
3           -1.0000          0.6649         0.6286  
4            0.2000          0.8727         0.7900  
5            0.2000          0.8472         0.7533  
6            0.2000          0.8653         0.7703  
7            0.2000          0.8788         0.6496  
8            0.2000          0.8726         0.7467  
9            0.2000          0.9407         0.7297  
10           0.2000          0.9367         0.7612  
11           0.2000          0.8365         0.7612  
12           0.2000          0.8710         0.7585  
13          -1.0000          0.8038         0.7152  
14           0.2000          0.8727         0.7900  
15           0.2000          0.8460         0.6654  
16           0.2000          0.8419         0.7283  
17           0.2000          0.8743         0.7507  
18          -1.0000          0.5894         0.5945  
19          -1.0000          0.7685         0.7520  
20          -1.0000          0.7919         0.7100  
21          -1.0000          0.5865         0.5906  
22          -1.0000          0.5852         0.6010  
23          -1.0000          0.5790         0.5459  
>>> df_exp.fillna('')
   split_random_state torch_random_state  \
0        1460940.0000        433994.0000   
1        1461049.0000        841794.0000   
2                                          
3                                          
4        1460940.0000        433994.0000   
5        1566490.0000        202925.0000   
6        1461626.0000        667307.0000   
7        1460820.0000        475689.0000   
8        1461523.0000        968824.0000   
9        1461348.0000        945794.0000   
10       1461995.0000        433994.0000   
11       1460331.0000        155048.0000   
12       1460223.0000        417554.0000   
13                                         
14       1460940.0000        433994.0000   
15       1459833.0000        656168.0000   
16       1460013.0000         48167.0000   
17       1562131.0000        186757.0000   
18                                         
19                                         
20                                         
21                                         
22                                         
23                                         

                                    filename case_sensitive  num_stopwords  \
0    disaster_tweets_cnn_pipeline_24365.json           True              0   
1    disaster_tweets_cnn_pipeline_24352.json           True              0   
2   disaster_tweets_cnn_pipeline_172823.json                             0   
3    disaster_tweets_cnn_pipeline_26119.json                             0   
4    disaster_tweets_cnn_pipeline_24350.json           True              0   
5    disaster_tweets_cnn_pipeline_26108.json           True              0   
6    disaster_tweets_cnn_pipeline_24361.json           True              0   
7    disaster_tweets_cnn_pipeline_24348.json           True              0   
8    disaster_tweets_cnn_pipeline_24359.json           True              0   
9    disaster_tweets_cnn_pipeline_24358.json           True              0   
10   disaster_tweets_cnn_pipeline_24368.json           True              0   
11   disaster_tweets_cnn_pipeline_24339.json           True              0   
12   disaster_tweets_cnn_pipeline_24338.json           True              0   
13  disaster_tweets_cnn_pipeline_173132.json                             0   
14   disaster_tweets_cnn_pipeline_24363.json           True              0   
15   disaster_tweets_cnn_pipeline_24331.json           True              0   
16   disaster_tweets_cnn_pipeline_24334.json           True              0   
17   disaster_tweets_cnn_pipeline_26036.json           True              0   
18   disaster_tweets_cnn_pipeline_26105.json                             0   
19   disaster_tweets_cnn_pipeline_32808.json                             0   
20  disaster_tweets_cnn_pipeline_173147.json                             0   
21   disaster_tweets_cnn_pipeline_26113.json                             0   
22   disaster_tweets_cnn_pipeline_26104.json                             0   
23   disaster_tweets_cnn_pipeline_26110.json                             0   

        kernel_lengths   epochs  learning_rate  seq_len  vocab_size  \
0         [2, 3, 4, 5]  20.0000         0.0010       32        2000   
1         [2, 3, 4, 5]  20.0000         0.0010       32        2000   
2   [1, 2, 3, 4, 5, 6]                  0.0020       40        2000   
3                  [2]                  0.0010       32        2000   
4         [2, 3, 4, 5]  10.0000         0.0010       32        2000   
5                  [2]  10.0000         0.0010       32        2000   
6         [2, 3, 4, 5]  10.0000         0.0010       32        2000   
7         [2, 3, 4, 5]  10.0000         0.0010       32        2000   
8         [2, 3, 4, 5]  10.0000         0.0010       32        2000   
9         [2, 3, 4, 5]  20.0000         0.0010       32        2000   
10        [2, 3, 4, 5]  20.0000         0.0010       32        2000   
11        [2, 3, 4, 5]  10.0000         0.0010       32        2000   
12        [2, 3, 4, 5]  10.0000         0.0010       32        2000   
13        [2, 3, 4, 5]                  0.0015       40        2000   
14        [2, 3, 4, 5]  10.0000         0.0010       32        2000   
15        [2, 3, 4, 5]  10.0000         0.0010       32        2000   
16        [2, 3, 4, 5]  10.0000         0.0010       32        2000   
17        [2, 3, 4, 5]  10.0000         0.0010       32        2000   
18                 [2]                  0.0010       32        2000   
19  [1, 2, 3, 4, 5, 6]                  0.0010       40        2000   
20  [1, 2, 3, 4, 5, 6]  25.0000         0.0010       40        2000   
21                 [2]                  0.0010       32        2000   
22                 [2]                  0.0010       32        2000   
23                 [2]                  0.0010       32        2000   

   dropout_portion  train_accuracy  test_accuracy  
0           0.2000          0.9391         0.7835  
1           0.2000          0.9399         0.7257  
2                           0.9034         0.6614  
3                           0.6649         0.6286  
4           0.2000          0.8727         0.7900  
5           0.2000          0.8472         0.7533  
6           0.2000          0.8653         0.7703  
7           0.2000          0.8788         0.6496  
8           0.2000          0.8726         0.7467  
9           0.2000          0.9407         0.7297  
10          0.2000          0.9367         0.7612  
11          0.2000          0.8365         0.7612  
12          0.2000          0.8710         0.7585  
13                          0.8038         0.7152  
14          0.2000          0.8727         0.7900  
15          0.2000          0.8460         0.6654  
16          0.2000          0.8419         0.7283  
17          0.2000          0.8743         0.7507  
18                          0.5894         0.5945  
19                          0.7685         0.7520  
20                          0.7919         0.7100  
21                          0.5865         0.5906  
22                          0.5852         0.6010  
23                          0.5790         0.5459  
>>> df_exp.sort_values('test_accuracy')
    split_random_state  torch_random_state  \
23                 nan                 nan   
21                 nan                 nan   
18                 nan                 nan   
22                 nan                 nan   
3                  nan                 nan   
7         1460820.0000         475689.0000   
2                  nan                 nan   
15        1459833.0000         656168.0000   
20                 nan                 nan   
13                 nan                 nan   
1         1461049.0000         841794.0000   
16        1460013.0000          48167.0000   
9         1461348.0000         945794.0000   
8         1461523.0000         968824.0000   
17        1562131.0000         186757.0000   
19                 nan                 nan   
5         1566490.0000         202925.0000   
12        1460223.0000         417554.0000   
11        1460331.0000         155048.0000   
10        1461995.0000         433994.0000   
6         1461626.0000         667307.0000   
0         1460940.0000         433994.0000   
4         1460940.0000         433994.0000   
14        1460940.0000         433994.0000   

                                    filename case_sensitive  num_stopwords  \
23   disaster_tweets_cnn_pipeline_26110.json           None              0   
21   disaster_tweets_cnn_pipeline_26113.json           None              0   
18   disaster_tweets_cnn_pipeline_26105.json           None              0   
22   disaster_tweets_cnn_pipeline_26104.json           None              0   
3    disaster_tweets_cnn_pipeline_26119.json           None              0   
7    disaster_tweets_cnn_pipeline_24348.json           True              0   
2   disaster_tweets_cnn_pipeline_172823.json           None              0   
15   disaster_tweets_cnn_pipeline_24331.json           True              0   
20  disaster_tweets_cnn_pipeline_173147.json           None              0   
13  disaster_tweets_cnn_pipeline_173132.json           None              0   
1    disaster_tweets_cnn_pipeline_24352.json           True              0   
16   disaster_tweets_cnn_pipeline_24334.json           True              0   
9    disaster_tweets_cnn_pipeline_24358.json           True              0   
8    disaster_tweets_cnn_pipeline_24359.json           True              0   
17   disaster_tweets_cnn_pipeline_26036.json           True              0   
19   disaster_tweets_cnn_pipeline_32808.json           None              0   
5    disaster_tweets_cnn_pipeline_26108.json           True              0   
12   disaster_tweets_cnn_pipeline_24338.json           True              0   
11   disaster_tweets_cnn_pipeline_24339.json           True              0   
10   disaster_tweets_cnn_pipeline_24368.json           True              0   
6    disaster_tweets_cnn_pipeline_24361.json           True              0   
0    disaster_tweets_cnn_pipeline_24365.json           True              0   
4    disaster_tweets_cnn_pipeline_24350.json           True              0   
14   disaster_tweets_cnn_pipeline_24363.json           True              0   

        kernel_lengths   epochs  learning_rate  seq_len  vocab_size  \
23                 [2]      nan         0.0010       32        2000   
21                 [2]      nan         0.0010       32        2000   
18                 [2]      nan         0.0010       32        2000   
22                 [2]      nan         0.0010       32        2000   
3                  [2]      nan         0.0010       32        2000   
7         [2, 3, 4, 5]  10.0000         0.0010       32        2000   
2   [1, 2, 3, 4, 5, 6]      nan         0.0020       40        2000   
15        [2, 3, 4, 5]  10.0000         0.0010       32        2000   
20  [1, 2, 3, 4, 5, 6]  25.0000         0.0010       40        2000   
13        [2, 3, 4, 5]      nan         0.0015       40        2000   
1         [2, 3, 4, 5]  20.0000         0.0010       32        2000   
16        [2, 3, 4, 5]  10.0000         0.0010       32        2000   
9         [2, 3, 4, 5]  20.0000         0.0010       32        2000   
8         [2, 3, 4, 5]  10.0000         0.0010       32        2000   
17        [2, 3, 4, 5]  10.0000         0.0010       32        2000   
19  [1, 2, 3, 4, 5, 6]      nan         0.0010       40        2000   
5                  [2]  10.0000         0.0010       32        2000   
12        [2, 3, 4, 5]  10.0000         0.0010       32        2000   
11        [2, 3, 4, 5]  10.0000         0.0010       32        2000   
10        [2, 3, 4, 5]  20.0000         0.0010       32        2000   
6         [2, 3, 4, 5]  10.0000         0.0010       32        2000   
0         [2, 3, 4, 5]  20.0000         0.0010       32        2000   
4         [2, 3, 4, 5]  10.0000         0.0010       32        2000   
14        [2, 3, 4, 5]  10.0000         0.0010       32        2000   

    dropout_portion  train_accuracy  test_accuracy  
23              nan          0.5790         0.5459  
21              nan          0.5865         0.5906  
18              nan          0.5894         0.5945  
22              nan          0.5852         0.6010  
3               nan          0.6649         0.6286  
7            0.2000          0.8788         0.6496  
2               nan          0.9034         0.6614  
15           0.2000          0.8460         0.6654  
20              nan          0.7919         0.7100  
13              nan          0.8038         0.7152  
1            0.2000          0.9399         0.7257  
16           0.2000          0.8419         0.7283  
9            0.2000          0.9407         0.7297  
8            0.2000          0.8726         0.7467  
17           0.2000          0.8743         0.7507  
19              nan          0.7685         0.7520  
5            0.2000          0.8472         0.7533  
12           0.2000          0.8710         0.7585  
11           0.2000          0.8365         0.7612  
10           0.2000          0.9367         0.7612  
6            0.2000          0.8653         0.7703  
0            0.2000          0.9391         0.7835  
4            0.2000          0.8727         0.7900  
14           0.2000          0.8727         0.7900  
>>> ['\n'.join(c.split('_')) for c in ex.columns]
>>> ['\n'.join(c.split('_')) for c in df_ex.columns]
>>> ['\n'.join(c.split('_')) for c in df_exp.columns]
['split\nrandom\nstate',
 'torch\nrandom\nstate',
 'filename',
 'case\nsensitive',
 'num\nstopwords',
 'kernel\nlengths',
 'epochs',
 'learning\nrate',
 'seq\nlen',
 'vocab\nsize',
 'dropout\nportion',
 'train\naccuracy',
 'test\naccuracy']
>>> df_exp.sort_values('test_accuracy')
    split_random_state  torch_random_state  \
23                 nan                 nan   
21                 nan                 nan   
18                 nan                 nan   
22                 nan                 nan   
3                  nan                 nan   
7         1460820.0000         475689.0000   
2                  nan                 nan   
15        1459833.0000         656168.0000   
20                 nan                 nan   
13                 nan                 nan   
1         1461049.0000         841794.0000   
16        1460013.0000          48167.0000   
9         1461348.0000         945794.0000   
8         1461523.0000         968824.0000   
17        1562131.0000         186757.0000   
19                 nan                 nan   
5         1566490.0000         202925.0000   
12        1460223.0000         417554.0000   
11        1460331.0000         155048.0000   
10        1461995.0000         433994.0000   
6         1461626.0000         667307.0000   
0         1460940.0000         433994.0000   
4         1460940.0000         433994.0000   
14        1460940.0000         433994.0000   

                                    filename case_sensitive  num_stopwords  \
23   disaster_tweets_cnn_pipeline_26110.json           None              0   
21   disaster_tweets_cnn_pipeline_26113.json           None              0   
18   disaster_tweets_cnn_pipeline_26105.json           None              0   
22   disaster_tweets_cnn_pipeline_26104.json           None              0   
3    disaster_tweets_cnn_pipeline_26119.json           None              0   
7    disaster_tweets_cnn_pipeline_24348.json           True              0   
2   disaster_tweets_cnn_pipeline_172823.json           None              0   
15   disaster_tweets_cnn_pipeline_24331.json           True              0   
20  disaster_tweets_cnn_pipeline_173147.json           None              0   
13  disaster_tweets_cnn_pipeline_173132.json           None              0   
1    disaster_tweets_cnn_pipeline_24352.json           True              0   
16   disaster_tweets_cnn_pipeline_24334.json           True              0   
9    disaster_tweets_cnn_pipeline_24358.json           True              0   
8    disaster_tweets_cnn_pipeline_24359.json           True              0   
17   disaster_tweets_cnn_pipeline_26036.json           True              0   
19   disaster_tweets_cnn_pipeline_32808.json           None              0   
5    disaster_tweets_cnn_pipeline_26108.json           True              0   
12   disaster_tweets_cnn_pipeline_24338.json           True              0   
11   disaster_tweets_cnn_pipeline_24339.json           True              0   
10   disaster_tweets_cnn_pipeline_24368.json           True              0   
6    disaster_tweets_cnn_pipeline_24361.json           True              0   
0    disaster_tweets_cnn_pipeline_24365.json           True              0   
4    disaster_tweets_cnn_pipeline_24350.json           True              0   
14   disaster_tweets_cnn_pipeline_24363.json           True              0   

        kernel_lengths   epochs  learning_rate  seq_len  vocab_size  \
23                 [2]      nan         0.0010       32        2000   
21                 [2]      nan         0.0010       32        2000   
18                 [2]      nan         0.0010       32        2000   
22                 [2]      nan         0.0010       32        2000   
3                  [2]      nan         0.0010       32        2000   
7         [2, 3, 4, 5]  10.0000         0.0010       32        2000   
2   [1, 2, 3, 4, 5, 6]      nan         0.0020       40        2000   
15        [2, 3, 4, 5]  10.0000         0.0010       32        2000   
20  [1, 2, 3, 4, 5, 6]  25.0000         0.0010       40        2000   
13        [2, 3, 4, 5]      nan         0.0015       40        2000   
1         [2, 3, 4, 5]  20.0000         0.0010       32        2000   
16        [2, 3, 4, 5]  10.0000         0.0010       32        2000   
9         [2, 3, 4, 5]  20.0000         0.0010       32        2000   
8         [2, 3, 4, 5]  10.0000         0.0010       32        2000   
17        [2, 3, 4, 5]  10.0000         0.0010       32        2000   
19  [1, 2, 3, 4, 5, 6]      nan         0.0010       40        2000   
5                  [2]  10.0000         0.0010       32        2000   
12        [2, 3, 4, 5]  10.0000         0.0010       32        2000   
11        [2, 3, 4, 5]  10.0000         0.0010       32        2000   
10        [2, 3, 4, 5]  20.0000         0.0010       32        2000   
6         [2, 3, 4, 5]  10.0000         0.0010       32        2000   
0         [2, 3, 4, 5]  20.0000         0.0010       32        2000   
4         [2, 3, 4, 5]  10.0000         0.0010       32        2000   
14        [2, 3, 4, 5]  10.0000         0.0010       32        2000   

    dropout_portion  train_accuracy  test_accuracy  
23              nan          0.5790         0.5459  
21              nan          0.5865         0.5906  
18              nan          0.5894         0.5945  
22              nan          0.5852         0.6010  
3               nan          0.6649         0.6286  
7            0.2000          0.8788         0.6496  
2               nan          0.9034         0.6614  
15           0.2000          0.8460         0.6654  
20              nan          0.7919         0.7100  
13              nan          0.8038         0.7152  
1            0.2000          0.9399         0.7257  
16           0.2000          0.8419         0.7283  
9            0.2000          0.9407         0.7297  
8            0.2000          0.8726         0.7467  
17           0.2000          0.8743         0.7507  
19              nan          0.7685         0.7520  
5            0.2000          0.8472         0.7533  
12           0.2000          0.8710         0.7585  
11           0.2000          0.8365         0.7612  
10           0.2000          0.9367         0.7612  
6            0.2000          0.8653         0.7703  
0            0.2000          0.9391         0.7835  
4            0.2000          0.8727         0.7900  
14           0.2000          0.8727         0.7900  
>>> df_exp.columns = ['\n'.join(c.split('_')) for c in df_exp.columns]
>>> df_exp.sort_values('test_accuracy')
>>> df_exp.sort_values('test\naccuracy')
    split\nrandom\nstate  torch\nrandom\nstate  \
23                   nan                   nan   
21                   nan                   nan   
18                   nan                   nan   
22                   nan                   nan   
3                    nan                   nan   
7           1460820.0000           475689.0000   
2                    nan                   nan   
15          1459833.0000           656168.0000   
20                   nan                   nan   
13                   nan                   nan   
1           1461049.0000           841794.0000   
16          1460013.0000            48167.0000   
9           1461348.0000           945794.0000   
8           1461523.0000           968824.0000   
17          1562131.0000           186757.0000   
19                   nan                   nan   
5           1566490.0000           202925.0000   
12          1460223.0000           417554.0000   
11          1460331.0000           155048.0000   
10          1461995.0000           433994.0000   
6           1461626.0000           667307.0000   
0           1460940.0000           433994.0000   
4           1460940.0000           433994.0000   
14          1460940.0000           433994.0000   

                                    filename case\nsensitive  num\nstopwords  \
23   disaster_tweets_cnn_pipeline_26110.json            None               0   
21   disaster_tweets_cnn_pipeline_26113.json            None               0   
18   disaster_tweets_cnn_pipeline_26105.json            None               0   
22   disaster_tweets_cnn_pipeline_26104.json            None               0   
3    disaster_tweets_cnn_pipeline_26119.json            None               0   
7    disaster_tweets_cnn_pipeline_24348.json            True               0   
2   disaster_tweets_cnn_pipeline_172823.json            None               0   
15   disaster_tweets_cnn_pipeline_24331.json            True               0   
20  disaster_tweets_cnn_pipeline_173147.json            None               0   
13  disaster_tweets_cnn_pipeline_173132.json            None               0   
1    disaster_tweets_cnn_pipeline_24352.json            True               0   
16   disaster_tweets_cnn_pipeline_24334.json            True               0   
9    disaster_tweets_cnn_pipeline_24358.json            True               0   
8    disaster_tweets_cnn_pipeline_24359.json            True               0   
17   disaster_tweets_cnn_pipeline_26036.json            True               0   
19   disaster_tweets_cnn_pipeline_32808.json            None               0   
5    disaster_tweets_cnn_pipeline_26108.json            True               0   
12   disaster_tweets_cnn_pipeline_24338.json            True               0   
11   disaster_tweets_cnn_pipeline_24339.json            True               0   
10   disaster_tweets_cnn_pipeline_24368.json            True               0   
6    disaster_tweets_cnn_pipeline_24361.json            True               0   
0    disaster_tweets_cnn_pipeline_24365.json            True               0   
4    disaster_tweets_cnn_pipeline_24350.json            True               0   
14   disaster_tweets_cnn_pipeline_24363.json            True               0   

       kernel\nlengths   epochs  learning\nrate  seq\nlen  vocab\nsize  \
23                 [2]      nan          0.0010        32         2000   
21                 [2]      nan          0.0010        32         2000   
18                 [2]      nan          0.0010        32         2000   
22                 [2]      nan          0.0010        32         2000   
3                  [2]      nan          0.0010        32         2000   
7         [2, 3, 4, 5]  10.0000          0.0010        32         2000   
2   [1, 2, 3, 4, 5, 6]      nan          0.0020        40         2000   
15        [2, 3, 4, 5]  10.0000          0.0010        32         2000   
20  [1, 2, 3, 4, 5, 6]  25.0000          0.0010        40         2000   
13        [2, 3, 4, 5]      nan          0.0015        40         2000   
1         [2, 3, 4, 5]  20.0000          0.0010        32         2000   
16        [2, 3, 4, 5]  10.0000          0.0010        32         2000   
9         [2, 3, 4, 5]  20.0000          0.0010        32         2000   
8         [2, 3, 4, 5]  10.0000          0.0010        32         2000   
17        [2, 3, 4, 5]  10.0000          0.0010        32         2000   
19  [1, 2, 3, 4, 5, 6]      nan          0.0010        40         2000   
5                  [2]  10.0000          0.0010        32         2000   
12        [2, 3, 4, 5]  10.0000          0.0010        32         2000   
11        [2, 3, 4, 5]  10.0000          0.0010        32         2000   
10        [2, 3, 4, 5]  20.0000          0.0010        32         2000   
6         [2, 3, 4, 5]  10.0000          0.0010        32         2000   
0         [2, 3, 4, 5]  20.0000          0.0010        32         2000   
4         [2, 3, 4, 5]  10.0000          0.0010        32         2000   
14        [2, 3, 4, 5]  10.0000          0.0010        32         2000   

    dropout\nportion  train\naccuracy  test\naccuracy  
23               nan           0.5790          0.5459  
21               nan           0.5865          0.5906  
18               nan           0.5894          0.5945  
22               nan           0.5852          0.6010  
3                nan           0.6649          0.6286  
7             0.2000           0.8788          0.6496  
2                nan           0.9034          0.6614  
15            0.2000           0.8460          0.6654  
20               nan           0.7919          0.7100  
13               nan           0.8038          0.7152  
1             0.2000           0.9399          0.7257  
16            0.2000           0.8419          0.7283  
9             0.2000           0.9407          0.7297  
8             0.2000           0.8726          0.7467  
17            0.2000           0.8743          0.7507  
19               nan           0.7685          0.7520  
5             0.2000           0.8472          0.7533  
12            0.2000           0.8710          0.7585  
11            0.2000           0.8365          0.7612  
10            0.2000           0.9367          0.7612  
6             0.2000           0.8653          0.7703  
0             0.2000           0.9391          0.7835  
4             0.2000           0.8727          0.7900  
14            0.2000           0.8727          0.7900  
>>> print(df_exp.sort_values('test\naccuracy'))
>>> df_exp.columns = ['_'.join(c.split('\n')) for c in df_exp.columns]
>>> print(df_exp.sort_values('test_accuracy'))
>>> df_exp.sort_values('test_accuracy').to_csv('hyperparameter_tuning_sorted.csv')
>>> mv hyperparameter_tuning_sorted.csv ch05_hyperparameter_tuning_sorted.csv
>>> best = 'disaster_tweets_cnn_pipeline_24363.json'
>>> experiments[14]
{'seq_len': 32,
 'usecols': ['text', 'target'],
 'tokenizer': 'tokenize_re',
 'embeddings': [2000, 64],
 'kernel_lengths': [2, 3, 4, 5],
 'strides': [2, 2, 2, 2],
 'conv_output_size': 32,
 'in_channels': 32,
 'planes': 1,
 'out_channels': 32,
 'groups': 1,
 'epochs': 10,
 'batch_size': 12,
 'learning_rate': 0.001,
 'test_size': 0.1,
 'dropout_portion': 0.2,
 'num_stopwords': 0,
 'case_sensitive': True,
 'split_random_state': 1460940,
 'numpy_random_state': 433,
 'torch_random_state': 433994,
 're_sub': '[^A-Za-z0-9.?!]+',
 'vocab_size': 2000,
 'embedding_size': 64,
 'learning_curve': [[0.6614749431610107,
   0.6139249744562838,
   0.636482939632546],
  [0.5514588356018066, 0.6883666618011969, 0.7099737532808399],
  [0.4705458879470825, 0.7339074587651437, 0.7388451443569554],
  [0.35673093795776367, 0.7722960151802657, 0.7650918635170604],
  [0.28001248836517334, 0.7952123777550723, 0.7782152230971129],
  [0.3533312976360321, 0.8171069916800467, 0.7834645669291339],
  [0.22455070912837982, 0.8338928623558605, 0.7821522309711286],
  [0.24331556260585785, 0.8476134870821778, 0.7795275590551181],
  [0.2981152832508087, 0.8617720040869946, 0.7821522309711286],
  [0.11444409191608429, 0.8727193110494819, 0.7900262467191601]],
 'loss': 0.11444409191608429,
 'train_accuracy': 0.8727193110494819,
 'test_accuracy': 0.7900262467191601,
 'filename': 'disaster_tweets_cnn_pipeline_24363.json'}
>>> lc = pd.DataFrame(experiments[14])
>>> lc = pd.DataFrame(experiments[14]['hyperp'])
>>> experiments[14].keys()
dict_keys(['seq_len', 'usecols', 'tokenizer', 'embeddings', 'kernel_lengths', 'strides', 'conv_output_size', 'in_channels', 'planes', 'out_channels', 'groups', 'epochs', 'batch_size', 'learning_rate', 'test_size', 'dropout_portion', 'num_stopwords', 'case_sensitive', 'split_random_state', 'numpy_random_state', 'torch_random_state', 're_sub', 'vocab_size', 'embedding_size', 'learning_curve', 'loss', 'train_accuracy', 'test_accuracy', 'filename'])
>>> experiments[14]['learning_curve']
[[0.6614749431610107, 0.6139249744562838, 0.636482939632546],
 [0.5514588356018066, 0.6883666618011969, 0.7099737532808399],
 [0.4705458879470825, 0.7339074587651437, 0.7388451443569554],
 [0.35673093795776367, 0.7722960151802657, 0.7650918635170604],
 [0.28001248836517334, 0.7952123777550723, 0.7782152230971129],
 [0.3533312976360321, 0.8171069916800467, 0.7834645669291339],
 [0.22455070912837982, 0.8338928623558605, 0.7821522309711286],
 [0.24331556260585785, 0.8476134870821778, 0.7795275590551181],
 [0.2981152832508087, 0.8617720040869946, 0.7821522309711286],
 [0.11444409191608429, 0.8727193110494819, 0.7900262467191601]]
>>> lc = pd.DataFrame(experiments[4]['hyperp'])
>>> lc = experiments[4]['learning_curve']
>>> lc.columns = 'training_loss training_accuracy test_accuracy'.split()
>>> lc = pd.DataFrame(experiments[4]['learning_curve'], columns='training_loss training_accuracy test_accuracy'.split())
>>> lc
   training_loss  training_accuracy  test_accuracy
0         0.6615             0.6139         0.6365
1         0.5515             0.6884         0.7100
2         0.4705             0.7339         0.7388
3         0.3567             0.7723         0.7651
4         0.2800             0.7952         0.7782
5         0.3533             0.8171         0.7835
6         0.2246             0.8339         0.7822
7         0.2433             0.8476         0.7795
8         0.2981             0.8618         0.7822
9         0.1144             0.8727         0.7900
>>> lc1 = pd.DataFrame(experiments[14]['learning_curve'], columns='training_loss training_accuracy test_accuracy'.split())
>>> lc1
   training_loss  training_accuracy  test_accuracy
0         0.6615             0.6139         0.6365
1         0.5515             0.6884         0.7100
2         0.4705             0.7339         0.7388
3         0.3567             0.7723         0.7651
4         0.2800             0.7952         0.7782
5         0.3533             0.8171         0.7835
6         0.2246             0.8339         0.7822
7         0.2433             0.8476         0.7795
8         0.2981             0.8618         0.7822
9         0.1144             0.8727         0.7900
>>> lc[['training set', 'test set']].plot(linewidth=2, grid='on', ylabel='accuracy', xlabel='epochs', title=title)
>>> title = 'seq_len={seq_len} vocab_size={vocab_size} embedding_size={embedding_size} kernel_lengths={kernel_lengths}'.format(**pipelinexperiments[14])
>>> title = 'seq_len={seq_len} vocab_size={vocab_size} embedding_size={embedding_size} kernel_lengths={kernel_lengths}'.format(**experiments[14])
>>> title
'seq_len=32 vocab_size=2000 embedding_size=64 kernel_lengths=[2, 3, 4, 5]'
>>> lc.columns = 'loss training set test set'.split()
>>> lc.columns = ['loss', 'training set', 'test set']
>>> lc[['training set', 'test set']].plot(linewidth=2, grid='on', ylabel='accuracy', xlabel='epochs', title=title)
<AxesSubplot:title={'center':'seq_len=32 vocab_size=2000 embedding_size=64 kernel_lengths=[2, 3, 4, 5]'}, xlabel='epochs', ylabel='accuracy'>
>>> plt.show()
>>> !mv -i 'learning*.png' /home/hobs/code/tangibleai/nlpia-manuscript/manuscript/images/ch07/
>>> !mv -i ~/learning*.png /home/hobs/code/tangibleai/nlpia-manuscript/manuscript/images/ch07/
>>> !mv -i ~/learning*.svg /home/hobs/code/tangibleai/nlpia-manuscript/manuscript/images/ch07/
>>> cd /home/hobs/code/tangibleai/nlpia-manuscript/manuscript/images/ch07/
>>> ls *.png
>>> rm learning-curve-85-80.png
>>> mv underfit_learning_curve.png ../unused/ch07/learning_curve_underfit.png
>>> mv overfit_learning_curve.png ../unused/ch07/learning_curve_overfit.png
>>> hist -o -p -f hyperparameter_tuning_disaster_tweets.hist.md
