{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6c6b153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dataframe-image \n",
    "import dataframe_image as dfi\n",
    "# !pip install playsound pygobject\n",
    "from playsound import playsound\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "801c89b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "CHAPTER = 'ch07'\n",
    "BOOK_IMAGES_DIR = Path.home() / 'code' / 'tangibleai' / 'nlpia-manuscript' \n",
    "BOOK_IMAGES_DIR /= Path('manuscript') / 'images' / CHAPTER\n",
    "CODE_IMAGES_DIR = Path.home() / 'code' / 'tangibleai' / 'nlpia2' / 'src' / 'nlpia2' / 'images' / CHAPTER \n",
    "IMAGES_DIR = CODE_IMAGES_DIR\n",
    "IMAGES_DIR.mkdir(exist_ok=True,parents=True)\n",
    "\n",
    "def savefig(ax, filename, **kwargs):\n",
    "    filepath = IMAGES_DIR / filename\n",
    "    if isinstance(ax, (list, tuple)):\n",
    "        ax = ax[0]\n",
    "    if hasattr(ax, 'figure'):\n",
    "        return ax.figure.savefig(filepath, **kwargs)\n",
    "    if hasattr(ax, 'savefig'):\n",
    "        return ax.savefig(filepath, **kwargs)\n",
    "    return plt.savefig(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1dd15d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .Chapter imports\n",
    "\n",
    "HOME_DATA_DIR = Path.home() / '.nlpia2-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c411eda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_conv_out_seq_len(seq_len, kernel_len, stride=1, dilation=1, padding=0):\n",
    "    \"\"\" L_out = 1 + (L_in + 2 * padding - dilation * (kernel_size - 1) - 1) / stride \"\"\"\n",
    "    return 1 + (seq_len + 2 * padding - dilation * (kernel_len - 1) - 1) // stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d34f799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def total_out_seq_len(seq_len, kernel_lengths, pool_lengths=None, stride=1, dilation=1, padding=0):\n",
    "#     \"\"\" Calculate the number of encoding dimensions output from CNN layers\n",
    "\n",
    "#     From PyTorch docs:\n",
    "#       L_out = 1 + (L_in + 2 * padding - dilation * (kernel_size - 1) - 1) / stride\n",
    "#     But padding=0 and dilation=1, because we're only doing a 'valid' convolution.\n",
    "#     So:\n",
    "#       L_out = 1 + (L_in - (kernel_size - 1) - 1) // stride\n",
    "\n",
    "#     source: https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n",
    "#     \"\"\"\n",
    "#     if pool_lengths is None:\n",
    "#         pool_lengths = kernel_lengths\n",
    "#     out_pool_total = 0\n",
    "#     for kernel_len in kernel_lengths:\n",
    "#         conv_output_len = calc_conv_out_seq_len(\n",
    "#             seq_len=seq_len, kernel_len=kernel_len, stride=stride,\n",
    "#             dilation=dilation, padding=padding\n",
    "#         )\n",
    "#         out_pool = calc_conv_out_seq_len(\n",
    "#             seq_len=conv_output_len,\n",
    "#             kernel_len=kernel_len,\n",
    "#             stride=stride,\n",
    "#             dilation=dilation, padding=padding\n",
    "#         )\n",
    "#         out_pool_total += out_pool\n",
    "\n",
    "#     # return the len of a \"flattened\" vector that is passed into a fully connected (Linear) layer\n",
    "#     return out_pool_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d04a76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .CNN hyperparameters\n",
    "# [source,python]\n",
    "# ----\n",
    "class CNNTextClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, embeddings=torch.rand(10_000, 50)):\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq_len = 40                               # <1>\n",
    "        self.vocab_size = embeddings.shape[0]           # <2>\n",
    "        self.embedding_size = embeddings.shape[1]       # <3>\n",
    "        self.out_channels = 5                           # <4>\n",
    "        self.kernel_lengths = [2, 3, 4, 5, 6]           # <5>\n",
    "        self.stride = 1                                 # <6>\n",
    "        self.dropout = nn.Dropout(0)                    # <7>\n",
    "        self.pool_stride = self.stride                  # <8>\n",
    "# ----\n",
    "# <1> `N_`: assume a maximum text length of 40 tokens\n",
    "# <2> `V`: number of unique tokens (words) in your vocabulary\n",
    "# <3> `E`: number of word embedding dimensions (kernel input channels)\n",
    "# <4> `F`: number of filters (kernel output channels)\n",
    "# <5> `K`: number of columns of weights in each kernel\n",
    "# <6> `S`: number of time steps (tokens) to slide the kernel forward with each step\n",
    "# <7> `D`: portion of convolution output to ignore. 0 dropout increases overfitting\n",
    "# <8> `P`: pooling strides greater than 1 will increase feature reduction\n",
    "\n",
    "# .CNN embedding\n",
    "# [source,python]\n",
    "# ----\n",
    "        self.embed = nn.Embedding(\n",
    "            self.vocab_size,\n",
    "            self.embedding_size,                        # <1>\n",
    "            padding_idx=0)\n",
    "        state = self.embedding.state_dict()\n",
    "        state['weight'] = embeddings                    # <2>\n",
    "        \n",
    "# ----\n",
    "# <1> for pretrained 50-D GloVe vectors set embedding_size to 50\n",
    "\n",
    "        self.convolvers = []\n",
    "        self.poolers = []\n",
    "        self.total_out_len = 0\n",
    "        for i, kernel_len in enumerate(self.kernel_lengths):\n",
    "            self.convolvers.append(\n",
    "                nn.Conv1d(\n",
    "                    in_channels=self.embedding_size,\n",
    "                    out_channels=self.out_channels,\n",
    "                    kernel_size=kernel_len,\n",
    "                    stride=self.stride))\n",
    "            print(f'conv[{i}].weight.shape: {self.convolvers[-1].weight.shape}')\n",
    "            conv_output_len = calc_conv_out_seq_len(\n",
    "                seq_len=self.seq_len, kernel_len=kernel_len, stride=self.stride)\n",
    "            print(f'conv_output_len: {conv_output_len}')\n",
    "            self.poolers.append(\n",
    "                nn.MaxPool1d(\n",
    "                    kernel_size=conv_output_len,\n",
    "                    stride=self.stride))                            # <7>\n",
    "            self.total_out_len += calc_conv_out_seq_len(\n",
    "                seq_len=conv_output_len,\n",
    "                kernel_len=conv_output_len,\n",
    "                stride=self.stride)\n",
    "            print(f'pool_output_len: {pool_output_len}')\n",
    "            self.pool_lengths.append(pool_output_len)\n",
    "            # Given input size: (32x1x34). Calculated output size: (32x1x0). Output size is too small\n",
    "            print(f'poolers[{i}]: {self.poolers[-1]}')\n",
    "        print(f'sum(self.pool_lengths): {sum(self.pool_lengths)}')\n",
    "        self.conv_output_size = calc_output_seq_len(seq_len=self.seq_len, kernel_lengths=self.kernel_lengths)  # <8>\n",
    "        print(f'calc_output_seq_len: {self.conv_output_size}')\n",
    "        self.linear_layer = nn.Linear(self.out_channels * sum(self.pool_lengths), 1)\n",
    "        print(f'linear_layer: {self.linear_layer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7b78a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa016413",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, x):\n",
    "        \"\"\" Takes sequence of integers (token indices) and outputs binary class label \"\"\"\n",
    "\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        conv_outputs = []\n",
    "        for (conv, pool) in zip(self.convolvers, self.poolers):\n",
    "            z = conv(x)\n",
    "            z = torch.relu(z)\n",
    "            z = pool(z)\n",
    "            conv_outputs.append(z)\n",
    "\n",
    "        # The output of each convolutional layer is concatenated into a unique vector\n",
    "        union = torch.cat(conv_outputs, 2)\n",
    "        union = union.reshape(union.size(0), -1)\n",
    "\n",
    "        # The \"flattened\" vector is passed through a fully connected layer\n",
    "        out = self.linear_layer(union)\n",
    "        # Dropout is applied\n",
    "        out = self.dropout(out)\n",
    "        # Activation function is applied\n",
    "        out = torch.sigmoid(out)\n",
    "\n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2b69d0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .Learnable embedding layer\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "embedding = nn.Embedding(\n",
    "    num_embeddings=10000,  # <1>\n",
    "    embedding_dim=50,      # <2>\n",
    "    padding_idx=0)\n",
    "\n",
    "# <1> you must use the same size here as you use in your tokenizer\n",
    "# <2> the smallest useful GloVE embeddings have 50 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5710deb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(3000, 50, padding_idx=0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6b8d0964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  target\n",
      "0     Our Deeds are the Reason of this #earthquake M...       1\n",
      "1                Forest fire near La Ronge Sask. Canada       1\n",
      "2     All residents asked to 'shelter in place' are ...       1\n",
      "...                                                 ...     ...\n",
      "7850  And in its broadest sense, neural Darwinism im...      -1\n",
      "7851  In the 1980's, Edelman's theory was so novel t...      -1\n",
      "7852  Over a lifetime, I have written millions of wo...      -1\n",
      "\n",
      "[7853 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Listing 7.TBD12\n",
    "# .Load news posts\n",
    "\n",
    "df = pd.read_csv(HOME_DATA_DIR / 'news.csv')\n",
    "df = df[['text', 'target']]  # <1>\n",
    "print(df)\n",
    "\n",
    "# <1> you only need the text and binary newsworthiness label for your CNN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "565291c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('t', 5235), ('co', 4740), ('http', 4309), ('the', 3667), ('a', 2366), ('to', 2185)]\n"
     ]
    }
   ],
   "source": [
    "# // Listing 7.TBD13\n",
    "# .Most common words for your vocabulary\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "counts = Counter(chain(*[\n",
    "    re.findall(r'\\w+', t.lower()) for t in df['text']]))     # <1>\n",
    "vocab = [tok for tok, count in counts.most_common(4000)[3:]] # <2>\n",
    "vocab += ['sacred']\n",
    "\n",
    "print(counts.most_common(6))\n",
    "\n",
    "# <1> tokenizing, case folding, and occurrence counting\n",
    "# <2> ignore the 3 most frequent tokens (\"t\", \"co\", \"http\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bb7d962f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# // Listing 7.TBD14 \n",
    "# .Multipurpose padding function\n",
    "\n",
    "def pad(sequence, pad_value, seq_len):\n",
    "    padded = list(sequence)[:seq_len]\n",
    "    padded = padded + [pad_value] * (seq_len - len(padded))\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1c17d9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-27:12:35:27.193 DEBUG files.py:271 skiprows=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed.shape: (3846, 50)\n",
      "vocab:\n",
      "0             a\n",
      "1            to\n",
      "2            in\n",
      "         ...   \n",
      "3843    objects\n",
      "3844     chases\n",
      "3845     sacred\n",
      "Length: 3846, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# .Load embeddings and align with your vocabulary\n",
    "\n",
    "from nessvec.files import load_vecs_df\n",
    "\n",
    "glove = load_vecs_df(HOME_DATA_DIR / 'glove.6B.50d.txt')\n",
    "vocab = [tok for tok in vocab if tok in glove.index]         # <1>\n",
    "embed = glove.loc[vocab]                                     # <2>\n",
    "\n",
    "print(f'embed.shape: {embed.shape}')\n",
    "print(f'vocab:\\n{pd.Series(vocab)}')\n",
    "\n",
    "# <1> skip unknown embeddings; alternatively create zero vectors\n",
    "# <2> ensure your embedding matrix is in the same order as your vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "465fbe8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>are</th>\n",
       "      <th>sacred</th>\n",
       "      <th>.</th>\n",
       "      <th>&lt;pad&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-1.54</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>-0.54</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>-1.41</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0   words   are  sacred    .  <pad>\n",
       "0   -0.07  0.96    0.38  0.0    0.0\n",
       "1    0.66  0.01    1.32  0.0    0.0\n",
       "2   -0.06  0.22   -1.54  0.0    0.0\n",
       "..    ...   ...     ...  ...    ...\n",
       "47   0.82  0.14   -0.51  0.0    0.0\n",
       "48  -0.54 -0.38   -0.70  0.0    0.0\n",
       "49  -0.00 -0.39   -1.41  0.0    0.0\n",
       "\n",
       "[50 rows x 5 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Figure 7.TBD\n",
    "# cnn-embeddings-glove-words-are-sacred.drawio.png\n",
    "\n",
    "words = embed.loc[['words', 'are', 'sacred']].T\n",
    "words['.'] = 0.0\n",
    "words['<pad>'] = 0.0\n",
    "roundwords = words.round(2)\n",
    "roundwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5ee2049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNTextClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size=5, embedding_size=50):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=5,\n",
    "            embedding_dim=50,\n",
    "            padding_idx=0)\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=50,\n",
    "            out_channels=50,\n",
    "            groups=50,\n",
    "            kernel_size=2,\n",
    "            stride=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        embeddings = self.embedding(x)\n",
    "        print(f\"embeddings.size(): {embeddings.size()}\")\n",
    "        print(f\"embeddings:\\n{embeddings}\")\n",
    "        features = self.conv(embeddings)\n",
    "        print(f\"features.size(): {features.size()}\")\n",
    "        print(f\"features:\\n{features}\")\n",
    "        return features.squeeze()\n",
    "#             z = torch.relu(z)\n",
    "#             z = pool(z)\n",
    "#             conv_outputs.append(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a06cf5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_seq.dtype: torch.int64\n",
      "index_seq.size(): torch.Size([1, 50])\n",
      "embeddings.size(): torch.Size([1, 50, 50])\n",
      "embeddings:\n",
      "tensor([[[-1.0081, -1.1699, -1.4963,  ...,  2.3020,  0.2489,  0.5018],\n",
      "         [-0.2400, -0.6480, -1.3521,  ...,  0.5758, -1.1413,  1.3931],\n",
      "         [-0.3120,  0.6063,  1.8722,  ..., -0.8434, -0.6202, -0.5454],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "features.size(): torch.Size([1, 50, 49])\n",
      "features:\n",
      "tensor([[[ 0.7111,  0.7006,  0.8373,  ...,  1.6108, -0.9101,  0.4913],\n",
      "         [ 0.3074, -0.0479, -0.0912,  ...,  0.9394,  0.0321,  1.3873],\n",
      "         [ 0.7862,  1.0993,  0.8208,  ...,  0.5600,  0.5075,  0.5376],\n",
      "         ...,\n",
      "         [ 0.3920,  0.3920,  0.3920,  ...,  0.3920,  0.3920,  0.3920],\n",
      "         [ 0.0987,  0.0987,  0.0987,  ...,  0.0987,  0.0987,  0.0987],\n",
      "         [-0.3255, -0.3255, -0.3255,  ..., -0.3255, -0.3255, -0.3255]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7111,  0.7006,  0.8373,  ...,  1.6108, -0.9101,  0.4913],\n",
       "        [ 0.3074, -0.0479, -0.0912,  ...,  0.9394,  0.0321,  1.3873],\n",
       "        [ 0.7862,  1.0993,  0.8208,  ...,  0.5600,  0.5075,  0.5376],\n",
       "        ...,\n",
       "        [ 0.3920,  0.3920,  0.3920,  ...,  0.3920,  0.3920,  0.3920],\n",
       "        [ 0.0987,  0.0987,  0.0987,  ...,  0.0987,  0.0987,  0.0987],\n",
       "        [-0.3255, -0.3255, -0.3255,  ..., -0.3255, -0.3255, -0.3255]],\n",
       "       grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "cnn = CNNTextClassifier()\n",
    "index_seq = torch.tensor([[1, 2, 3, 4] + [0] * 46 ])\n",
    "print(f\"index_seq.dtype: {index_seq.dtype}\")\n",
    "print(f\"index_seq.size(): {index_seq.size()}\")\n",
    "cnn.forward(index_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "59dc0c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0527/123616.744617:WARNING:bluez_dbus_manager.cc(248)] Floss manager not present, cannot set Floss enable/disable.\n",
      "[0527/123616.786132:ERROR:sandbox_linux.cc(377)] InitializeSandbox() called with multiple threads in process gpu-process.\n",
      "[0527/123617.056794:INFO:headless_shell.cc(659)] Written to file /tmp/tmp2hzke_7k/temp.png.\n"
     ]
    }
   ],
   "source": [
    "filepath = Path('df-glove-vectors-sacred-pad-1.png')\n",
    "if filepath.is_file():\n",
    "    filepath.unlink()\n",
    "dfi.export(roundwords, filepath, max_rows=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163d64eb",
   "metadata": {},
   "source": [
    "![df-glove-vectors-sacred-pad.png](df-glove-vectors-sacred-pad.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aeb7c0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a52b38c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed.size(): torch.Size([3846, 50])\n",
      "Embedding(3846, 50)\n"
     ]
    }
   ],
   "source": [
    "# .Initialize your embedding layer with GloVE vectors\n",
    "\n",
    "import torch\n",
    "embed = torch.Tensor(embed.values)                         # <1>\n",
    "print(f'embed.size(): {embed.size()}')\n",
    "embed = nn.Embedding.from_pretrained(embed, freeze=False)  # <2>\n",
    "print(embed)\n",
    "\n",
    "# <1> convert Pandas DataFrame to a torch.Tensor\n",
    "# <2> freeze=False allows your Embedding layer to fine-tune your embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bea08f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .Initialize your CNN hyperparameters\n",
    "\n",
    "class CNNTextClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, embeddings=glove):\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq_len = 35                         # <1>\n",
    "        self.vocab_size = embeddings.shape[0]\n",
    "        self.embedding_size = embeddings.shape[1]\n",
    "        self.kernel_lengths = [2]                 # <2>\n",
    "        self.stride = 2\n",
    "        self.conv_output_size = 50                # <3>\n",
    "\n",
    "        self.dropout = nn.Dropout(.2)\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size + 1, self.embedding_size, padding_idx=0)\n",
    "\n",
    "        self.convolvers = []\n",
    "        self.poolers = []\n",
    "        for i, kernel_len in enumerate(self.kernel_lengths):\n",
    "            self.convolvers.append(nn.Conv1d(self.seq_len, self.conv_output_size, kernel_len, self.stride))\n",
    "            self.poolers.append(nn.MaxPool1d(kernel_len, self.stride))\n",
    "\n",
    "        self.encoding_size = self.cnn_output_size()\n",
    "        self.linear_layer = nn.Linear(self.encoding_size, 1)\n",
    "\n",
    "# <1> assume a maximum text length of 35 tokens\n",
    "# <2> only one kernel layer is needed for reasonable results\n",
    "# <3> the convolution output need not have the same number of channels as your embeddings\n",
    "\n",
    "\n",
    "    def cnn_output_size(self):\n",
    "        \"\"\" Calculate the number of encoding dimensions output from CNN layers\n",
    "\n",
    "        Convolved_Features = ((embedding_size + (2 * padding) - dilation * (kernel - 1) - 1) / stride) + 1\n",
    "        Pooled_Features = ((embedding_size + (2 * padding) - dilation * (kernel - 1) - 1) / stride) + 1\n",
    "\n",
    "        source: https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n",
    "        \"\"\"\n",
    "        out_pool_total = 0\n",
    "        for kernel_len in self.kernel_lengths:\n",
    "            out_conv = ((self.embedding_size - 1 * (kernel_len - 1) - 1) / self.stride) + 1\n",
    "            out_conv = math.floor(out_conv)\n",
    "            out_pool = ((out_conv - 1 * (kernel_len - 1) - 1) / self.stride) + 1\n",
    "            out_pool = math.floor(out_pool)\n",
    "            out_pool_total += out_pool\n",
    "\n",
    "        # Returns \"flattened\" vector (input for fully connected layer)\n",
    "        return out_pool_total * self.conv_output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "172620ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(embed).shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d305276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_rows = 40\n",
    "from pathlib import Path\n",
    "\n",
    "paths = list((Path.home() / '.nlpia2-data' / 'log').glob('*'))\n",
    "df = []\n",
    "for p in paths:\n",
    "    d = json.load(p.open())\n",
    "    df.append({k:d.get(k) for k in d.keys() if k not in ('learning_curve', 'y_test', 'y_train')})\n",
    "    df[-1]['filename'] = p.name[-12:-5]\n",
    "df = pd.DataFrame(df)\n",
    "df.sort_values('test_accuracy').round(2).tail(10).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed9640c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: both implementations look incorrect because pooling should not add additional dimensions/sequence elements to the encoding\n",
    "\n",
    "# .Compute the shape of the CNN output (the number of the output encoding vector dimensions)\n",
    "\n",
    "\n",
    "def conv1_output_encoding_size(self, embedding_size, kernel_lengths, stride, output_channels):\n",
    "    \"\"\" Calculate the number of encoding dimensions output from CNN layers\n",
    "\n",
    "    Convolved_Features = ((embedding_size + (2 * padding) - dilation * (kernel - 1) - 1) / stride) + 1\n",
    "    Pooled_Features = ((embedding_size + (2 * padding) - dilation * (kernel - 1) - 1) / stride) + 1\n",
    "\n",
    "    source: https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n",
    "    \"\"\"\n",
    "    out_pool_total = 0\n",
    "    for kernel_len in kernel_lengths:\n",
    "        out_conv = (\n",
    "            (embedding_size - 1 * (kernel_len - 1) - 1) / stride) + 1\n",
    "        out_conv = math.floor(out_conv)\n",
    "        out_pool = ((out_conv - 1 * (kernel_len - 1) - 1) / stride) + 1\n",
    "        out_pool = math.floor(out_pool)\n",
    "        out_pool_total += out_pool\n",
    "\n",
    "    # return the len of a \"flattened\" vector that is passed into a fully connected (Linear) layer\n",
    "    return out_pool_total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc7079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_output_seq_len(input_seq_len, kernel_lengths, stride):\n",
    "    \"\"\" Calculate the number of encoding dimensions output from CNN layers\n",
    "\n",
    "    From PyTorch docs:\n",
    "      L_out = 1 + (L_in + 2 * padding - dilation * (kernel_size - 1) - 1) / stride\n",
    "    But padding=0 and dilation=1, because we're only doing a 'valid' convolution.\n",
    "    So:\n",
    "      L_out = 1 + (L_in - (kernel_size - 1) - 1) // stride\n",
    "\n",
    "    source: https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n",
    "    \"\"\"\n",
    "    out_pool_total = 0\n",
    "    for kernel_len in kernel_lengths:\n",
    "        out_conv = (\n",
    "            (input_seq_len - 1 * (kernel_len - 1) - 1) / stride) + 1\n",
    "        out_conv = math.floor(out_conv)\n",
    "        out_pool = ((out_conv - 1 * (kernel_len - 1) - 1) / stride) + 1\n",
    "        out_pool = math.floor(out_pool)\n",
    "        out_pool_total += out_pool\n",
    "\n",
    "    # return the len of a \"flattened\" vector that is passed into a fully connected (Linear) layer\n",
    "    return out_pool_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc7842f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_rows = 40\n",
    "\n",
    "\n",
    "def best_hyperparms():\n",
    "    paths = list((Path.home() / '.nlpia2-data' / 'log').glob('*'))\n",
    "    df = []\n",
    "    for p in paths:\n",
    "        d = json.load(p.open())\n",
    "        df.append({k: d.get(k) for k in d.keys() if k not in ('learning_curve', 'y_test', 'y_train')})\n",
    "        df[-1]['filename'] = p.name[-12:-5]\n",
    "    df = pd.DataFrame(df)\n",
    "    df.sort_values('test_accuracy').round(2).tail(10).T\n",
    "\n",
    "\n",
    "def learning_curve(filepath=Path.home() / '.nlpia2-data' / 'log' / 'disaster_tweets_cnn_pipeline_14728.json'):\n",
    "    with Path(filepath).open() as fin:\n",
    "        results = json.load(fin)\n",
    "\n",
    "    curve = pd.DataFrame(results['learning_curve'],\n",
    "                         columns=['loss', 'training_accuracy', 'test_accuracy'])\n",
    "    return curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245b5519",
   "metadata": {},
   "outputs": [],
   "source": [
    "curve = learning_curve()\n",
    "\n",
    "accuracy = curve[['training_accuracy', 'test_accuracy']]\n",
    "accuracy.plot(linewidth=3, xlabel='Epochs', ylabel='Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcac947",
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig(ax, IMAGES_DIR / 'learning-curve-85-80.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3309b241",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
