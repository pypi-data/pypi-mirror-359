>>> from char_rnn_from_scratch_refactored import *
>>> rnn3 = RNN(vocab_size=len(meta3['char2i']), n_hidden=128, n_categories=len(meta3['categories']))
>>> load_model_meta(filepath2)
>>> filepath2 = 'char_rnn_from_scratch_refactored-1_311-17min_28sec'
>>> load_model_meta(filepath2)
{'n_hidden': 128,
 'losses': [2.8671472182273865,
  2.821946612358093,
  2.768683669805527,
  2.7211787350177765,
  2.6067244070768356,
  2.4925117932260035,
  2.3585279549360276,
  2.328374351620674,
  2.2578060274720193,
  2.1829546317458153,
  2.133399403221905,
  2.081378333926201,
  2.0949830952584745,
  2.047797423169017,
  1.9575361510664224,
  1.9046579734161495,
  1.9634913218319416,
  1.8884026370942593,
  1.8125248789479955,
  1.875375485625118,
  1.8152059813300148,
  1.8272708428502082,
  1.727591314867139,
  1.7891129239257426,
  1.7041916830940171,
  1.698797692248947,
  1.6969761557490566,
  1.7509024651125074,
  1.7166071840180084,
  1.6962616560962052,
  1.7260312985219062,
  1.6468711970821024,
  1.5964861724972725,
  1.6509366240557284,
  1.627774701528251,
  1.6151056950660423,
  1.6209595620185138,
  1.6122024231052492,
  1.6082114250604063,
  1.5654603112242185,
  1.6160839821263215,
  1.5893379600048065,
  1.4944561518342234,
  1.4522892135940493,
  1.4541176807638259,
  1.5510416472572834,
  1.5438817436133103,
  1.4844900171358604,
  1.4944846707405524,
  1.4412343637180747,
  1.5037207862595097,
  1.498835348906694,
  1.4255677196120378,
  1.512218948060181,
  1.411633187209023,
  1.3554458860089507,
  1.438542943635024,
  1.4003859191554948,
  1.4319411287871189,
  1.3899608713750131,
  1.3670121905936394,
  1.4038232047640486,
  1.3753902504259021,
  1.4618199928840623,
  1.3995068790671503,
  1.4296890313476325,
  1.311817888944177,
  1.4120396478781185,
  1.435596553599229,
  1.4244902503085322],
 'train_time': '17:28',
 'categories': ['Arabic',
  'Irish',
  'Spanish',
  'French',
  'German',
  'English',
  'Korean',
  'Vietnamese',
  'Scottish',
  'Japanese',
  'Polish',
  'Greek',
  'Czech',
  'Italian',
  'Portuguese',
  'Russian',
  'Dutch',
  'Chinese'],
 'char2i': {'g': 0,
  'J': 1,
  'j': 2,
  'l': 3,
  'X': 4,
  'e': 5,
  'L': 6,
  'H': 7,
  ' ': 8,
  "'": 9,
  'w': 10,
  'O': 11,
  'U': 12,
  'E': 13,
  'c': 14,
  'F': 15,
  'a': 16,
  'Q': 17,
  'y': 18,
  'u': 19,
  'I': 20,
  'W': 21,
  ',': 22,
  'p': 23,
  'b': 24,
  'z': 25,
  'G': 26,
  'T': 27,
  't': 28,
  'q': 29,
  'S': 30,
  'm': 31,
  'd': 32,
  'K': 33,
  'n': 34,
  'i': 35,
  'x': 36,
  'Y': 37,
  'M': 38,
  'R': 39,
  'r': 40,
  'N': 41,
  '-': 42,
  'f': 43,
  'Z': 44,
  's': 45,
  'D': 46,
  'P': 47,
  'o': 48,
  ';': 49,
  'v': 50,
  'k': 51,
  'V': 52,
  'h': 53,
  'C': 54,
  'A': 55,
  '.': 56,
  'B': 57},
 'state_dict': OrderedDict([('i2h.weight',
               tensor([[-0.0857, -0.2011,  0.0355,  ...,  0.0254,  0.0691, -0.0199],
                       [-0.0454,  0.0275, -0.1081,  ...,  0.0356, -0.0539,  0.0066],
                       [ 0.0774, -0.0716, -0.1427,  ...,  0.0391, -0.0303,  0.0045],
                       ...,
                       [-0.1663,  0.0812,  0.1243,  ..., -0.0265, -0.1084, -0.0283],
                       [ 0.0013, -0.0473,  0.0264,  ..., -0.1098, -0.0496, -0.0729],
                       [-0.0564, -0.1088, -0.0805,  ..., -0.0360,  0.0612,  0.0022]])),
              ('i2h.bias',
               tensor([-0.1421,  0.1197,  0.0206, -0.0216, -0.0465,  0.0084,  0.0399, -0.1008,
                        0.0488,  0.0414,  0.0149, -0.0192,  0.0714,  0.0813,  0.1196,  0.0112,
                        0.0037,  0.0078,  0.0458,  0.0376, -0.0958,  0.0224, -0.0571,  0.0138,
                        0.0034,  0.0606,  0.0106, -0.0571,  0.1492, -0.0139,  0.0270,  0.0327,
                       -0.1947,  0.0612,  0.0612,  0.0210, -0.0606,  0.0097,  0.0434,  0.0023,
                       -0.0638,  0.0689, -0.1105, -0.1213, -0.0420, -0.1569,  0.0402, -0.1228,
                       -0.0392,  0.0563,  0.1197, -0.1060, -0.1890,  0.0144, -0.0224, -0.0591,
                        0.0101, -0.1126,  0.0497, -0.0485,  0.0767,  0.1616, -0.0911, -0.0362,
                       -0.0288, -0.0963,  0.0228, -0.0245, -0.0553, -0.0207,  0.1048, -0.0617,
                       -0.0693,  0.0278,  0.0483,  0.0691, -0.1587, -0.0689, -0.0175, -0.1212,
                        0.0694,  0.0540,  0.1198,  0.2359,  0.1043,  0.0321,  0.1130, -0.1440,
                       -0.1777,  0.0637, -0.1824,  0.0253, -0.0216, -0.0067,  0.0351, -0.1525,
                       -0.0446, -0.0250,  0.1283, -0.0049, -0.0675, -0.0628,  0.0426,  0.0314,
                        0.0076,  0.0061, -0.0149,  0.0145,  0.0504,  0.0748,  0.2153, -0.0430,
                        0.0005,  0.0421,  0.0010, -0.1259,  0.0233,  0.0853, -0.0032, -0.0050,
                       -0.1043,  0.0894,  0.0833,  0.0103,  0.0728, -0.0484, -0.0383,  0.0222])),
              ('i2o.weight',
               tensor([[-0.2815, -0.0672, -0.0370,  ..., -0.1170, -0.1320,  0.0742],
                       [-0.3031,  0.0410,  0.0293,  ..., -0.1706, -0.0377, -0.3837],
                       [-0.2143,  0.0183, -0.0483,  ...,  0.0527, -0.1062,  0.0707],
                       ...,
                       [-0.1438, -0.0411, -0.0318,  ..., -0.2376,  0.0815,  0.1580],
                       [-0.0332,  0.0162,  0.1559,  ...,  0.1144, -0.0276, -0.0189],
                       [ 1.2896,  0.0726, -0.0898,  ...,  0.0743,  0.0072, -0.1444]])),
              ('i2o.bias',
               tensor([-0.0449,  0.0489, -0.0954,  0.1373,  0.2341,  0.3368,  0.5902,  0.4655,
                        0.0224, -0.1709, -0.3004, -0.7787,  0.1166, -0.3638, -0.2438,  0.0113,
                       -0.0683,  0.0934]))]),
 'model': None}
>>> meta3 = _
>>> meta3
{'n_hidden': 128,
 'losses': [2.8671472182273865,
  2.821946612358093,
  2.768683669805527,
  2.7211787350177765,
  2.6067244070768356,
  2.4925117932260035,
  2.3585279549360276,
  2.328374351620674,
  2.2578060274720193,
  2.1829546317458153,
  2.133399403221905,
  2.081378333926201,
  2.0949830952584745,
  2.047797423169017,
  1.9575361510664224,
  1.9046579734161495,
  1.9634913218319416,
  1.8884026370942593,
  1.8125248789479955,
  1.875375485625118,
  1.8152059813300148,
  1.8272708428502082,
  1.727591314867139,
  1.7891129239257426,
  1.7041916830940171,
  1.698797692248947,
  1.6969761557490566,
  1.7509024651125074,
  1.7166071840180084,
  1.6962616560962052,
  1.7260312985219062,
  1.6468711970821024,
  1.5964861724972725,
  1.6509366240557284,
  1.627774701528251,
  1.6151056950660423,
  1.6209595620185138,
  1.6122024231052492,
  1.6082114250604063,
  1.5654603112242185,
  1.6160839821263215,
  1.5893379600048065,
  1.4944561518342234,
  1.4522892135940493,
  1.4541176807638259,
  1.5510416472572834,
  1.5438817436133103,
  1.4844900171358604,
  1.4944846707405524,
  1.4412343637180747,
  1.5037207862595097,
  1.498835348906694,
  1.4255677196120378,
  1.512218948060181,
  1.411633187209023,
  1.3554458860089507,
  1.438542943635024,
  1.4003859191554948,
  1.4319411287871189,
  1.3899608713750131,
  1.3670121905936394,
  1.4038232047640486,
  1.3753902504259021,
  1.4618199928840623,
  1.3995068790671503,
  1.4296890313476325,
  1.311817888944177,
  1.4120396478781185,
  1.435596553599229,
  1.4244902503085322],
 'train_time': '17:28',
 'categories': ['Arabic',
  'Irish',
  'Spanish',
  'French',
  'German',
  'English',
  'Korean',
  'Vietnamese',
  'Scottish',
  'Japanese',
  'Polish',
  'Greek',
  'Czech',
  'Italian',
  'Portuguese',
  'Russian',
  'Dutch',
  'Chinese'],
 'char2i': {'g': 0,
  'J': 1,
  'j': 2,
  'l': 3,
  'X': 4,
  'e': 5,
  'L': 6,
  'H': 7,
  ' ': 8,
  "'": 9,
  'w': 10,
  'O': 11,
  'U': 12,
  'E': 13,
  'c': 14,
  'F': 15,
  'a': 16,
  'Q': 17,
  'y': 18,
  'u': 19,
  'I': 20,
  'W': 21,
  ',': 22,
  'p': 23,
  'b': 24,
  'z': 25,
  'G': 26,
  'T': 27,
  't': 28,
  'q': 29,
  'S': 30,
  'm': 31,
  'd': 32,
  'K': 33,
  'n': 34,
  'i': 35,
  'x': 36,
  'Y': 37,
  'M': 38,
  'R': 39,
  'r': 40,
  'N': 41,
  '-': 42,
  'f': 43,
  'Z': 44,
  's': 45,
  'D': 46,
  'P': 47,
  'o': 48,
  ';': 49,
  'v': 50,
  'k': 51,
  'V': 52,
  'h': 53,
  'C': 54,
  'A': 55,
  '.': 56,
  'B': 57},
 'state_dict': OrderedDict([('i2h.weight',
               tensor([[-0.0857, -0.2011,  0.0355,  ...,  0.0254,  0.0691, -0.0199],
                       [-0.0454,  0.0275, -0.1081,  ...,  0.0356, -0.0539,  0.0066],
                       [ 0.0774, -0.0716, -0.1427,  ...,  0.0391, -0.0303,  0.0045],
                       ...,
                       [-0.1663,  0.0812,  0.1243,  ..., -0.0265, -0.1084, -0.0283],
                       [ 0.0013, -0.0473,  0.0264,  ..., -0.1098, -0.0496, -0.0729],
                       [-0.0564, -0.1088, -0.0805,  ..., -0.0360,  0.0612,  0.0022]])),
              ('i2h.bias',
               tensor([-0.1421,  0.1197,  0.0206, -0.0216, -0.0465,  0.0084,  0.0399, -0.1008,
                        0.0488,  0.0414,  0.0149, -0.0192,  0.0714,  0.0813,  0.1196,  0.0112,
                        0.0037,  0.0078,  0.0458,  0.0376, -0.0958,  0.0224, -0.0571,  0.0138,
                        0.0034,  0.0606,  0.0106, -0.0571,  0.1492, -0.0139,  0.0270,  0.0327,
                       -0.1947,  0.0612,  0.0612,  0.0210, -0.0606,  0.0097,  0.0434,  0.0023,
                       -0.0638,  0.0689, -0.1105, -0.1213, -0.0420, -0.1569,  0.0402, -0.1228,
                       -0.0392,  0.0563,  0.1197, -0.1060, -0.1890,  0.0144, -0.0224, -0.0591,
                        0.0101, -0.1126,  0.0497, -0.0485,  0.0767,  0.1616, -0.0911, -0.0362,
                       -0.0288, -0.0963,  0.0228, -0.0245, -0.0553, -0.0207,  0.1048, -0.0617,
                       -0.0693,  0.0278,  0.0483,  0.0691, -0.1587, -0.0689, -0.0175, -0.1212,
                        0.0694,  0.0540,  0.1198,  0.2359,  0.1043,  0.0321,  0.1130, -0.1440,
                       -0.1777,  0.0637, -0.1824,  0.0253, -0.0216, -0.0067,  0.0351, -0.1525,
                       -0.0446, -0.0250,  0.1283, -0.0049, -0.0675, -0.0628,  0.0426,  0.0314,
                        0.0076,  0.0061, -0.0149,  0.0145,  0.0504,  0.0748,  0.2153, -0.0430,
                        0.0005,  0.0421,  0.0010, -0.1259,  0.0233,  0.0853, -0.0032, -0.0050,
                       -0.1043,  0.0894,  0.0833,  0.0103,  0.0728, -0.0484, -0.0383,  0.0222])),
              ('i2o.weight',
               tensor([[-0.2815, -0.0672, -0.0370,  ..., -0.1170, -0.1320,  0.0742],
                       [-0.3031,  0.0410,  0.0293,  ..., -0.1706, -0.0377, -0.3837],
                       [-0.2143,  0.0183, -0.0483,  ...,  0.0527, -0.1062,  0.0707],
                       ...,
                       [-0.1438, -0.0411, -0.0318,  ..., -0.2376,  0.0815,  0.1580],
                       [-0.0332,  0.0162,  0.1559,  ...,  0.1144, -0.0276, -0.0189],
                       [ 1.2896,  0.0726, -0.0898,  ...,  0.0743,  0.0072, -0.1444]])),
              ('i2o.bias',
               tensor([-0.0449,  0.0489, -0.0954,  0.1373,  0.2341,  0.3368,  0.5902,  0.4655,
                        0.0224, -0.1709, -0.3004, -0.7787,  0.1166, -0.3638, -0.2438,  0.0113,
                       -0.0683,  0.0934]))]),
 'model': None}
>>> rnn3 = RNN(vocab_size=len(meta3['char2i']), n_hidden=128, n_categories=len(meta3['categories']))
>>> RNN??
>>> rnn3 = RNN(vocab_size=len(meta3['char2i']), n_hidden=128, n_categories=len(meta3['categories']))
>>> rnn3.load_state_dict(meta3['state_dict'])
<All keys matched successfully>
>>> predict_category("Davletyarov", categories=meta3['categories'], char2i=meta3['char2i'], model=rnn3)
'Russian'
>>> predict_category??
>>> evaluate_tensor??
>>> model.__call__
>>> rnn3.__call__
<bound method Module._call_impl of RNN(
  (i2h): Linear(in_features=186, out_features=128, bias=True)
  (i2o): Linear(in_features=186, out_features=18, bias=True)
  (softmax): LogSoftmax(dim=1)
)>
>>> predict_category("Bilal", categories=meta3['categories'], char2i=meta3['char2i'], model=rnn3)
'Scottish'
>>> predict_category("Turan", categories=meta3['categories'], char2i=meta3['char2i'], model=rnn3)
'Vietnamese'
>>> meta3['categories']
['Arabic',
 'Irish',
 'Spanish',
 'French',
 'German',
 'English',
 'Korean',
 'Vietnamese',
 'Scottish',
 'Japanese',
 'Polish',
 'Greek',
 'Czech',
 'Italian',
 'Portuguese',
 'Russian',
 'Dutch',
 'Chinese']
>>> predict_category("Doestoevsky", categories=meta3['categories'], char2i=meta3['char2i'], model=rnn3)
'Russian'
>>> predict_category("Nakamoto", categories=meta3['categories'], char2i=meta3['char2i'], model=rnn3)
'Japanese'
>>> dir(rnn3)
['T_destination',
 '__annotations__',
 '__call__',
 '__class__',
 '__delattr__',
 '__dict__',
 '__dir__',
 '__doc__',
 '__eq__',
 '__format__',
 '__ge__',
 '__getattr__',
 '__getattribute__',
 '__gt__',
 '__hash__',
 '__init__',
 '__init_subclass__',
 '__le__',
 '__lt__',
 '__module__',
 '__ne__',
 '__new__',
 '__reduce__',
 '__reduce_ex__',
 '__repr__',
 '__setattr__',
 '__setstate__',
 '__sizeof__',
 '__str__',
 '__subclasshook__',
 '__weakref__',
 '_apply',
 '_backward_hooks',
 '_buffers',
 '_call_impl',
 '_forward_hooks',
 '_forward_pre_hooks',
 '_get_backward_hooks',
 '_get_name',
 '_is_full_backward_hook',
 '_load_from_state_dict',
 '_load_state_dict_pre_hooks',
 '_maybe_warn_non_full_backward_hook',
 '_modules',
 '_named_members',
 '_non_persistent_buffers_set',
 '_parameters',
 '_register_load_state_dict_pre_hook',
 '_register_state_dict_hook',
 '_replicate_for_data_parallel',
 '_save_to_state_dict',
 '_slow_forward',
 '_state_dict_hooks',
 '_version',
 'add_module',
 'apply',
 'bfloat16',
 'buffers',
 'children',
 'cpu',
 'cuda',
 'double',
 'dump_patches',
 'eval',
 'extra_repr',
 'float',
 'forward',
 'get_buffer',
 'get_extra_state',
 'get_parameter',
 'get_submodule',
 'half',
 'i2h',
 'i2o',
 'init_hidden',
 'load_state_dict',
 'modules',
 'n_categories',
 'n_hidden',
 'named_buffers',
 'named_children',
 'named_modules',
 'named_parameters',
 'parameters',
 'register_backward_hook',
 'register_buffer',
 'register_forward_hook',
 'register_forward_pre_hook',
 'register_full_backward_hook',
 'register_module',
 'register_parameter',
 'requires_grad_',
 'set_extra_state',
 'share_memory',
 'softmax',
 'state_dict',
 'to',
 'to_empty',
 'train',
 'training',
 'type',
 'xpu',
 'zero_grad']
>>> rnn3.i2o
Linear(in_features=186, out_features=18, bias=True)
>>> rnn3.i2o.shape
>>> rnn3.i2o.size
>>> rnn3.i2o.size()
>>> rnn3.i2o.weight
Parameter containing:
tensor([[-0.2815, -0.0672, -0.0370,  ..., -0.1170, -0.1320,  0.0742],
        [-0.3031,  0.0410,  0.0293,  ..., -0.1706, -0.0377, -0.3837],
        [-0.2143,  0.0183, -0.0483,  ...,  0.0527, -0.1062,  0.0707],
        ...,
        [-0.1438, -0.0411, -0.0318,  ..., -0.2376,  0.0815,  0.1580],
        [-0.0332,  0.0162,  0.1559,  ...,  0.1144, -0.0276, -0.0189],
        [ 1.2896,  0.0726, -0.0898,  ...,  0.0743,  0.0072, -0.1444]],
       requires_grad=True)
>>> rnn3.i2o.weight.size()
torch.Size([18, 186])
>>> len(char2i)
>>> char2i = meta3['char2i']
>>> len(char2i)
58
>>> char2i
{'g': 0,
 'J': 1,
 'j': 2,
 'l': 3,
 'X': 4,
 'e': 5,
 'L': 6,
 'H': 7,
 ' ': 8,
 "'": 9,
 'w': 10,
 'O': 11,
 'U': 12,
 'E': 13,
 'c': 14,
 'F': 15,
 'a': 16,
 'Q': 17,
 'y': 18,
 'u': 19,
 'I': 20,
 'W': 21,
 ',': 22,
 'p': 23,
 'b': 24,
 'z': 25,
 'G': 26,
 'T': 27,
 't': 28,
 'q': 29,
 'S': 30,
 'm': 31,
 'd': 32,
 'K': 33,
 'n': 34,
 'i': 35,
 'x': 36,
 'Y': 37,
 'M': 38,
 'R': 39,
 'r': 40,
 'N': 41,
 '-': 42,
 'f': 43,
 'Z': 44,
 's': 45,
 'D': 46,
 'P': 47,
 'o': 48,
 ';': 49,
 'v': 50,
 'k': 51,
 'V': 52,
 'h': 53,
 'C': 54,
 'A': 55,
 '.': 56,
 'B': 57}
>>> predict_category("Nakamoto", model=rnn3)
'Japanese'
>>> predict_category("O'Neal", model=rnn3)
'Irish'
>>> predict_category("Pie're", model=rnn3)
'French'
>>> predict_category("Piere", model=rnn3)
'French'
>>> 58 + 128
186
>>> len(meta3['categories'])
18
>>> rnn3??
>>>     def forward(self, char_tens, hidden):  # <2> x = input = char_tens
...         combined = torch.cat((char_tens, hidden), 1)
...         hidden = self.i2h(combined)
...         output = self.i2o(combined)
...         output = self.softmax(output)
...         return output, hidden
...
>>> forward
<function __main__.forward(self, char_tens, hidden)>
>>> rnn3.forward = forward
>>> predict_category("Piere", model=rnn3)
>>> rnn3??
>>> class RNN(nn.Module):
...     def __init__(self, vocab_size, n_hidden, n_categories):
...         super(RNN, self).__init__()
... 
...         self.n_hidden = n_hidden
...         self.n_categories = n_categories  # <1> n_categories = n_outputs (one-hot)
... 
...         self.i2h = nn.Linear(vocab_size + n_hidden, n_hidden)
...         self.i2o = nn.Linear(vocab_size + n_hidden, n_categories)
...         self.softmax = nn.LogSoftmax(dim=1)
... 
...     def forward(self, char_tens, hidden):  # <2> x = input = char_tens
...         combined = torch.cat((char_tens, hidden), 1)
...         
...         hidden = self.i2h(combined)
...         print(f"hidden: {hidden.size()}")
...         output = self.i2o(combined)
...         output = self.softmax(output)
...         return output, hidden
... 
...     def init_hidden(self):
...         return torch.zeros(1, self.n_hidden)
...
>>> rnn3 = RNN(vocab_size=len(meta3['char2i']), n_hidden=128, n_categories=len(meta3['categories']))
>>> rnn3.load_state_dict(meta3['state_dict'])
<All keys matched successfully>
>>> predict_category("Piere", model=rnn3)
'French'
>>> hist -f working/test_piere_load_model_1_311.hist.py
>>> pwd
'/home/hobs/code/tangibleai/nlpia2/src/nlpia2/ch08/nlpia_ch08_char_rnn_nationality'
>>> hist -f test_piere_load_model_1_311.hist.py
>>> hist -o -p -f test_piere_load_model_1_311.hist.md
