{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5107f084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "D_EMBEDDING = 512\n",
    "MAX_TOKENS = 1  # 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ceb1af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model=D_EMBEDDING, dropout=0.1, max_len=MAX_TOKENS):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)  # <1>\n",
    "        self.d_model = d_model  # <2>\n",
    "        self.max_len = max_len  # <3>\n",
    "        pe = torch.zeros(max_len, d_model)  # <4>\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
    "                             (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # <5>\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]  # <6>\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b7b5d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_position = PositionalEncoding()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba0ae520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = torch.tensor(np.arange(D_EMBEDDING * MAX_TOKENS).reshape(MAX_TOKENS, D_EMBEDDING))\n",
    "X_encoded = encode_position(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e303600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb16553d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset opus_books (/home/hobs/.cache/huggingface/datasets/opus_books/de-en/1.0.0/e8f950a4f32dc39b7f9088908216cd2d7e21ac35f893d04d39eb594746af2daf)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6945bb8c72f472a98b0485cf69aa5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset  # <1>\n",
    "opus = load_dataset('opus_books', 'de-en')\n",
    "opus\n",
    "sents = opus['train'].train_test_split(test_size=.1)\n",
    "sents\n",
    "next(iter(sents['test']))  # <1>\n",
    "DEVICE = torch.device(\n",
    "    'cuda' if torch.cuda.is_available()\n",
    "    else 'cpu')\n",
    "SRC = 'en'  # <1>\n",
    "TGT = 'de'  # <2>\n",
    "SOS, EOS = '<s>', '</s>'\n",
    "PAD, UNK, MASK = '<pad>', '<unk>', '<mask>'\n",
    "SPECIAL_TOKS = [SOS, PAD, EOS, UNK, MASK]\n",
    "VOCAB_SIZE = 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b52f64f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer  # <3>\n",
    "tokenize_src = ByteLevelBPETokenizer()\n",
    "tokenize_src.train_from_iterator(\n",
    "    [x[SRC] for x in sents['train']['translation']],\n",
    "    vocab_size=10000, min_frequency=2,\n",
    "    special_tokens=SPECIAL_TOKS)\n",
    "PAD_IDX = tokenize_src.token_to_id(PAD)\n",
    "tokenize_tgt = ByteLevelBPETokenizer()\n",
    "tokenize_tgt.train_from_iterator(\n",
    "    [x[TGT] for x in sents['train']['translation']],\n",
    "    vocab_size=10000, min_frequency=2,\n",
    "    special_tokens=SPECIAL_TOKS)\n",
    "assert PAD_IDX == tokenize_tgt.token_to_id(PAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "752f30ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from typing import Optional\n",
    "class CustomDecoderLayer(nn.TransformerDecoderLayer):\n",
    "    def forward(self, tgt: Tensor, memory: Tensor,\n",
    "            tgt_mask: Optional[Tensor] = None,\n",
    "            memory_mask: Optional[Tensor] = None,\n",
    "            tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "            mem_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        \"\"\"Like decode but returns multi-head attention weights.\"\"\"\n",
    "        tgt2 = self.self_attn(\n",
    "            tgt, tgt, tgt, attn_mask=tgt_mask,\n",
    "            key_padding_mask=tgt_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "        tgt2, attention_weights = self.multihead_attn(\n",
    "            tgt, memory, memory,  # <1>\n",
    "            attn_mask=memory_mask,\n",
    "            key_padding_mask=mem_key_padding_mask,\n",
    "            need_weights=True)\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        tgt2 = self.linear2(\n",
    "            self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt, attention_weights  # <2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d8526b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDecoder(nn.TransformerDecoder):\n",
    "    def __init__(self, decoder_layer, num_layers, norm=None):\n",
    "        super().__init__(\n",
    "            decoder_layer, num_layers, norm)\n",
    "\n",
    "    def forward(self,\n",
    "            tgt: Tensor, memory: Tensor,\n",
    "            tgt_mask: Optional[Tensor] = None,\n",
    "            memory_mask: Optional[Tensor] = None,\n",
    "            tgt_key_padding_mask: Optional[Tensor] = None\n",
    "            ) -> Tensor:\n",
    "        \"\"\"Like TransformerDecoder but cache multi-head attention\"\"\"\n",
    "        self.attention_weights = []  # <1>\n",
    "        output = tgt\n",
    "        for mod in self.layers:\n",
    "            output, attention = mod(\n",
    "                output, memory, tgt_mask=tgt_mask,\n",
    "                memory_mask=memory_mask,\n",
    "                tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "            self.attention_weights.append(attention) # <2>\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "288a7b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange  # <1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "229c5d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationTransformer(nn.Transformer):  # <2>\n",
    "    def __init__(self,\n",
    "            device=DEVICE,\n",
    "            src_vocab_size: int = VOCAB_SIZE,\n",
    "            src_pad_idx: int = PAD_IDX,\n",
    "            tgt_vocab_size: int = VOCAB_SIZE,\n",
    "            tgt_pad_idx: int = PAD_IDX,\n",
    "            max_sequence_length: int = 100,\n",
    "            d_model: int = 512,\n",
    "            nhead: int = 8,\n",
    "            num_encoder_layers: int = 6,\n",
    "            num_decoder_layers: int = 6,\n",
    "            dim_feedforward: int = 2048,\n",
    "            dropout: float = 0.1,\n",
    "            activation: str = \"relu\"\n",
    "        ):\n",
    "\n",
    "        decoder_layer = CustomDecoderLayer(\n",
    "            d_model, nhead, dim_feedforward,  # <3>\n",
    "            dropout, activation)\n",
    "        decoder_norm = nn.LayerNorm(d_model)\n",
    "        decoder = CustomDecoder(\n",
    "            decoder_layer, num_decoder_layers,\n",
    "            decoder_norm)  # <4>\n",
    "\n",
    "        super().__init__(\n",
    "            d_model=d_model, nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout, custom_decoder=decoder)\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.tgt_pad_idx = tgt_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "        self.src_emb = nn.Embedding(\n",
    "            src_vocab_size, d_model)  # <5>\n",
    "        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_enc = PositionalEncoding(\n",
    "            d_model, dropout, max_sequence_length)  # <6>\n",
    "        self.linear = nn.Linear(\n",
    "            d_model, tgt_vocab_size)  # <7>\n",
    "    def _make_key_padding_mask(self, t, pad_idx):\n",
    "        mask = (t == pad_idx).to(self.device)\n",
    "        return mask\n",
    "\n",
    "    def prepare_src(self, src, src_pad_idx):\n",
    "        src_key_padding_mask = self._make_key_padding_mask(\n",
    "            src, src_pad_idx)\n",
    "        src = rearrange(src, 'N S -> S N')\n",
    "        src = self.pos_enc(self.src_emb(src)\n",
    "            * math.sqrt(self.d_model))\n",
    "        return src, src_key_padding_mask\n",
    "    def prepare_tgt(self, tgt, tgt_pad_idx):\n",
    "        tgt_key_padding_mask = self._make_key_padding_mask(\n",
    "            tgt, tgt_pad_idx)\n",
    "        tgt = rearrange(tgt, 'N T -> T N')\n",
    "        tgt_mask = self.generate_square_subsequent_mask(\n",
    "            tgt.shape[0]).to(self.device)\n",
    "        tgt = self.pos_enc(self.tgt_emb(tgt)\n",
    "            * math.sqrt(self.d_model))\n",
    "        return tgt, tgt_key_padding_mask, tgt_mask\n",
    "    def forward(self, src, tgt):\n",
    "        src, src_key_padding_mask = self.prepare_src(\n",
    "            src, self.src_pad_idx)\n",
    "        tgt, tgt_key_padding_mask, tgt_mask = self.prepare_tgt(\n",
    "            tgt, self.tgt_pad_idx)\n",
    "        memory_key_padding_mask = src_key_padding_mask.clone()\n",
    "        output = super().forward(\n",
    "            src, tgt, tgt_mask=tgt_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=memory_key_padding_mask)\n",
    "        output = rearrange(output, 'T N E -> N T E')\n",
    "        return self.linear(output)\n",
    "    def init_weights(self):\n",
    "        def _init_weights(m):\n",
    "            if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "                nn.init.xavier_uniform_(m.weight.data)\n",
    "        self.apply(_init_weights);  # <1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1c65dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationTransformer(nn.Transformer):\n",
    "#    global supertransformer\n",
    "    def __init__(self,\n",
    "            device=DEVICE,\n",
    "            src_vocab_size: int = 10000,\n",
    "            src_pad_idx: int = PAD_IDX,\n",
    "            tgt_vocab_size: int  = 10000,\n",
    "            tgt_pad_idx: int = PAD_IDX,\n",
    "            max_sequence_length: int = 100,\n",
    "            d_model: int = 512,\n",
    "            nhead: int = 8,\n",
    "            num_encoder_layers: int = 6,\n",
    "            num_decoder_layers: int = 6,\n",
    "            dim_feedforward: int = 2048,\n",
    "            dropout: float = 0.1,\n",
    "            activation: str = \"relu\"\n",
    "            ):\n",
    "        decoder_layer = CustomDecoderLayer(\n",
    "            d_model, nhead, dim_feedforward,\n",
    "            dropout, activation)\n",
    "        decoder_norm = nn.LayerNorm(d_model)\n",
    "        decoder = CustomDecoder(\n",
    "            decoder_layer, num_decoder_layers, decoder_norm)\n",
    "\n",
    "        super().__init__(\n",
    "            d_model=d_model, nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout, custom_decoder=decoder)\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.tgt_pad_idx = tgt_pad_idx\n",
    "        self.device = device\n",
    "        self.src_emb = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.pos_enc = PositionalEncoding(\n",
    "            d_model, dropout, max_sequence_length)\n",
    "        self.linear = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def init_weights(self):\n",
    "        def _init_weights(m):\n",
    "            if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "                nn.init.xavier_uniform_(m.weight.data)\n",
    "        self.apply(_init_weights);\n",
    "\n",
    "    def _make_key_padding_mask(self, t, pad_idx=PAD_IDX):\n",
    "        mask = (t == pad_idx).to(self.device)\n",
    "        return mask\n",
    "\n",
    "    def prepare_src(self, src, src_pad_idx):\n",
    "        src_key_padding_mask = self._make_key_padding_mask(\n",
    "            src, src_pad_idx)\n",
    "        src = rearrange(src, 'N S -> S N')\n",
    "        src = self.pos_enc(self.src_emb(src)\n",
    "            * math.sqrt(self.d_model))\n",
    "        return src, src_key_padding_mask\n",
    "\n",
    "    def prepare_tgt(self, tgt, tgt_pad_idx):\n",
    "        tgt_key_padding_mask = self._make_key_padding_mask(\n",
    "            tgt, tgt_pad_idx)\n",
    "        tgt = rearrange(tgt, 'N T -> T N')\n",
    "        tgt_mask = self.generate_square_subsequent_mask(\n",
    "            tgt.shape[0]).to(self.device)      # <1>\n",
    "        tgt = self.pos_enc(self.tgt_emb(tgt)\n",
    "            * math.sqrt(self.d_model))\n",
    "        return tgt, tgt_key_padding_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src, src_key_padding_mask = self.prepare_src(\n",
    "            src, self.src_pad_idx)\n",
    "        tgt, tgt_key_padding_mask, tgt_mask = self.prepare_tgt(\n",
    "            tgt, self.tgt_pad_idx)\n",
    "        memory_key_padding_mask = src_key_padding_mask.clone()\n",
    "        # supertransformer = super()\n",
    "        # print(help(supertransformer.forward))\n",
    "        # forward(\n",
    "        #     src: torch.Tensor,\n",
    "        #     tgt: torch.Tensor,\n",
    "        #     src_mask: Optional[torch.Tensor] = None,\n",
    "        #     tgt_mask: Optional[torch.Tensor] = None,\n",
    "        #     memory_mask: Optional[torch.Tensor] = None,\n",
    "        #     src_key_padding_mask: Optional[torch.Tensor] = None,\n",
    "        #     tgt_key_padding_mask: Optional[torch.Tensor] = None,\n",
    "        #     memory_key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor method of __main__.TranslationTransformer instance\n",
    "        output = super().forward(\n",
    "            src=src,\n",
    "            tgt=tgt,\n",
    "            src_mask=None,\n",
    "            tgt_mask=tgt_mask,\n",
    "            memory_mask=None,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=memory_key_padding_mask,\n",
    "            )\n",
    "        output = rearrange(output, 'T N E -> N T E')\n",
    "        return self.linear(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2123c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
