{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d371533",
   "metadata": {},
   "source": [
    "## References\n",
    "- [analyticsvidhya.com/blog](https://www.analyticsvidhya.com/blog/2020/08/build-a-natural-language-generation-nlg-system-using-pytorch/)\n",
    "- [drive.google.com ... plots_text.pickle](https://drive.google.com/file/d/1PakdWMKYNyC5-2G_CSlLtkBsHezFpMHJ/view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7009c119",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-all-titles-in-ns0.gz\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/hobs/nessvec-data/corpora/enwikipedia'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnessvec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwikicrawl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnessvec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DATA_DIR\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m(\u001b[49m\u001b[43mDATA_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcorpora\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menwikipedia\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m      6\u001b[0m pages \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/nessvec/lib/python3.9/pathlib.py:1252\u001b[0m, in \u001b[0;36mPath.open\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, buffering\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1247\u001b[0m          errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;124;03m    Open the file pointed by this path and return a file object, as\u001b[39;00m\n\u001b[1;32m   1250\u001b[0m \u001b[38;5;124;03m    the built-in open() function does.\u001b[39;00m\n\u001b[1;32m   1251\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mopener\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_opener\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nessvec/lib/python3.9/pathlib.py:1120\u001b[0m, in \u001b[0;36mPath._opener\u001b[0;34m(self, name, flags, mode)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_opener\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, flags, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0o666\u001b[39m):\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;66;03m# A stub for the opener argument to built-in open()\u001b[39;00m\n\u001b[0;32m-> 1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/hobs/nessvec-data/corpora/enwikipedia'"
     ]
    }
   ],
   "source": [
    "from nessvec.wikicrawl import *\n",
    "from nessvec.constants import DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30ece237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:00<00:00, 129024.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ukranian_wikipedia_63.json'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikicrawl(, search_results=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1acec4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia as wiki\n",
    "\n",
    "queries = split_query_str('prosocial,win-win,supercooperator,regenerative economy')\n",
    "wiki.set_lang('en')\n",
    "\n",
    "results = []\n",
    "for q in queries:\n",
    "    results.append(wiki.search(q, results=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6667914d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Prosocial behavior',\n",
       " 'Social behavior',\n",
       " 'Helping behavior',\n",
       " 'Facial resemblance',\n",
       " 'Vicarious embarrassment',\n",
       " 'Behavior management',\n",
       " 'Sympathy',\n",
       " 'Peer group',\n",
       " 'David Sloan Wilson',\n",
       " 'Social identity theory']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki.search('prosocial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e207f7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<WikipediaPage 'Social behavior'>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = wiki.page('Social behavior')\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12bd8d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1967733'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.pageid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ede47ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.342792272567749\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "page.content\n",
    "dt = time.time() - t0\n",
    "\n",
    "print(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d949d133",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def search(queries=list(QUERIES.values()), results=50, lang='en'):\n",
    "    \"\"\" Search Wikipedia with the query strings listed in queries\n",
    "\n",
    "    >>> search('hello,world')\n",
    "    \"\"\"\n",
    "    from tqdm import tqdm\n",
    "    import wikipedia as wiki\n",
    "    queries = split_query_str(queries)\n",
    "    wiki.set_lang(lang)\n",
    "\n",
    "    title_pages = OrderedDict()\n",
    "    for q in queries:\n",
    "        title_pages.update(\n",
    "            OrderedDict(zip(wiki.search(q, results=results), [None] * results)))\n",
    "    for title in tqdm(title_pages):\n",
    "        try:\n",
    "            title_pages[title] = wiki.page(title)\n",
    "        except wiki.DisambiguationError:\n",
    "            pass\n",
    "        except Exception:\n",
    "            pass\n",
    "    return title_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a35ef035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:39<00:00,  1.36it/s]\n"
     ]
    }
   ],
   "source": [
    "prosocial_search_query = 'prosocial,win-win,supercooperator,regenerative economy,ethic AI,human compatible AI'\n",
    "title_pages = search(prosocial_search_query, results=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4f109fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['Prosocial behavior', 'Social behavior', 'Helping behavior', 'Facial resemblance', 'Behavior management', 'Vicarious embarrassment', 'Sympathy', 'David Sloan Wilson', 'Peer group', 'Personal distress', 'Winâ€“win game', 'Win Win (film)', 'How to Win Friends and Influence People', 'Win Butler', 'Minute to Win It', 'Win Gatchalian', 'No-win situation', \"List of college men's basketball coaches with 600 wins\", 'Win It All', 'Win Myint', 'Martin Nowak', 'Roger Highfield', 'Yoh Iwasa', 'Circular economy', 'Regenerative agriculture', 'Regenerative design', 'Regenerative economic theory', 'World Economic Forum', 'Regenerative medicine', 'John B. Fullerton', 'Cradle-to-cradle design', 'Blue economy', 'Economy of Finland', 'Hacker ethic', 'Jargon File', 'YoungBoy Never Broke Again', 'Golden Rule', 'Hackers: Heroes of the Computer Revolution', 'Hacker', 'Stellaris (video game)', 'Val Kilmer', 'Ethics', 'United States', 'Human Compatible', 'Center for Human-Compatible Artificial Intelligence', 'AI alignment', 'Existential risk from artificial general intelligence', 'Friendly artificial intelligence', 'AI takeover', 'Artificial intelligence', 'Stuart J. Russell', 'Bart Selman', 'Human-Centered Artificial Intelligence'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_pages.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d3794bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_WikipediaPage__continued_query',\n",
       " '_WikipediaPage__load',\n",
       " '_WikipediaPage__title_query_param',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'categories',\n",
       " 'content',\n",
       " 'coordinates',\n",
       " 'html',\n",
       " 'images',\n",
       " 'links',\n",
       " 'original_title',\n",
       " 'pageid',\n",
       " 'parent_id',\n",
       " 'references',\n",
       " 'revision_id',\n",
       " 'section',\n",
       " 'sections',\n",
       " 'summary',\n",
       " 'title',\n",
       " 'url']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(title_pages['Winâ€“win game'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c15522c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [01:44<00:00,  1.97s/it]\n"
     ]
    }
   ],
   "source": [
    "title_pages_dict = create_pages_data(title_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4eea1b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_json(pages, filename=\"wikicrawl_dump_json\"):\n",
    "    import json\n",
    "    filepath = f\"{filename}_{len(pages)}.json\"\n",
    "\n",
    "    with open(filepath, \"w\", encoding=\"utf8\") as fout:\n",
    "        json.dump(pages, fout, ensure_ascii=False, indent=4)\n",
    "\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "826d375a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikicrawl_dump_json_53.json'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump_json(title_pages_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6ea40b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "more wikicrawl_dump_json_53.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ac7167",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open\n",
    "json.dump(title_pages_dict, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ae1c578",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apostrophes': 'â€˜â€™â€›â›âœ',\n",
       " 'commas': 'â€š,',\n",
       " 'quotes': ['\"',\n",
       "  'Â«',\n",
       "  'Â»',\n",
       "  'â€˜',\n",
       "  'â€™',\n",
       "  'â€š',\n",
       "  'â€›',\n",
       "  'â€œ',\n",
       "  'â€',\n",
       "  'â€ž',\n",
       "  'â€Ÿ',\n",
       "  'â€¹',\n",
       "  'â€º',\n",
       "  'â›',\n",
       "  'âœ',\n",
       "  'â',\n",
       "  'âž',\n",
       "  'âŸ',\n",
       "  'â ',\n",
       "  'â®',\n",
       "  'â¯',\n",
       "  'â¹‚',\n",
       "  'ã€',\n",
       "  'ã€ž',\n",
       "  'ã€Ÿ',\n",
       "  'ï¼‚',\n",
       "  'ðŸ™¶',\n",
       "  'ðŸ™·',\n",
       "  'ðŸ™¸',\n",
       "  '\\U000e0022'],\n",
       " 'hyphens': ['-',\n",
       "  '\\xad',\n",
       "  'ÖŠ',\n",
       "  'á€',\n",
       "  'á †',\n",
       "  'â€',\n",
       "  'â€‘',\n",
       "  'â€§',\n",
       "  'âƒ',\n",
       "  'â¸—',\n",
       "  'â¸š',\n",
       "  'â¹€',\n",
       "  'ã‚ ',\n",
       "  'ï¹£',\n",
       "  'ï¼',\n",
       "  'ðº­',\n",
       "  '\\U000e002d'],\n",
       " 'dashes': ['â€’',\n",
       "  'â€“',\n",
       "  'â€”',\n",
       "  'â“',\n",
       "  'âŠ',\n",
       "  'â‘ˆ',\n",
       "  'â”„',\n",
       "  'â”…',\n",
       "  'â”†',\n",
       "  'â”‡',\n",
       "  'â”ˆ',\n",
       "  'â”‰',\n",
       "  'â”Š',\n",
       "  'â”‹',\n",
       "  'â•Œ',\n",
       "  'â•',\n",
       "  'â•Ž',\n",
       "  'â•',\n",
       "  'â¤Œ',\n",
       "  'â¤',\n",
       "  'â¤Ž',\n",
       "  'â¤',\n",
       "  'â¤',\n",
       "  'â¥ª',\n",
       "  'â¥«',\n",
       "  'â¥¬',\n",
       "  'â¥­',\n",
       "  'â©œ',\n",
       "  'â©',\n",
       "  'â«˜',\n",
       "  'â«¦',\n",
       "  'â¬·',\n",
       "  'â¸º',\n",
       "  'â¸»',\n",
       "  'â¹ƒ',\n",
       "  'ã€œ',\n",
       "  'ã€°',\n",
       "  'ï¸±',\n",
       "  'ï¸²',\n",
       "  'ï¹˜',\n",
       "  'ðŸ’¨']}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nessvec.text import quotes_hyphens_dashes\n",
    "charmap = quotes_hyphens_dashes()\n",
    "charmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "362aa207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>barry is a private with the 101st airborne div...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chinese exorcist one-eyebrow priest  leads a p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>while playing baseball on a busy street in gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thadeous and fabious ([[danny mcbride  are son...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{{plot}} jung su-ji is a quiet, mysterious bea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  barry is a private with the 101st airborne div...\n",
       "1  chinese exorcist one-eyebrow priest  leads a p...\n",
       "2  while playing baseball on a busy street in gre...\n",
       "3  thadeous and fabious ([[danny mcbride  are son...\n",
       "4  {{plot}} jung su-ji is a quiet, mysterious bea..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(451)\n",
    "\n",
    "df = pd.DataFrame(movie_plots, columns=['text'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75061812",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e9e34b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def tokenize(text, pattern=r'\\w+(?:\\'\\w+)?|[^\\w\\s]'):\n",
    "    r\"\"\" Split English text into words, ignoring 1 internal punctuation\"\n",
    "\n",
    "    `return list(re.findall(r'\\w+(?:\\'\\w+)?|[^\\w\\s]', text))`\n",
    "    \"\"\"\n",
    "    return list(re.findall(pattern, text))\n",
    "\n",
    "\n",
    "def create_seq(text, seq_len=5):\n",
    "    sequences = []\n",
    "    tokens = tokenize(text)\n",
    "    if len(tokens) > seq_len:\n",
    "        for i in range(seq_len, len(tokens)):\n",
    "            ngram = tokens[i - seq_len:i + 1]\n",
    "            sequences.append(\" \".join(ngram))  \n",
    "    else:\n",
    "        sequences = [\" \".join(tokens)]\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab312854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a village in rural thailand is celebrating loy krathong, when the festivities are disrupted by the d...'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_plots[-1][:100] + '...'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dab9dfc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12394           rope\n",
       "3069     concurrence\n",
       "14150        suppose\n",
       "4738         elegant\n",
       "3107        conflict\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "vocab = sorted(set(chain(*(set(tokenize(s)) for s in movie_plots))))\n",
    "int2token = pd.Series(vocab)\n",
    "token2int = pd.Series(int2token.index.values, index=int2token.values)\n",
    "int2token.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2ca5cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a village in rural thailand is',\n",
       " 'village in rural thailand is celebrating',\n",
       " 'in rural thailand is celebrating loy']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "five_grams = create_seq(movie_plots[-1])\n",
    "five_grams[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d03151",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
