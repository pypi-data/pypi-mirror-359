,Prompt,Task,Training Flops,Parameters,Model,Author,Year,Reference,Prompt-Task
0,Few-shot,Addition/subtraction (3 digit),2.3E+22,13B,GPT-3,Brown et al.,2020,Brown et al. (2020),Few-shot: Addition/subtraction (3 digit)
1,Few-shot,Addition/subtraction (4-5 digit),3.1E+23,175B,,,,,Few-shot: Addition/subtraction (4-5 digit)
2,Few-shot,MMLU Benchmark (57 topic avg.),3.1E+23,175B,GPT-3,Hendrycks et al.,2021,Hendrycks et al. (2021),Few-shot: MMLU Benchmark (57 topic avg.)
3,Few-shot,Toxicity classification (CivilComments),1.3E+22,7.1B,Gopher,Rae et al.,2021,Rae et al. (2021),Few-shot: Toxicity classification (CivilComments)
4,Few-shot,Truthfulness (Truthful QA),5.0E+23,280B,,,,,Few-shot: Truthfulness (Truthful QA)
5,Few-shot,MMLU Benchmark (26 topics),5.0E+23,280B,,,,,Few-shot: MMLU Benchmark (26 topics)
6,Few-shot,Grounded conceptual mappings,3.1E+23,175B,GPT-3,Patel & Pavlick,2022,Patel & Pavlick (2022),Few-shot: Grounded conceptual mappings
7,Few-shot,MMLU Benchmark (30 topics),5.0E+23,70B,Chinchilla,Hoffmann et al.,2022,Hoffmann et al. (2022),Few-shot: MMLU Benchmark (30 topics)
8,Few-shot,Word in Context (WiC) benchmark,2.5E+24,540B,PaLM,Chowdhery et al.,2022,Chowdhery et al. (2022),Few-shot: Word in Context (WiC) benchmark
10,Augmented,Instruction following (finetuning),1.3E+23,68B,FLAN,Wei et al.,2022,Wei et al. (2022),Augmented: Instruction following (finetuning)
11,Augmented,Scratchpad: 8-digit addition (finetuning),8.9E+19,40M,LaMDA,Nye et al.,2021,Nye et al. (2021),Augmented: Scratchpad: 8-digit addition (finetuning)
12,Augmented,Using open-book knowledge for fact checking,1.3E+22,7.1B,Gopher,Rae et al.,2021,Rae et al. (2021),Augmented: Using open-book knowledge for fact checking
13,Augmented,Chain-of-thought: Math word problems,1.3E+23,68B,LaMDA,Wei et al.,2022,Wei et al. (2022),Augmented: Chain-of-thought: Math word problems
14,Augmented,Chain-of-thought: StrategyQA,2.9E+23,62B,PaLM,Chowdhery et al.,2022,Chowdhery et al. (2022),Augmented: Chain-of-thought: StrategyQA
15,Augmented,Differentiable search index,3.3E+22,11B,T5,Tay et al.,2022,Tay et al. (2022),Augmented: Differentiable search index
16,Augmented,Self-consistency decoding,1.3E+23,68B,LaMDA,Wang et al.,2022,Wang et al. (2022),Augmented: Self-consistency decoding
17,Augmented,Leveraging explanations in prompting,5.0E+23,280B,Gopher,Lampinen et al.,2022,Lampinen et al. (2022),Augmented: Leveraging explanations in prompting
18,Augmented,Least-to-most prompting,3.1E+23,175B,GPT-3,Zhou et al.,2022,Zhou et al. (2022),Augmented: Least-to-most prompting
19,Augmented,Zero-shot chain-of-thought reasoning,3.1E+23,175B,GPT-3,Kojima et al.,2022,Kojima et al. (2022),Augmented: Zero-shot chain-of-thought reasoning
20,Augmented,Calibration via P(True),2.6E+23,52B,Anthropic,Kadavath et al.,2022,Kadavath et al. (2022),Augmented: Calibration via P(True)
21,Augmented,Multilingual chain-of-thought reasoning,2.9E+23,62B,PaLM,Shi et al.,2022,Shi et al. (2022),Augmented: Multilingual chain-of-thought reasoning
22,Augmented,Ask me anything prompting,1.4E+22,6B,EleutherAI,Arora et al.,2022,Arora et al. (2022),Augmented: Ask me anything prompting
