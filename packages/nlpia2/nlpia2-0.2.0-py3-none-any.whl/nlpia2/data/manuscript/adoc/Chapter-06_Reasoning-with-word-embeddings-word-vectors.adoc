= Natural Language Processing in Action, Second Edition
:chapter: 6
:part: 2
:secnums:
:sectnumoffset: 5
:leveloffset: 1
:imagesdir: .
:xrefstyle: short
:figure-caption: Figure {chapter}.
:listing-caption: Listing {chapter}.
:table-caption: Table {chapter}.
:stem: latexmath
:encoding: UTF-8
:!figure:
:!listing:

= Reasoning with word embeddings (word vectors)

This chapter covers

* Understanding word embeddings or word vectors
* Representing meaning with a vector
* Customizing word embeddings to create domain-specific nessvectors
* Reasoning with word embeddings
* Visualizing the meaning of words

_Word embeddings_ are perhaps the most approachable and generally useful tools in your NLP toolbox.
They can give your NLP pipeline a general understanding of words.
In this chapter you will learn how to apply word embeddings to real world applications.
And just as importantly you'll learn where not to use word embeddings.
And hopefully these examples will help you dream up new and interesting applications in business as well as in your personal life.

You can think of word vectors as sorta like lists of attributes for Dota 2 heroes or roll playing game (RPG) characters and monsters.
Now imagine that there was no text on these character sheets or profiles.
You would want to keep all the numbers in a consistent order so you knew what each number meant.
That's how word vectors work.
The numbers aren't labeled with their meaning.
They are just put in a consistent _slot_ or location in the vector.
That way when you add or subtract or multiply two word vectors together the attribute for "strength" in one vector lines up with the strength attribute in another vector.
Likewise for "agility" and "intelligence" and alignment or philosophy attributes in D&D (Dungeons and Dragons).

Thoughtful roll playing games often encourage deeper thinking about philosophy and words with subtle combinations of character personalities such "chaotic good" or "lawful evil."
I'm eternally grateful to my childhood Dungeon Master for opening my eyes to the false dichotomies suggested by words like "good" and "evil" or "lawful" and "chaotic".footnote:[Thank you Marc for your chaotic good influence on me!]
The word vectors you'll learn about here have room for every possible attribute of all the words in almost any text and any language.
And the word vector attributes or features are intertwined with each other in complex ways that can handle concepts like "lawful evil", "benevolent dictator" and "altruistic spite" with ease.

Learning word embeddings are often categorized as a _representation learning_ algorithm.footnote:[Representation learning methodology on Papers With Code (https://paperswithcode.com/area/methodology/representation-learning)]
The goal of any word embedding is to build a compact numerical representation of a word's "character".
These numerical representations enable a machine to process your words (or your Dota 2 character) are what a machine needs to process words in a meaningful way.

== This is your brain on words

Word embeddings are vectors we use to represent meaning.
And your brain is where meaning is stored.
Your brain is "on" words -- it is affected by them.
Just as chemicals affect a brain, so do words.
"This is your brain on drugs" was a popular slogan of the 80's anti-narcotics Television advertising campaign that featured a pair of eggs sizzling in a frying pan.footnote:["This is your brain on drugs" (https://en.wikipedia.org/wiki/This_Is_Your_Brain_on_Drugs)]

Fortunately words are much more gentle and helpful influencers than chemicals.
The image of your brain on words shown in figure 6.1 looks a little different than eggs sizzling in a frying pan.
The sketch gives you one way to imagine the neurons sparking and creating thoughts inside your brain as you read one of these sentences.
Your brain connects the meaning of these words together by firing signals to the appropriate neighbor neurons for associated words.
Word embeddings are vector representations of these connections between words.
And so they are also a crude representation of the node embeddings for the network of neuron connections in your brain.footnote:[See "Recap: Node Embeddings" by Ted Kye for San Diego Machine Learning Book Club (https://github.com/SanDiegoMachineLearning/bookclub/blob/master/graph/graphml-05-GNN1.pdf)]

[id=word_brain_embedding_figure, reftext={chapter}.{counter:figure}]
.Word embeddings in your brain
image::../images/ch06/word-brain-embedding_drawio.png[alt="Figure 6. 1: connections between clusters of neurons representing each word in the phrase 'read the words in'",width=100%,link="../images/ch06/word-brain-embedding_drawio.png"]

You can think of a word embedding as a vector representation of the pattern of neurons firing in your brain when you think about an individual word.
Whenever you think of a word, the thought creates a wave of electrical charges and chemical reactions in your brain originating at the neurons associated with that word or thought.
Neurons within your brain fire in waves, like the circluar ripples emanating out from a pebble dropped in a pond.
But these electrical signals are selectively flowing out through some neurons and not others.

As you read the words in this sentence you are sparking flashes of activity in your neurons like those in the sketch in figure <<word_brain_embedding_figure>>.
In fact, researchers have found surprising similarity in the patterns of artificial neural network weights for word embeddings, and the patterns of activity within your brain as you think about words.footnote:["Robust Evaluation of Language-Brain Encoding Experiments" (https://arxiv.org/abs/1904.02547) by Lisa Beinborn (https://beinborn.eu/)] footnote:[
footnote:["Linkng human cognitive patterns to NLP models" (https://soundcloud.com/nlp-highlights/130-linking-human-cognitive-patterns-to-nlp-models-with-lisa-beinborn) interview of Lisa Beinborn (https://beinborn.eu/)]

Electrons flowing out from neurons are like children running out of a school doorway when the school bell rings for recess.
The word or thought is like the school bell.
Of course your thoughts and the electrons in your brain are much faster than students.
You don't even have to speak or hear the word to trigger its pattern in your brain.
You just have to think it.
And like kids running out to the playground, the electrons never flow along the same paths twice.
Just as the meaning of a word evolves over time, your embedding of a word is constantly evolving.
Your brain is a never-ending language learner not too different from Cornell's Never Ending Language Learner system.footnote:["Never-Ending Language Learning" by T. Mitchell et al at Cornell (http://proai.org/NELL_aaai15.pdf)]

Some people have gotten carried away with this idea, and they imagine that you can accomplish a form of mind control with words.
When I was looking for information about NLP research on Reddit I got distracted by the `r/NLP` subreddit.
It's not what you think.
It turns out that some motivational speakers have name-squatted the word "NLP" on reddit for their 70's era "Neuro-linguistic Programming" money-making schemes.footnote:["Neuro-linguistic programming" explanation on Wikipedia (https://en.wikipedia.org/wiki/Neuro-linguistic_programming)"] footnote:["r/NLP/ Subreddit (https://www.reddit.com/r/NLP)"] footnote:["An authentic NLP subreddit at "r/NaturalLanguage/" (https://www.reddit.com/r/NaturalLanguage)"]
Fortunately word embeddings can handle this ambiguity and misinformation just fine.

You don't even have to tell the word embedding algorithm what you want the word "NLP" to mean.
It will figure out the most useful and popular meaning of the acronym based on how it is used within the text that you use to train it.
The algorithm for creating word embeddings is a self-supervised machine learning algorithm.
This means you will not need a dictionary or thesaurus to feed your algorithm.
You just need a lot of text.
Later in this chapter you will just gather up a bunch of Wikipedia articles to use as your training set.
But any text in any language will do, as long as contains a lot of words that you are interested in.

There's another "brain on words" to think about.
Words not only affect the way you think but they affect how you communicate.
And you are sorta like a neuron in the collective consciousness, the brain of society.
That "sorta" word is an especially powerful pattern of neural connections for me, because I learned what it means from Daniel Dennet's _Intuition Pumps_ book.footnote:[_Intuition Pumps and Other Tools for Thinking_ by Daniel Dennett p.96]
It invokes associations with complex ideas and words such as the concept of "gradualism" used by Turing to explain how the mechanisms behind both AI and a calculator are exactly the same.
Darwin used this concept of gradualism to explain how language-comprehending human brains can evolve from single cell organisms through simple mechanisms.

== Applications

Well, what are these awesome word embeddings good for?
Word embeddings can be used anywhere you need a machine to understand words or short N-grams.
Here are some examples of N-grams where word embeddings haven proven useful in the real world:

* Hashtags
* Tags and Keywords
* Named entities (People, Places, Things)
* Titles (Songs, Poems , Books, Articles)
* Job titles & business names
* Web page titles
* Web URLs and file paths
* Wikipedia article titles

Even there are many practical applications where your NLP pipeline could take advantage of the ability to understand these phrases using word embeddings:

* Semantic search for jobs, web pages, ...
* Tip-of-your-tongue word finder
* Rewording a title or sentence
* Sentiment shaping
* Answer word analogy questions
* Reasoning with words and names


And in the academic world researchers use word embeddings to solve some of the 200+ NLP problems: footnote:[Papers With Code topic "Word Embeddings" (https://paperswithcode.com/task/word-embeddings)]

* Part-of-Speech tagging
* Named Entity Recognition (NER)
* Analogy querying
* Similarity querying
* Transliteration
* Dependency parsing

=== Search for meaning

In the old days (20 years ago) search engines tried to find all the words you typed based on their TF-IDF scores in web pages.
And good search engines attempted would augment your search terms with synonyms.
They would sometimes even alter your words to guess what you actually "meant" when you typed a particular combination of words.
So if you searched for "sailing cat" they would change cat to catamaran to disambiguate your search for you.
Behind the scenes, while ranking your results, search engines might even change a query like "positive sum game" to "nonzero sum game" to send you to the correct Wikipedia page.

Then information retrieval researchers discovered how to make latent semantic analysis more effective -- word embeddings.
In fact, the GloVE word embedding algorithm is just latent semantic analysis on millions of sentences extracted from web pages.footnote:[Standford's open source GloVE algorithm in C (https://github.com/stanfordnlp/GloVe) and Python (https://github.com/lapis-zero09/compare_word_embedding/blob/master/glove_train.py)]
These new word embeddings (vectors) made it possible for search engines to directly match the "meaning" of your query to web pages, without having to guess your intent.
The embeddings for your search terms provide a direct numerical representation of the _intent_ of your search based on the average meaning of those words on the Internet.

[WARNING]
====
Word embeddings do not represent _your_ intended interpretation of words.
They represent the average meaning of those words for everyone that composed the documents and pages that were used to train the word embedding language model.
This means that word embeddings contain all the biases and stereotypes of all the people that composed the web pages used to train the model.
====

Search engines no longer need to do synonym substitution, stemming, lemmatization, case-folding and disambiguation based on hard-coded rules.
They create word embeddings based on the text in all the pages in their search index.
Unfortunately the dominant search engines decided to use this new-found power to match word embeddings with products and ads rather than real words.
Word embeddings for AdWords and iAds are weighted based on how much a marketer has paid to distract you from your intended search.
Basically, big tech makes it easy for corporations to bribe the search engine so that it manipulates you and trains you to become their consumption zombie.

If you use a more honest search engine such as Startpage,footnote:[Startpage proviacy-protecting web search (https://www.startpage.com/)] DISROOT,footnote:[DISROOT nonprofit search engine (https://search.disroot.org)] or Wolfram Alpha footnote:[Wolfram Alpha uses state-of-the art NLP (https://wolframalpha.com/)] you will find they give you what you're actually looking for.
And if you have some dark web or private pages and documents you want to use a as a knowledge base for your organization or personal life you can self-host a search engine with cutting edge NLP: Elastic Search,footnote:[ElasticSearch backend source code (https://github.com/elastic/elasticsearch) and frontend SearchKit demo (https://demo.searchkit.co/type/all?query=prosocial%20AI)] Meilisearch,footnote:[Meilisearch source code and self-hosting docker images (https://github.com/meilisearch/meilisearch) and managed hosting (https://www.meilisearch.com/)] SearX,footnote:[SearX git repository (https://github.com/searx/searx)] Apache Solr,footnote:[Apache Solr home page and Java source code (https://solr.apache.org/)] Apache Lucene,footnote:[Apache Lucene home page (https://lucene.apache.org/)], Qwant,footnote:[Qwant web search engine is based in Europe where regulations protect you from manipulation and deception (https://www.qwant.com/)] or Sphinx.footnote:[Sphinx home page and C source code (http://sphinxsearch.com/)]
Even PostgreSQL beats the major search engines for full-text search precision.
It will surprise you how much clearer you see the world when you are using an honest-to-goodness search engine.

These semantic search engines use vector search under the hood to query a word and document embedding (vector) database.

Open source Python tools such as NBOOST or PynnDescent let you integrate word embeddings with into your favorite TF-IDF search algorithm.footnote:["How to Build a Semantic Search Engine in 3 minutes" by Cole Thienes and Jack Pertschuk (http://mng.bz/yvjG)]
Of if you want a scalable way to search your fine tuned embeddings and vectors you can use Approximate Nearest Neighbor algorithms to index whatever vectors your like.footnote:[PynnDescent Python package (https://pypi.org/project/pynndescent/)]

That's the nice thing about word embeddings.
All that vector algebra math you are used to, such as calculating distance, that will also work for word embeddings.
Only now that distance represents how far apart the words are in _meaning_ rather than physical distance.
And these new embeddings are much more compact and dense with meaning than than the thousands of dimensions you are used to with TF-IDF vectors.

You can use the meaning distance to search a database of words for all job titles that are _near_ the job title you had in mind for your job search.
This may reveal additional job titles you hadn't even thought of.
Or your search engine could be designed to add additional words to your search query to make sure related job titles were returned.
This would be like an autocomplete search box that understands what words mean - called _semantic search_.

[source,python]
----
>>> from nessvec.indexers import Index  # <1>
>>> index = Index(num_vecs=100_000)  # <2>
>>> index.get_nearest("Engineer").round(2)
Engineer       0.00
engineer       0.23
Engineers      0.27
Engineering    0.30
Architect      0.35
engineers      0.36
Technician     0.36
Programmer     0.39
Consultant     0.39
Scientist      0.39
----
<1> `pip install nessvec` (see Appendix A)
<2> 100k of the 1M embeddings in this FastText vocabulary

You can see that finding the nearest neighbors of an word embedding is kind of like looking up a word in a Thesaurus.
But this is a much fuzzier and complete thesaurus than you'll find at your local book shop or online dictionary.
And you will soon see how you can customize this dictionary to work within any domain you like.
For example you could train it to work with job postings only from the UK or perhaps even India or Australia, depending on your region of interest.
Or you could train it to work better with tech jobs in Silicon Valley rather than finance and banking jobs in New York.
You can even train it on 2-grams and 3-grams if you want it to work on longer job titles like "Software Developer" or "NLP Engineer".

Another nice thing about word embeddings is that they are _fuzzy_.
You may have noticed several nearby neighbors of "Engineer" that you'd probably not see in a thesaurus.
And you can keep expanding the list as far as you like.
So if you were thinking of a Software Engineer rather than an Architect you might want to scan the `get_nearest()` list for another word to do a search for, such as "Programmer":


[source,python]
----
>>> index.get_nearest("Programmer").round(2)
Programmer    -0.00
programmer     0.28
Developer      0.33
Programmers    0.34
Programming    0.37
Engineer       0.39
Software       0.40
Consultant     0.42
programmers    0.42
Analyst        0.42
dtype: float64
>>> index.get_nearest("Developer").round(2)
Developer     -0.00
developer      0.25
Developers     0.25
Programmer     0.33
Software       0.35
developers     0.37
Designer       0.38
Architect      0.39
Publisher      0.39
Development    0.40
----

Well that's surprising.
It seems that the title "Developer" is often also associated with the word "Publisher."
I would have never guessed why this would be before having worked with the Development Editors, Development Managers, and even a Tech Development Editor at Manning Publishing.
Just today these "Developers" cracked the whip to get me moving on writing this Chapter.

=== Combining word embeddings

Another nice thing about word embeddings is that you can combine them any way you like to create new words!
Well, of course, you can combine multiple words the old fashioned way just appending the strings together.
In Python you do that with addition or the `+` operator:

[source,python]
----
>>> "Chief" + "Engineer"
'ChiefEngineer'
>>> "Chief" + " " + "Engineer"
'Chief Engineer'
----

Word embedding math works even better than that.
You can add the meanings of the words together to try to find a single word that captures the meaning of the two words you added together

[source,python]
----
>>> chief = (index.data[index.vocab["Chief"]]
...     + index.data[index.vocab["Engineer"]])
>>> index.get_nearest(chief)
Engineer     0.110178
Chief        0.128640
Officer      0.310105
Commander    0.315710
engineer     0.329355
Architect    0.350434
Scientist    0.356390
Assistant    0.356841
Deputy       0.363417
Engineers    0.363686
----

So if you want to one day become a "Chief Engineer" it looks like "Scientist", "Architect", and "Deputy" might also be job titles you'll encounter along the way.

What about that tip-of-your-tongue word finder application mentioned at the beginning of this chapter?
Have you ever tried to recall a famous person's name while only have a general impression of them, like maybe this:

[quote]
____
She invented something to do with physics in Europe in the early 20th century.
____

If you enter that sentence into Google or Bing, you may not get the direct answer you are looking for, "Marie Curie."
Google Search will most likely only give you links to lists of famous physicists, both men and women.

You would have to skim several pages to find the answer you are looking for.
But once you found "Marie Curie," Google or Bing would keep note of that.
They might get better at providing you search results the next time you look for a scientist.
(At least, that is what it did for us in researching this book.
We had to use private browser windows to ensure that your search results would be similar to ours.)

With word embeddings, you can search for words or names that combine the meaning of the words "woman," "Europe," "physics," "scientist," and "famous," and that would get you close to the token "Marie Curie" that you are looking for.
And all you have to do to make that happen is add up the vectors for each of those words that you want to combine:

[source,python]
----
>>> answer_vector = wv['woman'] + wv['Europe'] + wv['physics'] +
...     wv['scientist']
----

In this chapter, we show you the exact way to do this query.
You can even see how you might be able to use word embedding math to subtract out some of the gender bias within a word:

[source,python]
----
>>> answer_vector = wv['woman'] + wv['Europe'] + wv['physics'] +\
...     wv['scientist'] - wv['male'] - 2 * wv['man']
----

With word embeddings, you can take the "man" out of "woman"!

=== Analogy questions

What if you could rephrase your question as an analogy question?
What if your "query" was something like this:

[quote]
____
Who is to nuclear physics what Louis Pasteur is to germs?
____

Again, Google Search, Bing, and even Duck Duck Go are not much help with this one.footnote:[Try them all if you don't believe us.]
But with word embeddings, the solution is as simple as subtracting "germs" from "Louis Pasteur" and then adding in some "physics":

[source,python]
----
>>> answer_vector = wv['Louis_Pasteur'] - wv['germs'] + wv['physics']
----

And if you are interested in trickier analogies about people in unrelated fields, such as musicians and scientists, you can do that, too.

[quote]
____
Who is the Marie Curie of music?
____

OR

[quote]
____
Marie Curie is to science as who is to music?
____

Can you figure out what the vector space math would be for that question?

You might have seen questions like these on the English analogy section of standardized tests such as SAT, ACT, or GRE exams.
Sometimes they are written in formal mathematical notation like this:

[source,code]
----
MARIE CURIE : SCIENCE :: ? : MUSIC
----

Does that make it easier to guess the vector math for these words?
One possibility is this:

[source,python]
----
>>> wv['Marie_Curie'] - wv['science'] + wv['music']
----

And you can answer questions like this for things other than people and occupations, like perhaps sports teams and cities:

[quote]
____
The Timbers are to Portland as what is to Seattle?"
____

In standardized test form, that is:

[source,code]
----
TIMBERS : PORTLAND :: ? : SEATTLE
----

But, more commonly, standardized tests use English vocabulary words and ask less fun questions, like the following:

[source,code]
----
WALK : LEGS :: ? : MOUTH
----

OR

[source,code]
----
ANALOGY : WORDS :: ? : NUMBERS
----

All those "tip of the tongue" questions are a piece of cake for word embeddings, even though they are not multiple choice.
It can be difficult to get analogy questions right, even when you have multiple choice options to choose from.
NLP comes to the rescue with word embeddings.

Word embeddings can be used to answer even these vague questions and analogy problems.
Word embeddings can help you remember any word or name on the tip of your tongue, as long as the vector for the answer exists in your vocabulary.
(For Google's pretrained Word2Vec model, your word is almost certainly within the 100B word news feed that Google trained it on, unless your word was created after 2013.)
And embeddings work well even for questions that you cannot even pose in the form of a search query or analogy.

You can learn about some of the math with embeddings in the "analogical reasoning" section later in this chapter.

=== Word2Vec Innovation

Words that are used near each other sort of pile up on top of each other in our minds and eventually define what those words mean within the connections of the neurons of our brains.
As a toddler you hear people talking about things like "soccer balls," "fire trucks," "computers," and "books," and you can gradually figure out what each of them is.
The surprising thing is that your machine does not need a body or brain to understand words as well as a toddler.

A child can learn a word after pointing out objects in the real world or a picture book a few times.
A child never needs to read a dictionary or thesaurus.
Like a child, a machine "figures it out" without a dictionary or thesaurus or any other supervised machine learning dataset.
A machine does not even need to see objects or pictures.
The machine is completely self-supervised by the way you parse the text and set up the dataset.
All you need is a lot of text.

In previous chapters, you could ignore the nearby context of a word.
All you needed to do was count up the uses of a word within the same _document_.
It turns out, if you make your documents very very short, these counts of co-occurrences become useful for representing the meaning of words themselves.
This was the key innovation of Tomas Mikolov and his Word2vec NLP algorithm.
John Rubert Firth popularized the concept that "a word is characterized by the company it keeps."footnote:[See wikipedia article (https://en.wikipedia.org/wiki/John_Rupert_Firth)]
But to make word embeddings useful required Tomas Mikolov's focus on a very small "company" of words and the computational power of 21st century computers as well as massive corpora machine-readable text.
You do not need a dictionary or thesaurus to train your word embeddings.
You only need a large body of text.

That is what you are going to do in this chapter.
You are going to teach a machine to be a sponge, like a toddler.
You are going to help machines figure out what words mean, without ever explicitly labeling words with their dictionary definitions.
All you need is a bunch of random sentences pulled from any random book or web page.
Once you tokenize and segment those sentences, which you learned how to do in previous chapters, your NLP pipeline will get smarter and smarter each time it reads a new batch of sentences.

In Chapters 2 and 3 you isolated words from their neighbors and only worried about whether they were present or absent in each _document_.
You ignored the effect the neighbors of a word have on its meaning and how those relationships affect the overall meaning of a statement.
Our bag-of-words concept jumbled all the words from each document together into a statistical bag.
In this chapter, you will create much smaller bags of words from a "neighborhood" of only a few words, typically fewer than ten tokens.
You will also ensure that these neighborhoods have boundaries to prevent the meaning of words from spilling over into adjacent sentences.
This process will help focus your word-embedding language model on the words that are most closely related to one another.

Word embeddings can help you identify synonyms, antonyms, or words that just belong to the same category, such as people, animals, places, plants, names, or concepts.
We could do that before, with semantic analysis in Chapter 4, but your tighter limits on a word's neighborhood will be reflected in tighter accuracy on the word embeddings.
Latent semantic analysis (LSA) of words, _n_-grams, and documents did not capture all the literal meanings of a word, much less the implied or hidden meanings.
Some of the connotations of a word are fuzzier for LSA's oversized bags of words.

.Word embeddings
[IMPORTANT, definition]
====
Word embeddings (sometimes called _word vectors_) are high-dimensional numerical vector representations of what a word means, including its literal and implied meaning.
So word embeddings can capture the _connotation_ of words.
Somewhere inside an embedding, there is a score for  "peopleness," "animalness," "placeness," "thingness" and even "conceptness."
And a word embedding combines all those scores, and all the other _ness_ of words, into a dense vector (no zeros) of floating point values.
====

The density and high (but not too high) dimensionality of word embeddings is a source of their power as well as their limitations.
This is why dense, high-dimensional embeddings are most valuable when you use them in your pipeline along side sparse hyper-dimensional TFIDF vectors or discrete bag-of-words vectors.

== Artificial Intelligence Relies on Embeddings

Word embeddings were a big leap forward in not only natural language understanding accuracy but also a breakthrough in the hope for Artificial General Intelligence, or AGI.

Do you think you could tell the difference between intelligent and unintelligent messages from a machine?
It may not be as obvious as you think.
Even the "deep minds" at BigTech were fooled by the surprisingly unintelligent answers from their latest and greatest chatbots in 2023, Bing and Bard.
Simpler, more authentic conversational search tools such as you.com and neeva.com and their chat interfaces outperform BigTech search on most Internet research tasks.

The philosopher Douglas Hofstader pointed out a few things to look out for when measuring intelligence. footnote[Douglas R. Hofstadter, "Gödel, Escher, Bach: an Eternal Golden Braid (GEB), p. 26]

* flexibility
* dealing with ambiguity
* ignoring irrelevant details
* finding similarities and analogies
* generating new ideas

You'll soon see how word embeddings can enable these aspects of intelligence within your software.
For example, word embeddings make it possible to respond with flexibility by giving words fuzziness and nuance that previous representations like TF-IDF vectors could not.
In previous iterations of your chatbot, you would have to enumerate all the possible ways to say "Hi" if you want your bot to be flexible in its response to common greetings.

But with word embeddings you can recognize the *meaning* of the word "hi", "hello", and "yo" all with a single embedding vector.
And you can create embeddings for all the concepts your bot is likely to encounter by just feeding it as much text as you can find.
There is no need to hand-craft your vocabularies anymore.

[CAUTION]
====
Like word embeddings, intelligence itself is a high-dimensional concept.
This makes _Artificial General Intelligence_ (AGI) an elusive target.
Be careful not to allow your users or your bosses to think that your chatbot is generally intelligent, even if it appears to achieve all of Hofstadter's "essential elements."
====


== Word2Vec

In 2012, Tomas Mikolov, an intern at Microsoft, found a way to embed the meaning of words into vector space.
Word embeddings or word vectors typically have 100 to 500 dimensions, depending on the breadth of information in the corpus used to train them.
Mikolov trained a neural network to predict word occurrences near each target word.
Mikolov used a network with a single hidden layer, so almost any linear machine learning model will also work.
Logistic regression, truncated SVD, linear discriminant analysis, or Naive Bayes would all work well and were used successfully by others to duplicate Mikolov's results.
In 2013, once at Google, Mikolov and his teammates released the software for creating these word vectors and called it "Word2Vec."footnote:["Efficient Estimation of Word Representations in Vector Space" Sep 2013, Mikolov, Chen, Corrado, and Dean (https://arxiv.org/pdf/1301.3781.pdf).]

The Word2Vec language model learns the meaning of words merely by processing a large corpus of unlabeled text.
No one has to label the words in the Word2Vec vocabulary.
No one has to tell the Word2Vec algorithm that "Marie Curie" is a scientist, that the "Timbers" are a soccer team, that Seattle is a city, or that Portland is a city in both Oregon and Maine.
And no one has to tell Word2Vec that soccer is a sport, or that a team is a group of people, or that cities are both "places" as well as "communities."
Word2Vec can learn that and much more, all on its own!
All you need is a corpus large enough to mention "Marie Curie," "Timbers," and "Portland" near other words associated with science, soccer, or cities.

This unsupervised nature of Word2Vec is what makes it so powerful.
The world is full of unlabeled, uncategorized, and unstructured natural language text.

_Unsupervised_ learning and _supervised_ learning are two radically different approaches to machine learning.

.Supervised learning
[IMPORTANT, definition]
====
In supervised learning, a human or team of humans must label data with the correct value for the target variable.
An example of a label is the "spam" categorical label for an SMS message in chapter 4.
A more difficult label for a human might be a percentage score for the hotness connotation of the word "red" or "fire".
Supervised learning is what most people think of when they think of machine learning.
A supervised model can only get better if it can measure the difference between the expected output (the label) and its predictions.
====

In contrast, unsupervised learning enables a machine to learn directly from data, without any assistance from humans.
The training data does not have to be organized, structured, or labeled by a human.
So unsupervised learning algorithms like Word2Vec are perfect for natural language text.

.Unsupervised learning
[IMPORTANT, definition]
====
In unsupervised learning, you train the model to perform a task, but without any labels, only the raw data.
Clustering algorithms such as k-means or DBSCAN are examples of unsupervised learning.
Dimension reduction algorithms like principal component analysis (PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE) are also unsupervised machine learning techniques.
In unsupervised learning, the model finds patterns in the relationships between the data points themselves.
An unsupervised model can get smarter (more accurate) just by throwing more data at it.
====

Instead of trying to train a neural network to learn the target word meanings directly (on the basis of labels for that meaning) you can teach the network to predict words near the target word in your sentences.
So in this sense, you do have labels: the nearby words you are trying to predict.
But because the labels are coming from the dataset itself and require no hand-labeling, the Word2Vec training algorithm is definitely an unsupervised learning algorithm.

Another domain where this unsupervised training technique is used in time series modeling.
Time series models are often trained to predict the next value in a sequence based on a window of previous values.
Time series problems are remarkably similar to natural language problems in a lot of ways because they deal with ordered sequences of values (words or numbers).

And the prediction itself is not what makes Word2Vec work.
The prediction is merely a means to an end.
What you do care about is the internal representation, the vector, that Word2Vec gradually builds up to help it generate those predictions.
This representation will capture much more of the meaning of the target word (its semantics) than the word-topic vectors that came out of latent semantic analysis (LSA) and latent Dirichlet allocation (LDiA) in chapter 4.

[NOTE]
====
Models that learn by trying to re-predict the input using a lower-dimensional internal representation are called _autoencoders_.
This may seem odd to you.
It is like asking the machine to echo back what you just asked them, only they cannot write the question down as you are saying it.
The machine has to compress your question into shorthand.
And it has to use the same shorthand algorithm (function) for all the questions you ask it.
The machine learns a new shorthand (vector) representation of your statements.

If you want to learn more about unsupervised deep learning models that create compressed representations of high-dimensional objects like words, search for the term "autoencoder."footnote:[See the web page titled "Unsupervised Feature Learning and Deep Learning Tutorial" (http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/).]
They are also a common way to get started with neural nets, because they can be applied to almost any dataset.
====

Word2Vec will learn about things you might not think to associate with all words.
Did you know that every word has some geography, sentiment (positivity), and gender associated with it?
If any word in your corpus has some quality, like "placeness", "peopleness", "conceptness" or "femaleness", all the other words will also be given a score for these qualities in your word vectors.
The meaning of a word "rubs off" on the neighboring words when Word2Vec learns word vectors.

All words in your corpus will be represented by numerical vectors, similar to the word-topic vectors discussed in chapter 4.
Only this time the "topics" mean something more specific, more precise.
In LSA, words only had to occur in the same document to have their meaning "rub off" on each other and get incorporated into their word-topic vectors.
For Word2Vec word vectors, the words must occur near each other -- typically fewer than five words apart and within the same sentence.
And Word2Vec word vector "topic" weights can be added and subtracted to create new word vectors that mean something!

////
KM: Nice explanation below. Is there a way you could illustrate this visually for the readers?
HL: I haven't thought of a good one yet.
////

A mental model that may help you understand word vectors is to think of word vectors as a list of weights or scores.
Each weight or score is associated with a specific dimension of meaning for that word.

[id=compute_nessvector_code, reftext={chapter}.{counter:listing}]
.Compute nessvector
[source,python]
----
>>> from nessvec.examples.ch06.nessvectors import *  # <1>
>>> nessvector('Marie_Curie').round(2)
placeness     -0.46
peopleness     0.35  # <2>
animalness     0.17
conceptness   -0.32
femaleness     0.26
----
<1> Don't import this module unless you have a lot of RAM and a lot of time. The pretrained `word2vec` model is huge.
<2> Get creative with nessvec dimensions you find fun, like "trumpness" and "ghandiness". How about an `nessvec` PR?

You can compute "nessvectors" for any word or _n_-gram in the Word2Vec vocabulary using the tools from  `nlpia` (https://gitlab.com/tangibleai/nessvec/-/blob/main/src/nessvec/examples/ch06/nessvectors.py). And this approach will work for any "ness" components that you can dream up.


Mikolov developed the Word2Vec algorithm while trying to think of ways to numerically represent words in vectors.
He wasn't satisfied with the less accurate word sentiment math you did in chapter 4.
He wanted to do _analogical reasoning_, like you just did in the previous section with those analogy questions.
This concept may sound fancy, but really it just means that you can do math with word vectors and that the answer makes sense when you translate the vectors back into words.
You can add and subtract word vectors to _reason_ about the words they represent and answer questions similar to your examples above, like the following.
(For those not up on sports in the US, the Portland Timbers and Seattle Sounders are Major League Soccer teams.)

[source,code]
----
wv['Timbers'] - wv['Portland'] + wv['Seattle'] = ?
----

Ideally, you'd like this math (word vector reasoning) to give you this:

[source,code]
----
wv['Seattle_Sounders']
----

Similarly, your analogy question "'Marie Curie' is to 'physics' as ____ is to 'classical music'?" can be thought about as a math expression like this:

[source,code]
----
wv['Marie_Curie'] - wv['physics'] + wv['classical_music'] = ?
----

In this chapter, we want to improve on the LSA word vector representations we introduced in chapter 4.
Topic vectors constructed from entire documents using LSA are great for document classification, semantic search, and clustering.
But the topic-word vectors that LSA produces aren't accurate enough to be used for semantic reasoning or classification and clustering of short phrases or compound words.
You'll soon learn how to train the single-layer neural networks required to produce these more accurate, more fun, word vectors.
And you'll see why they have replaced LSA word-topic vectors for many applications involving short documents or statements.

=== Analogy reasoning

Word2Vec was first presented publicly in 2013 at the ACL conference.footnote:[See the PDF "Linguistic Regularities in Continuous Space Word Representations" by Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig (https://www.aclweb.org/anthology/N13-1090).]
The talk with the dry-sounding title "Linguistic Regularities in Continuous Space Word Representations" described a surprisingly accurate language model.
Word2Vec embeddings were four times more accurate (45%) compared to equivalent LSA models (11%) at answering analogy questions like those above.footnote:[See Radim Řehůřek's interview of Tomas Mikolov (https://rare-technologies.com/rrp#episode_1_tomas_mikolov_on_ai).]
The accuracy improvement was so surprising, in fact, that Mikolov's initial paper was rejected by the International Conference on Learning Representations.footnote:[See "ICRL2013 open review" (https://openreview.net/forum?id=idpCdOWtqXd60&noteId=C8Vn84fqSG8qa).]
Reviewers thought that the model's performance was too good to be true.
It took nearly a year for Mikolov's team to release the source code and get accepted to the Association for Computational Linguistics.

Suddenly, with word vectors, questions like

[source,code]
----
Portland Timbers + Seattle - Portland = ?
----

can be solved with vector algebra (see figure 6.1).

[id=geometry_of_word2vec_math_figure, reftext={chapter}.{counter:figure}]
.Geometry of Word2Vec math
image::../images/ch06/vector_add.png[Geometry of Word2Vec Math,width=100%,link="../images/ch06/vector_add.png"]

The `word2vec` language model "knows" that the terms "Portland" and "Portland Timbers" are roughly the same distance apart as "Seattle" and "Seattle Sounders".
And those vector displacements between the words in each pair are in roughly the same direction.
So the `word2vec` model can be used to answer your sports team analogy question.
You can add the difference between "Portland" and "Seattle" to the vector that represents the "Portland Timbers".
That should get you close to the vector for "Seattle Sounders".


*Equation 6.1 Compute the answer to the soccer team question*

image::../images/ch06/equations/equation_6_1.png[]
// [stem]
// ++++
// \begin{bmatrix} 0.0168\\ 0.007\\ 0.247\\ ... \end{bmatrix} + \begin{bmatrix} 0.093\\ -0.028\\ -0.214\\ ... \end{bmatrix} - \begin{bmatrix} 0.104\\  0.0883\\ -0.318\\ ... \end{bmatrix} = \begin{bmatrix} 0.006\\ -0.109\\  0.352\\ ... \end{bmatrix}
// ++++


After adding and subtracting word vectors, your resultant vector will almost never exactly equal one of the vectors in your word vector vocabulary. Word2Vec word vectors usually have 100s of dimensions, each with continuous real values.
Nonetheless, the vector in your vocabulary that is closest to the resultant will often be the answer to your NLP question.
The English word associated with that nearby vector is the natural language answer to your question about sports teams and cities.

Word2Vec allows you to transform your natural language vectors of token occurrence counts and frequencies into the vector space of much lower-dimensional Word2Vec vectors.
In this lower-dimensional space, you can do your math and then convert them back to a natural language space.
You can imagine how useful this capability is to a chatbot, search engine, question-answering system, or information extraction algorithm.

[NOTE]
====
The initial paper in 2013 by Mikolov and his colleagues was able to achieve an answer accuracy of only 40%.
But back in 2013, the approach outperformed any other semantic reasoning approach by a significant margin.
Since its initial publication, the performance of Word2Vec has improved further.
This was accomplished by training it on extremely large corpora.
The reference implementation was trained on the 100 billion words from the Google News Corpus.
This is the pre-trained model you'll see used in this book a lot.
====

The research team also discovered that the difference between a singular and a plural word is often roughly the same magnitude, and in the same direction:


*Equation 6.2 Distance between the singular and plural versions of a word*

image::../images/ch06/equations/equation_6_2.png[]
// [stem]
// ++++
// \vec{x}_{coffee} - \vec{x}_{coffees} \approx \vec{x}_{cup} - \vec{x}_{cups} \approx \vec{x}_{cookie} - \vec{x}_{cookies}
// ++++

//image::../images/ch06/stem2.png[]

But their discovery didn't stop there.
They also discovered that distance relationships go far beyond simple singular versus plural relationships.
Distances apply to other semantic relationships.
The Word2Vec researchers soon discovered they could answer questions that involve geography, culture, and demographics, like this:

[source,text]
----
"San Francisco is to California as what is to Colorado?"
----

[source,code]
----
San Francisco - California + Colorado = Denver
----

==== More reasons to use word vectors

Vector representations of words are useful not only for reasoning and analogy problems but also for all the other things you use natural language vector space models for.
From pattern matching to modeling and visualization, your NLP pipeline's accuracy and usefulness will improve if you know how to use the word vectors from this chapter.

For example, later in this chapter, we show you how to visualize word vectors on 2D "semantic maps" like the one shown in Figure <<word_vectors_for_ten_us_cities_projected_onto_a_2d_map_figure>>.
You can think of this like a cartoon map of a popular tourist destination or one of those impressionistic maps you see on bus stop posters.
In these cartoon maps, things that are close to each other semantically as well as geographically get squished together.
For cartoon maps, the artist adjusts the scale and position of icons for various locations to match the "feel" of the place.
With word vectors, the machine too can have a feel for words and places and how far apart they should be.

So your machine will be able to generate impressionistic maps like the one in figure 6.3 using word vectors you are learning about in this chapter.footnote:[You can find the code for generating these interactive 2D word plots in http://mng.bz/M5G7.]

[id=word_vectors_for_ten_us_cities_projected_onto_a_2d_map_figure, reftext={chapter}.{counter:figure}]
.Word vectors for ten US cities projected onto a 2D map
image::../images/ch06/us-10-city-word-vector-pca-map-labeled.png["Figure 6.3", width=100%, link="../images/ch06/us-10-city-word-vector-pca-map-labeled.png", alt="Figure 6.3: 2D plot with two blue dots labeled Chicago and Philadelphia in the upper right. San Jose, San Diego, and Los Angeles form a triangle to the far left. Houston and Dallas are at the bottom center, nearly on top of each other, and Austin is right above them. Jacksonville is to the lower right. Phoenix, AZ is right in the middle of the plot."]

If you're familiar with these US cities, you might realize that this isn't an accurate geographic map, but it's a pretty good semantic map.
I, for one, often confuse the two large Texas cities, Houston and Dallas, and they have almost identical word vectors.
And the word vectors for the big California cities make a nice triangle of culture in my mind.

And word vectors are great for chatbots and search engines too.
For these applications, word vectors can help overcome some of the rigidity, brittleness of pattern, or keyword matching.
Say you were searching for information about a famous person from Houston, Texas, but didn't realize they'd moved to Dallas.
From Figure <<word_vectors_for_ten_us_cities_projected_onto_a_2d_map_figure>>, you can see that a semantic search using word vectors could easily figure out a search involving city names such as Denver and Houston.
And even though character-based patterns wouldn't understand the difference between "tell me about a Denver omelette" and "tell me about the Denver Nuggets", a word vector pattern could.
Patterns based on word vectors would likely be able to differentiate between the food item (omelette) and the basketball team (Nuggets) and respond appropriately to a user asking about either.

=== Learning word embeddings

Word embeddings are vectors that represent the meaning (semantics) of words.
However, the meaning of words is an elusive, fuzzy thing to capture.
An isolated individual word has a very ambiguous meaning.
Here are some of the things that can affect the meaning of a word:

* Whose thought is being communicated with the word
* Who is the word intended to be understood by
* The context (where and when) the word is being used
* The domain knowledge or background knowledge assumed
* The sense of the word intended

Your brain will likely understand a word quite differently than mine.
And the meaning of a word in your brain changes over time.
You learn new things about a word as you make new connections to other concepts.
And as you learn new concepts and words, you learn new connections to these new words depending on the impression of the new words on your brain.
Embeddings are used to represent this evolving pattern of neuron connections in your brain created by the new word.
And these new vectors have 100s of dimensions.

Imagine a young girl who says "My mommy is a doctor."footnote:[See Part III. "Tools for thinking about Meaning or Content" p 59 and chapter 15 "Daddy is a doctor" p. in the book "Intuition Pumps and Other Tools for Thinking" by Daniel C. Dennett]
Imagine what the word "doctor" means to her.
And then think about how her understanding of that word, her NLU processing algorithm, evolves as she grows up.
Over time she will learn to differentiate between a medical doctor (M.D.) and an academic doctor of philosophy (Ph.D.).
Imagine what that word means to her just a few years later when she herself begins to think about the possibility of applying to med school or a Ph.D. program.
And imagine what that word means to her father or her mother, the doctor.
And imagine what that word means to someone who doesn't have access to healthcare.

Creating useful numerical representations of words is tricky.
The meaning you want to encode or embed in the vector depends not only on whose meaning you want to represent but also on when and where you want your machine to process and understand that meaning.
In the case of GloVe, Word2Vec and other early word embeddings the goal was to represent the "average" or most popular meaning.
The researchers creating these representations were focused on analogy problems and other benchmark tests that measure human and machine understanding of words.
For example, we used pretrained fastText word embeddings for the code snippets earlier in this chapter.

[TIP]
====
Pretrained word vector representations are available for corpora like Wikipedia, DBPedia, Twitter, and Freebase.footnote:[See the web page titled "GitHub - 3Top/word2vec-api: Simple web service providing a word embedding model" (https://github.com/3Top/word2vec-api#where-to-get-a-pretrained-model).]
These pretrained models are great starting points for your word vector applications.

- Google provides a pretrained `word2vec` model based on English Google News articles.footnote:[Original Google 300D Word2Vec model on Google Drive (https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM)]
- Facebook published their word models, called _fastText_, for 294 languages.footnote:[See the web page titled "GitHub - facebookresearch/fastText: Library for fast text representation and classification." (https://github.com/facebookresearch/fastText).]
====

Fortunately, once you've decided your "audience" or "users" for the word embeddings, you only need to gather up example usages of those words.
Word2Vec, GloVe, and fastText are all unsupervised learning algorithms.
All you need is some raw text from the _domain_ that you and your users are interested in.
If you are mainly interested in medical doctors you can train your embeddings on a collection of texts from medical journals.
Or if you want the most general understanding of words represented in your vectors, ML engineers often use Wikipedia and online news articles to capture the meaning of words.
After all, Wikipedia represents our collective understanding of everything in the world.

Now that you have your corpus how exactly do you create a training set for your word embedding language model?
In the early days there were two main approaches:

1. _Continuous bag-of-words_ (CBOW)
2. Continuous _skip-gram_

The _continuous bag-of-words_ (CBOW) approach predicts the target word (the output or "target" word) from the nearby context words (input words).
The only difference with the bag-of-words (BOW) vectors you learned about in chapter 3 is that a CBOWs are created for a continuously sliding window of words within each document.
So you will have almost as many CBOW vectors as you have words in the sequence of words from all of your documents.
Whereas for the BOW vectors you only had one vector for each document.
This gives your word embedding training set a lot more information to work with so it will produce more accurate embedding vectors.
With the CBOW approach, you create a huge number of tiny synthetic documents from every possible phrase you can extract from your original documents.

[id=cbow_neural_network_architecture, reftext={chapter}.{counter:figure}]
.CBOW neural network architecture
image::../images/ch06/word2vec-cbow-whatever-affects-one_drawio.png["Figure 6.4",width=100%,alt="Figure 6.4: CBOW neural network architecture showing 5 highlighted words in the phrase 'Whatever affects one affects us all'. Those 5 words then propogate through the network top to bottom to converge in the weights used for the word embedding at a single dense vector in the middle for each of the 5 words.",link="../images/ch06/word2vec-cbow-whatever-affects-one_drawio.png"]

For the _skip-gram_ approach you also create this huge number of synthetic documents.
You just reverse the prediction target so that you're using the CBOW targets to predict the CBOW features. predicts the context words ("target" words) from a word of interest (the input word).
Though these may seem like your pairs of words are reversed, you will see soon that the results are almost mathematically equivalent.

[id=skip_gram_neural_network_architecture, reftext={chapter}.{counter:figure}]
.Skip-gram neural network architecture
image::../images/ch06/word2vec-skip-gram-whatever-affects-one_drawio.png["Figure 6.5", width=100%, alt="Figure 6.5: Skip-gram neural network architecture showing 5 highlighted words in the phrase 'Whatever affects one affects us all'. The center word being skipped, 'one', propogates through the network top to bottom to be split into 4 neighbor predictions producing the 4 weights used for the word embedding at a single dense vector in the middle for each of the 4 words.", link="../images/ch06/word2vec-skip-gram-whatever-affects-one_drawio.png"]

You can see how the two neural approaches produce the same number of training examples and create the same number of training examples for both the skip-gram and CBOW approach.

==== Skip-gram approach

In the skip-gram training approach, you predict a word in the neighborhood of the context word.
Imagine your corpus contains this wise rejection of individualism by Bayard Rustin and Larry Dane Brimner.footnote:[Wikipedia on Bayard Rustin (https://en.wikipedia.org/wiki/Bayard_Rustin) a civil right leader and Larry Dane Brimner (https://en.wikipedia.org/wiki/Larry_Dane_Brimner) an author of more than 150 children's books]

.Rustin on Individualism
[quote#rustin-individualism, Bayard Rustin, "_We Are One: The Story of Bayard Rustin_, 2007, p.46_ by Larry Dane Brimner"]
____
We are all one. And if we don't know it, we will find out the hard way.
____


[IMPORTANT, definition]
.Definition
====
A _skip-gram_ is a 2-gram or pair of grams where each gram is within the neighborhood of each other.
As usual, the grams can be whatever chunks of text your tokenizer is designed to predict - usually words.
====

For the continuous skip-gram training approach, skip-grams are word pairs that skip over zero to four words to create the skip-gram pair.
When training word embeddings using the Word2Vec skip-gram method, the first word in a skip-gram is called the "context" word.
The context word is the input to the Word2Vec neural network.
The second word in the skip-gram pair is often called the "target" word.
The target word is the word that the language model and embedding vector is being trained to predict - the output.

[id=training_input_and_output_example_for_the_skip-gram_approach_figure, reftext={chapter}.{counter:figure}]
.Training input and output example for the skip-gram approach
image::../images/ch06/we-are-all-one_drawio.png["Flow diagram for creating 2-grams for a window halfwidth of 3 (full width of 6) and skips of 1, 2, 3 and -1, -2, -3",width=100%,alt="Figure 6.3: Skip-gram method for creating training set of paired context and target words",link="../images/ch06/we-are-all-one_drawio.png"]

In Figure <<training_input_and_output_example_for_the_skip-gram_approach_figure>>, you can see how the neural network architecture looks like for the skip-gram approach to creating word embeddings.


==== What is softmax?

The _softmax function_ is often used as the activation function in the output layer of neural networks when the network's goal is to learn classification problems. The softmax will squash the output results between 0 and 1, and the sum of all output notes will always add up to 1. That way, the results of an output layer with a softmax function can be considered as probabilities.

For each of the _K_ output nodes, the softmax output value of the can be calculated using the normalized exponential function:



image::../images/ch06/equations/equation_6_3.png[]
// [stem]
// ++++
// \sigma(z)_j = \frac{e^{z_j}}{\sum_{k=1}^{k}{e^{z_k}}}
// ++++

If your output vector of a three-neuron output layer looks like this:

//[[example_3d_vector_eq]]
*Equation 6.3 Example 3D vector*

image::../images/ch06/equations/equation_6_4.png[]
// [stem]
// ++++
// v = \begin{bmatrix}0.5\\ 0.9\\ 0.2\end{bmatrix}
// ++++



The "squashed" vector after the softmax activation would look like this:


//[[example_3d_vector_after_softmax_eq]]
//[width=66%]
*Equation 6.4 Example 3D vector after softmax*

image::../images/ch06/equations/equation_6_5.png[]
// [stem]
// ++++
// \sigma(v) = \begin{bmatrix}0.309\\ 0.461\\ 0.229 \end{bmatrix}
// ++++



Notice that the sum of these values (rounded to 3 significant digits) is approximately 1.0, like a probability distribution.

Figure 6.4 shows the numerical network input and output for the first two surrounding words. In this case, the input word is "Monet", and the expected output of the network is either "Claude" or "painted", depending on the training pair.

[id=network-example, reftext={chapter}.{counter:figure}]
.Network example for the skip-gram training
image::../images/ch06/skipgram.png[Network example for the skip-gram training,width=100%,alt="Figure 6.4: Network example for the skip-gram training",link="../images/ch06/skipgram.png"]

[NOTE]
====
When you look at the structure of the neural network for word embedding, you'll notice that the implementation looks similar to what you discovered in chapter 5.
====

=== Learning meaning without a dictionary

For this Word2Vec training example you won't need to use a dictionary, such as `wiktionary.org` to explicitly define the meaning of words.
Instead you can just have Word2Vec read text that contains meaningful sentences.
You'll use the WikiText2 corpus that comes with PyTorch in the `torchtext` package.

[source,python]
----
>>> import torchtext

>>> dsets = torchtext.datasets.WikiText2()
>>> num_texts = 10000

>>> filepath = DATA_DIR / f'WikiText2-{num_texts}.txt'
>>> with open(filepath, 'wt') as fout:
...     fout.writelines(list(dsets[0])[:num_texts])
----

To make it even less mysterious you can look at the text file you just created with about 10,000 paragraphs of from the `WikiText2` dataset:

[source,python]
----
>>> !tail -n 3 ~/nessvec-data/WikiText2-10000.txt

When Marge leaves Dr. Zweig 's office , she says ,
" Whenever the wind whistles through the leaves ,
I 'll think , Lowenstein , Lowenstein … " .
This is a reference to The Prince of Tides ; the <unk> is Dr. Lowenstein .

= = Reception = =
----

The 99,998th paragraph just happens to contain the abbreviation "Dr.".
In this case the abbreviation is for the word "doctor."
You can use this to practice your "Mommy is a doctor" intuition pump.
So you'll soon find out whether Word2Vec can learn what a doctor really is.
Or maybe it will get confused by street addresses that use "Dr." to mean "drive".

Conveniently, the WikiText2 dataset has already tokenized the text into words for you.
Words are delimited with a single space (`" "`) character.
So your pipeline doesn't have to decide whether "Dr." is the end of a sentence or not.
If the text was not tokenized, your NLP pipeline would need to remove periods from tokens at the ends of all sentences.
Even the heading delimiter text `"=="` has been split into two separate tokens `"="` and `"="`.
And paragraphs are delimited by a newline (`"\n"`) character.
And many "paragraphs" will be created for Wikipedia headings such as "== Reception ==" as well as retaining all empty lines between paragraphs.

You can utilize a sentence boundary detector or sentence segmenter such as SpaCy to split paragraphs into sentences.
This would prevent your training pairs of words from spilling over from one sentence to the other.
Honoring sentence boundaries with your Word2Vec can improve the accuracy of your word embeddings.
But we'll leave that to you to decide if you need the extra boost in accuracy.

One critical piece of infrastructure that your pipeline here can handle is the memory management for large corpora.
If you were training your word embeddings on millions of paragraphs you will need to use a dataset object that manages the text on disk, only loading into RAM or the GPU what is needed.
The Hugging Face Hub `datasets` package can handle this for you:

[source,python]
----
>>> import datasets
>>> dset = datasets.load_dataset('text', data_files=str(filepath))
>>> dset
DatasetDict({
    train: Dataset({
        features: ['text'],
        num_rows: 10000
    })
})
----

But you still need to tell Word2Vec what a word is.
This is the only "supervising" of Word2Vec dataset that you need to worry about.
And you can use the simplest possible tokenizer from Chapter 2 to achieve good results.
For this space-delimited tokenized text, you can just use the `str.split()` method.
And you can use case folding with `str.lower()` to cut your vocabulary size in half.
Surprisingly, this is enough for Word2Vec to learn the meaning and connotation of words sufficiently well for the magic of analogy problems like you might see on an SAT test and even reason about the real-world objects and people.

[source,python]
----
def tokenize_row(row):
    row['all_tokens'] = row['text'].lower().split()
    return row
----

Now you can use your tokenizer on the torchtext dataset that contains this iterable sequence of rows of data, each with a "text" key for the WikiText2 data.

[source,python]
----
>>> dset = dset.map(tokenize_row)
>>> dset

DatasetDict({
    train: Dataset({
        features: ['text', 'tokens'],
        num_rows: 10000
    })
})
----

You'll need to compute the vocabulary for your dataset to handle the one-hot encoding and decoding for your neural network.

[source,python]
----
>>> vocab = list(set(
...     [tok for row in dset['train']['tokens'] for tok in row]))
>>> vocab[:4]
['cast', 'kaifeng', 'recovered', 'doctorate']

>>> id2tok = dict(enumerate(vocab))
>>> list(id2tok.items())[:4]
[(0, 'cast'), (1, 'kaifeng'), (2, 'recovered'), (3, 'doctorate')]

>>> tok2id = {tok: i for (i, tok) in id2tok.items()}
>>> list(tok2id.items())[:4]
[('cast', 0), ('kaifeng', 1), ('recovered', 2), ('doctorate', 3)]
----

The one remaining feature engineering step is to create the skip-gram pairs using by windowizing the token sequences and then pairing up the skip-grams within those windows.

[source,python]
----
WINDOW_WIDTH = 10

>>> def windowizer(row, wsize=WINDOW_WIDTH):
    """ Compute sentence (str) to sliding-window of skip-gram pairs. """
...    doc = row['tokens']
...    out = []
...    for i, wd in enumerate(doc):
...        target = tok2id[wd]
...        window = [
...            i + j for j in range(-wsize, wsize + 1, 1)
...            if (i + j >= 0) & (i + j < len(doc)) & (j != 0)
...        ]

...        out += [(target, tok2id[doc[w]]) for w in window]
...    row['moving_window'] = out
...    return row
----

Once you apply the windowizer to your dataset it will have a 'window' key where the windows of tokens will be stored.

[source,python]
----
>>> dset = dset.map(windowizer)
>>> dset
DatasetDict({
    train: Dataset({
        features: ['text', 'tokens', 'window'],
        num_rows: 10000
    })
})
----

Here's your skip_gram generator function:

[source,python]
----
>>> def skip_grams(tokens, window_width=WINDOW_WIDTH):
...    pairs = []
...    for i, wd in enumerate(tokens):
...        target = tok2id[wd]
...        window = [
...            i + j for j in
...            range(-window_width, window_width + 1, 1)
...            if (i + j >= 0)
...            & (i + j < len(tokens))
...            & (j != 0)
...        ]

...        pairs.extend([(target, tok2id[tokens[w]]) for w in window])
    # huggingface datasets are dictionaries for every text element
...    return pairs
----
Your neural network only needs the pairs of skip-grams from the windowed data:

[source,python]
----
>>> from torch.utils.data import Dataset

>>> class Word2VecDataset(Dataset):
...    def __init__(self, dataset, vocab_size, wsize=WINDOW_WIDTH):
...        self.dataset = dataset
...        self.vocab_size = vocab_size
...        self.data = [i for s in dataset['moving_window'] for i in s]
...
...    def __len__(self):
...        return len(self.data)
...
...    def __getitem__(self, idx):
...        return self.data[idx]
----

And your DataLoader will take care of memory management for you.
This will ensure your pipeline is reusable for virtually any size corpus, even all of Wikipedia.

[source,python]
----
from torch.utils.data import DataLoader

dataloader = {}
for k in dset.keys():
    dataloader = {
        k: DataLoader(
            Word2VecDataset(
                dset[k],
                vocab_size=len(vocab)),
            batch_size=BATCH_SIZE,
            shuffle=True,
            num_workers=CPU_CORES - 1)
    }
----

You need a one-hot encoder to turn your word pairs into one-hot vector pairs:

[source,python]
----
def one_hot_encode(input_id, size):
    vec = torch.zeros(size).float()
    vec[input_id] = 1.0
    return vec
----

To dispell some of the magic of the examples you saw earlier, you'll train the network from scratch, just as you did in chapter 5.
You can see that a Word2Vec neural network is almost identical to your single-layer neural network from the previous chapter.


[source,python]
----
from torch import nn
EMBED_DIM = 100  # <1>

class Word2Vec(nn.Module):
    def __init__(self, vocab_size=len(vocab), embedding_size=EMBED_DIM):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, embedding_size)  # <2>
        self.expand = nn.Linear(embedding_size, vocab_size, bias=False)

    def forward(self, input):
        hidden = self.embed(input)  # <3>
        logits = self.expand(hidden)  # <4>
        return logits
----
<1> 100 is small but usable for many problems, 300 is more typical
<2> initialize the layers of your network only once when you instantiate the Word2Vec object
<3> the hidden layer embeds (encodes) the statistics of word usage in a lower dimensional vector
<4> the output layer expands (decodes) the 100-D hidden layer to predict one-hot vectors


Once you instantiate your Word2Vec model you are ready to create 100-D embeddings for the more than 20 thousand words in your vocabulary:

[source,python]
----
>>> model = Word2Vec()
>>> model

Word2Vec(
  (embed): Embedding(20641, 100)
  (expand): Linear(in_features=100, out_features=20641, bias=False)
)
----

If you have a GPU you can send your model to the GPU to speed up the training:

[source,python]
----
>>> import torch
>>> if torch.cuda.is_available():
...     device = torch.device('cuda')
>>> else:
...     device = torch.device('cpu')
>>> device

device(type='cpu')
----

Don't worry if you do not have a GPU.
On most modern CPUs this Word2Vec model will train in less than 15 minutes.

[source,python]
----
>>> model.to(device)

Word2Vec(
  (embed): Embedding(20641, 100)
  (expand): Linear(in_features=100, out_features=20641, bias=False)
)
----

Now is the fun part!
You get to watch as Word2Vec quickly learns the meaning of "Dr." and thousands of other tokens, just by reading a lot of text.
You can go get a tea or some chocolate or just have a 10 minute meditation to contemplate the meaning of life while your laptop contemplates the meaning of words.
First, let's define some training parameters

[source,python]
----
>>> from tqdm import tqdm  # noqa
>>> EPOCHS = 10
>>> LEARNING_RATE = 5e-4
EPOCHS = 10
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)
----

[source,python]
----
running_loss = []
pbar = tqdm(range(EPOCHS * len(dataloader['train'])))
for epoch in range(EPOCHS):
    epoch_loss = 0
    for sample_num, (center, context) in enumerate(dataloader['train']):
        if sample_num % len(dataloader['train']) == 2:
            print(center, context)
            # center: tensor([ 229,    0, 2379,  ...,  402,  553,  521])
            # context: tensor([ 112, 1734,  802,  ...,   28,  852,  363])
        center, context = center.to(device), context.to(device)
        optimizer.zero_grad()
        logits = model(input=context)
        loss = loss_fn(logits, center)
        if not sample_num % 10000:
            # print(center, context)
            pbar.set_description(f'loss[{sample_num}] = {loss.item()}')
        epoch_loss += loss.item()
        loss.backward()
        optimizer.step()
        pbar.update(1)
    epoch_loss /= len(dataloader['train'])
    running_loss.append(epoch_loss)

save_model(model, loss)
----

=== Computational tricks of Word2Vec

After the initial publication, the performance of `word2vec` models have been improved through various computational tricks.
In this section, we highlight the three key improvements that help word embeddings achieve greater accuracy with less computational resources or training data:

1. Add frequent bigrams to the vocabulary
2. Undersampling (subsampling) frequent tokens
3. Undersampling of negative examples

==== Frequent bigrams

Some words often occur in combination with other words creating a compound word.
For example "Aaron" is often followed by "Swartz" and "AI" is often followed by "Ethics".
Since the word "Swartz" would follow the word "Aaron" with an above average probability you probably want to create a single word vector for "Aaron Swartz" as a single compound proper noun.
In order to improve the accuracy of the Word2Vec embedding for their applications involving proper nouns and compound words, Mikolov's team included some bigrams and trigrams in their Word2Vec vocabulary.
The team footnote:[The publication by the team around Tomas Mikolov (https://arxiv.org/pdf/1310.4546.pdf) provides more details.] used co-occurrence frequency to identify bigrams and trigrams that should be considered single terms using the following scoring function:

*Equation 6.5 Bigram scoring function*
image::../image/06/equations/equation_6_6.png[]
// [stem]
// ++++
// score(w_{i}, w_{j}) = \frac{count(w_{i} w_{j}) - \delta}{count(w_{i}) \times count(w_{j})}
// ++++


When words occur often enough next to each other, they will be included in the Word2Vec vocabulary as a pair term.
You'll notice that the vocabulary of many word embeddings models such as Word2vec contains terms like "New_York" or "San_Francisco".
That way, these terms will be represented as a single vector instead of two separate ones, such as for "San" and "Francisco".

Another effect of the word pairs is that the word combination often represents a different meaning than the sum of the vectors for the individual words.
For example, the MLS soccer team "Portland Timbers" has a different meaning than the individual words "Portland" or "Timbers".
But by adding oft-occurring bigrams like team names to the `word2vec` model, they can easily be included in the one-hot vector for model training.

==== Undersampling frequent tokens

Another accuracy improvement to the original algorithm was to undersample (subsample) frequent words.
This is also referred to as "undersampling the majority classes" in order to balance the class weights.
Common words such as the articles "the" and "a" often don't contain a lot of information and meaning relevant to most NLP problems so they are referred to as stop words.
Mikolov and others often chose to _subsample_ these words.
Subsampling just means you randomly ignore them during your sampling of the corpus of continuous skip-grams or CBOWs.
Many blogger will take this to the extreme and completely remove them from the corpus during prepossessing.
Though subsampling or filtering stopwords may help your word embedding algorithm train faster, it can sometimes be counterproductive.
And with modern computers and applications, a 1% improvement in training time is not likely to be worth the loss in precision of your word vectors.
And the co-occurrence of stop words with other "words" in the corpus might create less meaningful connections between words muddying the Word2Vec representation with this false semantic similarity training.

[IMPORTANT]
====
All words carry meaning, including stop words. So stop words should not be completely ignored or skipped while training your word vectors or composing your vocabulary. In addition, because word vectors are often used in generative models (like the model Cole used to compose sentences in this book), stop words and other common words must be included in your vocabulary and are allowed to affect the word vectors of their neighboring words.
====

To reduce the emphasis on frequent words like stop words, words are sampled during training in inverse proportion to their frequency. The effect of this is similar to the IDF affect on TF-IDF vectors. Frequent words are given less influence over the vector than the rarer words. Tomas Mikolov used the following equation to determine the probability of sampling a given word.
This probability determines whether or not a particular word is included in a particular skip-gram during training:

*Equation 6.6 Subsampling probability in Mikolov's Word2Vec paper*

image::../image/06/equations/equation_6_7.png[]
// [stem]
// ++++
// P(w_{i}) = 1 - \sqrt{\frac{t}{f(w_{i})}}
// ++++

The `word2vec` C++ implementation uses a slightly different sampling probability than the one mentioned in the paper, but it has the same effect:

*Equation 6.7 Subsampling probability in Mikolov's `word2vec` code*

image::../image/06/equations/equation_6_8.png[]
// [stem]
// ++++
// P(w_{i}) = \frac{f(w_{i})-t}{f(w_{i})} - \sqrt{\frac{t}{f(w_{i})}}
// ++++


In the preceding equations, `f(wpass:n[~i~])` represents the frequency of a word across the corpus, and `t` represents a frequency threshold above which you want to apply the subsampling probability.
The threshold depends on your corpus size, average document length, and the variety of words used in those documents. Values between `10pass:n[^-5^]` and `10pass:n[^-6^]` are often found in the literature.

If a word shows up 10 times across your entire corpus, and your corpus has a vocabulary of one million distinct words, and you set the subsampling threshold to `10pass:n[^-6^]`, the probability of keeping the word in any particular _n_-gram is 68%. You would skip it 32% of the time while composing your _n_-grams during tokenization.

Mikolov showed that subsampling improves the accuracy of the word vectors for tasks such as answering analogy questions.

==== Negative sampling

One last trick that Mikolov came up with was the idea of negative sampling.
If a single training example with a pair of words is presented to the network it will cause all weights for the network to be updated.
This changes the values of all the vectors for all the words in your vocabulary.
But if your vocabulary contains thousands or millions of words, updating all the weights for the large one-hot vector is inefficient.
To speed up the training of word vector models, Mikolov used negative sampling.

Instead of updating all word weights that weren't included in the word window, Mikolov suggested sampling just a few negative samples (in the output vector) to update their weights.
Instead of updating all weights, you pick _n_ negative example word pairs (words that don't match your target output for that example) and update the weights that contributed to their specific output.
That way, the computation can be reduced dramatically and the performance of the trained network doesn't decrease significantly.

[NOTE]
====
If you train your word model with a small corpus, you might want to use a negative sampling rate of 5 to 20 samples. For larger corpora and vocabularies, you can reduce the negative sample rate to as low as two to five samples, according to Mikolov and his team.
====


=== Using the `gensim.word2vec` module

If the previous section sounded too complicated, don't worry. Various companies provide their pretrained word vector models, and popular NLP libraries for different programming languages allow you to use the pretrained models efficiently.
In the following section, we look at how you can take advantage of the magic of word vectors. 
For word vectors you'll use the popular `gensim` library, which you first saw in chapter 4.

If you've already installed the `nlpia` package,footnote:[See the README file at http://gitlab.com/tangibleai/nlpia2 for installation instructions.] you can download a pretrained `word2vec` model with the following command:

[source,python]
>>> from nlpia.data.loaders import get_data
>>> word_vectors = get_data('word2vec')

If that doesn't work for you, or you like to "roll your own," you can do a Google search for `word2vec` models pretrained on Google News documents.footnote:[Google hosts the original model trained by Mikolov on Google Drive https://bit.ly/GoogleNews-vectors-negative300[here].]
After you find and download the model in Google's original binary format and put it in a local path, you can load it with the `gensim` package like this:

[source,python]
>>> from gensim.models.keyedvectors import KeyedVectors
>>> word_vectors = KeyedVectors.load_word2vec_format(\
...     '/path/to/GoogleNews-vectors-negative300.bin.gz', binary=True)

Working with word vectors can be memory intensive. If your available memory is limited or if you don't want to wait minutes for the word vector model to load, you can reduce the number of words loaded into memory by passing in the `limit` keyword argument. In the following example, you'll load the 200k most common words from the Google News corpus:

[source,python]
----
>>> from gensim.models.keyedvectors import KeyedVectors
>>> from nlpia.loaders import get_data
>>> word_vectors = get_data('w2v', limit=200000)  # <1>
----
<1> This limits the memory footprint by only loading 200k of the 2M Word2Vec vectors

But keep in mind that a word vector model with a limited vocabulary will lead to a lower performance of your NLP pipeline if your documents contain words that you haven't loaded word vectors for.
Therefore, you probably only want to limit the size of your word vector model during the development phase.
For the rest of the examples in this chapter, you should use the complete Word2Vec model if you want to get the same results we show here.

The `gensim.KeyedVectors.most_similar()` method provides an efficient way to find the nearest neighbors for any given word vector.
The keyword argument `positive` takes a list of the vectors to be added together, similar to your soccer team example from the beginning of this chapter.
Similarly, you can use the `negative` argument for subtraction and to exclude unrelated terms. The argument `topn` determines how many related terms should be provided as a return value.

Unlike a conventional thesaurus, Word2Vec synonomy (similarity) is a continuous score, a distance.
This is because Word2Vec itself is a continuous vector space model.
Word2Vec high dimensionality and continuous values for each dimension enable it to capture the full range of meaning for any given word.
That's why analogies and even zeugmas, odd juxtopositions of multiple meanings within the same word, are no problem.
Handling analogies and zeugmas is a really big deal.
Understanding analogies and zeugmas takes human-level understanding of the world, including common sense knowledge and reasoning.footnote:[_Surfaces and Essences: Analogy as the Fuel and Fire of Thinking_ by Douglas Hoffstadter and Emmanuel Sander.]
Word embeddings are enough to give machines at least a passing understanding on the kinds of analogies you might see on an SAT quiz.

[source,python]
----
>>> word_vectors.most_similar(positive=['cooking', 'potatoes'], topn=5)
[('cook', 0.6973530650138855),
 ('oven_roasting', 0.6754530668258667),
 ('Slow_cooker', 0.6742032170295715),
 ('sweet_potatoes', 0.6600279808044434),
 ('stir_fry_vegetables', 0.6548759341239929)]
>>> word_vectors.most_similar(positive=['germany', 'france'], topn=1)
[('europe', 0.7222039699554443)]
----

Word vector models also allow you to determine unrelated terms. The `gensim` library provides a method called `doesnt_match`:

[source,python]
----
>>> word_vectors.doesnt_match("potatoes milk cake computer".split())
'computer'
----

To determine the most unrelated term of the list, the method returns the term with the highest distance to all other list terms.

If you want to perform calculations (such as the famous example _king + woman - man = queen_, which was the example that got Mikolov and his advisor excited in the first place), you can do that by adding a `negative` argument to the `most_similar` method call:

[source,python]
----
>>> word_vectors.most_similar(positive=['king', 'woman'],
...     negative=['man'], topn=2)
[('queen', 0.7118192315101624), ('monarch', 0.6189674139022827)]
----

The `gensim` library also allows you to calculate the similarity between two terms. If you want to compare two words and determine their cosine similarity, use the method `.similarity()`:

[source,python]
----
>>> word_vectors.similarity('princess', 'queen')
0.70705315983704509
----

If you want to develop your own functions and work with the raw word vectors, you can access them through Python's square bracket syntax (`[]`) or the `get()` method on a `KeyedVector` instance. You can treat the loaded model object as a dictionary where your word of interest is the dictionary key. Each float in the returned array represents one of the vector dimensions. In the case of Google's word model, your numpy arrays will have a shape of 1x300.

[source,python]
----
>>> word_vectors['phone']
array([-0.01446533, -0.12792969, -0.11572266, -0.22167969, -0.07373047,
       -0.05981445, -0.10009766, -0.06884766,  0.14941406,  0.10107422,
       -0.03076172, -0.03271484, -0.03125   , -0.10791016,  0.12158203,
        0.16015625,  0.19335938,  0.0065918 , -0.15429688,  0.03710938,
        ...
----

If you're wondering what all those numbers _mean_, you can find out. But it would take a lot of work.
You would need to examine some synonyms and see which of the 300 numbers in the array they all share.
Alternatively you can find the linear combination of these numbers that make up dimensions for things like "placeness" and "femaleness", like you did at the beginning of this chapter.

=== Generating your own Word vector representations

In some cases you may want to create your own domain-specific word vector models.
Doing so can improve the accuracy of your model if your NLP pipeline is processing documents that use words in a way that you wouldn't find on Google News before 2006, when Mikolov trained the reference `word2vec` model.
Keep in mind, you need a _lot_ of documents to do this as well as Google and Mikolov did.
But if your words are particularly rare on Google News, or your texts use them in unique ways within a restricted domain, such as medical texts or transcripts, a domain-specific word model may improve your model accuracy.
In the following section, we show you how to train your own `word2vec` model.

For the purpose of training a domain-specific `word2vec` model, you'll again turn to `gensim`, but before you can start training the model, you'll need to preprocess your corpus using tools you discovered in Chapter 2.

==== Preprocessing steps

First you need to break your documents into sentences and the sentences into tokens. The `gensim` `word2vec` model expects a list of sentences, where each sentence is broken up into tokens.
This prevents word vectors learning from irrelevant word occurrences in neighboring sentences.
Your training input should look similar to the following structure:

[source,python]
----
>>> token_list
[
  ['to', 'provide', 'early', 'intervention/early', 'childhood', 'special',
   'education', 'services', 'to', 'eligible', 'children', 'and', 'their',
   'families'],
  ['essential', 'job', 'functions'],
  ['participate', 'as', 'a', 'transdisciplinary', 'team', 'member', 'to',
   'complete', 'educational', 'assessments', 'for']
  ...
]
----

To segment sentences and then convert sentences into tokens, you can apply the various strategies you learned in chapter 2.
Let's add another one: Detector Morse is a sentence segmenter that improves upon the accuracy segmenter available in NLTK and `gensim` for some applications.footnote:[Detector Morse, by Kyle Gorman and OHSU on pypi and at https://github.com/cslu-nlp/DetectorMorse]
It has been pretrained on sentences from years of text in the Wall Street Journal.
So if your corpus includes language similar to that in the WSJ, Detector Morse is likely to give you the highest accuracy currently possible.
You can also retrain Detector Morse on your own dataset if you have a large set of sentences from your domain.
Once you've converted your documents into lists of token lists (one for each sentence), you're ready for your `word2vec` training.


==== Train your domain-specific `word2vec` model

Get started by loading the _word2vec_ module:

[source,python]
----
>>> from gensim.models.word2vec import Word2Vec
----

The training requires a few setup details.

.Parameters to control word2vec model training
[source,python]
----
>>> num_features = 300  # <1>
>>> min_word_count = 3  # <2>
>>> num_workers = 2  # <3>
>>> window_size = 6  # <4>
>>> subsampling = 1e-3  # <5>
----
<1> Number of vector elements (dimensions) to represent the word vector
<2> Min number of word count to be considered in the `word2vec` model. If your corpus is small, reduce the min count. If you're training with a large corpus, increase the min count.
<3> Number of CPU cores used for the training. If you want to set the number of cores dynamically, check out `import multiprocessing; num_workers = multiprocessing.cpu_count()`.
<4> Context window size
<5> Subsampling rate for frequent terms

Now you're ready to start your training.

.Instantiating a word2vec model
[source,python]
----
>>> model = Word2Vec(
...     token_list,
...     workers=num_workers,
...     size=num_features,
...     min_count=min_word_count,
...     window=window_size,
...     sample=subsampling)
----

Depending on your corpus size and your CPU performance, the training will take a significant amount of time.
For smaller corpora, the training can be completed in minutes.
But for a comprehensive word model, the corpus will contain millions of sentences.
You need to have several examples of all the different ways all the different words in your corpus are used.
If you start processing larger corpora, such as the Wikipedia corpus, expect a much longer training time and a much larger memory consumption.

In addition, Word2Vec models can consume quite a bit of memory. But remember that only the weight matrix for the hidden layer is of interest.
Once you've trained your word model, you can reduce the memory footprint by about half if you freeze your model and discard the unnecessary information.
The following command will discard the unneeded output weights of your neural network:

[source,python]
----
>>> model.init_sims(replace=True)
----

The `init_sims` method will freeze the model, storing the weights of the hidden layer and discarding the output weights that predict word co-ocurrences.
The output weights aren't part of the vector used for most Word2Vec applications.
But the model cannot be trained further once the weights of the output layer have been discarded.

You can save the trained model with the following command and preserve it for later use:

[source,python]
----
>>> model_name = "my_domain_specific_word2vec_model"
>>> model.save(model_name)
----

If you want to test your newly trained model, you can use it with the same method you learned in the previous section.

.Loading a saved `word2vec` model
[source,python]
----
>>> from gensim.models.word2vec import Word2Vec
>>> model_name = "my_domain_specific_word2vec_model"
>>> model = Word2Vec.load(model_name)
>>> model.most_similar('radiology')
----

== Word2Vec alternatives 

Word2Vec was a breakthrough, but it relies on a neural network model that must be trained using backpropagation.
Since Mikolov first popularized word embeddings, researchers have come up with increasingly more accurate and efficient ways to embed the meaning of words in a vector space.

1. Word2vec
2. GloVE
3. fastText

Stanford NLP researchers footnote:[Stanford GloVe Project (https://nlp.stanford.edu/projects/glove/).] led by Jeffrey Pennington set about to understand the reason why Word2Vec worked so well and to find the cost function that was being optimized.
They started by counting the word co-occurrences and recording them in a square matrix.
They found they could compute the singular value decomposition (SVD)footnote:[See chapter 5 and Appendix C for more details on SVD.] of this co-occurrence matrix, splitting it into the same two weight matrices that Word2Vec produces.footnote:[_GloVe: Global Vectors for Word Representation_ by Jeffrey Pennington, Richard Socher, and Christopher D. Manning: https://nlp.stanford.edu/pubs/glove.pdf]
The key was to normalize the co-occurrence matrix the same way.
But in some cases, the Word2Vec model failed to converge to the same global optimum that the Stanford researchers were able to achieve with their SVD approach.
It's this direct optimization of the global vectors of word co-occurrences (co-occurrences across the entire corpus) that gives GloVe its name.

GloVe can produce matrices equivalent to the input weight matrix and output weight matrix of Word2Vec, producing a language model with the same accuracy as Word2Vec but in much less time.
GloVe speeds the process by using text data more efficiently.
GloVe can be trained on smaller corpora and still converge.footnote:[Gensim's comparison of Word2Vec and GloVe performance: https://rare-technologies.com/making-sense-of-Word2Vec/#glove_vs_word2vec]
And SVD algorithms have been refined for decades, so GloVe has a head start on debugging and algorithm optimization.
Word2Vec relies on backpropagation to update the weights that form the word embeddings.
Neural network backpropagation is less efficient than more mature optimization algorithms such as those used within SVD for GloVe.

Even though Word2Vec first popularized the concept of semantic reasoning with word vectors, your workhorse should probably be GloVe to train new word vector models.
With GloVe you'll be more likely to find the global optimum for those vector representations, giving you more accurate results.
And spaCy utilizes as its default embedding algorithm, so that when you run: 

[source,python]
----
>>> import spacy
>>> 
>>> nlp = spacy.load("en_core_web_sm")
>>> text = "This is an example sentence."
>>> doc = nlp(text)
>>>
>>> for token in doc:
...    print(token.text, token.vector)
----

The results are computed using GloVe under the hood! 

Advantages of GloVe:

- Faster training
- Better RAM/CPU efficiency (can handle larger documents)
- More efficient use of data (helps with smaller corpora)
- More accurate for the same amount of training

=== fastText

Researchers from Facebook took the concept of Word2Vec one step further footnote:[Enriching Word Vectors with Subword Information, Bojanowski et al.: https://arxiv.org/pdf/1607.04606.pdf] by adding a new twist to the model training.
The new algorithm, which they named fastText, predicts the surrounding _n_-_character_ grams rather than just the surrounding words, like Word2Vec does.
For example, the word "whisper" would generate the following 2- and 3-character grams:

[source,python]
----
['wh', 'whi', 'hi', 'his', 'is', 'isp', 'sp', 'spe', 'pe', 'per', 'er']
----

fastText is then training a vector representation for every _n_-character gram (called "subwords"), which includes words, misspelled words, partial words, and even single characters.
The advantage of this approach is that it handles rare or new words much better than the original Word2Vec approach.

The fastText tokenizer will create vectors for two halves of a longer word if the longer word used much less often than the subwords that make it up.
For example, fastText might create vectors for "super" and "woman" if your corpus only mentions "Superwoman" once or twice but uses "super" and "woman" thousands of times.
And if your fastText language model encounters the word "Superwoman" in the real world after training is over, it sums the vectors for "Super" and "woman" together to create a vector for the word "Superwoman".
This reduces the number of words that fastText will have to assign the generic Out of Vocabulary (OOV) vector to.
In the "mind" of your NLU pipeline, the OOV word vector looks like "Unkown Word".
It has the same effect as if you heard a foreign word in a completely unfamiliar language.
While Word2vec only "knows" how to embed words it has seen before, fastText is much more flexible due to its subword approach.
It is also relatively lightweight and operates faster. 

As part of the fastText release, Facebook published pretrained fastText models for 294 languages.
On the Github page of Facebook research, footnote:[See the web page titled "fastText/pretrained-vectors.md" (https://github.com/facebookresearch/fastText/blob/main/docs/pretrained-vectors.md).] you can find models ranging from _Abkhazian_ to _Zulu_. 
The model collection even includes rare languages such as _Saterland Frisian_, which is only spoken by a handful of Germans.
The pretrained fastText models provided by Facebook have only been trained on the available Wikipedia corpora.
Therefore the vocabulary and accuracy of the models will vary across languages.

We've included the fastText logic for creating new vectors for OOV words in the `nessvec` package.
We've also added an enhancement to the fastText pipeline to handle misspellings and typos using Peter Norvig's famously elegant spelling corrector algorithm.footnote:[Spelling corrector code and explanation by Peter Norvig (https://norvig.com/spell-correct.html)]
This will give you the best of both worlds, an understandable training algorithm and a robust inference or prediction model when you need to use your trained vectors in the real world.

==== Power up your NLP with pretrained model

Supercharge your NLP pipeline by taking advantage of the open source pretrained embeddings from the most powerful corporations on the planet.
Pretrained fastText vectors are available in almost every language conceivable.
An
If you want to see the all the options available for your word embeddings check out the fastText model repository (https://github.com/facebookresearch/fastText/blob/main/docs/pretrained-vectors.md).
And for multilingual power you can find combined models for many of the 157 languages supported in the Common Crawl version of fastText embeddings (https://fasttext.cc/docs/en/crawl-vectors.html).
If you want you can download all the different versions of the embeddings for your language using the _bin+text_ links on the fastText pages.
But if you want to save some time and just download the 1 million

[WARNING]
====
The _bin+text_ `wiki.en.zip` file (https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.zip) is _9.6 GB_.
The text-only `wiki.en.vec` file is _6.1 GB_ (https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.vec).
If you use the `nessvec` package rather than `gensim` it will download just the 600MB `wiki-news-300d-1M.vec.zip` file (https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip).
That `wiki-news-300d-1M.vec.zip` contains the 300-D vectors for the 1 million most popular words (case-insensitive) from Wikipedia and news web pages.
====

The `nessvec` package will create a memory-mapped `DataFrame` of all your pretrained vectors.
The memory-mapped file (`.hdf5`) keeps you from running out of memory (RAM) on your computer by lazy-loading just the vectors you need, when you need them.

[source,python]
----
>>> from nessvec.files import load_fasttext
>>> df = load_fasttext()  # <1>
>>> df.head().round(2)
      0     1     2    ...   297   298   299
,    0.11  0.01  0.00  ...  0.00  0.12 -0.04
the  0.09  0.02 -0.06  ...  0.16 -0.03 -0.03
.    0.00  0.00 -0.02  ...  0.21  0.07 -0.05
and -0.03  0.01 -0.02  ...  0.10  0.09  0.01
of  -0.01 -0.03 -0.03  ...  0.12  0.01  0.02
>>> df.loc['prosocial']  # <2>
0      0.0004
1     -0.0328
2     -0.1185
        ...
297    0.1010
298   -0.1323
299    0.2874
Name: prosocial, Length: 300, dtype: float64
----
<1> This will download data to the `$HOME/.nlpia2-data/` directory
<2> Use the standard `DataFrame` API to retrieve any embedding you like

[NOTE]
====
To turbocharge your word embedding pipeline you can use Bloom embeddings.
Bloom embeddings aren't a new algorithm for creating embeddings, but a faster more accurate indexing approach for storing and retrieving a high dimensional vector.
The vectors in a Bloom embedding table each represent the meaning of two or more words combined together.
The trick is to subtract out the words you don't need in order to recreate the original embedding that you're looking for.
Fortunately SpaCy has implemented all this efficiency under the hood with its v2.0 language model.
This is how SpaCy can create word embeddings for millions of words while storing only 20k unique vectors.footnote:[SpaCy medium language model docs (https://spacy.io/models/en#en_core_web_md)]
====

=== Word2Vec vs LSA

You might now be wondering how word embeddings compare to the LSA topic-word vectors of chapter 4.
These are word embeddings that you created using PCA (principal component analysis) on your TF-IDF vectors.
And LSA also gives you topic-document vectors which you used as embeddings of entire documents.
LSA topic-document vectors are the sum of the topic-word vectors for all the words in whatever document you create the embedding for.
If you wanted to get a word vector for an entire document that is analogous to topic-document vectors, you'd sum all the word vectors for your document.
That's pretty close to how Doc2vec document vectors work.

If your LSA matrix of topic vectors is of size `Npass:n[~words~] × Npass:n[~topics~]`, the LSA word vectors are the rows of that LSA matrix.
These row vectors capture the meaning of words in a sequence of around 200 to 300 real values, like Word2Vec does.
And LSA topic-word vectors are just as useful as Word2Vec vectors for finding both related and unrelated terms.
As you learned in the GloVe discussion, Word2Vec vectors can be created using the exact same SVD algorithm used for LSA.
But Word2Vec gets more use out of the same number of words in its documents by creating a sliding window that overlaps from one document to the next.
This way it can reuse the same words five times before sliding on.

What about incremental or online training?
Both LSA and Word2Vec algorithms allow adding new documents to your corpus and adjusting your existing word vectors to account for the co-occurrences in the new documents.
But only the existing "bins" in your lexicon can be updated.
Adding completely new words would change the total size of your vocabulary and therefore your one-hot vectors would change.
That requires starting the training over if you want to capture the new word in your model.

LSA trains faster than Word2Vec does.
And for long documents, it does a better job of discriminating and clustering those documents.
In fact, Stanford researchers used this faster PCA-based method to train the GloVE vectors.
You can compare the three most popular word embeddings using the `nessvec` package.footnote:[Nessvec source code (https://gitlab.com/tangibleai/nessvec) and tutorial videos (https://proai.org/nessvec-videos)]


The "killer app" for Word2Vec is the semantic reasoning that it made possible.
LSA topic-word vectors can do that, too, but it usually isn't accurate.
You'd have to break documents into sentences and then only use short phrases to train your LSA model if you want to approach the accuracy and "wow" factor of Word2Vec reasoning.
With Word2Vec you can determine the answer to questions like _Harry Potter + "University = Hogwarts_
As a great example for domain-specific `word2vec` models, check out the models for words from Harry Potter, the Lord of the Rings by footnote:[Niel Chah's  `word2vec4everything` repository (https://github.com/nchah/word2vec4everything)].

Advantages of LSA:

* Faster training
* Better discrimination between longer documents

Advantages of Word2Vec and GloVe:

* More efficient use of large corpora
* More accurate reasoning with words, such as answering analogy questions


=== Statis vs contextualized embeddings

There are two kinds of word embeddings you may encounter in the real world: static and contextualized. 

Static word embeddings can be used on individual words or N-Grams in isolation.
And once the training is completed, the vectors remain fixed.
These are the kinds of word embeddings you'll use for analogy and other word vector reasoning problems you want to solve.
You'll train a language model to create static word embeddings here.
The context of a word will only be used to train the model.
Once your word embeddings are trained, you will not use the context of a word's usage to adjust your word embeddings at all as you are _using_ your trained word embeddings.
This means that the different senses or meanings of a word are all smushed together into a single static vector.
All the embeddings we have seen so far - Word2Vec, GloVe and fasttext - are static embeddings. 
Word2Vec will return the same embedding for the word "bank" in the name "World Bank" and in the expression "river bank".

In contrast, contextualized word embeddings can be updated or refined based on the embeddings and words that come before or after.
And the order a word appears relative to other words matters for contextualized word embeddings.
This means that for NLU of the bigram "not happy" it would have an embedding much closer to the embedding of "unhappy" for contextualized word embeddings than for static word embeddings.

As you can imagine, contextualized embeddings can be much more useful for a variety of applications, 
such as semantic search. 
A huge breakthrough in creating them came with the introduction of bi-directional transformer neural networks, such as BERT (Bi-directional Encoder Representations for Transformers), which we're going to cover in depth in Chapter 9.
BERT embeddings outperformed older algorithms such as World2Vec and GloVe because it takes into account not only the context to the right and to the left of the word it embeds but also the order of the words in the sentence. 
As such, it became a popular choice for many NLP applications. 

=== Visualizing word relationships

The semantic word relationships can be powerful and their visualizations can lead to interesting discoveries.
In this section, we demonstrate steps to visualize the word vectors in 2D.

To get started, let's load all the word vectors from the Google Word2Vec model of the Google News corpus.
As you can imagine, this corpus included a lot of mentions of "Portland" and "Oregon" and a lot of other city and state names.
You'll use the `nlpia` package to keep things simple, so you can start playing with Word2Vec vectors quickly.

.Load a pretrained `FastText` language model using `nlpia`
[source,python]
----
>>> from nessvec.indexers import Index
>>> index = Index()  # <1>
>>> vecs = index.vecs
>>> vecs.shape
(3000000, 300)
----
<1> Downloads the pretrained FastText embedding vectors to `\~/.nessvec-data/`


[WARNING]
====
The Google News `word2vec` model is huge: 3 million words with 300 vector dimensions each.
The complete word vector model requires 3 GB of available memory.
If your available memory is limited or you quickly want to load a few most frequent terms from the word model, check out chapter 13.
====

This `KeyedVectors` object in `gensim` now holds a table of 3 million Word2Vec vectors.
We loaded these vectors from a file created by Google to store a Word2Vec model that they trained on a large corpus based on Google News articles.
There should definitely be a lot of words for states and cities in all those news articles.
The following listing shows just a few of the words in the vocabulary, starting at the 1 millionth word:

.Examine word2vec vocabulary frequencies
[source,python]
----
>>> import pandas as pd
>>> vocab = pd.Series(wv.vocab)
>>> vocab.iloc[1000000:100006]
Illington_Fund             Vocab(count:447860, index:2552140)
Illingworth                 Vocab(count:2905166, index:94834)
Illingworth_Halifax       Vocab(count:1984281, index:1015719)
Illini                      Vocab(count:2984391, index:15609)
IlliniBoard.com           Vocab(count:1481047, index:1518953)
Illini_Bluffs              Vocab(count:2636947, index:363053)
----

Notice that compound words and common _n_-grams are joined together with an underscore character ("\_").
Also notice that the "value" in the key-value mapping is a `gensim` `Vocab` object that contains not only the index location for a word, so you can retrieve the Word2Vec vector, but also the number of times it occurred in the Google News corpus.

As you've seen earlier, if you want to retrieve the 300-D vector for a particular word, you can use the square brackets on this `KeyedVectors` object to `__getitem__` any word or _n_-gram:

[source,python]
----
>>> wv['Illini']
array([ 0.15625   ,  0.18652344,  0.33203125,  0.55859375,  0.03637695,
       -0.09375   , -0.05029297,  0.16796875, -0.0625    ,  0.09912109,
       -0.0291748 ,  0.39257812,  0.05395508,  0.35351562, -0.02270508,
       ...
       ])
----

The reason we chose the 1 millionth word (in lexical alphabetic order) is because the first several thousand "words" are punctuation sequences like "\#\#\#\#\#" and other symbols that occurred a lot in the Google News corpus.
We just got lucky that "Illini" showed up in your list.
The word "Illini" refers to a group of people, usually football players and fans, rather than a single geographic region like "Illinois" -- where most fans of the "Fighting Illini" live.
Let's see how close this "Illini" vector is to the vector for "Illinois":

.Distance between "Illinois" and "Illini"
[source,python]
----
>>> import numpy as np
>>> np.linalg.norm(wv['Illinois'] - wv['Illini'])  # <1>
3.3653798
>>> cos_similarity = np.dot(wv['Illinois'], wv['Illini']) / (
...     np.linalg.norm(wv['Illinois']) *\
...     np.linalg.norm(wv['Illini']))  # <2>
>>> cos_similarity
0.5501352
>>> 1 - cos_similarity # <3>
0.4498648
----
<1> Euclidean distance
<2> Cosine similarity is the normalized dot product
<3> Cosine distance

These distances mean that the words "Illini" and "Illinois" are only moderately close to one another in meaning.

Now let's retrieve all the Word2Vec vectors for US cities so you can use their distances to plot them on a 2D map of meaning.
How would you find all the cities and states in that Word2Vec vocabulary in that `KeyedVectors` object?
You could use cosine distance like you did in the previous listing to find all the vectors that are close to the words "state" or "city".

But rather than reading through all 3 million words and word vectors, lets load another dataset containing a list of cities and states (regions) from around the world.

.Some US city data
[source,python]
----
>>> from nlpia.data.loaders import get_data
>>> cities = get_data('cities')
>>> cities.head(1).T
geonameid                       3039154
name                          El Tarter
asciiname                     El Tarter
alternatenames     Ehl Tarter,Эл Тартер
latitude                        42.5795
longitude                       1.65362
feature_class                         P
feature_code                        PPL
country_code                         AD
cc2                                 NaN
admin1_code                          02
admin2_code                         NaN
admin3_code                         NaN
admin4_code                         NaN
population                         1052
elevation                           NaN
dem                                1721
timezone                 Europe/Andorra
modification_date            2012-11-03
----

This dataset from Geocities contains a lot of information, including latitude, longitude, and population.
You could use this for some fun visualizations or comparisons between geographic distance and Word2Vec distance.
But for now you're just going to try to map that Word2Vec distance on a 2D plane and see what it looks like.
Let's focus on just the United States for now:

.Some US state data
[source,python]
----
>>> us = cities[(cities.country_code == 'US') &\
...     (cities.admin1_code.notnull())].copy()
>>> states = pd.read_csv(\
...     'http://www.fonz.net/blog/wp-content/uploads/2008/04/states.csv')
>>> states = dict(zip(states.Abbreviation, states.State))
>>> us['city'] = us.name.copy()
>>> us['st'] = us.admin1_code.copy()
>>> us['state'] = us.st.map(states)
>>> us[us.columns[-3:]].head()
                     city  st    state
geonameid
4046255       Bay Minette  AL  Alabama
4046274              Edna  TX    Texas
4046319    Bayou La Batre  AL  Alabama
4046332         Henderson  TX    Texas
4046430           Natalia  TX    Texas
----

Now you have a full state name for each city in addition to its abbreviation.
Let's check to see which of those state names and city names exist in your Word2Vec vocabulary:

[source,python]
----
>>> vocab = pd.np.concatenate([us.city, us.st, us.state])
>>> vocab = np.array([word for word in vocab if word in wv.wv])
>>> vocab[:10]
----

Even when you only look at United States cities, you'll find a lot of large cities with the same name, like Portland, Oregon and Portland, Maine.
So let's incorporate into your city vector the essence of the state where that city is located.
To combine the meanings of words in Word2Vec, you add the vectors together.
That's the magic of "Analogy reasoning."

Here's one way to add the Word2Vecs for the states to the vectors for the cities and put all these new vectors in a big DataFrame.
We use either the full name of a state or just the abbreviations (whichever one is in your Word2Vec vocabulary).

.Augment city word vectors with US state word vectors
[source,python]
----
>>> city_plus_state = []
>>> for c, state, st in zip(us.city, us.state, us.st):
...     if c not in vocab:
...         continue
...     row = []
...     if state in vocab:
...         row.extend(wv[c] + wv[state])
...     else:
...         row.extend(wv[c] + wv[st])
...     city_plus_state.append(row)
>>> us_300D = pd.DataFrame(city_plus_state)
----

Depending on your corpus, your word relationship can represent different attributes, such as geographical proximity or cultural or economic similarities. But the relationships heavily depend on the training corpus, and they will reflect the corpus.

[WARNING]
.Word vectors are biased!
====
Word vectors learn word relationships based on the training corpus.
If your corpus is about finance then your "bank" word vector will be mainly about businesses that hold deposits.
If your corpus is about geology the your "bank" word vector will be trained on associations with rivers and streams.
And if you corpus is mostly about a matriarchal society with women bankers and men washing clothes in the river, then your word vectors would take on that gender bias.

The following example shows the gender bias of a word model trained on Google News articles.
If you calculate the distance between "man" and "nurse" and compare that to the distance between "woman" and "nurse", you'll be able to see the bias.

[source,python]
----
>>> word_model.distance('man', 'nurse')
0.7453
>>> word_model.distance('woman', 'nurse')
0.5586
----

Identifying and compensating for biases like this is a challenge for any NLP practitioner that trains her models on documents written in a biased world.
====

The news articles used as the training corpus share a common component, which is the semantical similarity of the cities. Semantically similar locations in the articles seems to interchangeable and therefore the word model learned that they are similar. If you would have trained on a different corpus, your word relationship might have differed.

Cities that are similar in size and culture are clustered close together despite being far apart geographically, such as San Diego and San Jose, or vacation destinations such as Honolulu and Reno.

Fortunately you can use conventional algebra to add the vectors for cities to the vectors for states and state abbreviations. As you discovered in chapter 4, you can use tools such as the principal components analysis (PCA) to reduce the vector dimensions from your 300 dimensions to a human-understandable 2D representation.
PCA enables you to see the projection or "shadow" of these 300D vectors in a 2D plot.
Best of all, the PCA algorithm ensures that this projection is the best possible view of your data, keeping the vectors as far apart as possible.
PCA is like a good photographer that looks at something from every possible angle before composing the optimal photograph.

You don't even have to normalize the length of the vectors after summing the city + state + abbrev vectors, because PCA  takes care of that for you.

We saved these "augmented" city word vectors in the `nlpia` package so you can load them to use in your application.
In the following code, you use PCA to project them onto a 2D plot.

.Bubble chart of US cities
[source,python]
----
>>> from sklearn.decomposition import PCA
>>> pca = PCA(n_components=2)  # <1>
>>> us_300D = get_data('cities_us_wordvectors')
>>> us_2D = pca.fit_transform(us_300D.iloc[:, :300])  # <2>
----
<1> The 2D vectors producted by PCA are for visualization. We retain the original 300-D Word2Vec vectors for any vector reasoning you might want to do.
<2> The last column of this DataFrame contains the city name, which is also stored in the DataFrame index.

Figure 6.8 shows the 2D projection of all these 300-D word vectors for US cities:

[id=google-news-word2Vec, reftext={chapter}.{counter:figure}]
.Google News Word2Vec 300-D vectors projected onto a 2D map using PCA
image::../images/ch06/us-city-word-vector-pca-map.png[Google News Word2Vec 300-D vectors projected onto a 2D map using PCA,width=100%,link="../images/ch06/us-city-word-vector-pca-map.png,caption="300-D word vectors produced by adding the vectors for cities to states and projected onto a 2D map using PCA"]

[NOTE]
====
Low semantic distance (distance values close to zero) represent high similarity between words.
The semantic distance, or "meaning" distance, is determined by the words occurring nearby in the documents used for training. The Word2Vec vectors for two terms are _close_ to each other in word vector space if they are often used in similar contexts (used with similar words nearby). For example "San Francisco" is _close_ to "California" because they  often occur nearby in sentences and the distribution of words used near them are similar. A large distance between two terms expresses a low likelihood of shared context and shared meaning (they are semantically dissimilar), such as "cars" and "peanuts".
====

If you'd like to explore the city map shown in figure 6.8, or try your hand at plotting some vectors of your own, listing 6.12 shows you how.
We built a wrapper for Plotly's offline plotting API that should help it handle DataFrames where you've "denormalized" your data.
The Plotly wrapper expects a DataFrame with a row for each sample and column for features you'd like to plot.
These can be categorical features (such as time zones) and continuous real-valued features (such as city population).
The resulting plots are interactive and useful for exploring many types of machine learning data, especially vector-representations of complex things such as words and documents.

[id=bubble_plot_of_US_city_word_vectors_code, reftext={chapter}.{counter:listing}]
.Bubble plot of US city word vectors
[source,python]
----
>>> import seaborn
>>> from matplotlib import pyplot as plt
>>> from nlpia.plots import offline_plotly_scatter_bubble
>>> df = get_data('cities_us_wordvectors_pca2_meta')
>>> html = offline_plotly_scatter_bubble(
...     df.sort_values('population', ascending=False)[:350].copy()\
...         .sort_values('population'),
...     filename='plotly_scatter_bubble.html',
...     x='x', y='y',
...     size_col='population', text_col='name', category_col='timezone',
...     xscale=None, yscale=None,  # 'log' or None
...     layout={}, marker={'sizeref': 3000})
{'sizemode': 'area', 'sizeref': 3000}
----

To produce the 2D representations of your 300-D word vectors, you need to use a dimension reduction technique.
We used PCA.
To reduce the amount of information lost during the compression from 300-D to 2D, reducing the range of information contained in the input vectors helps.
So you limited your word vectors to those associated with cities.
This is like limiting the domain or subject matter of a corpus when computing TF-IDF (term frequency - inverse document frequency) or BOW vectors.

For a more diverse mix of vectors with greater information content, you'll probably need a nonlinear embedding algorithm such as t-SNE (t-Distributed Stochastic Neighbor Embedding).
We talk about t-SNE and other neural net techniques in later chapters.
t-SNE will make more sense once you've grasped the word vector embedding algorithms here.

=== Making Connections

In this section, we are going to construct what is known as a _graph_.footnote:[See this Wiki page titled, 'Graph (abstract data type'): https://en.wikipedia.org/wiki/Graph_(abstract_data_type)]
The _graph_ data structure is ideal for representing relations in data. At the core, a _graph_ can be be characterized as having _entities_ (_nodes_ or _vertices_) that are connected together through _relationships_ or _edges_. Social networks are great examples of where the _graph_ data structure is ideal to store the data. We will be using a particular type of _graph_ in this section, an _undirected graph_. This type of _graph_ is one where the _relationships_ do not have a direction. An example of this non-directed relationship could be a friend connection between two people on Facebook, since neither can be the friend of the other without reciprocation. Another type of _graph_ is the _directed graph_. This type of _graph_ has relationships that go one way. This type of relationship can be seen in the example of Followers or Following on Twitter. You can follow someone without them following you back, and thus you can have Followers without having to reciprocate the relationship.

To visualize the relationships between ideas and thoughts in this chapter you can create an _undirected graph_ with connections (edges) between sentences that have similar meaning.
You'll use a force-directed layout engine to push all the similar concepts nodes together into clusters.
But first you need some sort of embedding for each sentence.
Sentences are designed to contain a single thought
How would you use word embeddings to create an embedding for a sentence?

You can apply what you learned about word embeddings from previous sections to create sentence embeddings.
You will just average all the embeddings for each word in a sentence to create a single 300-D embedding for each sentence.

==== Extreme summarization

What does a sentence embedding or thought vector actually contain?
That depends on how you create it.
You've already seen how to use SpaCy and `nessvec` to create word embeddings.
You can create sentence embeddings by averaging all the word embeddings for a sentence;

[source,python]
----
from nessvec.files import load_glove
----

==== Extract natural language from the NLPiA manuscript

You can download any of the chapters of this book in `adoc` format from the `src/nlpia2/data/manuscript` directory in the `nlpia2` project (https://gitlab.com/tangibleai/nlpia2/).
The examples here will use the `adoc` manuscript for chapter 6.
If you ever write a book or software documentation yourself don't do this.
The recursive loop of testing and editing code withing the text that you are processing with that code will break your brain.
But you can now enjoy the fruits of all those headaches by processing the words your reading right now.

.Download the adoc text from the `nlpia2` repo
[source,python]
----
>>> import requests
>>> repo = 'https://gitlab.com/tangibleai/nlpia2/-/raw/main'
>>> name = 'Chapter-06_Reasoning-with-word-embeddings-word-vectors.adoc'
>>> url = f'{repo}/src/nlpia2/data/{name}'
>>> adoc_text = requests.get(url)
----

Now you need to save that text to an `.adoc` file so that you can use a commandline tool to render it to html.

.Write the adoc string to disk
[source,python]
----
>>> from pathlib import Path
>>> path = Path.cwd() / name
>>> with path.open('w') as fout:
...     fout.write(adoc_text)
----

Now you will want to render the `adoc` text into HTML to make it easier to separate out the natural language text from the formatting characters and other "unnatural" text.
You can use the Python package called `Asciidoc3` to convert any _AsciiDoc_ (.adoc) text file into HTML.

.Convert AsciiDoc file to HTML
[source,python]
----
>>> import subprocess
>>> subprocess.run(args=[   # <1>
...     'asciidoc3', '-a', '-n', '-a', 'icons', path.name])
----
<1> The Asciidoc3 application can render html from an adoc file.

Now that you have an HTML text file, you can use the _BeautifulSoup_ package to extract the text.

[source,python]
----
>>> if os.path.exists(chapt6_html) and os.path.getsize(chapt6_html) > 0:
...     chapter6_html = open(chapt6_html, 'r').read()
...     bsoup = BeautifulSoup(chapter6_html, 'html.parser')
...     text = bsoup.get_text()  # <1>

----
<1> BeautifulSoup.get_text() extracts the natural language text from HTML.

Now that you have the text for this chapter, you can run the small English language  model from _spaCy_ to get the sentence embedding vectors.
SpaCy will average the _token_ vectors within a `Doc` object.footnote:[spaCy's vector attribute for the Span object defaults to the average of the token vectors (https://spacy.io/api/span#vector)]
In addition to getting the sentence vectors, you also want to retrieve the _noun phrases_ footnote:[See the Wiki page titled, 'Noun phrase': https://en.wikipedia.org/wiki/Noun_phrase] footnote:[spaCy's Span.noun_chunks: https://spacy.io/api/span#noun_chunks] from each sentence that will be the labels for our sentence vectors.

.Getting Sentence Embeddings and Noun Phrases with spaCy
[source,python]
----
>>> import spacy
>>> nlp = spacy.load('en_core_web_md')
>>> config = {'punct_chars': None}
>>> nlp.add_pipe('sentencizer', config=config)
>>> doc = nlp(text)
>>> sentences = []
>>> noun_phrases = []
>>> for sent in doc.sents:
...     sent_noun_chunks = list(sent.noun_chunks)
...     if sent_noun_chunks:
...         sentences.append(sent)
...         noun_phrases.append(max(sent_noun_chunks))
>>> sent_vecs = []
>>> for sent in sentences:
...    sent_vecs.append(sent.vector)
----

Now that you have sentence vectors and noun phrases, you should normalize footnote:[See the Wiki page title, 'Norm (mathematics) -- Euclidean norm': https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm] the sentence vectors so that all your vectors have a length (or _2-norm_) of 1.
The 2-norm is computed the same way you compute the length of the diagonal across a right triangle, you add up the square of the length of the dimensions and then you take the square root of the sum of those squares.


[source,python]
----
>>> import numpy as np
>>> vector = np.array([1, 2, 3, 4])  # <1>
>>> np.sqrt(sum(vector**2)) 
5.47...
>>> np.linalg.norm(vector)  # <2>
5.47...
----
<1> Imagine you had a vector with the values of 1, 2, 3 and 4 in each of 4 dimensions.
<2> The numpy `linalg` module has a norm function that makes it a little easier to compute the 2-norm.

Normalizing the data in the 300-dimensional vector gets all the values on the same scale while still retaining what differentiates them. footnote:[See the web page titled, 'Why Data Normalization is necessary for Machine Learning models': http://mng.bz/aJ2z]

.Normalize the Sentence Vector Embeddings with NumPy
[source,python]
----
>>> import numpy as np
>>> for i, sent_vec in enumerate(sent_vecs):
...     sent_vecs[i] = sent_vec / np.linalg.norm(sent_vec)
----

With the sentence vectors normalized, you can get the similarity between all those vectors and each other.
When you compute the pairwise similarity between all the possible pairs of objects in a list of object that creates a square matrix called a _similarity matrix_ or _affinity matrix_. footnote:[See this web page titled, 'Affinity Matrix': https://deepai.org/machine-learning-glossary-and-terms/affinity-matrix]
If you use the dot product of each vector with all the others your are computing the cosine similarity that you are familiar with from previous chapters.

.Getting the Similarity/Affinity Matrix
[source,python]
----
>>> np_array_sent_vecs_norm = np.array(sent_vecs)
>>> similarity_matrix = np_array_sent_vecs_norm.dot(
...     np_array_sent_vecs_norm.T)  # <1>
----
<1> By computing the dot product on matrices you are _vectorizing_ the operation which is much faster than using a for loop.

The similarity matrix is calculated by taking the _dot product_ between the normalized matrix of sentence embeddings (_N_ by 300 dimensions) with the transpose of itself.
This gives a matrix with shape _N_ by _N_ matrix, one row and column for each sentence in this chapter.
The upper diagonal half of the matrix has the exact same values as the lower diagonal half.
This is because of the commutative property of multiplication.
The similarity between one vecgtor and another is the same no matter which direction you do the multiplication or the similarity computation.

////
HL: hyperspace/sphere discussion?
any vectors pointing in a similar direction will give a weighted sum of their values (dot product) that is close to 1 when they are similar, since the vectors are normalized and all have the same magnitude, but in different directions; think of a sphere in hyper space -- a hypersphere with n-dimensions (an _n-sphere_). footnote:[See the Wiki page titled, 'n-sphere': https://en.wikipedia.org/wiki/N-sphere] These weighted sums will be the value of the undirected edges in the graph, and the nodes are the indexes from the similarity matrix. For example: index_i is one node, and index_j is another node (where 'i' represents rows and 'j' represents columns in the matrix).
////

With the similarity matrix, you can now create an undirected graph using the similarities between sentence vectors to create graph edges between those sentences that are similar.
The code below uses a library called `NetworkX` footnote:[See the NetworkX web page for more information: https://networkx.org/] to create the _undirected graph_ data structure.
Internally the data is stored in an in nested dictionaries -- a dictionary of dictionaries of dictionaries... -- "[dictionaries] all the way down". footnote:[NetworkX docs have more detail (https://networkx.org/documentation/stable/reference/introduction.html#data-structure)]
Like a linked list, the nested dictionaries allow for quick lookups of sparse data.
You computed the similarity matrix as a dense matrix with the dot product, but you will need to make it sparse because you don't want every sentence to be connected to every other sentence in your graph.
You are going to break the links between any sentence pairs that are far apart (have low similarity).

.Creating the Undirected Graph
[source,python]
----
>>> import re
>>> import networkx as nx
>>> similarity_matrix = np.triu(similarity_matrix, k=1)  # <1>
>>> iterator = np.nditer(similarity_matrix, flags=['multi_index'],
    [CA]order='C')
>>> node_labels = dict()
>>> G = nx.Graph()
>>> pattern = re.compile(
...    r'[\w\s]*[\'\"]?[\w\s]+\-?[\w\s]*[\'\"]?[\w\s]*'
...    )  # <2>
>>> for edge in iterator:
...     key = 0
...     value = ''
...     if edge > 0.95:  # <3>
...         key = iterator.multi_index[0]
...         value = str(noun_phrases[iterator.multi_index[0]])
...         if pattern.fullmatch(value)
            [CA]and (value.lower().rstrip() != 'figure'):
...             node_labels[key] = value
...         G.add_node(iterator.multi_index[0])
...         G.add_edge(iterator.multi_index[0], iterator.multi_index[1],
            [CA]weight=edge)
----
<1> np.triu turns the lower triangle in the matrix (k=1 means to include the diagonal in the matrix) into zeros; this allows us to create a single check for the threshold.
<2> This regular expression pattern will help us clean the node labels dictionary of values we do not necessarily want as labels for the nodes
<3> This threshold is arbitrary. It seemed to be a good cut-off point for this data.

With the shiny new graph (network) you've assembled, you can now use `matplotlib.pyplot` to visualize it.

.Plotting the Undirected Graph
[source,python]
----
>>> import matplotlib.pyplot as plt
>>> plt.subplot(1, 1, 1)  # <1>
>>> pos = nx.spring_layout(G, k=0.15, seed=42)  # <1>
>>> nx.draw_networkx(G, pos=pos, with_labels=True, labels=node_labels,
    [CA]font_weight='bold')
>>> plt.show()
----
<1> initialize a single figure (plot) with one subplot filling the window
<2> `k` affects the node separation -- the average "resting" length of the spring forces in the connections between nodes

Finally you can see your _undirected graph_ show the clusters of concepts in the natural language of this book!
Each _node_ represents the average word embedding for a sentence in this chapter.
And the _edges_ (or lines) represent the connections between the meaning of those sentences that mean similar things.
Looking at the plot, you can see the central big cluster of nodes (sentences) are connected the most.
And there are other smaller clusters of nodes further out from the central cluster for topics such as sports and cities.

[id=concepts-to-each-other, reftext={chapter}.{counter:figure}]
.Connecting concepts to each other with word embeddings
image::../images/ch06/adjacency_graph_ch_6_with_labels_bold.png[alt="An undirected graph or network diagram of the natural language in chapter 6 using sentence embeddings and noun phrases as labels",width=100%,link="../images/ch06/adjacency_graph_ch_6_with_labels_bold.png"]

The dense cluster of concepts in the center should contain some information about the central ideas of this chapter and how they are related.
Zooming in you can see these passages are mostly about words and numbers to represent words, because that's what this chapter is about.


[id=undirected-graph-plot, reftext={chapter}.{counter:figure}]
.Undirected Graph Plot of Chapter 6 Center Zoom-in
image::../images/ch06/adjacency_graph_ch_6_zoom_in_center_with_labels_bold.png[Center zoom-in of the undirected graph plot of chapter  using sentence embeddings and noun phrases as labels,width=100%,link="../images/ch06/adjacency_graph_ch_6_zoom_in_center_with_labels_bold.png]

The end of this chapter includes some exercises that you can do to practice what we have covered in this section.

=== Unnatural words

Word embeddings such as Word2Vec are useful not only for English words but also for any sequence of symbols where the sequence and proximity of symbols is representative of their meaning.
If your symbols have semantics, embeddings may be useful.
As you may have guessed, word embeddings also work for languages other than English.

Embedding works also for pictorial languages such as traditional Chinese and Japanese (Kanji) or the mysterious hieroglyphics in Egyptian tombs.
Embeddings and vector-based reasoning even works for languages that attempt to obfuscate the meaning of words.
You can do vector-based reasoning on a large collection of "secret" messages transcribed from "Pig Latin" or any other language invented by children or the Emperor of Rome.
A _Caesar cipher_ footnote:[See the web page titled "Caesar cipher - Wikipedia" (https://en.wikipedia.org/wiki/Caesar_cipher).] such as RO13 or a _substitution cipher_ footnote:[See the web page titled "Substitution cipher - Wikipedia" (https://en.wikipedia.org/wiki/Substitution_cipher).] are both vulnerable to vector-based reasoning with Word2Vec.
You don't even need a decoder ring (shown in figure 6.9).
You just need a large collection of messages or _n_-grams that your Word2Vec embedder can process to find co-occurrences of words or symbols.

[id=decoder-rings, reftext={chapter}.{counter:figure}]
.Decoder rings
image::../images/ch06/decoder-rings.png[Google News Word2Vec 300-D vectors projected onto a 2D map using PCA,width=100%,link="../images/ch06/decoder-rings.png]

Word2Vec has even been used to glean information and relationships from unnatural words or ID numbers such as college course numbers (CS-101), model numbers (Koala E7270 or Galaga Pro), and even serial numbers, phone numbers, and zip codes. footnote:[See the web page titled "A non-NLP application of Word2Vec – Towards Data Science" (https://medium.com/towards-data-science/a-non-nlp-application-of-word2vec-c637e35d3668).]
To get the most useful information about the relationship between ID numbers like this, you'll need a variety of sentences that contain those ID numbers. And if the ID numbers often contain a structure where the position of a symbol has meaning, it can help to tokenize these ID numbers into their smallest semantic packet (such as words or syllables in natural languages).



== Summary

* Word vectors and vector-oriented reasoning can solve some surprisingly subtle problems like analogy questions and nonsynonomy relationships between words.
* To keep your word vectors current and improve their relevance to the current events and concepts you are interested in you can retrain and fine tune your word embeddings with `gensim` or PyTorch.
* The `nessvec` package is a fun new tool for helping you find that word on the tip of your tongue or visualize the "character sheet" of a word.
* Word embeddings can reveal some surprising hidden meanings of the names of people, places, businesses and even occupations
* A PCA projection of word vectors for cities and countries can reveal the cultural closeness of places that are geographically far apart.
* The key to turning latent semantic analysis vectors into more powerful word vectors is to respect sentence boundaries when creating your _n_-grams
* Machines can easily pass the word analogies section of standardized tests with nothing more than pretrained word embeddings


== Test yourself

1. Use pretrained word embeddings to compute the strength, agility, and intelligence of Dota 2 heroes based only on the natural language summary.footnote:[The `nessvec` and `nlpia2` packages contain FastText, GloVE and Word2vec loaders (https://gitlab.com/tangibleai/nessvec). nlpia2 contain `ch06_dota2_wiki_heroes.hist.py` (https://gitlab.com/tangibleai/nlpia2/-/blob/main/src/nlpia2/etl/ch06_dota2_wiki_heroes.hist.py) `dota2-heroes.csv` (https://gitlab.com/tangibleai/nlpia2/-/raw/main/.nlpia2-data/dota2-heroes.csv?inline=false)]
2. Visualize the graph of connections between concepts in another chapter of this book (or any other text) that you'd like to understand better.
3. Try combining graph visualizations of the word embeddings for all the chapters of this book.
4. Give examples for how word vectors enable at least two of Hofstadter's eight elements of intelligence
5. Fork the nessvec repository and create your own visualizations or nessvector "character sheets" for your favorite words or famous people. Perhaps the "mindfulness", "ethicalness", "kindness", or "impactfulness" of your heros? Humans are complex and the words used to describe them are multidimensional.
