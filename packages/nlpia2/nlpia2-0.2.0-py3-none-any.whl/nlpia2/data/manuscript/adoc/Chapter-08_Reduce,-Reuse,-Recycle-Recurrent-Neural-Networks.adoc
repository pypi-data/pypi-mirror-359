= Natural Language Processing in Action, Second Edition
:chapter: 8
:part: 2
:sectnums:
:sectnumoffset: 7
:figure-caption: Figure {chapter}.
:listing-caption: Listing {chapter}.
:table-caption: Table {chapter}.
:leveloffset: 1
:xrefstyle: short
// :imagesdir: .
// :icons!:
:stem: latexmath
:toc:
:source-highlighter: coderay
:bibliography-database: dl4nlp.bib
:bibliography-style: ieee
:index: []
:!figure:
:!listing:

// SUM: RNNs add recurrence to neural networks, a paradigm shift that enables deep learning to achieve truly intelligent behavior like conversation and composition of longer texts.
= Reduce, Reuse, Recycle Your Words (RNNs and LSTMs)

This chapter covers

* Unrolling recursion so you can understand how to use it for NLP
* Implementing word and character-based RNNs in PyTorch
* Identifying applications where RNNs are your best option
* Re-engineering your datasets for training RNNs
* Customizing and tuning your RNN structure for your NLP problems
* Understanding backprop (backpropagation) in time
* Combining long and short-term memory mechanisms to make your RNN smarter

// SUM: RNNs add recurrence to neural networks, a paradigm shift or phase shift that enables DL to achieve truly intelligent behavior. Unlike CNNs which must truncate your texts at a maximum length, RNNs enable your model to keep reading and reading, (or writing and writing) without limit until you tell them to stop.

An _RNN_ (Recurrent Neural Network) recycles tokens.
Why would you want to recycle and reuse your words?
To build a more sustainable NLP pipeline of course! ;)
_Recurrence_ is just another word for recycling.
An RNN uses recurrence to allow it to remember the tokens it has already read and reuse that understanding to predict the target variable.
And if you use RNNs to predict the next word, RNNs can generate, going on and on and on, until you tell them to stop.
This sustainability or regenerative ability of RNNs is their superpower.

It turns out that your NLP pipeline can predict the next tokens in a sentence much better if it remembers what it has already read and understood.
But, wait, didn't a CNN "remember" the nearby tokens with a kernel or filter of weights?
It did!
But a CNN can only _remember_ a limited window, that is a few words long.
By recycling the machine's understanding of each token before moving to the next one, an RNN can remember something about _all_ of the tokens it has read.
This makes your machine reader much more sustainable, it can keep reading and reading and reading...for as long as you like.

But wait, isn't recursion dangerous?
If that's the first thought that came to you when you read recurrence, you're not alone.
Anyone who has taken an algorithms class has probably broken a function, an entire program, or even taken down an entire web server, but using recurrence the wrong way.
The key to doing recurrence correctly and safely is that you must always make sure your algorithm is _reducing_ the amount of work it has to do with each recycling of the input.
This means you need to delete something from the input before you call the function again with that input.
For your NLP RNN, this comes naturally as you _pop_ (remove) a token off of the _stack_ (the text string) before you feed that input back into your network.

[NOTE]
====
Technically "recurrence" and "recursion" are two different things. footnote:[Mathematics forum StackExchange question about recurrence and recursion (https://math.stackexchange.com/questions/931035/recurrence-vs-recursive)]
But most mathematicians and computer scientists use both words to explain the same concept - recycling a portion of the output back into the input to perform an operation repeatedly in sequence. footnote:[MIT Open Courseware lectures for CS 6.005 "Software Construction" (https://ocw.mit.edu/ans7870/6/6.005/s16/classes/10-recursion/)]
But as with all natural language words, the concepts are fuzzy and it can help to understand them both when building _Recurrent_ Neural Networks.
As you'll see in the code for this chapter, an RNN doesn't have a function that calls itself recursively the way you normally think of recursion.
The `.forward(x)` method is called in a `for` loop that is outside of the RNN itself.
====

RNNs are _neuromorphic_.
This is a fancy way of saying that researchers are mimicking how they think brains work when they design artificial neural nets such as RNNs.
You can use what you know about how your own brain works to come up with ideas for how to process text with artificial neurons.
And your brain is recurrently processing the tokens that you are reading right now.
So recurrence must be a smart, efficient way to use your brain resources to understand text.

As you read this text you are recycling what you already know about the previous words before updating your prediction of what's going to happen next.
And you don't stop predicting until you reach the end of a sentence or paragraph or whatever you're trying to understand.
Then you can pause at the end of a text and process all of what you've just read.
Just like the RNNs in this chapter, the RNN in your brain uses that pause at the end to encode, classify, and _get something out of_ the text.
And because RNNs are always predicting, you can use them to predict words that your NLP pipeline should say.
So RNNs are great not only for reading text data but also for tagging and writing text.

RNNs are a game changer for NLP.
They have spawned an explosion of practical applications and advancements in deep learning and AI.

== What are RNNs good for?
// SUM: Unlike CNNs which must truncate your texts at a maximum length, RNNs enable your model to keep reading and reading, (or writing and writing) without limit until you tell them to stop. Because RNNs can process variable length text they enable new applications such as tagging or classifying individual tokens and generating text.

The previous deep learning architectures you've learned about are great for processing short bits of text - usually individual sentences.
RNNs promise to break through that text length barrier and allow your NLP pipeline to ingest an infinitely long sequence of text.
And not only can they process unending text, but they can also _generate_ text for as long as you like.
RNNs open up a whole new range of applications like generative conversational chatbots and text summarizers that combine concepts from many different places within your documents.

[cols="d,d,d"]
|===
| **Type** | **Description** | **Applications**
|  One to Many  | One input tensor used to generate a sequence of output tensors | Generate chat messages, answer questions, describe images
|  Many to One  |   sequence of input tensors gathered up into a single output tensor |   Classify or tag text according to its language, intent, or other characteristics
|  Many to Many |  a sequence of input tensors used to generate a sequence of output tensors | Translate, tag, or anonymize the tokens within a sequence of tokens, answer questions, participate in a conversation
|===


This is the superpower of RNNs, they process sequences of tokens or vectors.
You are no longer limited to processing a single, fixed-length vector.
So you don't have to truncate and pad your input text to make your round text the right shape to fit into a square hole.
And an RNN can generate text sequences that go on and on forever if you like.
You don't have to stop or truncate the output at some arbitrary maximum length that you decide ahead of time.
Your code can dynamically decide when enough is enough.

// Figure 8.1
[id=recycling-tokens, reftext={chapter}.{counter:figure}]
.Recycling tokens creates endless options
image::../images/ch08/rnn-unrolled-many-to-many_drawio.png[alt="Figure 8.1: Four block diagrams of different kinds of RNNs based on the number of inputs and number of outputs: 1. one-to-one - one input tensor and one output tensor 2. many-to-one - sequence of input tensors gathered up into a single output tensor 3. one-to-many - one input tensor and a sequence of many output tensors 4. many-to-many - a sequence of input tensors producing a sequence of output tensors .", width=95%]

You can use RNNs to achieve state-of-the-art performance on many of the tasks you're already familiar with, even when your text is shorter than infinity `;)`.

* translation
* summarization
* classification
* question answering

And RNNs are one of the most efficient and accurate ways to accomplish some new NLP tasks that you will learn about in this chapter:

* generating new text such as paraphrases, summaries or even answers to questions
* tagging individual tokens
* diagramming the grammar of sentences, as you did in English class
* creating language models that predict the next token

If you read through the RNNs that are at the top of the leader board on Papers with Code footnote:[Papers with Code query for RNN applications (https://proai.org/pwc-rnn)] you can see that RNNs are the most efficient approach for many applications.

RNNs aren't just for researchers and academics.
Let's get real.
In the real world, people are using RNNs to:

* spell checking and correction
* autocompletion of natural language or programming language expressions
* classify sentences for grammar checking or FAQ chatbots
* classify questions or generate answers to those questions
* generate entertaining conversational text for chatbots
* named entity recognition (NER) and extraction
* classify, predict, or generate names for people, babies, and businesses
* classify or predict subdomain names (for security vulnerability scanning

You can probably guess what most of those applications are about, but you're probably curious about that last one (subdomain prediction).
A subdomain is that first part of a domain name in a URL, the `www` in `www.lesswrong.com` or `en` in `en.wikipedia.org`.
Why would anyone want to predict or guess subdomains?
Dan Meisler did a talk on the critical role that subdomain guessers play in his cybersecurity toolbox.footnote:[Daniel Miessler's Unsupervised Learning podcast #340 (https://mailchi.mp/danielmiessler/unsupervised-learning-no-2676196) and the RNN source code (https://github.com/JetP1ane/Affinis)]
Once you know a subdomain, a hacker or pentester can scan the domain to find vulnerabilities in the server security.

And once you will soon be comfortable using RNNs to generate completely new words, phrases, sentences, paragraphs and even entire pages of text.
It can be so much fun playing around with RNNs that you could find yourself accidentally creating applications that open up opportunities for completely new businesses.

* suggest company, product or domain names footnote:[Ryan Stout's (https://github.com/ryanstout) BustAName app (https://bustaname.com/blog_posts)]
* suggest baby names
* sentence labeling and tagging
* autocomplete for text fields
* paraphrasing and rewording sentences
* inventing slang words and phrases

=== RNNs can handle any sequence
In addition to NLP, RNNs are useful for any sequence of numerical data, such as time series.
You just need to represent the objects in your sequence as numerical vectors.
For natural language words this is often the word embedding.
But you can also see how a city government might represent daily or hourly electric scooter rentals, freeway traffic, or weather conditions as vectors.
And often they will want to predict all of this simultaneously in one vector.
 
Because RNNs can output something for each and every element in a sequence, you can create an RNN that outputs a prediction for "tomorrow" -- the next sequence element after the one you currently know.
You can then use that prediction to predict the one after that, recursively.
This means that once you master backpropagation through time, you will be able to use RNNs to predict things such as:

* The next day's weather
* The next minute's web traffic volume
* The next second's Distributed Denial of Services (DDOS) web requests
* The action an automobile driver will take over the next 100 milliseconds
* The next image in a sequence of frames in a video clip

As soon as you have a prediction of the target variable you can measure the error - the difference between the model's output and the desired output.
This usually happens at the last time step in whatever sequence of events you are processing.

=== RNNs remember everything you tell them
// SUM: If you rolled a clean paint roller of the wet paint of a sign, it would smear all the letters together to create a single smudge at the end. The smudge gathers up all the paint from the previous letters into a single compact representation of the original text.

Have you ever accidentally touched wet paint and found yourself "reusing" that paint whenever you touched something?
And as a child, you might have fancied yourself an impressionistic painter as you shared your art with the world by finger-painting the walls around you.
You're about to learn how to build a more mindful impressionistic word painter.
In chapter 7 you imagined a lettering stencil as an analogy for processing text with CNNs.
Well now, instead of sliding a word stencil across the words in a sentence you are going to roll a paint roller across them... while they're still wet!

Imagine painting the letters of a sentence with slow-drying paint and laying it on thick.
And let's create a diverse rainbow of colors in your text.
Maybe you're even supporting LBGTQ pride week by painting the crosswalks and bike lanes in North Park.

// Figure 8.2
[id=rainbow-of-meaning, reftext={chapter}.{counter:figure}]
.A rainbow of meaning
image::../images/ch08/wet-paint-rainbow-lettering_drawio.png[alt="Figure 8.2: The letters 'Wet Paint' in a rainbow of color, one color for each letter.", width=50%]

Now, pick up a clean paint roller and roll it across the letters of the sentence from the beginning of the sentence to the end.
Your roller would pick up the paint from one letter and recycle it to lay it back down on top of the previous letters.
Depending on how big your roller is, a small number of letters (or parts of letters) would be rolled on top of the letters to the right.
All the letters after the first one would be smeared together to create a smudgy stripe that only vaguely resembles the original sentence.

// Figure 8.3
[id=pot-of-gold, reftext={chapter}.{counter:figure}]
.Pot of gold at the end of the rainbow
image::../images/ch08/wet-paint-rainbow-lettering-smudged_drawio.png[alt="Figure 8.3: The letters 'Wet Paint' in a rainbow of color, one color for each letter, rolled over with a clean paint roller while the paint is still wet. This would create a rainbow smudge at the end. A pink 'm' at the end seems to dominate. But the 'm' was created by duplicating the 'n' in the word 'Paint!'. And because this is the last thing your paint roller touches it covers over the exclamation point and all the other smudged letters.", width=50%]

The smudge gathers up all the paint from the previous letters into a single compact representation of the original text.
But is it a useful, meaningful representation?
For a human reader, all you've done is create a multicolored mess.
It wouldn't communicate much meaning to the humans reading it.
This is why humans don't use this _representation_ of the meaning of the text for themselves.
However, if you think about the smudge of characters you might be able to imagine how a machine might interpret it.
And for a machine, it is certainly much more dense and compact than the original sequence of characters.

In NLP we want to create compact, dense vector representations of text.
Fortunately, that representation we're looking for is hidden on your paint roller!
As your fresh clean roller got smeared with the letters of your text it gathered up a _memory_ of all the letters you rolled it across.
This is analogous to the word embeddings you created in Chapter 6.
But this embedding approach would work on much longer pieces of text.
You could keep rolling the roller forever across more and more text, if you like, squeezing more and more text into the compact representation.

In previous chapters, your tokens were mostly words or word n-grams.
You need to expand your idea of a token to include individual characters.
The simplest RNNs use characters rather than words as tokens.
This is called a character-based RNN.
Just as you had word and token embeddings in previous chapters you can think of characters having meaning too.
Now does it make more sense how this smudge at the end of the "Wet Paint!" lettering represents an embedding of all the letters of the text?

One last imaginary step might help you bring out the hidden meaning in this thought experiment.
In your mind, check out that embedding on your paint roller.
In your mind roll it out on a fresh clean piece of paper.
Keep in mind the paper and your roller are only big enough to hold a single letter.
That will _output_ a compact representation of the paint roller's memory of the text.
And that output is hidden inside your roller until you decide to use it for something.
That's how the text embeddings work in an RNN.
The embeddings are _hidden_ inside your RNN until you decide to output them or combine them with something else to reuse them.
In fact, this vector representation of your text is stored in a variable called `hidden` in many implementations of RNNs.

[IMPORTANT]
====
RNN embeddings are different from the word and document embeddings you learned about in Chapters 6 and 7.
An RNN is gathering up meaning over time or text position.
An RNN encodes meaning into this vector for you to reuse with subsequent tokens in the text.
This is like the Python `str.encode()` function for creating a multi-byte representation of Unicode text characters.
The order in which the sequence of tokens is processed matters a lot to the end result, the encoding vector.
So you probably want to call RNN embeddings "encodings", "encoding vectors" or "encoding tensors."
This vocabulary shift was encouraged by Garrett Lander on a project to do NLP on extremely long and complex documents, such as patient medical records or The Meuller Report.footnote:[Garrett Lander, Al Kari, and Chris Thompson contributed to our project to unredact the Meuller report (https://proai.org/unredact)]
This new vocabulary made it a lot easier for his team to develop a shared mental model of the NLP pipeline.
====

Keep your eye out for the hidden layer later in this chapter.
The activation values are stored in the variable `h` or `hidden`.
These activation values within this tensor are your embeddings up to that point in the text.
It's overwritten with new values each time a new token is processed as your NLP pipeline is gathering up the meaning of the tokens it has read so far.
In figure <<ch8_best_figure>> you can see how this blending of meaning in an embedding vector is much more compact and blurry than the original text.

[id=ch8_best_figure, reftext={chapter}.{counter:figure}]
.Gather up meaning into one spot
image::../images/ch08/wet-paint-rainbow-lettering-smudged-encoding_drawio.png[alt="The letters 'Wet Paint' in a rainbow of color, one color for each letter and then stamped in reducing opacity and increasing transparency one on top of the other.. And because this is the last thing your paint roller touched, it obscures the exclamation point and all the other smudged letters processed earlier.", width=30%]

You could read into the paint smudge something of the meaning of the original text, just like in a Rorschach inkblot test.
Rorschach inkblots are smudges of ink or paint on flashcards used to spark people's memories and test their thinking or mental health.footnote:[Rorsharch test Wikipedia article (https://en.wikipedia.org/wiki/Rorschach_test)]
Your smudge of paint from the paint roller is a vague, impressionistic representation of the original text.
And it's a much more compact representation of the text.
This is exactly what you were trying to achieve, not just creating a mess.
You could clean your roller, rinse and repeat this process on a new line of text to get a different smudge with a different _meaning_ for your neural network.
Soon you'll see how each of these steps is analogous to the actual mathematical operations going on in an RNN layer of neurons.

Your paint roller has smeared many of the letters at the end of the sentence so that the last exclamation point at the end is almost completely unintelligible.
But that unintelligible bit at the end is exactly what your machine needs to understand the entire sentence within the limited surface area of the paint roller.
You have smudged all the letters of the sentence together onto the surface of your roller.
And if you want to see the message embedded in your paint roller, you just roll it out onto a clean piece of paper.

In your RNN you can accomplish this by outputting the hidden layer activations after you've rolled your RNN over the tokens of some text.
The encoded message probably won't say much to you as a human, but it gives your paint roller, the machine, a hint at what the entire sentence said.
Your paint roller gathered an impression of the entire sentence.
We even use the word "gather" to express understanding of something someone says, as in "I gather from what you just said, that rolling paint rollers over wet paint are analogous to RNNs."

Your paint roller has compressed, or encoded, the entire sentence of letters into a short smudgy impressionistic stripe of paint.
In an RNN this smudge is a vector or tensor of numbers.
Each position or dimension in the encoding vector is like a color in your paint smudge.
Each encoding dimension holds an aspect of meaning that your RNN has been designed to keep track of.
The impressions that the paint made on your roller (the hidden layer activations) were continuously recycled till you got to the end of the text.
And then you reused all those smudges on your roller to create a new impression of the entire sentence.

=== RNNs hide their understanding
// SUM: An RNN has a loop that recycles or feeds back the hidden layer output back into that same layer by combining it with the input for the next token in the text.

The key change for an RNN is that it maintains a hidden embedding by recycling the meaning of each token as it reads them one at a time.
This hidden vector of weights contains everything the RNN has understood up to the point in the text it is reading.
This means you can't run the network all at once on the entire text you're processing.
In previous chapters, your model learns a function that maps one input to one output.
But, as you'll soon see, an RNN learns a _program_ that keeps running on your text until it's done.
An RNN needs to read your text one token at a time.

An ordinary feedforward neuron just multiplies the input vector by a bunch of weights to create an output.
No matter how long your text is, a CNN or feedforward neural network will have to do the exact same number of multiplications to compute the output prediction.
The neurons of a linear neural network all work together to compose a new vector to represent your text.
You can see in Figure <<ordinary-feedforward-neuron>> that a normal feedforward neural network takes in a vector input (`x`), multiplies it by a matrix of weights (`W`), applies an activation function, and then outputs a transformed vector (`y`).
Feedforward network layers transform can only transform one vector into another.

[id=ordinary-feedforward-neuron, reftext={chapter}.{counter:figure}]
.Ordinary feedforward neuron
image::../images/ch08/neuron-feedforward_drawio.png[alt="Figure 8.5: Block diagram of ordinary feedforward neuron taking in a vector x from the bottom, multiplies it by a matrix of weights (W), applies an activation function (shown as the S curve of the sigmoid function) and then outputs a transformed vector (y) out the top.", width=50%]

With RNNs, your neuron never gets to see the vector for the entire text.
Instead, an RNN must process your text one token at a time.
To keep track of the tokens it has already read it records a hidden vector (`h`) that can be passed along to its future self - the exact same neuron that produced the hidden vector in the first place.
In computer science terminology this hidden vector is called a _state_.
That's why Andrej Karpathy and other deep learning researchers get so excited about the effectiveness of RNNs.
RNNs enable machines to finally learn Turing complete programs rather than just isolated functions.footnote:["The unreasonable effectiveness of RNNs" (https://karpathy.github.io/2015/05/21/rnn-effectiveness)]

// Figure 8.6
[id=neuron-with-recurrence, reftext={chapter}.{counter:figure}]
.A neuron with recurrence
image::../images/ch08/neuron-with-recurrence_drawio.png[alt="Figure 8.6: Block diagram of a recurrent neuron taking in a vector x from the bottom, and the hidden state vector from the left-hand side. First, it concatenates the two tensors together then multiplies this concatenated tensor by two different matrice of weights (W_c2h and W_c2y) to output two transformed tensors (y and h). The output is squashed with an activation function (shown as the S curve of the sigmoid function) before outputting the transformed vector (y) out the top. In this simple recurrence approach, no activation function is applied to the hidden tensor output.", width=95%]

If you unroll your RNN it begins to look a lot like a chain... a Markov Chain, in fact.
But this time your window is only one token wide and you're reusing the output from the previous token, combined with the current token before rolling forward to the next token in your text.
Fortunately, you started doing something similar to this when you slid the CNN window or kernel across the text in Chapter 7.

How can you implement neural network recurrence in Python?
Luckily, you don't have to try to wrap around a recursive function call like you may have encountered in coding interviews.
Instead, all you have to do is create a variable to store the hidden state separate from the inputs and outputs.
And you need to have a separate matrix of weights to use for computing that hidden tensor.
<<listing-recurrence-pytorch>> implements a minimal RNN from scratch, without using PyTorch's `RNNBase` class.

[#listing-recurrence-pytorch, reftext={chapter}.{counter:listing}]
.Recurrence in PyTorch
[source,python]
----
>>> from torch import nn

>>> class RNN(nn.Module):
...
...     def __init__(self,
...             vocab_size, hidden_size, output_size):  # <1>
...         super().__init__()
...         self.W_c2h = nn.Linear(
...             vocab_size + hidden_size, hidden_size)  # <2>
...         self.W_c2y = nn.Linear(vocab_size + hidden_size, output_size)
...         self.softmax = nn.LogSoftmax(dim=1)
...
...     def forward(self, x, hidden):  # <3>
...         combined = torch.cat((x, hidden), axis=1)  # <4>
...         hidden = self.W_c2h(combined)  # <5>
...         y = self.W_c2y(combined)  # <6>
...         y = self.softmax(y)
...         return y, hidden  # <7>
----
<1> `vocab_size` and `hidden_size` used to allocate space for the `combined` inputs
<2> Add `W_c2h1`, `W_c2h2`, ... `Linear` layers of the same size for deeper learning
<3> `x` = one-hot vector for latest token, `hidden` = latest encoding vector
<4> Concatenate one-hot token vector with the latest hidden (encoding) vector
<5> `nn.Linear` dot product transforms `combined` vector into a `hidden` vector
<6> Dot product transforms `combined` vector into `y` (output vector of category likelihoods)
<7> Notice that both the input and output include the `hidden` encoding vector - it's reused on the next token

You can see how this new RNN neuron now outputs more than one thing.
Not only do you need to return the output or prediction, but you also need to output the hidden state tensor to be reused by the "future self" neuron.

Of course, the PyTorch implementation has many more features.
PyTorch RNNs can even be trained from left to right and right to left simultaneously!
This is called a bidirectional language model.
Of course, your problem needs to be "noncausal" for a bidirectional language model to be of any use. 
A noncausal model in NLP for English just means that you want your language model to predict words that occur before (to the left of) other words that you already know.
A common noncausal application is to predict interior words that have been masked out intentionally or accidentally corrupted during OCR (Optical Character Recognition).
If you're curious about bidirectional RNNs, all of the PyTorch RNN models (RNNs, GRUs, LSTMs, and even Transformers) include an option to turn on bidirectional recurrence.footnote:[PyTorch `RNNBase` class source code (https://github.com/pytorch/pytorch/blob/75451d3c81c88eebc878fb03aa5fcb89328989d9/torch/nn/modules/rnn.py#L44)]
For question-answering models and other difficult problems, you will often see a 5-10% improvement in the accuracy of bidirectional models when you compare them to the default forward direction (causal) language models.
This is simply because their embeddings of a bidirectional language model are more balanced, forgetting as much about the beginning of the text as they forget about the end of the text.

=== RNNs remember everything you tell them
// SUM: CNNs have a limited window of memory - the kernel lengths. So RNNs give models an infinite memory for the previous tokens in the text it has already processed.

To see how RNNs retain a memory of all the tokens of a document you can unroll the neuron diagram in Figure 8.7.
You create copies of the neuron to show the "future selves" in the `for` loop that is iterating through your tokens.
This is like unrolling a `for` loop, when you just copy and paste the lines of code within the loop the appropriate number of times.

// Figure 8.7
[id=unroll-an-RNN, reftext={chapter}.{counter:figure}]
.Unroll an RNN to reveal its hidden secrets
image::../images/ch08/rnn-unrolled_drawio.png[alt="Figure 8.7: A many to many RNN unrolled inputs come in from the bottom hidden vectors come in from the left and are passed along to the right to subsequent time steps of the weight matrix which includes both W_i2o to transform the input to the output and W_i2h to transform the input into the hidden weights used in the next time step. The current time step is t but the neurons keep unrolling to the right of the weights for t until the end of the text you are processing (N tokens).", width=80%]

Figure 8.7 shows an RNN passes the hidden state along to the next "future self" neuron, sort of like Olympic relay runners passing the baton.
But this baton is imprinted with more and more memories as it is recycled over and over again within your RNN.
You can see how the tensors for the input tokens are modified many, many times before the RNN finally sees the last token in the text.

Another nice feature of RNNs is that you can tap into an output tensor anywhere along the way.
This means you can tackle challenges like machine translation, named entity recognition, anonymization and deanonymization of text, and even unredaction of government documents.footnote:[Portland Python User Group presentation on unredacting the Meuller Report (https://proai.org/unredact)]

These two features are what make RNNs unique.

1. You can process as many tokens as you like in one text document.
2. You can output anything you need after each token is processed.

That first feature is not such a big deal.
As you saw with CNNs, if you want to process long text, you just need to make room for them in your max input tensor size.
In fact, the most advanced NLP models to date, _transformers_, create a max length limit and pad the text just like CNNs.

However, that second feature of RNNs is a really big deal.
Imagine all the things you can do with a model that labels each and every token in a sentence.
Linguists spend a lot of time diagramming sentences and labeling tokens.
RNNs and deep learning have revolutionized the way linguistics research is done.
Just look at some of the linguistic features that SpaCy can identify for each word in some example "hello world" text in listing <<figure-spacy-tags-tokens>>.

[id=figure-spacy-tags-tokens, reftext={chapter}.{counter:listing}]
.SpaCy tags tokens with RNNs
[source,python]
----
>>> import pandas as pd
>>> from nlpia2.spacy_language_model import nlp
>>>
>>> tagged_tokens = list(nlp('Hello world. Goodbye now!'))
>>> interesting_tags = 'text dep_ head lang_ lemma_ pos_ sentiment'
>>> interesting_tags = (interesting_tags +  'shape_ tag_').split()
>>> pd.DataFrame([
...         [getattr(t, a) for a in interesting_tags]
...         for t in tagged_tokens],
...     columns=interesting_tags)
      text    dep_     head lang_   lemma_   pos_  sentiment shape_ tag_
0    Hello    intj    world    en    hello   INTJ        0.0  Xxxxx   UH
1    world    ROOT    world    en    world   NOUN        0.0   xxxx   NN
2        .   punct    world    en        .  PUNCT        0.0      .    .
3  Goodbye    ROOT  Goodbye    en  goodbye   INTJ        0.0  Xxxxx   UH
4      now  advmod  Goodbye    en      now    ADV        0.0    xxx   RB
5        !   punct  Goodbye    en        !  PUNCT        0.0      !    .
----

It's all well and good to have all that information - all that output whenever you want it.
And you're probably excited to try out RNNs on really long text to see how much it can actually remember.

== Predict someone's nationality from only their last name

To get you up to speed quickly on recycling, you'll start with the simplest possible token -- the lowly character (letter or punctuation).
You are going to build a model that can predict the nationality of last names, also called "surnames" using only the letters in the names to guide the predictions.
This kind of model may not sound all that useful to you.
You might even be worried that it could be used to harm individuals from particular cultures.

Like you, the authors' LinkedIn followers were suspicious when we mentioned we were training a model to predict the demographic characteristics of names.
Unfortunately, businesses and governments do indeed use models like this to identify and target particular groups of people, often with harmful consequences.
But these models can also be used for good.
We use them to help our nonprofit and government customers anonymize their conversational AI datasets.
Volunteers and open-source contributors can then train NLP models from these anonymized conversation datasets to identify healthcare or education content that can be helpful for users, while simultaneously protecting user privacy.

This multilingual dataset will give you a chance to learn how to deal with diacritics and other embellishments that are common for non-English words.
To keep it interesting, you will remove these character embellishments and other giveaways in the Unicode characters of multilingual text.
That way your model can learn the patterns you really care about rather than "cheating" based on this leakage.
The first step in processing this dataset is to _asciify_ it - convert it to pure ASCII characters.
For example, the Unicode representation of the Irish name "O’Néàl" has an "acute accent" over the "e" and a "grave accent" over the "a" in this name.
And the apostrophe between the "O" and "N" can be a special directional apostrophe that could unfairly clue your model into the nationality of the name, if you don't _asciify_ it.
You will also need to remove the cedilla embellishment that is often added to the letter "C" in Turkish, Kurdish, Romance and other alphabets.

[source,python]
----
>>> from nlpia2.string_normalizers import Asciifier
>>> asciify = Asciifier()

>>> asciify("O’Néàl")
"O'Neal"
>>> asciify("Çetin")
'Cetin'
----

Now that you have a pipeline that "normalizes" the alphabet for a broad range of languages, your model will generalize better.
Your model will be useful for almost any Latin script text, even text transliterated into Latin script from other alphabets.
You can use this exact same model to classify any string in almost any language.
You just need to label a few dozen examples in each language you are interested in "solving" for.

Now let's see if you've created a _solvable problem_.
A solvable machine learning problem is one where:

1. You can imagine a human answering those same questions
2. There exists a correct answer for the vast majority of "questions" you want to ask your model
3. You don't expect a machine to achieve accuracy much better than a well-trained human expert

Think about this problem of predicting the country or dialect associated with a surname.
Remember we've removed a lot of the clues about the language, like the characters and embellishments that are unique to non-English languages.
Is it solvable?

Start with the first question above.
Can you imagine a human could identify a person's nationality from their asciified surname alone?
Personally, I often guess wrong when I try to figure out where one of my students is from, based on their surname.
I will never achieve 100% accuracy in real life and neither will a machine.
So as long as you're OK with an imperfect model, this is a solvable problem.
And if you build a good pipeline, with lots of labeled data, you should be able to create an RNN model that is at least as accurate as humans like you or I.
It may even be more accurate than a well-trained linguistics expert, which is pretty amazing when you think about it.
This is where the concept of AI comes from, if a machine or algorithm can do intelligent things, we call it AI.

Think about what makes this problem hard.
There is no one-to-one mapping between surnames and countries.
Even though surnames are generally shared between parents and children for generations, people tend to move around.
And people can change their nationality, culture, and religion.
All these things affect the names that are common for a particular country.
And sometimes individuals or whole families decide to change their last name, especially immigrants, expats and spies.
People have a lot of different reasons for wanting to blend in.footnote:[Lex Fridman interview with ex-spy Andrew Bustamante (https://lexfridman.com/andrew-bustamante)]
That blending of culture and language is what makes humans so awesome at working together to achieve great things, including AI.
RNNs will give your nationality prediction model the same flexibility.
And if you want to change your name, this model can help you craft it so that it invokes the nationality that you want people (and machines) to perceive of you.

Take a look at some random names from this dataset to see if you can find any character patterns that are reused in multiple countries.

[id=load-the, reftext={chapter}.{counter:listing}]
.Load the
[source,python]
----
>>> repo = 'tangibleai/nlpia2'  # <1>
>>> filepath = 'src/nlpia2/data/surname-nationality.csv.gz'
>>> url = f"https://gitlab.com/{repo}/-/raw/main/{filepath}"
>>> df = pd.read_csv(url)  # <2>
>>> df[['surname', 'nationality']].sort_values('surname').head(9)
        surname nationality
16760   Aalbers       Dutch
16829   Aalders       Dutch
35706  Aalsburg       Dutch
35707     Aalst       Dutch
11070     Aalto     Finnish
11052  Aaltonen     Finnish
10853     Aarab    Moroccan
35708     Aarle       Dutch
11410    Aarnio     Finnish
----
<1> Tangible AI's augmented version of the original PyTorch surname dataset
<2> `read_csv` can read from URLs or file paths, but you may need to specify `compression='gzip'` for some URLs.

Take a quick look at the data before diving in.
It seems the Dutch like their family names (surnames) to be at the beginning of the roll call.
Several Dutch surnames begin with "Aa."
In the US there are a lot of business names that start with "AAA" for similar reasons.
And it seems that Moroccan, Dutch, and Finnish languages and cultures tend to encourage the use of the trigram "Aar" at the beginning of words.
So you can expect some confusion among these nationalities.
Don't expect to achieve 90% accuracy on a classifier.

You also want to count up the unique categories in your dataset so you know how many options your model will have to choose from.

[id=listing-unique-nationalities-in-the-dataset, reftext={chapter}.{counter:listing}]
.Unique nationalities in the dataset
[source,python]
----
>>> df['nationality'].nunique()
37
>>> sorted(df['nationality'].unique())
['Algerian', 'Arabic', 'Brazilian', 'Chilean', 'Chinese', 'Czech', 'Dutch',
 'English', 'Ethiopian', 'Finnish', 'French', 'German', 'Greek',
 'Honduran', 'Indian', 'Irish', 'Italian', 'Japanese', 'Korean',
 'Malaysian', 'Mexican', 'Moroccan', 'Nepalese', 'Nicaraguan', 'Nigerian',
 'Palestinian', 'Papua New Guinean', 'Peruvian', 'Polish', 'Portuguese',
 'Russian', 'Scottish', 'South African', 'Spanish', 'Ukrainian',
 'Venezuelan', 'Vietnamese']
----

In listing <<listing-unique-nationalities-in-the-dataset>> you can see the thirty-seven unique nationalities and language categories that were collected from multiple sources.
This is what makes this problem difficult.
It's like a multiple-choice question where there are 36 wrong answers and only one correct answer.
And these region or language categories often overlap.
For example, Algerian is considered to be an Arabic language, and Brazilian is a dialect of Portuguese.
There are several names that are shared across these nationality boundaries.
So the model can't get the correct answer for all of the names. 
It can only try to return the right answer as often as possible.

The diversity of nationalities and data sources helped us do name substitution to anonymize messages exchanged within our multilingual chatbots.
That way can share conversation design datasets in open-source projects like the chatbots discussed in Chapter 12 of this book.
RNN models are great for anonymization tasks, such as named entity recognition and generation of fictional names.
They can even be used to generate fictional, but realistic social security numbers, telephone numbers, and other PII (Personally Identifiable Information).
To build this dataset we augmented the PyTorch RNN tutorial dataset with names scraped from public APIs that contained data for underrepresented countries in Africa, South and Central America, and Oceania.

When we were building this dataset during our weekly mob programming on Manning's Twitch channel, Rochdi Khalid pointed out that his last name is Arabic.
And he lives in Casablanca, Morocco where Arabic is an official language, along side French and Berber.
This dataset is a mashup of data from a variety of sources.footnote:[There's more info and data scraping code in the nlpia2 package (https://proai.org/nlpia-ch08-surnames)] some of which create labels based on broad language labels such as "Arabic" and others are labeled with their specific nationality or dialect, such as Moroccan, Algerian, Palestinian, or Malaysian.

Dataset bias is one of the most difficult biases to compensate for unless you can find data for the groups you want to elevate.
Besides public APIs, you can also mine your internal data for names.
Our anonymization scripts strip out names from multilingual chatbot dialog.
We added those names to this dataset to ensure it is a representative sample of the kinds of users that interact with our chatbots.
You can use this dataset for your own projects where you need a truly global slice of names from a variety of cultures.

Diversity has its challenges.
As you might imagine some spellings of these transliterated names are reused across national borders and even across languages.
Translation and transliteration are two separate NLP problems that you can solve with RNNs.
The word "नमस्कार" can be _translated_ to the English word "hello".
But before your RNN would attempt to translate a Nepalese word it would _transliterate_ the Nepalese word "नमस्कार" into the word "namaskāra" which uses only the Latin character set.
Most multilingual deep learning pipelines utilize the Latin character set (Romance script alphabet) to represent words in all languages.

[NOTE]
====
Transliteration is when you translate the characters and spellings of words from one language's alphabet to another, making it possible to represent words using the Latin character set (Romance script alphabet) used in Europe and the Americas.
A simple example is the removal or adding of the acute accent from the French character "é", as in "resumé" (resume) and "école" (school).
Transliteration is a lot harder for non-Latin alphabets such as Nepalese.
====


Here's how you can calculate just how much overlap there is within each of your categories (nationalities).

[source,python]
----
>>> fraction_unique = {}
>>> for i, g in df.groupby('nationality'):
>>>     fraction_unique[i] = g['surname'].nunique() / len(g)
>>> pd.Series(fraction_unique).sort_values().head(7)
Portuguese           0.860092
Dutch                0.966115
Brazilian            0.988012
Ethiopian            0.993958
Mexican              0.995000
Nepalese             0.995108
Chilean              0.998000
----

In addition to the overlap _across_ nationalities, the PyTorch tutorial dataset contained many duplicated names within nationalities.
More than 94% of the Arabic names were duplicates, some of which are shown in listing <<listing-surname-oversampling>>.
Other nationalities and languages such as English, Korean, and Scottish appear to have been deduplicated.
Duplicates in your training set make your model fit more closely to common names than to less frequently occurring names.
Duplicating entries in your datasets is a brute-force way of "balancing" your dataset or enforcing statistics about the frequency of phrases to help it predict popular names and heavily populated countries more accurately.
This technique is sometimes referred to as "oversampling the minority class" because it boosts the frequency and accuracy of underrepresented classes in your dataset.

If you're curious about the original surname data check out the PyTorch "RNN Classification Tutorial".footnote:[PyTorch RNN Tutorial by Sean Robertson (https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)]
There were only 108 unique Arabic surnames among the 2000 Arabic examples in Arabic.txt.footnote:[The original PyTorch RNN Tutorial surname dataset with duplicates (https://download.pytorch.org/tutorial/data.zip)]

[id=listing-surname-oversampling, reftext={chapter}.{counter:listing}]
.Surname oversampling
----
>>> arabic = [x.strip() for x in open('.nlpia2-data/names/Arabic.txt')]
>>> arabic = pd.Series(sorted(arabic))
0       Abadi
1       Abadi
2       Abadi
        ...
1995    Zogby
1996    Zogby
1997    Zogby
Length: 2000, dtype: object
----

This means that even a relatively simple model (like the one shown in the PyTorch tutorial) should be able to correctly label popular names like Abadi and Zogby as Arabic.
And you can anticipate your model's confusion matrix statistics by counting up the number of nationalities associated with each name in the dataset.

You are going to use a deduplicated dataset that you loaded in listing <<listing-surname-oversampling>>.
We have counted up the duplicates to give you the statistics for these duplicates without burdening you with downloading a bloated dataset.
And you will use a balanced sampling of countries to encourage your model to treat all categories and names equally.
This means your model will predict rare names and rare countries just as accurately as popular names from popular countries.
This balanced dataset will encourage your RNN to generalize from the linguistic features it sees in names.
Your model will be more likely to recognize patterns of letters that are common among many different names, especially those that help the RNN distinguish between countries.
We've included information on how to obtain accurate usage frequency statistics for names in the `nlpia2` repository on GitLab.footnote:[iPython `history` log in the `nlpia2` repository on GitLab with examples for scraping surname data (https://proai.org/nlpia-ch08-surnames)]
You'll need to keep this in mind if you intend to use this model in the real world on a more random sample of names.


[id=listing-name-nationality-overlap, reftext={chapter}.{counter:listing}]
.Name nationality overlap
[source,python]
----
>>> df.groupby('surname')
>>> overlap = {}
... for i, g in df.groupby('surname'):
...     n = g['nationality'].nunique()
...     if n > 1:
...         overlap[i] = {'nunique': n, 'unique': list(g['nationality'].unique())}
>>> overlap.sort_values('nunique', ascending=False)
         nunique                                             unique
Michel         6  [Spanish, French, German, English, Polish, Dutch]
Abel           5        [Spanish, French, German, English, Russian]
Simon          5            [Irish, French, German, English, Dutch]
Martin         5       [French, German, English, Scottish, Russian]
Adam           5          [Irish, French, German, English, Russian]
...          ...                                                ...
Best           2                                  [German, English]
Katz           2                                  [German, Russian]
Karl           2                                    [German, Dutch]
Kappel         2                                    [German, Dutch]
Zambrano       2                                 [Spanish, Italian]
----

To help diversify this dataset and make it a little more representative of real-world statistics, we added some names from India and Africa.
And we compressed the dataset by counting the duplicates.
The resulting dataset of surnames combines data from the PyTorch RNN tutorial with anonymized data from multilingual chatbots.footnote:[PyTorch character-based RNN tutorial (https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)]
In fact, we use this name classification and generation model to anonymize names in our chatbot logs.
This allows us to _default to open_ with both NLP datasets as well as software.footnote:[Qary (https://docs.qary.ai) combines technology and data from all our multilingual chatbots (https://tangibleai.com/our-work)]

[IMPORTANT]
====
A great way to find out if a machine learning pipeline has a chance of solving your problem, pretend you are the machine.
Give yourself training on a few of the examples in your training set.
Then try to answer a few of the "questions" in your test set without looking at the correct label.
Your NLP pipeline should probably be able to solve your problem almost as well as you could.
And in some cases, you might find machines are much better than you because they can balance many patterns in their head more accurately than you can.
====

By computing the most popular nationality for each name in the dataset, it is possible to create a confusion matrix, using the most common nationality as the "true" label for a particular name.
This can reveal several quirks in the dataset that should influence what the model learns and how well it can perform this task.
There is no confusion at all for Arabic names because there are very few unique Arabic names and none of them are included in the other nationalities.
And a significant overlap exists between Spanish, Portuguese, Italian and English names.
Interestingly, for the 100 Scottish names in the dataset, None of them are most commonly labeled as Scottish.
Scottish names are more often labeled as English and Irish names.
This is because there are thousands of English and Irish names, but only 100 Scottish names in the original PyTorch tutorial dataset.

// Figure 8.8
[id=dataset-is-confused, reftext={chapter}.{counter:figure}]
.The dataset is confused even before training
image::../images/ch08/confusion-pytorch-tutorial.png[alt="Figure 8.8: Dataset confusion matrix in a heat map showing no confusion for Arabic and signicant confusion between Spanish, Portuguese, Italian and English names. Interestingly, for the 100 Scottish names in the dataset, None of them are most commonly labeled as Scottish, rather than English. This is because there are thousands of English and Irish names, but only 100 Scottish names in the dataset.", width=85%]

We've added 26 more nationalities to the original PyTorch dataset.
This creates much more ambiguity or overlap in the class labels.
Many names are common in multiple different regions of the world.
An RNN can deal with this ambiguity quite well, using the statistics of patterns in the character sequences to guide its classification decisions.


=== Build an RNN from scratch

Here's the heart of your `RNN` class in <<listing-heart-rnn>>
Like all Python classes, a PyTorch Module class has an `__init__()` method where you can set some configuration values that control how the rest of the class works.
For an RNN you can use the `__init__()` method to set the hyperparameters that control the number of neurons in the hidden vector as well as the size of the input and output vectors.

For an NLP application that relies on tokenizers, it's a good idea to include the tokenizer parameters within the init method to make it easier to instantiate again from data saved to disk.
Otherwise, you'll find that you end up with several different models saved on your disk.
And each model may use a different vocabulary or dictionary to tokenize and vectorize your data.
Keeping all those models and tokenizers connected is a challenge if they aren't stored together in one object.

The same goes for the vectorizers in your NLP pipeline.
Your pipeline must be consistent about where it stores each word for your vocabulary.
And you also have to be consistent about the ordering of your categories if your output is a class label.
You can easily get confused if you aren't exactly consistent with the ordering of your category labels each time you reuse your model.
The output will be garbled nonsense labels if the numerical values used by your model aren't consistently mapped to human-readable names for those categories.
If you store your vectorizers in your model class (see listing <<listing-heart-rnn>>), it will know exactly which category labels it wants to apply to your data.

[id=listing-heart-rnn, reftext={chapter}.{counter:listing}]
.Heart of an RNN
[source,python]
----
>>> class RNN(nn.Module):

>>> def __init__(self, n_hidden=128, categories, char2i):  # <1>
...     super().__init__()
...     self.categories = categories
...     self.n_categories = len(self.categories)  # <2>
...     print(f'RNN.categories: {self.categories}')
...     print(f'RNN.n_categories: {self.n_categories}')

...     self.char2i = dict(char2i)
...     self.vocab_size = len(self.char2i)

...     self.n_hidden = n_hidden

...     self.W_c2h = nn.Linear(self.vocab_size + self.n_hidden,
[CA] self.n_hidden)
...     self.W_c2y = nn.Linear(self.vocab_size + self.n_hidden,
[CA] self.n_categories)
...     self.softmax = nn.LogSoftmax(dim=1)

>>> def forward(self, x, hidden):  # <3>
...     combined = torch.cat((x, hidden), 1)
...     hidden = self.W_c2h(combined)
...     y = self.W_c2y(combined)
...     y = self.softmax(y)
...     return y, hidden  # <4>
----
<1> add hyperparameters to your `__init__` methods so you can compare architectures
<2> n_categories = n_outputs (one-hot)
<3> x = input = a single one-hot character vector
<4> RNNs return two things processing each token, the output prediction and the hidden encoding vector

Technically, your model doesn't need the full `char2i` vocabulary.
It just needs the size of the one-hot token vectors you plan to input into it during training and inference.
Likewise for the category labels.
Your model only really needs to know the number of categories.
The names of those categories are meaningless to the machine.
But by including the category labels within your model you can print them to the console whenever you want to debug the internals of your model.

=== Training an RNN, one token at a time

The dataset of 30000+ surnames for 37+ countries in the `nlpia2` project is manageable, even on a modest laptop.
So you should be able to train it using the in a reasonable amount of time.
If your laptop has 4 or more CPU cores and 6 GB or more RAM, the training will take about 30 minutes.
And if you limit yourself to only 10 countries, 10000 surnames, and get lucky (or smart) with your choice of learning rate, you can train a good model in two minutes.

Rather than using the built-in `torch.nn.RNN` layer you can build your first RNN from scratch using plain old `Linear` layers.
This will generalize your understanding so you can design your own RNNs for almost any application.

[id=training-on-a-single-sample, reftext={chapter}.{counter:listing}]
.Training on a single sample must loop through the characters
[source,python]
----
>>> def train_sample(model, category_tensor, char_seq_tens,
...                 criterion=nn.NLLLoss(), lr=.005):
    """ Train for one epoch (one example name nationality tensor pair) """
...    hidden = torch.zeros(1, model.n_hidden)  # <1>
...    model.zero_grad()  # <2>
...    for char_onehot_vector in char_seq_tens:
...        category_predictions, hidden = model(  # <3>
...            x=char_onehot_vector, hidden=hidden)  # <4>
...    loss = criterion(category_predictions, category_tensor)
...    loss.backward()

...    for p in model.parameters():
...        p.data.add_(p.grad.data, alpha=-lr)

...    return model, category_predictions, loss.item()
----
<1> Initialize the hidden layer to zeros before computing the output for the first token
<2> An RNN starts fresh at the first token of each example text
<3> A PyTorch `Module` (model) object is callable because it redirects `.__call__()` to `.forward()`
<4> Notice that the hidden state vector is both an input and an output of your model's `.forward()` method

The `nlpia2` package contains a script to orchestrate the training process and allow you to experiment with different hyperparameters.

[source,python]
----
>>> %run classify_name_nationality.py  # <1>
    surname  nationality
0   Tesfaye    Ethiopian
...
[36241 rows x 7 columns]
----
<1> the `%run` command within ipython console (REPL) is equivalent to the `python` command in the terminal

[TIP]
====
You want to use the `%run` magic command within the iPython console rather than running your machine learning scripts in the terminal using the `python` interpreter.
The ipython console is like a debugger.
It allows you to inspect all the global variables and functions after your script finishes running.
And if you cancel the run or if there is an error that halts the script, you will still be able to examine the global variables without having to start over from scratch.
====

Once you launch the `classify_name_nationality.py` script it will prompt you with several questions about the model's hyperparameters.
This is one of the best ways to develop an instinct about deep learning models.
And this is why we chose a relatively small dataset and small problem that can be successfully trained in a reasonable amount of time.
This allows you to try many different hyperparameter combinations and fine tune your intuitions about NLP while fine tuning your model.

Listing <<listing-interactive-prompts-hyperparameters>> shows some hyperparameter choices that will give you pretty good results.
But we've left you room to explore the "hyperspace" of options on your own.
Can you find a set of hyperparameters that can identify a broader set of nationalities with better accuracy?

[id=listing-interactive-prompts-hyperparameters, reftext={chapter}.{counter:listing}]
.Interactive prompts so you can play with hyperparameters
----
How many nationalities would you like to train on? [10]? 25
model: RNN(
    n_hidden=128,
    n_categories=25,
    categories=[Algerian..Nigerian],
    vocab_size=58,
    char2i['A']=6
)

How many samples would you like to train on? [10000]? 1500

What learning rate would you like to train with? [0.005]? 0.010

  2%|▊        | 30/1500 [00:06<05:16,  4.64it/s]000030 2% 00:06 3.0791
  [CA] Haddad => Arabic (1) ✓
000030 2% 00:06 3.1712 Cai => Moroccan (21) ✗ should be Nepalese (22=22)
----

Even this simplified RNN model with only 128 neurons and 1500 epochs takes several minutes to converge to a decent accuracy.
This example was trained on a laptop with a 4-core (8-thread) i7 Intel processor and 64 GB of RAM.
If your computing resources are more limited, you can train a simpler model on only 10 nationalities and it should converge much more quickly.
Keep in mind that many names were assigned to multiple nationalities.
And some of the nationality labels were more general language labels like "Arabic" that apply to many many countries.
So you don't expect to get very high accuracy, especially when you give the model many nationalities (categories) to choose from.

[id=training-output-log, reftext={chapter}.{counter:listing}]
.Training output log
[source,python]
----
001470 98% 06:31 1.7358 Maouche => Algerian (0) ✓
001470 98% 06:31 1.8221 Quevedo => Mexican (20) ✓
...
001470 98% 06:31 0.7960 Tong => Chinese (4) ✓
001470 98% 06:31 1.2560 Nassiri => Moroccan (21) ✓
  mean_train_loss: 2.1883266236980754
  mean_train_acc: 0.5706666666666667
  mean_val_acc: 0.2934249263984298
100%|███████████| 1500/1500 [06:39<00:00,  3.75it/s]
----

Looks like the RNN achieved 57% accuracy on the training set and 29% accuracy on the validation set.
This is an unfair measure of the model's usefulness.
Because the dataset was deduplicated before splitting into training and validation sets, there is only one row in the dataset for each name-nationality combination.
This means that a name that is associated with one nationality in the training set will likely be associated with a _different_ nationality in the validation set.
This is why the PyTorch tutorial doesn't create test or validation datasets in the official docs.
They don't want to confuse you.

Now that you understand the ambiguity in the dataset you can see how hard the problem is and that this RNN does a really good job of generalizing from the patterns it found in the character sequences.
It generalizes to the validation set much better than random chance.
Random guesses would have achieved 4% accuracy on 25 categories (`1/25 == .04`) even if there was no ambiguity in the nationality associated with each name.

Let's try it on some common surnames that are used in many countries.
An engineer named Rochdi Khalid helped create one of the diagrams in this chapter.
He lives and works in Casablanca, Morrocco.
Even though Morocco isn't the top prediction for "Khalid", Morocco is in second place!

[source,python]
----
>>> model.predict_category("Khalid")
'Algerian'
>>> predictions = topk_predictions(model, 'Khalid', topk=4)
>>> predictions
        text  log_loss nationality
rank
0     Khalid     -1.17    Algerian
1     Khalid     -1.35    Moroccan
2     Khalid     -1.80   Malaysian
3     Khalid     -2.40      Arabic
----

The top 3 predictions are all for Arabic-speaking countries.
I don't think there are expert linguists that could do this prediction as fast or as accurately as this RNN model did.

Now it's time to dig deeper and examine some more predictions to see if you can figure out how only 128 neurons can predict someone's nationality so well.

=== Understanding the results

To use a model like this in the real world you will need to be able to explain how it works to your boss.
Germany, Finland, and the Netherlands (and soon in all of the EU) are regulating how AI can be used, to force businesses to explain their AI algorithms so users can protect themselves.footnote:[AI algorithm registry launched in Amsterdam in 2020 (https://algoritmeregister.amsterdam.nl/en/ai-register/)]
Businesses won't be able to hide their exploitative business practices within algorithms for long.footnote:["EU Artificial Intelligence Act (https://artificialintelligenceact.eu/) and the accepted OECD AI Council recommendations (https://legalinstruments.oecd.org/en/instruments/OECD-LEGAL-0449)"]
You can imagine how governments and businesses might use a nationality prediction algorithm for evil.
Once you understand how this RNN works you'll be able to use that knowledge to trick algorithms into doing what's right, elevating rather than discriminating against historically disadvantaged groups and cultures.

Perhaps the most important piece of an AI algorithm is the metric you used to train it.
You used `NLLLoss` for the PyTorch optimization training loop in listing <<training-on-a-single-sample>>.
The `NLL` part stands for "Negative Log Likelihood".
You should already know how to invert the `log()` part of that expression.
Try to guess what the mathematical function and Python code is to invert the `log()` function before checking out the code snippet below.
As with most ML algorithms, `log` means natural log, sometimes written as _ln_ or _log to the base e_.

[source,python]
----
>>> predictions = topk_predictions(model, 'Khalid', topk=4)
>>> predictions['likelihood'] = np.exp(predictions['log_loss'])
>>> predictions
        text  log_loss nationality  likelihood
rank
0     Khalid     -1.17    Algerian        0.31
1     Khalid     -1.35    Moroccan        0.26
2     Khalid     -1.80   Malaysian        0.17
3     Khalid     -2.40      Arabic        0.09
----

This means that the model is only 31% confident that Rochdi is Algerian.
These probabilities (likelihoods) can be used to explain how confident your model is to your boss or teammates or even your users.

If you're a fan of "debug by print" you can modify your model to print out anything you're interested in about the math the model uses to make predictions.
PyTorch models can be instrumented with print statements whenever you want to record some of the internal goings on.
If you do decide to use this approach, you only need to `.detach()` the tensors from the GPU or CPU where they are located to bring them back into your working RAM for recording in your model class.

A nice feature of RNNs is that the predictions are built up step by step as your `forward()` method is run on each successive token.
This means you may not even need to add print statements or other instrumentation to your model class.
Instead, you can just make predictions of the hidden and output tensors for parts of the input text.

You may want to add some `predict_*` convenience functions for your model class to make it easier to explore and explain the model's predictions.
If you remember the `LogisticRegression` model in Scikit-Learn, it has a `predict_proba` method to predict probabilities in addition to the `predict` method used to predict the category.
An RNN has an additional hidden state vector you may sometimes want to examine for clues as to how the network is making predictions.
So you can create a `predict_hidden` method to output the 128-D hidden tensor and a `predict_proba` to show you the predicted probabilities for each of the target categories (nationalities).

[source,python]
----
>>> def predict_hidden(self, text="Khalid"):
...    text_tensor = self.encode_one_hot_seq(text)
...    with torch.no_grad():  # <1>
...        hidden = self.hidden_init
...        for i in range(text_tensor.shape[0]):  # <2>
...            y, hidden = self(text_tensor[i], hidden)  # <3>
...    return hidden
----
<1> for making predictions outside a backpropagation and training loop, you can disable gradient calculation
<2> each row is the tensor representing a character-level token (letter) in the text
<3> all `nn.Module`-derived objects are callable and `self()` is synonymous with `self.forward()`

This `predict_hidden` convenience method converts the text (surname) into a tensor before iterating through the one-hot tensors to run the forward method (or just the model's `self`).

[source,python]
----
>>> def predict_proba(self, text="Khalid"):
...    text_tensor = self.encode_one_hot_seq(text)
...    with torch.no_grad():
...        hidden = self.hidden_init
...        for i in range(text_tensor.shape[0]):
...            y, hidden = self(text_tensor[i], hidden)
...    return y  # <1>
----
<1> `predict_proba` and `predict_hidden` methods are the same except for the tensor they return

This `predict_hidden` method gives you access to the most interesting part of the model where the "logic" of the predictions is taking place.
The hidden layer evolves as it learns more and more about the nationality of a name with each character.

Finally, you can use a `predict_category` convenience method to run the model's forward pass predictions to predict the nationality of a name.

[source,python]
----
>>> def predict_category(self, text):
...    tensor = self.encode_one_hot_seq(text)
...    y = self.predict_proba(tensor)  # <1>
...    pred_i = y.topk(1)[1][0].item()  # <2>
...    return self.categories[pred_i]
----
<1> The `predict_proba` method computes the `softmax()` of the output tensor to approximate the probability of each category
<2> PyTorch tensors have a `topk` method that finds the top-ranked elements of any tensor

The key thing to recognize is that for all of these methods, you don't necessarily have to input the entire string for the surname.
It is perfectly fine to reevaluate the first part of the surname text over and over again, as long as you reset the hidden layer each time.

If you input an expanding window of text you can see how the predictions and hidden layer evolve in their understanding of the surname.
During mob programming sessions with other readers of this book, we noticed that nearly all names started out with predictions of "Chinese" as the nationality for a name until after the 3rd or 4th character.
This is perhaps because so many Chinese surnames contain 4 (or fewer) characters.footnote:[Thank you Tiffany Kho for pointing this out.]

Now that you have helper functions you can use them to record the hidden and category predictions as the RNN is run on each letter in a name.

[source,python]
----
>>> text = 'Khalid'
>>> pred_categories = []
>>> pred_hiddens = []

>>> for i in range(1, len(text) + 1):
...    pred_hiddens.append(model.predict_hidden(text[:i]))  # <1>
...    pred_categories.append(model.predict_category(text[:i]))

>>> pd.Series(pred_categories, input_texts)
# K          English
# Kh         Chinese
# Kha        Chinese
# Khal       Chinese
# Khali     Algerian
# Khalid      Arabic
----
<1>  run the RNN on the text 'K', then 'Kh', 'Kha', 'Khal', ...

And you can create a 128 x 6 matrix of all the hidden layer values in a 6-letter name.
The list of PyTorch tensors can be converted to a list of lists and then a DataFrame to make it easier to manipulate and explore.

[source,python]
----
>>> hiddens = [h[0].tolist() for h in hiddens]
>>> df_hidden = pd.DataFrame(hidden_lists, index=list(text))
>>> df_hidden = df_hidden.T.round(2)  # <1>

>>> df_hidden
    0     1     2     3     4     5   ...  122   123   124   125   126   127
K  0.10 -0.06 -0.06  0.21  0.07  0.04 ... 0.16  0.12  0.03  0.06 -0.11  0.11
h -0.03  0.03  0.02  0.38  0.29  0.27 ...-0.08  0.04  0.12  0.30 -0.11  0.37
a -0.06  0.14  0.15  0.60  0.02  0.16 ...-0.37  0.22  0.30  0.33  0.26  0.63
l -0.04  0.18  0.14  0.24 -0.18  0.02 ... 0.27 -0.04  0.08 -0.02  0.46  0.00
i -0.11  0.12 -0.00  0.23  0.03 -0.19 ...-0.04  0.29 -0.17  0.08  0.14  0.24
d  0.01  0.01 -0.28 -0.32  0.10 -0.18 ... 0.09  0.14 -0.47 -0.02  0.26 -0.11
[6 rows x 128 columns]
----
<1> use `pd.options.display.float_format = '{:.2f}'` to preserve internal precision

This wall of numbers contains everything your RNN "thinks" about the name as it is reading through it.

[TIP]
====
There are some Pandas display options that will help you get a feel for the numbers in a large DataFrame without TMI ("too much information").
Here are some of the settings that helped improve the printouts of tables in this book

To display only 2 decimal places of precision for floating point values try: `pd.options.display.float_format = '{:.2f}'`.

To display a maximum of 12 columns and 7 rows of data from your DataFrame: `pd.options.display.max_columns = 12` and `pd.options.display.max_rows = 7`

These only affect the displayed representation of your data, not the internal values used when you do addition or multiplication.
====

As you've probably done with other large tables of numbers, it's often helpful to find patterns by correlating it with other numbers that are interesting to you.
For example, you may want to find out if any of the hidden weights are keeping track of the RNN's position within the text - how many characters it is from the beginning or end of the text.

[source,python]
----
>>> position = pd.Series(range(len(text)), index=df_hidden.index)
>>> pd.DataFrame(position).T
#    K  h  a  l  i  d
# 0  0  1  2  3  4  5

>>> df_hidden_raw.corrwith(position).sort_values()
# 11   -0.99
# 84   -0.98
# 21   -0.97
#       ...
# 6     0.94
# 70    0.96
# 18    0.96
----

Interestingly our hidden layer has room in its hidden memory to record the position in many different places.
And the strongest correlation seems to be negative.
These are likely helping the model to estimate the likelihood of the current character being the last character in the name.
When we looked at a wide range of example names, the predictions only seemed to converge on the correct answer at the very last character or two.
Andrej Karpathy experimented with several more ways to glean insight from the weights of your RNN model in his blog post "The unreasonable effectiveness of RNNs" in the early days of discovering RNNs. footnote:[footnote:["The unreasonable effectiveness of RNNs" by Andrej Karpathy (https://karpathy.github.io/2015/05/21/rnn-effectiveness)]]

=== Multiclass classifiers vs multi-label taggers
// SUM you can deal with classifier ambiguity by allowing multi-hot vectors for your output and creating a tagging model rather than a classifier.

How can you deal with the ambiguity of multiple different correct nationalities for surnames?
The answer is multi-label classification or tagging rather than the familiar multiclass classification.
Because the terms "multiclass classification" and "multi-label classification" sound so similar and are easily confused, you probably want to use the term "multi-label tagging" or just "tagging" instead of "multi-label classification."
And if you're looking for the `sklearn` models suited to this kind of problem you want to search for "multi-output classification."

Multi-label taggers are made for ambiguity.
In NLP intent classification and tagging is full of intent labels that have fuzzy overlapping boundaries.
We aren't talking about a graffiti war between Banksy and Bario Logan street artists when we say "taggers".
We're talking about a kind of machine learning model that can assign multiple discrete labels to an object in your dataset.

A multiclass classifier has multiple different categorical labels that are matched to objects, one label for each object.
A categorical variable takes on only one of several mutually exclusive classes or categories.
For example, if you wanted to predict both the language and the gender associated with first names (given names), then that would require a multiclass classifier.
But if you want to label a name with all the relevant nationalities and genders that are appropriate, then you would need a tagging model.

This may seem like splitting hairs to you, but it's much more than just semantics.
It's the semantics (meaning) of the text that you are processing that is getting lost in the noise of bad advice on the Internet.
David Fischer at ReadTheDocs.com (RTD) and the organizer for San Diego Python ran into these misinformed blog posts when he started learning about NLP to build a Python package classifier.
Ultimately he ended up building a tagger, which gave RTD advertisers more effective placements for their ads and gave developers reading documentation more relevant advertisements.

[TIP]
====
To turn any multi-class classifier into a multi-label tagger you must change your activation function from `softmax` to an element-wise `sigmoid` function.
A softmax creates a probability distribution across all the mutually exclusive categorical labels.
A sigmoid function allows every value to take on any value between zero and one, such that each dimension in your multi-label tagging output represents the independent binary probability of that particular label applying to that instance.
====

== Backpropagation through time
// SUM: The backpropagation algorithm rolls back through previous tokens it has already "read", and for each word it adjusts the weights for word embeddings and hidden layer encodings to try to incrementally improve (optimize) predictions of your target variable. For a typical language model you areBackpropagation predicting the next token, so each back propagation step is like a logistic regression on the previous layer.

Backpropagation for RNNs is a lot more work than for CNNs.
The reason training an RNN is so computationally expensive is that it must perform the forward and backward calculations many times for each text example - once for each token in the text.
And then it has to do all that again for the next layer in the RNN.
And this sequence of operations is really important because the computation for one token depends on the previous one.
You are recycling the output and hidden state tensors back into the calculation for the next token.
For CNNs and fully connected neural networks, the forward and backward propagation calculations could run all at once on the entire layer.
The calculations for each token in your text did not affect the calculation for the neighboring tokens in the same text.
RNNs do forward and backward propagation in time, from one token in the sequence to the next.

But you can see in the unrolled RNN in Figure 8.7 that your training must propagate the error back through all the weight matrix multiplications.
Even though the weight matrices are the same, or `tied` for all the tokens in your data, they must work on each and every token in each of your texts.
So your training loop will need to loop through all the tokens backward to ensure that the error at each step of the way is used to adjust the weights.

The initial error value is the distance between the final output vector and the "true" vector for the label appropriate for that sample of text.
Once you have that difference between the truth and the predicted vector, you can work your way back through time (tokens) to propagate that error to the previous time step (previous token).
The PyTorch package will use something very similar to the chain rule that you used in algebra or calculus class to make this happen.
PyTorch calculates the gradients it needs during forward propagation and then multiplies those gradients by the error for each token to decide how much to adjust the weights and improve the predictions.

And once you've adjusted the weights for all the tokens in one layer you do the same thing again for all the tokens on the next layer.
Working your way from the output of the network all the way back to the inputs (tokens) you will eventually have to "touch" or adjust all of the weights many times for each text example.
Unlike backpropagation through a linear layer or CNN layer, the backpropagation on an RNN must happen serially, one token at a time.

An RNN is just a normal feedforward neural network "rolled up" so that the Linear weights are multiplied again and again for each token in your text.
If you unroll it you can see all the weight matrices that need to be adjusted.
And like the CNN, many of the weight matrices are shared across all of the tokens in the unrolled view of the neural network computational graph.
An RNN is one long kernel that reuses "all" of the weights for each text document.
The weights of an RNN are one long, giant kernel.
At each time step, it is the _same_ neural network, just processing a different input and output at that location in the text.


[TIP]
====
In all of these examples, you have been passing in a single training example, the _forward pass_, and then backpropagating the error.
As with any neural network, this forward pass through your network can happen after each training sample, or you can do it in batches.
And it turns out that batching has benefits other than speed.
But for now, think of these processes in terms of just single data samples, single sentences, or documents.
====

In chapter 7 you learned how to process a string all at once with a CNN.
CNNs can recognize patterns of meaning in text using kernels (matrices of weights) that represent those patterns.
CNNs and the techniques of previous chapters are great for most NLU tasks such as text classification, intent recognition, and creating embedding vectors to represent the meaning of text in a vector.
CNNs accomplish this with overlapping windows of weights that can detect almost any pattern of meaning in text.

// Figure 8.10
[id=convolution-with-embeddings, reftext={chapter}.{counter:figure}]
.1D convolution with embeddings
image::../images/ch07/cnn-stride-text-words-are-sacred_transparent_drawio.png[alt="Figure 8.9: The words 'Words are sacred' with rows for t=0, t=1, and every step of the 2-word window sliding across the text and into the <PAD> tokens at the end of the string.", width=80%, link="../images/ch07/cnn-stride-text-words-are-sacred_transparent_drawio.png"]

In Chapter 7 you imagined striding the kernel window over your text, one step at a time.
But in reality, the machine is doing all the multiplications in parallel.
The order of operations doesn't matter.
For example, the convolution algorithm can do the multiplication on the pair of words and then hop around to all the other possible locations for the window.
It just needs to compute a bunch of dot products and then sum them all up or pool them together at the end.
Addition is commutative (order doesn't matter).
And none of the convolution dot products depend on any of the others.
In fact, on a GPU these matrix multiplications (dot products) are all happening _in parallel_ at approximately the _same_ time.

But an RNN is different.
With an RNN you're recycling the output of one token back into the dot product you're doing on the next token.
So even though we talked about RNNs working on any length text, to speed things up, most RNN pipelines truncate and pad the text to a fixed length.
This unrolls the RNN matrix multiplications so that
And you need two matrix multiplications for an RNN compared to one multiplication for a CNN.
You need one matrix of weights for the hidden vector and another for the output vector.

If you've done any signal processing or financial modeling you may have used an RNN without knowing it.
The recurrence part of a CNN is called 'auto-regression" in the world of signal processing and quantitative financial analysis.
An  _auto-regressive moving average_ (ARMA) model is an RNN in disguise.footnote:[ARMA model explanation (https://en.wikipedia.org/wiki/Autoregressive_model)]

In this chapter, you are learning about a new way to structure the input data.
Just as in a CNN, each token is associated with a time (`t`) or position within the text.
The variable `t` is just another name for the index variable in your sequence of tokens.

You will even see places where you use the integer value of `t` to retrieve a particular token in the sequence of tokens with an expression such as `token = tokens[t]`.
So when you see `t-1` or `tokens[t-1]` you know that it is referring to the preceding time step or token.
And `t+1` and `tokens[t+1]` refers to the next time step or token.
In past chapters, you may have seen that we sometimes used `i` for this index value.

Now you will use multiple different indexes to keep track of what has been passed into the network and is being output by the network:

* `t` or `token_num`: time step or token position for the current tensor being input to the network
* `k` or `sample_num`: sample number within a batch for the text example being trained on
* `b` or `batch_num`: batch number of the set of samples being trained
* `epoch_num`: number of epochs that have passed since the start of training

// Figure 8.11
[id=data-fed, reftext={chapter}.{counter:figure}]
.Data fed into a recurrent network
image::../images/ch08/rnn_input.png[alt="", width=80%, link="../images/ch08/rnn_input.png"]

This 2-D tensor representation of a document is similar to the "player piano" representation of text in chapter 2.
Only this time you are creating a dense representation of each token using word embeddings.

For an RNN you no longer need to process each text sample all at once.
Instead, you process text one token at a time.

In your recurrent neural net, you pass in the word vector for the first token and get the network's output. 
You then pass in the second token, but you also pass in the output from the first token! 
And then pass in the third token along with the output from the second token. 
And so on. 
The network has a concept of before and after, cause and effect - some vague notion of time (see Figure 8.8).

=== Initializing the hidden layer in an RNN
// SUM: You can initialize the hidden layer with zeros or small random values, or even create some arbitrary structure in your initial hidden weights to guide the training so that the end result is a more explainable hidden layer
// SUM: Keras stateful=True equivalents in PyTorch (https://datascience.stackexchange.com/a/66035)

There's a chicken-and-egg problem with the hidden layer when you restart the training of an RNN on each new document.
For each text string you want to process, there is no "previous" token or previous hidden state vector to recycle back into the network.
You don't have anything to prime the pump with and start the recycling (recurrence) loop.
Your model's `forward()` method needs a vector to concatenate with the input vector so that it will be the right size for multiplying by `W_c2h` and `W_c2o`.

The most obvious approach is to set the initial hidden state to all zeroes and allow the biases and weights to quickly ramp up to the best values during the training on each sample.
This can be great for any of the neurons that are keeping track of time, the position in the token sequence that is currently (recurrently) being processed.
But there are also neurons trying to predict how far from the end of the sequence you are.
And your network has a defined polarity with 0 for off and 1 for on.
So you may want your network to start with a mix of zeros and ones for your hidden state vector.
Better yet you can use some gradient or pattern of values between zero and 1 which is your particular "secret sauce", based on your experience with similar problems.

Getting creative and being consistent with your initialization of deep learning networks has the added benefit of creating more "explainable" AI.
You will often create a predictable structure in your weights.
And by doing it the same way each time you will know where to look within all the layers.
For example, you will know which positions in the hidden state vector are keeping track of position (time) within the text.

To get the full benefit of this consistency in your initialization values you will also need to be consistent with the ordering of your samples used during training.
You can sort your texts by their lengths, as you did with CNNs in Chapter 7.
But many texts will have the same length, so you will also need a sort algorithm that consistently orders the samples with the same length.
Alphabetizing is an obvious option, but this will tend to trap your model in local minima as it's trying to find the best possible predictions for your data.
It would get really good at the "A" names but do poorly on "Z" names.
So don't pursue this advanced seeding approach until you've fully mastered the random sampling and shuffling that has proven so effective.

As long as you are consistent throughout the training process, your network will learn the biases and weights that your network needs to layer on top of these initial values.
And that can create a recognizable structure in your neural network weights.

[TIP]
====
In some cases, it can help to seed your neural networks with an initial hidden state other than all zeros.
Johnathon Frankle and Michael Carbin found that being intentional about reuse of good initialization values can be key to helping a network find the _global minimum_ loss achievable for a particular dataset "Lottery Ticket Hypothesis" paper, footnote:[https://arxiv.org/pdf/1803.03635.pdf]
Their approach is to initialize all weights and biases using a random seed that can be reused in subsequent training.
====

Now your network is remembering something! Well, sort of. A few things remain for you to figure out. For one, how does backpropagation even work in a structure like this?

Another approach that is popular in the Keras community is to retain the hidden layer from a previous batch of documents.
This "pre-trained" hidden layer embedding gives your language model information about the context of the new document - the text that came before it.
However, this only makes sense if you've maintained the order of your documents within the batches and across the batches that you are training.
In most cases, you shuffle and reshuffle your training examples with each epoch.
You do this when you want your model to work equally well at making predictions "cold" without any priming by reading similar documents or nearby passages of text.

So unless you are trying to squeeze out every last bit of accuracy you can for a really difficult problem you should probably just reset it to zeros every time to start feeding a new document into your model.
And if you do use this _stateful_ approach to training an RNN, make sure you will be able to warm up your model on context documents for each prediction it needs to make in the real world (or on your test set).
And make sure you prepare your documents in a consistent order and can reproduce this document ordering for a new set of documents that you need to make prediction on with your model.


== Remembering with recurrent networks
// SUM: An RNN remembers previous words in the text they are processing and can keep adding more and more patterns to its memory as it processes a theoretically limitless amount of text.

An RNN remembers previous words in the text they are processing and can keep adding more and more patterns to its memory as it processes a theoretically limitless amount of text.
This can help it understand patterns that span the entire text and recognize the difference between two texts that have dramatically different meanings depending on where words occur.

_I apologize for the lengthy letter. I didn't have time to write a shorter one._

_I apologize for the short letter. I didn't have time to write a lengthy one._

Swapping the words "short" and "lengthy", flips the meaning of this Mark Twain quote.
Knowing Mark Twain's dry sense of humor and passion for writing, can you tell which quote is his?
It's the one where he apologizes for the lengthy letter.
He's making light of the fact that editing and writing concisely is hard work.
It's something that smart humans can still do better than even the smartest AI.

The CNNs you learned about in Chapter 7 would have a hard time making the connection between these two sentences about lengthy and short letters, whereas RNNs make this connection easily.
This is because CNNs have a limited window of text that they can recognize patterns within.
To make sense of an entire paragraph, you would have to build up layers of CNNs with overlapping kernels or windows of text that they understand.
RNNs do this naturally.
RNNs remember something about every token in the document they've read.
They remember everything you've input into them until you tell them you are done with that document.
This makes them better at summarizing lengthy Mark Twain letters and makes them better at understanding his long sophisticated jokes.

Mark Twain was right.
Communicating things concisely requires skill, intelligence and attention to detail.
In the paper "Attention is All You Need" Ashish Vaswani revealed how transformers can add an attention matrix that allows RNNs to accurately understand much longer documents.footnote:["Attention Is All You Need" by Ashish Vaswani et al (https://arxiv.org/abs/1706.03762)]
In chapter 9 you'll see this attention mechanism at work, as well as the other tricks that make the transformer approach to RNNs the most successful and versatile deep learning architecture so far.

Summarization of lengthy text is still an unsolved problem in NLP.
Even the most advanced RNNs and transformers make elementary mistakes.
In fact, The Hutter Prize for Artificial Intelligence will give you 5000 Euros for each one percent improvement in the compression (lossless summarization) of Wikipedia.footnote:[https://en.wikipedia.org/wiki/Hutter_Prize]
The Hutter Prize focuses on the compression of the symbols within Wikipedia.
You're going to learn how to compress the meaning of text.
That's even harder to do well.
And it's hard to measure how well you've done it.

You will have to develop generally intelligent machines that understand common sense logic and can organize and manipulate memories and symbolic representations of those memories.
That may seem hopeless, but it's not.
The RNNs you've built so far can remember everything in one big hidden representation of their understanding.
Can you think of a way to give some structure to that memory, so that your machine can organize its thoughts about text a bit better?
What if you gave your machine a separate way to maintain both short-term memories and long-term memories?
This would give it a working memory that it could then store in long-term memory whenever it ran across a concept that was important to remember.

=== Word-level Language Models

All the most impressive language models that you've read about use words as their tokens, rather than individual characters.
So, before you jump into GRUs and LSTMs you will need to rearrange your training data to contain sequences of word IDs rather than character (letter) IDs.
And you're going to have to deal with much longer documents than just surnames, so you will want to `batchify` your dataset to speed it up.

Take a look at the Wikitext-2 dataset and think about how you will preprocess it to create a sequence of token IDs (integers).

[source,python]
----
>>> lines = open('data/wikitext-2/train.txt').readlines()
>>> for line in lines[:4]:
...     print(line.rstrip()[:70])

 = Valkyria Chronicles III =
 =======

 Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 ,
 [CA] lit
----

Oh wow, this is going to be an interesting dataset.
Even the English language version of Wikipedia contains a lot of other natural languages in it, such as Japanese in this first article.
If you use your tokenization and vocabulary-building skills from previous chapters you should be able to create a Corpus class like the one used in the RNN examples coming up.footnote:[The full source code is in the nlpia2 package (https://gitlab.com/tangibleai/nlpia2/-/blob/main/src/nlpia2/ch08/rnn_word/data.py)]

[source,python]
----
>>> from nlpia2.ch08.data import Corpus

>>> corpus = Corpus('data/wikitext-2')
>>> corpus.train
tensor([ 4,  0,  1,  ..., 15,  4,  4])
----

And you always want to make sure that your vocabulary has all the info you need to generate the correct words from the sequence of word IDs:

[source,python]
----
>>> vocab = corpus.dictionary
>>> [vocab.idx2word[i] for i in corpus.train[:7]]
['<eos>', '=', 'Valkyria', 'Chronicles', 'III', '=', '<eos>']
----

Now, during training your RNN will have to read each token one at a time.
That can be pretty slow.
What if you could train it on multiple passages of text simultaneously?
You can do this by splitting your text into batches or _batchifying_ your data.
These batches can each become columns or rows in a matrix that PyTorch can more efficiently perform math on within a _GPU_ (Graphics Processing Unit).

In the `nlpia2.ch08.data` module you'll find some functions for batchifying long texts.

[source,python]
----
>>> def batchify_slow(x, batch_size=8, num_batches=5):
...    batches = []
...    for i in range(int(len(x)/batch_size)):
...        if i > num_batches:
...            break
...        batches.append(x[i*batch_size:i*batch_size + batch_size])
...    return batches
>>> batches = batchify_slow(corpus.train)
----

[source,python]
----
>>> batches
[tensor([4, 0, 1, 2, 3, 0, 4, 4]),
 tensor([ 5,  6,  1,  7,  8,  9,  2, 10]),
 tensor([11,  8, 12, 13, 14, 15,  1, 16]),
 tensor([17, 18,  7, 19, 13, 20, 21, 22]),
 tensor([23,  1,  2,  3, 24, 25, 13, 26]),
 tensor([27, 28, 29, 30, 31, 32, 33, 34])]
----

One last step and your data is ready for training.
You need to `stack` the tensors within this list so that you have one large tensor to iterate through during your training.

[source,python]
----
>>> torch.stack(batches)
tensor([[4, 0, 1, 2, 3, 0, 4, 4],
        [ 5,  6,  1,  7,  8,  9,  2, 10],
        [11,  8, 12, 13, 14, 15,  1, 16],
        ...
----

=== Gated Recurrent Units (GRUs)
// SUM: GRUs use logic gates as additional activation functions and weigth matrics which give a single GRU cell or unit the ability to recognize more complex patterns than a single neuron RNN.

For short text, ordinary RNNs with a single activation function for each neuron works well.
All your neurons need to do is recycle and reuse the hidden vector representation of what they have read so far in the text.
But ordinary RNNs have a short attention span that limits their ability to understand longer texts.
The influence of the first token in a string fades over time as your machine reads more and more of the text.
That's the problem that GRU (Gated Recurrent Unit) and LSTM (Long and Short Term Memory) neural networks aim to fix.

How do you think you could counteract fading memory of early tokens in a text string?
How could you stop the fading, but just for a few important tokens at the beginning of a long text string?
What about adding an `if` statement to record or emphasize particular words in the text.
That's what GRUs do.
GRUs add `if` statements, called _logic gates_ (or just "gates"), to RNN neurons.

The magic of machine learning and backpropagation will take care of the if statement conditions for you, so you don't have to adjust logic gate thresholds manually.
Gates in an RNN learn the best thresholds by adjusting biases and weights that affect the level of a signal that triggers a zero or 1 output (or something in between).
And the magic of back-propagation in time will train the LSTM gates to let important signals (aspects of token meaning) pass through and get recorded in the hidden vector and cell state vector.

But wait, you probably thought we already had if statements in our network.
After all, each neuron has a nonlinear activation function that acts to squash some outputs to zero and push others up close to 1.
So the key isn't that LSTMs add gates (activation functions) to your network.
The key is that the new gates are _inside_ the neuron and connected in a way that creates a structure to your neural network that wouldn't naturally just emerge from a normal linear, fully-connected layer of neurons.
And that structure was intentionally designed with a purpose, reflecting what researchers thing would help RNN neurons deal with this long-term memory problem.


In addition to the original RNN output gate, GRUs add two new logic gates or activation functions within your recurrent unit.

1. Reset gate: What parts of the hidden layer should be blocked because they are no longer relevant to the current output.
2. Update gate: What parts of the hidden layer should matter to the current output (now, at time `t`).

You already had an activation function on the output of your RNN layer.
This output logic gate is called the "new" logic gate in a GRU.


[source,python]
----
>>> r = sigmoid(W_i2r.mm(x) + b_i2r +    W_h2r.mm(h) + b_h2r)  # <1>
>>> z = sigmoid(W_i2z.mm(x) + b_i2z +    W_h2z.mm(h) + b_h2z)  # <2>

>>> n =    tanh(W_i2n.mm(x) + b_i2n + r∗(W_h2n.mm(h) + b_h2n))  # <3>
----
<1> reset gate
<2> update gate
<3> new gate

So when you are thinking about how many units to add to your neural network to solve a particular problem, each LSTM or GRU unit gives your network a capacity similar to 2 "normal" RNN neurons or hidden vector dimensions.
A unit is just a more complicated, higher-capacity neuron, and you can see this if you count up the number of "learned parameters" in your LSTM model and compare it to those of an equivalent RNN.

[NOTE]
====
You're probably wondering why we started using the word "unit" rather than "neuron" for the elements of this neural net.
Researchers use the terms "unit" or "cell" to describe the basic building blocks of an LSTM or GRU neural network because they are a bit more complicated than a neuron.
Each unit or cell in an LSTM or GRU contains internal gates and logic.
This gives your GRU or LSTM units more capacity for learning and understanding text, so you will probably need fewer of them to achieve the same performance as an ordinary RNN.
====

The _reset_, _update_, and _new_ logic gates are implemented with the fully-connected linear matrix multiplications and nonlinear activation functions you are familiar with from Chapter 5.
What's new is that they are implemented on each token recurrently and they are implemented on the hidden and input vectors in parallel.
Figure 8.12 shows how the input vector and hidden vector for a single token flow through the logic gates and output the prediction and hidden state tensors.

// Figure 8.12
[id=GRUs-add-capacity, reftext={chapter}.{counter:figure}]
.GRUs add capacity with logic gates
image::../images/ch08/gru_drawio.png[alt="", width=80%, link="../images/ch08/gru_drawio.png"]

If you have gotten good at reading data flow diagrams like Figure 8.12 you may be able to see that the GRU _update_ and _relevance_ logic gates are implementing the following two functions: footnote:[PyTorch docs for GRU layers (https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU)]

[source,python]
----
r = sigmoid(W_i2r.dot(x) + b_i2r + W_h2r.dot(h) + b_h2r)  # <1>
z = sigmoid(W_i2z.dot(x) + b_i2z + W_h2z.dot(h) + b_h2z)  # <2>
----
<1> reset
<2> update

Looking at these two lines of code you can see that inputs to the formula are exactly the same.
Both the hidden and input tensors are multiplied by weight matrices in both formulas.
And if you remember your linear algebra and matrix multiplication operations, you might be able to simplify the
And you may notice in the block diagram (figure 8.12) that the input and hidden tensors are concatenated together before the matrix multiplication by W_reset, the reset weight matrix.

Once you add GRUs to your mix of RNN model architectures, you'll find that they are much more efficient.
A GRU will achieve better accuracy with fewer learned parameters and less training time and less data.
The gates in a GRU give structure to the neural network that creates more efficient mechanisms for remembering important bits of meaning in the text.
To measure efficiency you'll need some code to count up the learned (trainable) parameters in your models.
This is the number of weight values that your model must adjust to optimize the predictions.
The requires_grad attribute is an easy way to check whether a particular layer contains learnable parameters or not.footnote:[PyTorch docs discussion about counting up learned parameters (https://discuss.pytorch.org/t/how-do-i-check-the-number-of-parameters-of-a-model/4325/9
)]

[source,python]
----
>>> def count_parameters(model, learned=True):
...     return sum(
...         p.numel() for p in model.parameters()  # <1>
...         if not learned or p.requires_grad  # <2>
...     )
----
<1> `p.numel()` is equivalent to `p.size().product()`
<2> Only learned parameters require the gradient calculation for backprop

The more weights or learned parameters there are, the greater the capacity of your model to learn more things about the data.
But the whole point of all the clever ideas, like convolution and recurrence, is to create neural networks that are efficient.
By choosing the right combination of algorithms, sizes and types of layers, you can reduce the number of weights or parameters your model must learn while simultaneously creating smarter models with greater capacity to make good predictions.

If you experiment with a variety of GRU hyperparameters using the `nlpia2/ch08/rnn_word/hypertune.py` script you can aggregate all the results with your RNN results to compare them all together.

[source,python]
----
>>> import jsonlines  # <1>

>>> with jsonlines.open('experiments.jsonl') as fin:
...     lines = list(fin)
>>> df = pd.DataFrame(lines)
>>> df.to_csv('experiments.csv')
>>> cols = 'learned_parameters rnn_type epochs lr num_layers'
>>> cols += ' dropout epoch_time test_loss'
>>> cols = cols.split()
>>> df[cols].round(2).sort_values('test_loss', ascending=False)
----
<1> The `jsonlines` package is great for incrementally saving your results

[source,python]
----
>>> df
     parameters  rnn_type  epochs   lr  layers  drop  time (s)  loss
3      13746478  RNN_TANH       1  0.5       5   0.0     55.46  6.90
155    14550478       GRU       1  0.5       5   0.2     72.42  6.89
147    14550478       GRU       1  0.5       5   0.0     58.94  6.89
146    14068078       GRU       1  0.5       3   0.0     39.83  6.88
1      13505278  RNN_TANH       1  0.5       2   0.0     32.11  6.84
..          ...       ...     ...  ...     ...   ...       ...   ...
133    13505278  RNN_RELU      32  2.0       2   0.2   1138.91  5.02
134    13585678  RNN_RELU      32  2.0       3   0.2   1475.43  4.99
198    14068078       GRU      32  2.0       3   0.0   1223.56  4.94
196    13585678       GRU      32  2.0       1   0.0    754.08  4.91
197    13826878       GRU      32  2.0       2   0.0    875.17  4.90
----

You can see from these experiments that GRUs are your best bet for creating language models that understand text well enough to predict the next word.
Surprisingly GRUs do not need as many layers as other RNN architectures to achieve the same accuracy.
And they take less time to train than RNNs to achieve comparable accuracy.

=== Long and Short-Term Memory (LSTM)

An LSTM neuron adds two more internal gates in an attempt to improve both the long-term and the short-term memory capacity of an RNN.
An LSTM retains the update and relevance gates but adds new gates for forgetting and the output gate.
four internal gates, each with a different purpose.
The first one is just the normal activation function that you are familiar with.

1. Forgetting gate (`f`): Whether to completely ignore some element of the hidden layer to make room in memory for future more important tokens.
2. Input or update gate (`i`): What parts of the hidden layer should matter to the current output (now, at time `t`).
3. Relevance or cell gate (`i`): What parts of the hidden layer should be blocked because they are not longer relevant to the current output.
4. Output gate (`o`): What parts of the hidden layer should be output, both to the neurons output as well as to the hidden layer for the next token in the text.

But what about that unlabeled `tanh` activation function at the upper right of Figure 8.12?
That's just the original output activation used to create the hidden state vector from the cell state.
The hidden state vector holds information about the most recently processed tokens; it's the short-term memory of the LSTM.
The cell state vector holds a representation of the meaning of the text over the long term, since the beginning of a document.

In Figure 8.13 you can see how these four logic gates fit together.
The various weights and biases required for each of the logic gates are hidden to declutter the diagram.
You can imagine the weight matrix multiplications happening within each of the activation functions that you see in the diagram.
Another thing to notice is that the hidden state is not the only recurrent input and output.
You've now got another encoding or state tensor called the _cell state_.
As before, you only need the hidden state to compute the output at each time step.
But the new cell state tensor is where the long and short-term memories of past patterns are encoded and stored to be reused on the next token.

// Figure 8.13
[id=LSTMs-add-forgetting, reftext={chapter}.{counter:figure}]
.LSTMs add a forgetting gate and a cell output
image::../images/ch08/lstm_drawio.png[alt="", width=80%, link="../images/ch08/lstm.png"]

One thing in this diagram that you'll probably only see in the smartest blog posts is the explicit linear weight matrix needed to compute the output tensor.footnote:[Thank you Rian Dolphin for your rigorous explanation (https://towardsdatascience.com/lstm-networks-a-detailed-explanation-8fae6aefc7f9)]
Even the PyTorch documentation glosses over this tidbit.
You'll need to add this fully connected linear layer yourself at whichever layer you are planning to compute predictions based on your hidden state tensor.

You're probably saying to yourself "Wait, I thought all hidden states (encodings) were the same, why do we have this new _cell state_ thing?"
Well, that's the long-term memory part of an LSTM.
The cell state is maintained separately so the logic gates can remember things and store them there, without having to mix them in with the shorter-term memory of the hidden state tensor.
And the cell state logic is a bit different from the hidden state logic.
It's designed to be selective in the things it retrains to keep room for things it learns about the text long before it reaches the end of the string.

The formulas for computing the LSTM logic gates and outputs are very similar to those for the GRU.
The main difference is the addition of 3 more functions to compute all the signals you need.
And some of the signals have been rerouted to create a more complicated network for storing more complex patterns of connections between long and short-term memory of the text.
It's this more complicated interaction between hidden and cell states that creates more "capacity" or memory and computation in one cell.
Because an LSTM cell contains more nonlinear activation functions and weights it has more information processing capacity.

[source,python]
----
r = sigmoid(W_i2r.mm(x) + b_i2r +    W_h2r.mm(h) + b_h2r)
z = sigmoid(W_i2z.mm(x) + b_i2z +    W_h2z.mm(h) + b_h2z)
n =    tanh(W_i2n.mm(x) + b_i2n + r∗(W_h2n.mm(h) + b_h2n))

f = sigmoid(W_i2f.mm(x) + b_i2f + W_h2f.mm(h) + b_h2f)  # <1>
i = sigmoid(W_i2i.mm(x) + b_i2i + W_h2i.mm(h) + b_h2i)  # <2>
g = tanh(W_i2g.mm(x) + b_i2g + W_h2y.mm(h) + b_h2g)  # <3>
o = sigmoid(W_i2o.mm(x) + b_i2o + W_h2o.mm(h) + b_h2o)  # <4>
c = f*c + i*g  # <5>
h = o*tanh(c)
----
<1> LSTM forgetting gate (GRU reset gate)
<2> LSTM input relevance gate (GRU update gate)
<3> LSTM cell gate, notice the redundant biases b_i2i and b_h2i
<4> LSTM output gate
<5> cell state


=== Give your RNN a tuneup
// SUM: Hyperparameter tuning tricks, like increasing dropout percentages and reducing the number of learnable weights can help an RNN improve its accuracy and generalization in the real world. Bigger isn't always better. And to get your model out of a rut, increase the temperature during runtime.

As you learned in Chapter 7, hyperparameter tuning becomes more and more important as your neural networks get more and more complicated.
Your intuitions about layers, network capacity and training time will get fuzzier and fuzzier as the models get complicated.
RNNs are particularly intuitive.
To jumpstart your intuition we've trained dozens of different basic RNNs with different combinations of hyperparameters such as the number of layers and number of hidden units in each layer.
You can explore all the hyperparameters that you are curious about using the code in `nlpia2/ch08`.footnote:[The `hypertune.py` script in the `ch08/rnn_word` module within the `nlpia2` Python package https://gitlab.com/tangibleai/nlpia2/-/blob/main/src/nlpia2/ch08/rnn_word/hypertune.py]

[source,python]
----
import pandas as pd
import jsonlines

with jsonlines.open('experiments.jsonl') as fin:
    lines = list(fin)
df = pd.DataFrame(lines)
df.to_csv('experiments.csv')
cols = 'rnn_type epochs lr num_layers dropout epoch_time test_loss'
cols = cols.split()
df[cols].round(2).sort_values('test_loss').head(10)
----

[source,text]
----
    epochs   lr  num_layers  dropout  epoch_time  test_loss
37      12  2.0           2      0.2       35.43       5.23
28      12  2.0           1      0.0       22.66       5.23
49      32  0.5           2      0.0       32.35       5.22
57      32  0.5           2      0.2       35.50       5.22
38      12  2.0           3      0.2       46.14       5.21
50      32  0.5           3      0.0       37.36       5.20
52      32  2.0           1      0.0       22.90       5.10
55      32  2.0           5      0.0       56.23       5.09
53      32  2.0           2      0.0       32.49       5.06
54      32  2.0           3      0.0       38.78       5.04
----

It's a really exciting thing to explore the hyperspace of options like this and discover surprising tricks for building accurate models.
Surprisingly, for this RNN language model trained on a small subset of Wikipedia, you can get great results without maximizing the size and capacity of the model.
You can achieve better accuracy with a 3-layer RNN than with a 5-layer RNN.
You just need to start with an aggressive learning rate and keep the dropout to a minimum.
And the fewer layers you have the faster the model will train.

[TIP]
====
Experiment often, and always document what things you tried and how well the model worked. 
This kind of hands-on work provides the quickest path toward an intuition that speeds up your model building and learning. 
Your lifelong goal is to train your mental model to predict which hyperparameter values will produce the best results in any given situation.
====

If you feel the model is overfitting the training data but you can't find a way to make your model simpler, you can always try increasing the `Dropout(percentage)`.
This is a sledgehammer that reduces overfitting while allowing your model to have as much complexity as it needs to match the data.
If you set the dropout percentage much above 50%, the model starts to have a difficult time learning.
Your learning will slow and the validation error may bounce around a lot.
But 20% to 50% is a pretty safe range for a lot of RNNs and most NLP problems.

If you're like Cole and me when we were getting started in NLP, you're probably wondering what a "unit" is.
All the previous deep learning models have used "neurons" as the fundamental unit of computation within a neural network.
Researchers use the more general term "unit" to describe the elements of an LSTM or GRU that contain internal gates and logic.
So when you are thinking about how many units to add to your neural network to solve a particular problem, each LSTM or GRU unit gives your network a capacity similar to two "normal" RNN neurons or hidden vector dimensions.
A unit is just a more complicated, higher-capacity neuron, and you can see this if you count up the number of "learned parameters" in your LSTM model and compare it to those of an equivalent RNN.

== Predicting
// SUM: Generating text from an encoding tensor requires running a decoder which is just an RNN language model trained to predict the next token (character, word, or N-gram)

The word-based RNN language model you trained for this chapter used the `WikiText-2` corpus.footnote:[PyTorch `torchtext` dataset (https://pytorch.org/text/0.8.1/datasets.html#wikitext-2)]
The nice thing about working with this corpus is that it is often used by researchers to benchmark their language model accuracy.
And the Wikipedia article text has already been tokenized for you.
Also, the uninteresting sections such as the References at the end of the articles have been removed.

Unfortunately, the PyTorch version of the WikiText-2 includes "<unk>" tokens that randomly replace, or mask, 2.7% of the tokens.
That means that your model will never get very high accuracy unless there is some predictable pattern that determines which tokens were masked with "<unk>".
But if you download the original raw text without the masking tokens you can train your language model on it and get a quick boost in accuracy.footnote:[Raw, unmasked text with "answers" for all the "unk" tokens (https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip)]
And you can compare the accuracy of your LSTM and GRU models to those of the experts that use this benchmark data.footnote:[AI researchers(https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/)]

Here is an example paragraph at the end of the masked training dataset `train.txt`.

[source,python]
----
>>> from nlpia2.ch08.rnn_word.data import Corpus
>>> corpus = Corpus('data/wikitext-2')
>>> passage = corpus.train.numpy()[-89:-35]
----

[source,python]
----
>>> ' '.join([vocab.idx2word[i] for i in passage])
Their ability at mimicry is so great that strangers have looked in vain
for the human they think they have just heard speak . <eos>
Common starlings are trapped for food in some Mediterranean countries .
The meat is tough and of low quality , so it is <unk> or made into <unk> .
----

It seems that the last Wikipedia article in the WikiText-2 benchmark corpus is about the common starling (a small bird in Europe).
And from the article, it seems that the starling appears to be good at mimicking human speech, just as your RNN can.

What about those "<unk>" tokens?
These are designed to test machine learning models.
Language models are trained with the goal of predicting the words that were replaced with the "<unk>" (unknown) tokens.
Because you have a pretty good English language model in your brain you can probably predict the tokens that have been masked out with all those "<unk>" tokens.

But if the machine learning model you are training thinks these are normal English words, you may confuse it.
The RNN you are training in this chapter is trying to discern the _meaning_ of the meaningless "<unk>" token, and this will reduce its understanding of all other words in the corpus.

[TIP]
====
If you want to avoid this additional source of error and confusion, you can try training your RNN on the unofficial raw text for the `wikitext-2` benchmark.
There is a one-to-one correspondence between the tokens of the official wikitext-2 corpus and the unofficial raw version in the nlpia2 repository. footnote:[`nlpia2` package with code and data for the rnn_word model code and datasets used in this chapter (https://gitlab.com/tangibleai/nlpia2/-/tree/main/src/nlpia2/ch08/rnn_word/data/wikitext-2)]
====

So how many "<eos>" and "<unk>" tokens are there in this training set?

[source,python]
----
>>> num_eos = sum([vocab.idx2word[i] == '<eos>' for i in
[CA] corpus.train.numpy()])
>>> num_eos
36718
>>> num_unk = sum([vocab.idx2word[i] == '<unk>' for i in
[CA] corpus.train.numpy()])
>>> num_unk
54625
>>> num_normal = sum([
...     vocab.idx2word[i] not in ('<unk>', '<eos>')
...     for i in corpus.train.numpy()])
>>> num_normal
1997285
>>> num_unk / (num_normal + num_eos + num_unk)
0.0261...
----

So 2.6% of the tokens have been replaced with the meaningless "<unk>" token.
And the "<eos>" token marks the newlines in the original text, which is typically the end of a paragraph in a Wikipedia article.

So let's see how well it does at writing new sentences similar to those in the WikiText-2 dataset, including the "<unk>" tokens.
We'll prompt the model to start writing with the word "The" to find out what's on the top of its "mind".

[source,python]
----
>>> import torch
>>> from preprocessing import Corpus
>>> from generate import generate_words
>>> from model import RNNModel

>>> corpus = Corpus('data/wikitext-2')
>>> vocab = corpus.dictionary
>>> with open('model.pt', 'rb') as f:
...    orig_model = torch.load(f, map_location='cpu')  # <1>

>>> model = RNNModel('GRU', vocab=corpus.dictionary, num_layers=1)  # <2>
>>> model.load_state_dict(orig_model.state_dict())
>>> words = generate_words(
...    model=model, vocab=vocab, prompt='The', temperature=.1)  # <3>
----
<1> load pickled model weights and RNNModel class
<2> because we updated the RNNModel class after the checkpoint was saved
<3> lower temperature makes the text less random and more repetitive

[source,python]
----
>>> print(' '.join(w for w in words))
...
= =  Valkyria Valkyria Valkyria Valkyria = = The kakapo is a common
[CA] starling , and the of the of the ,
...
----

The first line in the training set is "= Valkyria Chronicles III =" and the last article in the training corpus is titled "= Common Starling =".
So this GRU remembers how to generate text similar to text at the beginning and end of the text passages it has read.
So it surely seems to have both long and short-term memory capabilities.
This is exciting, considering we only trained a very simple model on a very small dataset.
But this GRI doesn't yet seem to have the capacity to store all of the English language patterns that it found in the two-million-token-long sequence.
And it certainly isn't going to do any sense-making any time soon.

[NOTE]
====
Sense-making is the way people give meaning to the experiences that they share.
When you try to explain to yourself why others are doing what they are doing, you are doing sense-making.
And you don't have to do it alone.
A community can do it as a group through public conversation mediated by social media apps and even conversational virtual assistants.
That's why it's often called "collective sense-making."
Startups like DAOStack are experimenting with chatbots that bubble up the best ideas of a community and use them for building knowledge bases and making decisions. footnote:[DAOStack platform for decentralized governance (https://daostack.io/deck/DAOstack-Deck-ru.pdf)]
====

You now know how to train a versatile NLP language model that you can use on word-level or character-level tokens.
You can use these models to classify text or even generate modestly interesting new text.
And you didn't have to go crazy on expensive GPUs and servers.

== Test yourself

* What are some tricks to improve "retention" for reading long documents with an RNN?
* What are some "unreasonably effective" applications for RNNs in the real world?
* How could you use a name classifier for good? What are some unethical uses of a name classifier?
* What are some ethical and prosocial AI uses for a dataset with millions of username-password pairs such as Mark Burnett's password dataset? footnote:[Alexander Fishkov's analysis (https://www.city-data.com/blog/1424-passwords-on-the-internet-publicly-available-dataset/) of Mark Burnett's (https://xato.net/author/mb/) ten million passwords (https://xato.net/passwords/ten-million-passwords)]
* Train an rnn_word model on the raw text, unmasked text for the Wikitext-2 dataset the proportion of tokens that are "<unk>". Did this improve the accuracy of your word-level RNN language model?
* Modify the dataset to label each name with a multi-hot tensor indicating all the nationalities for each name.footnote:[PyTorch community multi-label (tagging) data format example (https://discuss.pytorch.org/t/multi-label-classification-in-pytorch/905/45)] footnote:[Example `torchtext`` Dataset class multi-label text classification (https://discuss.pytorch.org/t/how-to-do-multi-label-classification-with-torchtext/11571/3)] How should you measure accuracy? Does your accuracy improve?

== Summary

* In natural language token sequences, an RNN can remember everything it has read up to that point, not just a limited window.
* Splitting a natural language statement along the dimension of time (tokens) can help your machine deepen its understanding of natural language.
* You can backpropagate errors back in time (token) as well as in the layers of a deep learning network.
* Because RNNs are particularly deep neural nets, RNN gradients are particularly temperamental, and they may disappear or explode.
* Efficiently modeling natural language character sequences wasn't possible until recurrent neural nets were applied to the task.
* Weights in an RNN are adjusted in aggregate across time for a given sample.
* You can use different methods to examine the output of recurrent neural nets.
* You can model the natural language sequence in a document by passing the sequence of tokens through an RNN backward and forward in time simultaneously.
