= Natural Language Processing in Action, Second Edition
:chapter: 11
:part: 3
:sectnumoffset: 1
:secnums:
:imagesdir: .
:xrefstyle: short
:figure-caption: Figure {chapter}.
:listing-caption: Listing {chapter}.
:table-caption: Table {chapter}.
:leveloffset: 0
:stem: latexmath
:toc:
:source-highlighter: coderay
:bibliography-database: dl4nlp.bib
:bibliography-style: ieee
:index::[]

= Information extraction and knowledge graphs (grounding)

This chapter covers

* Extracting named entities from text
* Understanding the structure of the sentence using dependency parsing
* Converting a dependency tree into a knowledge (fact)
* Building a knowledge graph from text


In the previous chapter (Chapter 10) you learned how to use large transformers to generate smart _sounding_ words.
But language models on their own are just faking it by predicting the next word that will _sound_ reasonable to you.
Your AI can't reason about the real world until you give them access to facts and knowledge about the world.
In Chapter 2 you learned how to do exactly this, but you didn't know it then.
You were able to tag tokens with their part of speech and their logical role in the meaning of a sentence (dependency tree).
This old-fashioned token tagging algorithm is all you need to give your generative language models (AI) 
knowledge about the real world. 
The goal of this chapter is to teach your bot to understand what it reads.
And you'll put that understanding into a flexible data structure designed to store knowledge, known as _knowledge graph_.
Then your bot can use that knowledge to make decisions and say smart stuff about the world.

Correctly parsing your text into _entities_ and discovering the _relations_ between them is how you'll go about extracting facts from the text.
A _knowledge graph_, also called _knowledge database_ (knowledge base) or a _semantic net_, is a database that stores knowledge as relationships between concepts.
Though you can use a relational database to store the relations and concepts, sometimes it is more appropriate to use a _graph_ data structure.
The nodes in the graph would be _entities_, while the edges would be relations between these entities.  

You can see an example of a knowledge graph in Figure <<figure-knowledge-graph>>.

[id=figure-knowledge-graph, reftext={chapter}.{counter:figure}]
.An example of a knowledge graph
image::../images/ch11/kg_150_biotech_company.graphviz.png[A network diagram of the first 150 entities in the NELL knowledge graph. The dataset starts with a biotech company named , width=85%, link="../images/ch11/kg_150_biotech_company.graphviz.png"]

Each fact you extract will create a new connection between the nodes of the graph - or possibly, create new nodes. 
This allows you to ask questions about the relationships between things using a query language such as GraphQL or Cypher or even SQL.

And your algorithms can then do fact-checking, not only on the text written by humans but also text generated by your NLP pipeline or AI.
Finally, your AI algorithms will be able to do introspection to let you know if what they are telling you might actually have some semblance of truth to it.

Your AI can use knowledge graphs to fill the _common sense knowledge_ gap in large language models and perhaps live up to a little bit of the hype around LLMs and AI.
This is the missing link in the NLP chain that you need to create true AI.
And you can use a knowledge graph to programmatically generate text that makes sense because it is grounded in facts in your database.
You can even infer new facts or _logical inferences_ about the world that aren't yet included in your knowledge base.

You may remember hearing about "inference" when people talk about forward propagation or prediction using deep learning models.
A deep learning language model uses statistics to estimate or guess the next word in the text that you prompt it with.
And deep learning researchers hope that one day, neural networks will be able to match the natural human ability to logically infer things and reason about the world.
But this isn't possible, because words don't contain all the knowledge about the world that a machine would need to process to make factually correct inferences.
So you're going to use a tried and true logical inference approach called "symbolic reasoning."

If you're familiar with the concept of a compiler then you may want to think of the dependency tree as a parse tree or abstract syntax tree (AST).
An AST defines the logic of a machine language expression or program.
You're going to use the natural language dependency tree to extract the logical relations within natural language text.
And this logic will help you _ground_ the statistical deep learning models so they can do more than merely make statistical "guesses" about the world as they did in previous chapters.

== Grounding

Once you have a knowledge graph, your chatbots and AI agents will have a way to correctly reason about the world in an explainable way.
And if you can extract facts from the text your deep learning model generates, you can check to see if that text agrees with the knowledge you've collected in your knowledge graph.
This is called _grounding_  when you maintain a knowledge graph and then use it to double-check the facts and reasoning in the generated text.
When you ground your language model you attach it to some ground truth facts about the world.

Grounding can also benefit your NLP pipeline in other ways.
Using a knowledge graph for the reasoning part of your algorithm can free up your language model to do what it does best -- generate plausible, grammatical text.
So you can fine tune your language model to have the tone that you want, without trying to build a chameleon that pretends to understand and reason about the world.
And your knowledge graph can be designed to contain just the facts about a world that you want your AI to understand -- whether it is facts about the real world that you have in mind or some fictional world that you are creating.
By separating the reasoning from the language you can create an NLP pipeline that both sounds correct and _is_ correct.

There are a few other terms that are often used when referring to this grounding process.
Sometimes it's referred to as _symbolic reasoning_ as opposed to the probabilistic reasoning of machine learning models.
_First order logic_ is one system for symbolic reasoning.footnote:[Wikipedia article "Symbolic AI" (https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence)]
This was the preferred approach to building expert systems and theorem provers before the data and processing power was available for machine learning and deep learning.
It's also called Good Old Fashioned AI or GOFAI, pronounced "Go Fie".
GOFAI is back in fashion as researchers attempt to build generally intelligent systems that we can rely on to make important decisions.

Another advantage of grounding your NLP pipeline is that you can use the facts in your knowledge base to _explain_ its reasoning.
If you ask an ungrounded LLM to explain why it said something unreasonable, it will just keep digging a hole for itself (and you) by making up more and more nonsense reasons.
You saw this in the previous chapters when LLMs confidently hallucinated (fabricated) nonexistent but plausible references and fictional people to explain where they got their nonsense from.
The key to creating AI you can trust is to put a floor of reason underneath it using a knowledge graph.
The first, and perhaps most important algorithm in this grounding process is _knowledge extraction_.

=== Going old-fashioned: information extraction with patterns
In this chapter, we'll also get back to methods you see in the very early chapters, like regular expressions. 
Why return to hard-coded (manually composed) regular expressions and patterns?
Because your statistical or data-driven approach to NLP has limits.
You want your machine learning pipeline to be able to do some basic things, such as answer logical questions or perform actions such as scheduling meetings based on NLP instructions.
And machine learning falls flat here.

Plus, as you'll see here, you can define a compact set of condition checks (a regular expression) to extract key bits of information from a natural language string.
And it can work for a broad range of problems.

Pattern matching (and regular expressions) continue to be the state-of-the-art approach for information extraction and related tasks.

Enough of a preamble.
Let's start the journey of knowledge extraction and grounding! 
But we have to cover an important step in processing your documents, to generate a proper input to your knowledge extraction pipeline.
We need to break our text into smaller units. 

== First things first: segmenting your text into sentences 
// SUM: How and why to segment text into sentences.

Before you can dive into extracting your knowledge from raw text, you need to break it down into chunks that your pipeline can work on. 
Document "chunking" is useful for creating semi-structured data about documents that can make it easier to search, filter, and sort documents for information retrieval.
And for information extraction, if you're extracting relations to build a knowledge base such as NELL or Freebase (more about them in a bit), you need to break it into parts that are likely to contain a fact or two.
When you divide natural language text into meaningful pieces, it's called _segmentation_.
The resulting segments can be phrases, sentences, quotes, paragraphs, or even entire sections of a long document.

Sentences are the most common chunk for most information extraction problems.
Sentences are usually punctuated with one of a few symbols (".", "?", "!", or a new line).
And grammatically correct English language sentences must contain a subject (noun) and a verb, which means they'll usually have at least one fact worth extracting.
Sentences are often self-contained packets of meaning that don't rely too much on preceding text to convey most of their information.

In addition to facilitating information extraction, you can flag some of those statements and sentences as being part of a dialog or being suitable for replies in a dialog.
Using a sentence segmenter allows you to train your chatbot on longer texts, such as books.
Choosing those books appropriately gives your chatbot a more literary, intelligent style than if you trained it purely on Twitter streams or IRC chats.
And these books give your chatbot access to a much broader set of training documents to build its common sense knowledge about the world.

Sentence segmentation is the first step in your information extraction pipeline.
It helps isolate facts from each other.
Most sentences express a single coherent thought.
And many of those thoughts are about real things in the real world.
And, most importantly, all the natural languages have sentences or logically cohesive sections of text of some sort.
And all languages have a widely shared process for generating them (a set of grammar "rules" or habits).

But segmenting text and identifying sentence boundaries is a bit trickier than you might think.
In English, for example, no single punctuation mark or sequence of characters always marks the end of a sentence.

=== Why won't `split('.!?')` work?
// SUM: Why you need a sentence segmenter instead of naively splitting on EOS punctuation.

Even a human reader might have trouble finding an appropriate sentence boundary within each of the following quotes.
And if they did find multiple sentences from each, they would be wrong for four out of five of these difficult examples:

_I went to G.T.You?_

_She yelled "It's right here!" but I kept looking for a sentence boundary anyway._

_I stared dumbfounded on as things like "How did I get here?", "Where am I?", "Am I alive?" flittered across the screen._

_The author wrote "'I don't think it's conscious.' Turing said."_

Even a human reader would have trouble finding an appropriate sentence boundary within each of these quotes and nested quotes and stories within a story.

More sentence segmentation "edge cases" such as this are available at tm-town.com. footnote:[See the web page titled "Natural Language Processing : TM-Town" (https://www.tm-town.com/natural-language-processing#golden_rules).] 

Technical text is particularly difficult to segment into sentences because engineers, scientists, and mathematicians tend to use periods and exclamation points to signify a lot of things besides the end of a sentence.
When we tried to find the sentence boundaries in this book, we had to manually correct several of the extracted sentences.

If only we wrote English like telegrams, with a "STOP" or unique punctuation mark at the end of each sentence.
But since we don't, you'll need some more sophisticated NLP than just `split('.!?')`.
Hopefully, you're already imagining a solution in your head.
If so, it's probably based on one of the two approaches to NLP you've used throughout this book:

* Manually programmed algorithms (regular expressions and pattern-matching)
* Statistical models (data-based models or machine learning)

We use the sentence segmentation problem to revisit these two approaches by showing you how to use regular expressions as well as more advanced methods to find sentence boundaries.
And you'll use the text of this book as a training and test set to show you some of the challenges.
Fortunately, you haven't inserted any newlines within sentences, to manually "wrap" text like in newspaper column layouts.
Otherwise, the problem would be even more difficult.
In fact, much of the source text for this book, in ASCIIdoc format, has been written with "old-school" sentence separators (two spaces after the end of every sentence), or with each sentence on a separate line.
This was so we could use this book as a training and test set for your segmenters.

=== Sentence segmentation with regular expressions
// SUM: Test some regular expressions on the TM-Town (translation memory company) dataset.

Regular expressions are just a shorthand way of expressing the tree of "`if...then`" rules (regular grammar rules) for finding character patterns in strings of characters.
As we mentioned in Chapters 1 and 2, regular expressions (regular grammars) are a particularly succinct way to specify the structure of a finite state machine.

Any formal grammar can be used by a machine in two ways:

* To recognize "matches" to that grammar
* To generate a new sequence of symbols

Not only can you use patterns (regular expressions) for extracting information from natural language, but you can also use them to generate strings that match that pattern!
Check out the `rstr` (short for "random string") package if you ever need to generate example strings that match a regular expresssion.footnote:["Rstr package on PyPi (https://pypi.org/project/rstr/).] for some of your information extraction patterns here.

This formal grammar and finite state machine approach to pattern matching has some other awesome features.
A true finite state machine is guaranteed to eventually stop (halt) in a finite number of steps.
So if you use a regular expression as your pattern matcher you know that you will always receive an answer to your question about whether you've found a match in your string or not.
It will never get caught in a perpetual loop... as long as you don't "cheat" and use look-aheads or look-backs in your regular expressions.
And because a regular expression is deterministic it always returns a match or non-match.
It will never give you less than 100% confidence or probability of there being a match.

So you'll stick to regular expressions that don't require these "look-back" or "look-ahead" cheats.
You'll make sure your regular expression matcher processes each character and moves ahead to the next character only if it matches -- sort of like a strict train conductor walking through the seats checking tickets.
If you don't have one, the conductor stops and declares that there's a problem, a mismatch, and he refuses to go on, or look ahead or behind you until he resolves the problem.
There are no "go-backs" or "do-overs" for train passengers, or for strict regular expressions.

Our regex or FSM has only one purpose in this case: identifying sentence boundaries.

If you do a web search for sentence segmenters,footnote:[See the web page titled "Python sentence segment at DuckDuckGo" (https://duckduckgo.com/?q=Python+sentence+segment&t=canonical&ia=qa).] you're likely to be pointed to various regular expressions intended to capture the most common sentence boundaries.
Here are some of them, combined and enhanced to give you a fast, general-purpose sentence segmenter.

The following regex would work with a few "normal" sentences.

[source,python]
>>> re.split(r'[!.?]+[ $]', "Hello World.... Are you there?!?! I'm going to Mars!")
['Hello World', 'Are you there', "I'm going to Mars!"]

Unfortunately, this `re.split` approach gobbles up the sentence-terminating token, and only retains it if it is the last character in a document or string.
But it does do a good job of ignoring the trickery of periods within doubly-nested quotes:

[source,python]
>>> re.split(r'[!.?] ', "The author wrote \"'I don't think it's conscious.' Turing said.\"")
['The author wrote "\'I don\'t think it\'s conscious.\' Turing said."']

It also ignores periods in quotes that terminate an actual sentence.
This can be a good thing or a bad thing, depending on your information extraction steps that follow your sentence segmenter.

What about abbreviated text, such as SMS messages and tweets?
Sometimes hurried humans squish sentences together, leaving no space surrounding periods.
Alone, the following regex could only deal with periods in SMS messages that have letters on either side, and it would safely skip over numerical values:

[source,python]
>>> re.split(r'(?<!\d)\.|\.(?!\d)', "I went to GT.You?")
['I went to GT', 'You?']

Even combining these two regexes into a monstrosity like `r'((?<!\d)\.|\.(?!\d))|([!.?]+)[ $]+'` isn't enough to get all the sentences right if we try to parse this chapter. 
You'd have to add a lot more "look-ahead" and "look-back" to improve the accuracy of a regex sentence segmenter - and we agreed that this might be dangerous!

If looking for all the edge cases and designing rules around them feel cumbersome, that's because it is. 
A better approach for sentence segmentation is to use a machine learning algorithm (often a single-layer neural net or logistic regression) trained on a labeled set of sentences.
Several packages contain such a model you can use to improve your sentence segmenter, such as spaCy footnote:[See the web page titled "Facts & Figures : spaCy Usage Documentation" (https://spacy.io/usage/facts-figures).] and Punkt package of NLTK footnote:[See the web page titled "nltk.tokenize package — NLTK 3.3 documentation" (http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.punkt).]

You can use the spaCy sentence segmenter (built into the parser) for most of your mission-critical applications.
spaCy has few dependencies and compares well with the others on accuracy and speed.
And actually, it does the sentence segmentation automatically as part of its default pipeline. 

Here's the simplest way to segment your text into sentences with spaCy:

[source,python]
----
>>> import spacy

>>> doc = nlp("Are you an M.D. Dr. Gebru? she forgot to spaCy this.")
>>> [s.text for s in doc.sents]
['Are you an M.D. Dr. Gebru?', 'she forgot to spaCy this.']
----

SpaCy's default parser is based on dependency extraction which we will cover later in this chapter.
A dependency parser identifies how each word depends on the other words in a sentence diagram, like the one you learned about in grammar school (elementary school).
The structure and meaning of the tokens within a sentence help the spacy sentence segmenter deal with ambiguous punctuation and capitalization accurately, but that takes time.
Speed is not important when you are only processing a few sentences, but what if you wanted to parse the AsciiDoc manuscript for Chapter 9 of this book?

[source,python]
----
>>> from nlpia2.text_processing.extractors import extract_lines
>>> t0 = time.time(); lines = extract_lines(
...     9, nlp=nlp); t1=time.time()  # <1>
>>> t1 - t0
15.98...
>>> t0 = time.time(); lines = extract_lines(9, nlp=None); t1=time.time()
>>> t1 - t0
0.022...
----
<1> The first argument can be a path to an `adoc` file or a chapter number.

Wow, that _is_ slow!
SpaCy is about 700 times slower than a regular expression.
If you have millions of documents instead of just this one chapter of text, then you will probably need to do something different.
For example, on a medical records parsing project we needed to switch to a regular expression tokenizer and sentence segmenter.
The regex parser reduced our processing time from weeks to days, but it also reduced the accuracy of the rest of our NLP pipeline.

SpaCy eventually caught up with our need for customization.
SpaCy now allows you to enable or disable any piece of the pipeline you like.
And it has a statistical sentence segmenter that doesn't rely on the other elements of the spaCy pipeline such as the word embeddings and named entity recognizer.
When you want to speed up your spaCy NLP pipeline you can remove all the elements you don't need and add back just the pipeline elements that you need.
First, check out the pipeline attribute of a spacy NLP pipeline, and use it to disable everything.

[source,python]
----
>>> nlp.pipeline
[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x...>),
 ('tagger', <spacy.pipeline.tagger.Tagger at 0x7...>),
 ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x...>),
 ('attribute_ruler',
  <spacy.pipeline.attributeruler.AttributeRuler at 0x...>),
 ('lemmatizer',
  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x...>),
 ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x...>)]
>>> nlp = spacy.load("en_core_web_md", exclude=[
...    'tok2vec', 'parser', 'lemmatizer',  # <1>
...    'ner', 'tagger', 'attribute_ruler'])
>>> nlp.pipeline  # <2>
[]
----
<1> The 'tok2vec', 'ner', and 'lemmatizer' algorithms are probably the slowest elements.
<2> You should see an empty list if you have successfully removed everything.

Now that you've cleaned your pipes, you can add back the important pieces that you need.
You will need the `senter` pipeline element for sentence segmentation for this speed read through Chapter 9.

[source,python]
----
>>> nlp.enable_pipe('senter')
>>> nlp.pipeline
[('senter', <spacy.pipeline.senter.SentenceRecognizer at 0x...>)]
>>> t0 = time.time(); lines2 = extract_lines(nlp=nlp); t1=time.time()
>>> t1 - t0
2.3...
----

That is a significant time saver -- 2.3 vs 11.5 seconds on an 8-core i7 laptop.
The statistical sentence segmenter is about 5x faster than the full spaCy pipeline.
The regular expression approach will still be much faster, but the statistical sentence segmenter will be more accurate.
You can estimate the accuracy of these two algorithms by comparing the lists of sentences to see if they produced the same lists of sentences.
This will not tell you which of the two approaches is correctly segmenting a particular text line, but at least you will see when the two algorithms agree.

[source,python]
----
>>> df_md = pd.DataFrame(lines)  # <1>
>>> df_fast = pd.DataFrame(lines2)
>>> (df_md['sents_spacy'][df_md.is_body]
...  == df_fast['sents_spacy'][df_fast.is_body]
...  ).sum() / df_md.is_body.sum()
0.93
----

So it appears that about 93% of the sentences of this book were segmented the same way with the slow and fast pipelines.
Look at some example segmentations to see which one might be better for your use cases.

[source,python]
----
>>> df_md['sents_spacy'][df_md.is_body]
37               [_Transformers_ are changing the world.]
                              ...
2049    [A personalized grammar checker may be your pe...
Name: sents_spacy, Length: 687, dtype: object

>>> df_fast['sents_spacy'][df_fast.is_body]
37             [_, Transformers_ are changing the world.]
                              ...
2049    [A personalized grammar checker may be your pe...
Name: sents_spacy, Length: 687, dtype: object
----

It looks like that opening sentence with the leading underscore character (\_) is bit more difficult for the faster statistical segmenter.
So you probably want to use the full spacy model whenever you are parsing Markdown or AsciiDoc text files.
The formatting characters will confuse a statistic segmenter if it has not been trained on similar text.

=== Sentence semantics

Now that you have your text segmented into sentences containing discrete facts, you are ready to start extracting those facts and give them structure in a knowledge graph.
To get started, create a heatmap of the BERT embeddings of all the sentences of chapter 9.

[source,python]
----
>>> import pandas as pd
>>> url = 'https://gitlab.com/tangibleai/nlpia2/-/raw/main/'
>>> url += 'src/nlpia2/data/nlpia_lines.csv'  # <1>
>>> df = pd.read_csv(url, index_col=0)
>>> df9 = df[df.chapter == 9].copy()
>>> df9.shape
(2075, 23)
----
<1> This data file contains the lines of AsciiDoc text from a draft of this book.

Take a look at this DataFrame.
It has columns that contain tags for each line of text.
You can use the tags to filter out the lines that you don't want to process.

[source,python]
----
>>> df9[['text', 'is_title', 'is_body', 'is_bullet']]
                           text  is_title  is_body  is_bullet
19057  = Stackable deep lear...      True    False      False
19058               :chapter: 9     False    False      False
19059                  :part: 3     False    False      False
19060                 :secnums:     False    False      False
19061             :imagesdir: .     False    False      False
...                         ...       ...      ...        ...
21124  * By keeping the inpu...     False    False       True
21125  * Transformers combin...     False    False       True
21126  * The GPT transformer...     False    False       True
21127  * Despite being more ...     False    False       True
21128  * If you chose a pret...     False    False       True

[2072 rows x 4 columns]
----

Now you can use the 'is_body' tag to process all the sentences within the body of the manuscript.
These lines should contain mostly complete sentences so that you can compare them semantically to each other to see a heatmap of how often we say similar things.
Now that you understand transformers such as BERT, you can use it to give you even more meaningful representations of this text than what SpaCy creates.

[source,python]
----
>>> texts = df9.text[df9.is_body]
>>> texts.shape
(687,)

>>> from sentence_transformers import SentenceTransformer
>>> minibert = SentenceTransformer('all-MiniLM-L12-v2')
>>> vecs = minibert.encode(list(texts))
>>> vecs.shape
(687, 384)
----

The MiniLM model is a multipurpose BERT transformer that has been obtimized and "distilled."
It provides high accuracy and speed and should not take long to download from Hugging Face.
Now you have 689 passages of text (mostly individual sentences).
The MiniLM language model has embedded them into a 384 dimensional vector space.
As you learned in Chapter 6, embedding vector semantic similarity is computed with the normalized dot product.

[source,python]
----
>>> from numpy.linalg import norm
>>> dfe = pd.DataFrame([list(v / norm(v)) for v in vecs])
>>> cos_sim = dfe.values.dot(dfe.values.T)
>>> cos_sim.shape
(687, 687)
----

Now you have a square matrix, one row and column for each passage of text and its BERT embedding vector.
And the value in each cell of the matrix contains the cosine similarity between that pair of embedding vectors.
If you label the columns and rows with the first few characters of the text passages, that will make it easier to interpret all this data with a heatmap.

[source,python]
----
>>> cos_sim = pd.DataFrame(cos_sim, columns=labels, index=labels)
>>> cos_sim
                This chapter c  References:  ...
This chapter c        1.000000     0.258608  ...
_Transformers_        0.187846     0.188828  ...
...                        ...          ...  ...
And even if yo        0.093767     0.079976  ...
A personalized        0.073603     0.078423  ...
----

// FIXME: finish with diagram
== A knowledge extraction pipeline

Once you have your sentences organized you can start extracting concepts and relations from natural language text.
For example, imagine a user says "Remind me to read aiindex.org on Monday."
You'd like that statement to trigger a calendar entry or alarm for the next Monday after the current date.
Easier said than done.

To trigger correct actions with natural language you need something like an NLU pipeline or parser that is a little less fuzzy than a transformer or large language model.
You need to know that "me" represents a particular kind of named entity: a person.
Named entities are natural language terms or n-grams that refer to a particular thing in the real world, such as a person, place or thing.
Sound familiar?
In English grammar, the part of speech (POS) for a person, place or thing is "noun".
So you’ll see that the POS tag that spaCy associates with the tokens for a named entity is "NOUN".

And the chatbot should know that it can expand or _resolve_ that word by replacing it with that person's username or other identifying information.
You'd also need your chatbot to recognize that "aiindex.org" is an abbreviated URL, which is a named entity - a name of a specific instance of something, like a website or company.
And you need to know that a normalized spelling of this particular kind of named entity might be "http://aiindex.org", "https://aiindex.org", or maybe even "https://www.aiindex.org".
Likewise, you need your chatbot to recognize that Monday is one of the days of the week (another kind of named entity called an "event") and be able to find it on the calendar.

For the chatbot to respond properly to that simple request, you also need it to extract the relation between the named entity "me" and the command "remind."
You'd even need to recognize the implied subject of the sentence, "you", referring to the chatbot, another person named entity.
And you need to teach the chatbot that reminders happen in the future, so it should find the soonest upcoming Monday to create the reminder.

And that's just a simple use case. 
You can construct a graph from scratch using your own common sense knowledge or the domain knowledge that you want your AI to know about.
But if you can extract knowledge from text you can build much larger knowledge graphs much quicker.
Plus, you will need this algorithm to double-check any text generated by your language models.

Knowledge extraction requires four main steps:

.Four stages of knowledge extraction
image::../images/ch11/knowledge-graph-extraction.drawio.png["Knowledge extraction pipeline showing the entities -- 'Gebru' and the title of her paper about the dangers of large language models. extracted from the passage: Gebru had ... She and five others coauthored a research paper, "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?'", width=85%, link="../images/ch11/knowledge-graph-extraction.drawio.png"]

Fortunately, the spaCy language models include the building blocks for knowledge extraction: named entity recognition, coreference resolution, and relation extraction.
You only need to know how to combine the results of each of these steps to connect the pieces together.
Let's look at each stage separately by looking at an article about Timnit Gebru, a thought leader in AI ethics. 
We'll continue using the spaCy nlp model we initialized in the previous section. 

Let's start by downloading the Wikipedia article about Timnit Gebru.

[source,python]
----
>>> from nlpia2 import wikipedia as wiki
>>> page = wiki.page('Timnit Gebru')
>>> text = page.content
----

Have you heard of Timnit Gebru before?
She's famous among people in your area of interest and she's written several influential papers:

[source,python]
----
>>> i1 = text.index('Stochastic')
>>> text[i1:i1+51]
'Stochastic Parrots: Can Language Models Be Too Big?'
----

That's a pretty interesting research paper title.
It certainly seems like something her bosses would be interested in publishing.
Let's start looking at the text from the perspective of your information extraction pipeline. 

== Entity Recognition

The first step in extracting knowledge about some __thing__ is to find the _things_ that you want to know about.
The most important things in natural language text are the names of people, places, and things.
In linguistics named things are called "named entities."
These are not just names - they might be things like dates, locations, and any piece of information that can be placed into your knowledge graph. 

As with sentences, you can go two ways about the task of Named Entity Recognition (NER) - using pattern-matching, and using the neural approach. 

You'll discover that there are cases in which regular expressions are as precise, or even more precise, than neural networks. 
Here are some keystone bits of quantitative information that are worth the effort of "hand-crafted" regular expressions:

* GPS locations
* Dates
* Prices
* Numbers

Let's make a quick detour to learn how to extract such numerical data in the next section.  

=== Pattern-based entity recognition: extracting GPS locations 
GPS locations are typical of the kinds of numerical data you'll want to extract from text using regular expressions.
GPS locations come in pairs of numerical values for latitude and longitude.
They sometimes also include a third number for altitude or height above sea level, but you'll ignore that for now.
Let's just extract decimal latitude/longitude pairs, expressed in degrees.
This will work for many Google Maps URLs.
Though URLs are not technically natural language, they are often part of unstructured text data, and you'd like to extract this bit of information, so your chatbot can know about places as well as things.

Let's use your decimal number pattern from previous examples, but let's be more restrictive and make sure the value is within the valid range for latitude (\+/- 90 deg) and longitude (+/- 180 deg).
You can't go any farther north than the North Pole (+90 deg) or farther south than the South Pole (-90 deg).
And if you sail from Greenwich England 180 deg east (+180 deg longitude), you'll reach the date line, where you're also 180 deg west (-180 deg) from Greenwich.

.Regular expression for GPS coordinates
[source,python]
----
>>> import re
>>> lat = r'([-]?[0-9]?[0-9][.][0-9]{2,10})'
>>> lon = r'([-]?1?[0-9]?[0-9][.][0-9]{2,10})'
>>> sep = r'[,/ ]{1,3}'
>>> re_gps = re.compile(lat + sep + lon)

>>> re_gps.findall('http://...maps/@34.0551066,-118.2496763...')
[(34.0551066, -118.2496763)]

>>> re_gps.findall("https://www.openstreetmap.org/#map=10/5.9666/116.0566")
[('5.9666', '116.0566')]

>>> re_gps.findall("Zig Zag Cafe is at 45.344, -121.9431 on my GPS.")
[('45.3440', '-121.9431')]
----

Numerical data is pretty easy to extract, especially if the numbers are part of a machine-readable string.
URLs and other machine-readable strings put numbers such as latitude and longitude in a predictable order, format, and units to make things easy for us.

However, if we want to extract people's names, nationalities, places and other things that don't have a standard format, things become much more complicated. 
We can of course account for all the names, locations, and organizations possible. 
But keeping such a collection up to date would be a tremendously laborious task. 
For this, we'll need the neural approach.

=== Named entity recognition with spaCy

Because NER is just a foundational task, you can imagine researchers have started trying to do it efficiently way before Neural Nets. 

However, the neural networks gave a huge boost to how fast and accurate NER can be performed on a text. 
Note that recognizing and categorizing named entities is not as straightforward as you might think. 
One of the common challenges of NER is _segmentation_, or defining boundaries of the named entity (is "New York" one named entity or two separate ones?) 
Another, even trickier one, is categorizing the type of the entity. 
For example, the name Washington can be used to signify a person (such as the writer Washington Irving), a location (Washington DC), an organization (Washington Post) and even a sports team (as in "Washington won two games in the last season").

So you can see how the _context_ of the entity - both the words that came before it and after it, potentially much later in the sentence - matters. 
That's why the popular approaches to NER with neural networks include multi-level CNNs, bi-directional transformers such as BERT, or bi-directional LSTMs. 
The last one, combined with a technique called Conditional Random Weights (CRF) is what spaCy uses in its named entity recognition module. 

Of course, you don't have to know how to build neural networks in order to extract the named entities from a text. 
The 'ents' attribute of a `doc` object that gets created once you run spaCy on a text contains a list of all those named entities.

[source,python]
----
>>> doc = nlpsm(text)
>>> doc.ents[:6]  # <1>
(Timnit Gebru, Amharic, 13, May 1983, Ethiopian, Black in AI)
----
<1> Get the first 6 named entities in the Wikipedia article.

The challenge of named entity recognition is closely related to a more basic problem - part-of-speech (POS) tagging. 
To recognize named entities in the sentence, you need to know which part of speech each word belongs to. 
In English grammar, the _part of speech_ (POS) for a person, place or thing is "noun".
And your named entity will often be a proper noun - a noun that refers to a _particular_ person, place or thing in the real world.
And the part of speech tag for relations is a _verb_.
The verb tokens will be used to connect the named entities to each other as the edges in your knowledge graph.

Part-of-speech tagging is also crucial to the next stage in our pipeline - dependency parsing. 
To determine the relationships between different entities inside the sentence, we will need to recognize the verbs in our sentence.

Luckily, spaCy already did that for you the moment you fed the text to it. 

[source,python]
----
>>> first_sentence = list(doc.sents)[0]
>>> ' '.join(['{}_{}'.format(tok, tok.pos_) for tok in first_sentence])
 'Timnit_PROPN Gebru_PROPN (_PUNCT Amharic_PROPN :_PUNCT ትምኒት_NOUN ገብሩ_ADV ;_PUNCT Tigrinya_PROPN :_PUNCT  _SPACE ትምኒት_NOUN ገብሩ_PROPN )_PUNCT born_VERB 13_NUM May_PROPN 1983_NUM is_AUX an_DET Eritrean_ADJ Ethiopian_PROPN -_PUNCT born_VERB computer_NOUN scientist_NOUN who_PRON works_VERB on_ADP algorithmic_ADJ bias_NOUN and_CCONJ data_NOUN mining_NOUN ._PUNCT'
----

Can you make sense of this?
PUNCT, NOUN and VERB are pretty self-explanatory; and you can guess that PROPN stands for Proper Noun. 
But what about CCONJ?
Luckily, you can let spaCy explain it to you.

[source,python]
----
>>> spacy.explain('CCONJ')
'coordinating conjunction'
----

Another tool spaCy gives you is the `tag_` property of each token. 
While the `pos_` tag gives you the part-of-speech or a particular token, the `tag_` gives you more information and details about the token. 
Let's see an example: 

[source,python]
----
>>> ' '.join(['{}_{}'.format(tok, tok.tag_) for tok in first_sentence])
'Timnit_NNP Gebru_NNP (_-LRB- Amharic_NNP :_: ትምኒት_NN ገብሩ_RB ;_: Tigrinya_NNP :_:  __SP ትምኒት_NN ገብሩ_NNP )_-RRB- born_VBN 13_CD May_NNP 1983_CD is_VBZ an_DT Eritrean_JJ Ethiopian_NNP -_HYPH born_VBN computer_NN scientist_NN who_WP works_VBZ on_IN algorithmic_JJ bias_NN and_CC data_NNS mining_NN ._.'
----

Wow, this looks much more cryptic. 
You can vaguely intuit the connection between PROPN and NNP, but what is VBZ?

[source,python]
----
>>> spacy.explain('VBZ')
'verb, 3rd person singular present'
----
That's for sure much more information, albeit served in a more cryptical form. 

Let's bring all the information about your tokens together in one table. 

[source,python]
----
>>> import pandas as pd
>>> from collections import OrderedDict
>>>
>>> def token_dict(token):
...    return OrderedDict( TOK=token.text,
...        POS=token.pos_, TAG=token.tag_, 
...        ENT_TYPE=token.ent_type_, DEP=token.dep_,)
>>>
>>> def doc_df(doc):
...    return pd.DataFrame([token_dict(tok) for tok in doc])
>>>
>>> doc_df(doc)
            TOK    POS    TAG ENT_TYPE       DEP
0        Timnit  PROPN    NNP           compound  # <1>
1         Gebru  PROPN    NNP              nsubj
2             (  PUNCT  -LRB-              punct
3       Amharic  PROPN    NNP              appos
4             :  PUNCT      :              punct
         ...    ...    ...      ...       ...
3277     Timnit  PROPN    NNP      ORG  compound  # <2>
3278      Gebru  PROPN    NNP      ORG      pobj
3279         at    ADP     IN               prep
3280  Wikimedia  PROPN    NNP      FAC  compound  # <3>
3281    Commons  PROPN    NNP      FAC      pobj
----
<1> "Timnit Gebru" is not recognized as an entity
<2> "Timnit Gebru" is recognized as an entity, but misclassified as "organization"
<3> "Wikimedia" is misclassified as "Buildings, airports, highways, bridges, etc."

You can guess what the columns 'TOK', 'POS' and 'TAG' contain. 
The fourth column 'ENT_TYPE', gives us information about the type of our named entity.
You can see that the small spaCy model didn't do great - it missed Timnit Gebru as a named entity in the first words of the text. 
When it did recognize it later, it thought that its entity type was an organization. 

Let's see if a larger model would do better:

[source,python]
----
>>> nlp = load('en_core_web_lg')
>>> doc = nlp(text)
>>> doc_df(doc)
            TOK    POS    TAG ENT_TYPE       DEP
0        Timnit  PROPN    NNP   PERSON  compound
1         Gebru  PROPN    NNP   PERSON     nsubj
2             (  PUNCT  -LRB-              punct
3       Amharic  PROPN    NNP     NORP     appos
4             :  PUNCT      :              punct
         ...    ...    ...      ...       ...
3278     Timnit  PROPN    NNP   PERSON  compound
3279      Gebru  PROPN    NNP   PERSON      pobj
3280         at    ADP     IN               prep
3281  Wikimedia  PROPN    NNP      ORG  compound
3282    Commons  PROPN    NNP      ORG      pobj
----

This looks better! 
Timnit Gebru is classified as a PERSON, and Wikimedia is properly tagged as ORG. 

So this will usually be the first algorithm in your pipeline, the spaCy language model that tokenizes your text and tags each token with the linguistic features you need for knowledge extraction.

Once you understand how a named entity recognizer works, you can expand the kinds of nouns and noun phrases you want to recognize and include in your knowledge graph.
This can help generalize your knowledge graph and help you create a more generally intelligent NLP pipeline.

But we haven't yet touched the last column of our dataframe, DEP, which indicates the token's role in the syntactic dependency tree.
Before we move to dependency parsing and relation extraction, we need to deal with step 2 of the pipeline - coreference resolution. 

== Coreference Resolution 

Imagine you're running NER on a text, and you obtain the list of entities that the model has recognized.
On closer inspection, you realize over half of them are duplicates because they're referring to the same terms!
This is where *Coreference resolution* comes in handy because it identifies all the mentions of a noun in a sentence.
This will consolidate mentions of the same _things_ in your knowledge graph instead of creating redundant nodes and edges and potentially creating incorrect relations.

Can you see the coreferences to "Timnit Gebru" in this sentence about that paper and her bosses:

[source,python]
----
>>> i0 = text.index('In a six-page')
>>> text_gebru = text[i0:i0+308]
>>> text_gebru
"In a six-page mail sent to an internal collaboration list, Gebru describes how she was summoned to a meeting at short notice where she was asked to withdraw the paper and she requested to know the names and reasons of everyone who made that decision, along with advice for how to revise it to Google's liking."
----

As a human, you can understand that "Gebru", "she" and "her" all relate. 
But it's trickier for a machine to recognize that, especially if "she" is mentioned before "Gebru" (a phenomenon called _cataphora_).

And that's a relatively simple case! 
Consider this sentence: "The city councilmen refused the demonstrators a permit because they feared violence".
Who does "they" in the sentence refer to?
Our common sense tells us that it refers to the "city councilmen" and the answer seems to be easy for us, but this task of identifying mentions using common sense is surprisingly difficult for deep learning models.
This task is called the Winograd schema challenge, also framed as "common-sense reasoning" or "common-sense inference" problem.

Let's see how NLP deals with this complicated task. 
Neural networks to the rescue! 

=== Coreference resolution with spaCy

NeuralCoref 4.0 was the fastest and most accurate entity resolver available in the open-source community.
So spaCy incorporated it into its "Universe" collection of pipelines and models.
NeuralCoref first uses spaCy for POS tagging and named entity recognition to extract the references or _mentions_ from the text. 
It then takes the words surrounding each mention and feeds them into feed-forward neural networks, which compute a score between each pair of mentions. 
Comparing these scores is how the network resolves what each mention refers to.

In order to use NeuralCoref you will need to import the `coreferee` package.
You cannot use the original `neuralcoref`` package within the latest spaCy package (3.5 or greater) because `neuralcoref` is no longer actively maintained.

You will also need to download another spaCy model, that's based on transformers:

[source,python]
----
>>> import spacy, coreferee
>>> nlptrf = spacy.load('en_core_web_trf')
>>> nlptrf.add_pipe('coreferee')
>>> doc_gebru = nlptrf(text_gebru)
>>> doc_gebru._.coref_chains.print()
0: Gebru(13), she(16), she(26), she(34)
1: advice(51), it(56)
----


=== Entity name normalization

Closely related to coreference resolution is the issue of _normalization_ of entities. 
The normalized representation of an entity is usually a string, even for numerical information such as dates.
For example, the normalized ISO format for Timnit Gebru's date of birth would be "1983-05-13".
A normalized representation for entities enables your knowledge base to connect all the different things that happened in the world on that same date to that same node (entity) in your graph.

You'd do the same for other named entities.
You'd correct the spelling of words and attempt to resolve ambiguities for names of objects, animals, people, places, and so on.
For example, San Francisco may be referred to, in different places as "San Fran", "SF", "'Frisco" or "Fog City".
Normalization of named entities ensures that spelling and naming variations don't pollute your vocabulary of entity names with confounding, redundant names.

A knowledge graph should normalize each kind of entity the same way, to prevent multiple distinct entities of the same type from sharing the same "name."
You don't want multiple person name entries in your database referring to the same physical person.
Even more importantly, the normalization should be applied consistently -- both when you write new facts to the knowledge base or when you read or query the knowledge base.

If you decide to change the normalization approach after the database has been populated, the data for existing entities in the knowledge should be "migrated", or altered, to adhere to the new normalization scheme.
Schemaless databases (key-value stores), like the ones used to store knowledge graphs or knowledge bases, are not free from the migration responsibilities of relational databases.
After all, schemaless databases are interface wrappers for relational databases under the hood.


== Dependency Parsing

In the previous section, you learned how to recognize and tag named entities in text.
Now you'll learn how to find relationships between these entities.
A typical sentence may contain several named entities of various types, such as geographic entities, organizations, people, political entities, times (including dates), artifacts, events, and natural phenomena.
And a sentence can contain several _relations_, too -- facts about the relationship between the named entities in the sentence

NLP researchers have identified two separate problems or models that can be used to identify how the words in a sentence work together to create meaning: _dependency parsing_ and _constituency parsing_.
_Dependency parsing_ will give your NLP pipelines the ability to diagram sentences like you learned to do in grammar school (elementary school).
And these tree data structures give your model a representation of the logic and grammar of a sentence.
This will help your applications and bots become a bit smarter about how they interpret sentences and act on them.

_Constituency parsing_ is another technique, and it's concerned with identifying the _constituent subphrases_ in a sentence. 
While dependency parsing deals with relationships between words, constituency parsing aims to parse a sentence into a series of constituents. 
These constituents can be, for example, a noun phrase ("My new computer") or a verb phrase ("has memory issues").
Its approach is more top-down, trying to iteratively break constituents into smaller units and relationships between them. 
Though constituency parsing can capture more syntactic information about the sentence, its results are slower to compute and more difficult to interpret. 
So we will focus on dependency parsing for now.  

But wait, you're probably wondering why understanding relationships between entities and sentence diagrams are so important.
After all, you've probably already forgotten how to create them yourself and have probably never used them in real life.
But that's only because you've internalized this model of the world.
We need to create that understanding in bots so they can be used to do the same things you do without thinking, from simple tasks like grammar checking to complex virtual assistants. 

Basically, dependency parsing will help your NLP pipelines for all those applications mentioned in Chapter 1... better.
Have you noticed how chatbots like GPT-3 often fall on their face when it comes to understanding simple sentences or having a substantive conversation?
As soon as you start to ask them about the logic or reasoning of the words they are "saying" they stumble.
Chatbot developers and conversation designers get around this limitation by using rule-based chatbots for substantive conversations like therapy and teaching.
The open-ended neural network models like PalM and GPT-3 are only used when the user tries to talk about something that hasn't yet been programmed into it.
And the language models are trained with the objective of steering the conversation back to something that the bot knows about and has rules for.

Dependency parsing, as the name suggests, relies on "dependencies" between the words in a sentence to extract information.
"Dependencies" between two words could refer to their grammatical, phrasal, or any custom relations.
But in the context of dependency parse trees, we refer to the grammatical relationships between word pairs of the sentence, one of them acting as the "head" and the other one the "dependent".
There exists one word in the sentence that isn't dependent on any other word in the parse tree, and this word is called the ROOT.
There are 37 "dependent" relations that a word could possibly have, and these relations are adapted from the *Universal Stanford Dependencies system*.

The spaCy package knows how to recognize these relations between words and phrases, and even plot the dependency diagrams for you. 
Let's try to do dependency parsing of a single sentence: 

[source,python]
----
>>> text = "Gebru was unethically fired from her Ethical AI team."
>>> doc_df(nlp(text))
           TOK    POS   TAG ENT_TYPE        DEP
0        Gebru  PROPN   NNP   PERSON  nsubjpass
1          was    AUX   VBD             auxpass
2  unethically    ADV    RB              advmod
3        fired   VERB   VBN                ROOT
4         from    ADP    IN                prep
5          her   PRON  PRP$                poss
6      Ethical  PROPN   NNP      ORG   compound
7           AI  PROPN   NNP      ORG   compound
8         team   NOUN    NN                pobj
9            .  PUNCT     .               punct
----

You can see that the ROOT of the sentence is the verb "fired".
This is because in our sentence, the word "fired" happens to be the main verb when you organize it into a Subject-Verb-Object triple.

And the role of the word "Gebru" serves as "nominal subject" of the sentence. 
Is there a dependency between them?

[source,python]
----
>>> print ("{:<11} | {:<9} | {:<25} | {:<20}"
...        .format('Token','Relation', 'Children', 'Meaning'))
>>> print ("-" * 75)

>>> for token in doc:
...    print ("{:<12} | {:<9} | {:<25} | {:<20}"
...            .format(str(token.text),
                    str(token.dep_), 
                    str([child for child in token.children]) ,
                     str(spacy.explain(token.dep_))[:17] ))

Token       | Relation  | Children                  | Meaning             
------------------------------------------------------------------------
Gebru        | nsubjpass | []                        | nominal subject (   
was          | auxpass   | []                        | auxiliary (passiv   
unethically  | advmod    | []                        | adverbial modifie   
fired        | ROOT      | [Gebru, was, unethically, from, .] | root                
from         | prep      | [team]                    | prepositional mod   
her          | poss      | []                        | possession modifi   
Ethical      | compound  | []                        | compound            
AI           | compound  | [Ethical]                 | compound            
team         | pobj      | [her, AI]                 | object of preposi   
.            | punct     | []                        | punctuation 
----

As expected, the ROOT has the most dependents - such as "Gebru", the subject of the sentence, and "unethically" (the adverb that describes the word "fired").
You can also see that every word, except the ROOT, is a child of another token. 
But surely, there is a better way to represent these relations?

Time for dependency diagrams to shine!
We'll use one of spaCy's sub-libraries called `displacy`.
It can generate a _scalable vector graphics_ SVG string (or a complete HTML page), which can be viewed as an image in a browser.
This visualization can help you find ways to use the tree to create tag patterns for relation extraction.

.Visualize a dependency tree
[source,python]
----
>>> from spacy.displacy import render
>>> sentence = "In 1541 Desoto wrote in his journal about the Pascagoula."
>>> parsed_sent = nlp(sentence)
>>> with open('pascagoula.html', 'w') as f:
...     f.write(render(docs=parsed_sent, page=True, options=dict(compact=True)))
----

When you open the file, you should see something like Figure <<figure-dependency-diagram>>.

[id=figure-dependency-diagram, reftext={chapter}.{counter:figure}]
.Dependency diagram for a sentence
image::../images/ch11/dependency_diagram.png[alt="Dependency tree for sentence about Timnit Gebru's firing",width=80%,link="../images/ch11/dependency_diagram.png"]

Before we explain the connection between dependency parsing and relation extraction, let's briefly dive into another tool at our disposal - constituency parsing.

=== Constituency parsing with `benepar`

Berkeley Neural Parser and Stanza have been the go-to options for the extraction of constituency relations in text.
Let's explore one of them, Berkeley Neural Parser.

This parser cannot be used on its own and requires either spaCy or NLTK to load it along with their existing models.
You want to use spaCy as your tokenizer and dependency tree parse because it is continually improving.

.Download the necessary packages
[source,python]
----
>>> import benepar
>>> benepar.download('benepar_en3')
----

After downloading the packages, we can test it out with a sample sentence.
But we will be adding `benepar` to spaCy's pipeline first.

[source,python]
----
>>> import spacy
>>> nlp = spacy.load("en_core_web_md")
>>> if spacy.__version__.startswith('2'):
...     nlp.add_pipe(benepar.BeneparComponent("benepar_en3"))
... else:
...     nlp.add_pipe("benepar", config={"model": "benepar_en3"})
>>> doc = nlp("She and five others coauthored a research paper,'On the
      Dangers of Stochastic Parrots:  Can Language Models Be Too Big?'")
>>> sent = list(doc.sents)[0]
>>> print(sent._.parse_string)
(S (NP (NP (PRP She)) (CC and) (NP (CD five) (NNS others))) (VP (VBD coauthored) (NP (NP (DT a) (NN research) (NN paper)) (, ,) (`` ') (PP (IN On) (NP (NP (DT the) (NNS Dangers)) (PP (IN of) (NP (NNP Stochastic) (NNPS Parrots))))) (: :) (MD Can) (NP (NN Language) (NNS Models)) (VP (VB Be) (ADJP (RB Too) (JJ Big))))) (. ?) ('' '))
----

Looks quite cryptic, right? 
In the example above, we generated a parsed string for the test sentence. The parse string includes various phrases and the POS tags of the tokens in the sentence. Some common tags you may notice in our parse string are NP ("Noun Phrase"), VP ("Verb Phrase"), S ("Sentence"), and PP ("Prepositional Phrase").
Now you can see how it's a bit more difficult to extract information from the constituency parser's output.
However, it can be useful to identify all the phrases in the sentence and use them in sentence simplification and/or summarization.

You now know how to extract the syntactic structure of sentences. 
How will it help you in your quest for an intelligent chatbot?

== From dependency parsing to relation extraction

We've come to the crucial stage of helping our bot learn from what it reads.
Take this sentence from Wikipedia:

_In 1983, Stanislav Petrov, a lieutenant colonel of the Soviet Air Defense Forces, saved the world from nuclear war._

If you were to take notes in a history class after reading or hearing something like that, you'd probably paraphrase things and create connections in your brain between concepts or words.
You might reduce it to a piece of knowledge, that thing that you "got out of it."
You'd like your bot to do the same thing.
You'd like it to "take note" of whatever it learns, such as the fact or knowledge that Stanislav Petrov was a lieutenant colonel.
This could be stored in a data structure something like this:

[source,python]
----
('Stanislav Petrov', 'is-a', 'lieutenant colonel')
----

This is an example of two named entity nodes ('Stanislav Petrov' and 'lieutenant colonel') and a relation or connection ('is a') between them in a knowledge graph or knowledge base.
When a relationship like this is stored in a form that complies with the RDF standard (resource description format) for knowledge graphs, it's referred to as an RDF triplet.
Historically these RDF triplets were stored in XML files, but they can be stored in any file format or database that can hold a graph of triplets in the form of `(subject, relation, object)`.
A collection of these triplets will be your knowledge graph!

Let's go ahead and create some fodder for your knowledge graph using the two approaches we know - patterns and machine learning. 

=== Pattern-based relation extraction

Remember how to extract character patterns, you used regular expressions?
Word patterns are just like regular expressions but for words instead of characters.
Instead of character classes, you have word classes.
For example, instead of matching a lowercase character, you might have a word pattern decision to match all the singular nouns ("NN" POS tag).footnote:[spaCy uses the "OntoNotes 5" POS tags: (https://spacy.io/api/annotation#pos-tagging)]
Some seed sentences are tagged with some correct relationships (facts) extracted from those sentences.
A POS pattern can be used to find similar sentences where the subject and object words might change or even the relationship words.

The simplest way to extract relations out of the text is to look for all  "Subject-Verb-Object" triplets using the "nsubj" and "dobj" tags of the ROOT word.
But let's do something a bit more complex. 
What if we want to extract information about meetings between historical figures from Wikipedia?
You can use the spaCy package in two different ways to match these patterns in latexmath:[O(1)] (constant time) no matter how many patterns you want to match:

* PhraseMatcher for any word/tag sequence patterns footnote:[See the web page titled "Code Examples : spaCy Usage Documentation" (https://spacy.io/usage/examples#phrase-matcher).]
* Matcher for POS tag sequence patterns footnote:[See the web page titled "Matcher : spaCy API Documentation" (https://spacy.io/api/matcher).]

Let's start with the latter. 

First, let's look at an example sentence and see the POS for every word: 
.Helper functions for spaCy tagged strings
[source,python]
----
>>> doc_dataframe(nlp("In 1541 Desoto met the Pascagoula."))
         ORTH       LEMMA    POS  TAG    DEP
0          In          in    ADP   IN   prep
1        1541        1541    NUM   CD   pobj
2      Desoto      desoto  PROPN  NNP  nsubj
3         met        meet   VERB  VBD   ROOT
4         the         the    DET   DT    det
5  Pascagoula  pascagoula  PROPN  NNP   dobj
6           .           .  PUNCT    .  punct
----

Now you can see the sequence of POS or TAG features that will make a good pattern.
If you're looking for "has-met" relationships between people and organizations, you'd probably like to allow patterns such as "PROPN met PROPN", "PROPN met the PROPN", "PROPN met with the PROPN", and "PROPN often meets with PROPN".
You could specify each of those patterns individually, or try to capture them all with some * or ? operators on "any word" patterns between your proper nouns:

[source,]
----
'PROPN ANYWORD? met ANYWORD? ANYWORD? PROPN'
----

Patterns in spaCy are much more powerful and flexible than the preceding pseudocode, so you have to be more verbose to explain exactly the word features you'd like to match.
In a spaCy pattern specification, you use a dictionary to capture all the parts-of-speech that you want to match for each token or word.

[source,python]
.Example spaCy POS pattern
----
>>> pattern = [
...     {'POS': {'IN': ['NOUN', 'PROPN']}, 'OP': '+'},
...     {'IS_ALPHA': True, 'OP': '*'},
...     {'LEMMA': 'meet'},
...     {'IS_ALPHA': True, 'OP': '*'},
...     {'POS': {'IN': ['NOUN', 'PROPN']}, 'OP': '+'}]
----

You can then extract the tagged tokens you need from your parsed sentence.

.Creating a POS pattern matcher with spaCy
[source,python]
----
>>> from spacy.matcher import Matcher
>>> doc = nlp("In 1541 Desoto met the Pascagoula.")
>>> matcher = Matcher(nlp.vocab)
>>> matcher.add(
...     key='met',
...     patterns=[pattern])
>>> matches = matcher(doc)
>>> matches
[(12280034159272152371, 2, 6)]  # <1>
>>> start = matches[0][1]
>>> stop = matches[0][2]
>>> doc[start:stop]  # <2>
Desoto met the Pascagoula
----
<1> list of 3-tuples with span ID, start token index, stop token index
<2> SpaCy lets you slice a document object on token indices just as you would for a Python list

A spacy matcher will list the pattern matches as 3-tuples containing match ID integers, plus the start and stop token indices (positions) for each match.
So you extracted a match from the original sentence from which you created the pattern, but what about similar sentences from Wikipedia?

.Using a POS pattern matcher
[source,python]
----
>>> doc = nlp("October 24: Lewis and Clark met their" \
...     "first Mandan Chief, Big White.")
>>> m = matcher(doc)[0]
>>> m
(12280034159272152371, 3, 11)

>>> doc[m[1]:m[2]]
Lewis and Clark met their first Mandan Chief

>>> doc = nlp("On 11 October 1986, Gorbachev and Reagan met at Höfði house")
>>> matcher(doc)
[]  # <1>
----
<1> The pattern doesn't match any substrings of the sentence from Wikipedia.

You need to add a second pattern to allow for the verb to occur after the subject and object nouns.

.Combine patterns together to handle more variations
[source,python]
----
>>> doc = nlp(
...     "On 11 October 1986, Gorbachev and Reagan met at Hofoi house"
...     )
>>> pattern = [
...     {'POS': {'IN': ['NOUN', 'PROPN']}, 'OP': '+'},
...     {'LEMMA': 'and'},
...     {'POS': {'IN': ['NOUN', 'PROPN']}, 'OP': '+'},
...     {'IS_ALPHA': True, 'OP': '*'},
...     {'LEMMA': 'meet'}
...     ]
>>> matcher.add('met', None, pattern)  # <1>
>>> matches = matcher(doc)
>>> pd.DataFrame(matches, columns=)
[(1433..., 5, 9),
 (1433..., 5, 11),
 (1433..., 7, 11),
 (1433..., 5, 12)]  # <2>

>>> doc[m[-1][1]:m[-1][2]]  # <3>
Gorbachev and Reagan met at Hofoi house
----
<1> This adds an additional pattern without removing the previous pattern.
<2> The '+' operators increase the number of overlapping alternative matches.
<3> The longest match is the last one in the list of matches.

So now you have your entities and a relationship.
You can even build a pattern that is less restrictive about the verb in the middle ("met") and more restrictive about the names of the people and groups on either side.
Doing so might allow you to identify additional verbs that imply that one person or group has met another, such as the verb "knows" or even passive phrases such as "had a conversation" or "became acquainted with".
Then you could use these new verbs to add relationships for new proper nouns on either side.

But you can see how you're drifting away from the original meaning of your seed relationship patterns.
This is called semantic drift.
To ensure that the new relations found in new sentences are truly analogous to the original seed (example) relationships, you often need to constrain the subject, relation, and object word meanings to be similar to those in the seed sentences.
The best way to do this is with some vector representation of the meaning of words.
Fortunately for you, spaCy tags words in a parsed document with not only their POS and dependency tree information but also provides the Word2Vec word vector.
You can use this vector to prevent the connector verb and the proper nouns on either side from drifting too far away from the original meaning of your seed pattern.footnote:[This is the subject of active research: https://nlp.stanford.edu/pubs/structuredVS.pdf.]

Using semantic vector representations for words and phrases has made automatic information extraction accurate enough to build large knowledge bases automatically.
But human supervision and curation are required to resolve much of the ambiguity in natural language text.


=== Neural relation extraction

Now that you've seen the pattern-based method for relation extraction, you can imagine that researchers have already tried to do the same with a neural network. 
The neural relation extraction task is traditionally classified into two categories: closed and open.

In _closed_ relation extraction, the model extracts relations only from a given list of relation types.
The advantages of this are that we can minimize the risk of getting untrue and bizarre relation labels between entities which makes us more confident about using them in real life.
But the limitation is that it needs human labelers to come up with a list of relevant labels for every category of text, which as you can imagine, can get tedious and expensive.

In _open_ relation extraction, the model tries to come up with its own set of probable labels for the named entities in the text.
This is suitable for processing large and generally unknown texts like Wikipedia articles and news entries.

Over the past few years, experiments with Deep Neural Networks have given strong results on triplet extraction and subsequently, most of the research on the topic now follow neural methods.

Unfortunately, there aren't as many out-of-the-box solutions for relation extraction as there are for the previous stages of the pipeline. 
What's more, your relation-extraction is usually going to be pretty targeted.
In most cases, you wouldn't want to extract ALL possible relations between entities, but only those that are relevant to the task you're trying to perform.
For example, you might want to extract interactions between drugs from a set of pharmaceutical documents.

One of the state-of-the-art models that is used nowadays to extract relations is LUKE (Language Understanding with Knowledge-based Embeddings).
LUKE uses _entity-aware attention_ - meaning that its training data included information on whether each token is an entity or not. 
It was also trained to be able to "guess" a masked entity in a Wikipedia-based dataset (rather than just guessing all masked words, like BERT model was trained).

SpaCy also includes some infrastructure to create your own relation extraction component, but that requires quite a bit of work. 
We won't cover it as part of this book. 
Fortunately, authors like Sofie Van Landeghem have created great resources footnote:["Implementing a custom trainable component for relation extraction": (https://explosion.ai/blog/relation-extraction)] for you to learn from if you want to custom-train a relation extractor for your particular needs. 

==== Training your relation extraction model

When training your relation extractor, you will need labeled data where the relations relative to your task are tagged properly in order for the model to learn to recognize them.
But big datasets are hard to create and label, so it's worth checking if some of the existing datasets used for benchmarking and finetuning state-of-the-art models already have the data that you need. 

DocRED and Stanford TACRED together are the de-facto benchmark datasets and models for relation extraction methods because of their size and the generality of the knowledge graphs

Stanford's Text Analysis Conference Relation Extraction Dataset (TACRED) contains more than 100,000 example natural language passages paired with their corresponding relations and entities.
It covers 41 relation types.
Over the past few years, researchers have improved TACRED's data quality and reduced ambiguity in the relation classes with datasets such as Re-TACRED and DocRED.

The Document Relation Extraction Dataset (DocRED) expands the breadth of natural language text that can be used for relation extraction because it includes relations that require parsing of multiple sentences of natural language text.
The training and validation dataset used to train DocRED is currently (in 2023) the largest human-annotated dataset for document-level relation extraction.
Most of the human-annotated knowledge graph data in DocRED is included in the Wikidata knowledge base.
And the corresponding natural language text examples can be found in the archived version of Wikipedia.

Now you have a better idea of how to take an unstructured text and turn it into a collection of facts. 
Time for the last stage of our pipeline - building a knowledge database.  

== Building your knowledge base

So, you have your relations extracted from your text. 
You could put them all into a big table; and yet, we keep talking about knowledge _graphs_. 
What really makes this particular way of structuring data so powerful?

Let's go back to Stanislav Petrov, whom we've met in the last chapter. 
What if we wanted to answer a question like "What is Stanislav Petrov's military rank?"
This is a question that a single relation triple (('Stanislav Petrov', 'is-a', 'lieutenant colonel')) isn't enough to answer - because your question-answering machine also needs to know that "lieutenant colonel" is a military rank. 
However, if you organize your knowledge as a graph, answering the question becomes possible.
Take a look at Figure <<figure-stanislav-knowledge-graph>> to understand how it happens. 

[id=figure-stanislav-knowledge-graph, reftext={chapter}.{counter:figure}]
.Stanislav knowledge graph
image::../images/ch11/Stanislav-Knowledge-Graph.png[Stanislav Knowledge Graph showing 'is-a' and 'is-famous-for' relations extracted, width=80%, link="../images/ch11/Stanislav-Knowledge-Graph.png"]

The red edge and node in this knowledge graph represent a fact that could not be directly extracted from the statement about Stanislav.
But this fact that "lieutenant colonel" is a military rank could be inferred from the fact that the title of a person who is a member of a military organization is a military rank.
This logical operation of deriving facts from a knowledge graph is called knowledge graph _inference_.
It can also be called querying a knowledge base, analogous to querying a relational database.
A whole field called Knowledge Base Question Answering is focused on finding ways to answer questions like this (they are called "multi-hop questions") more efficiently. 

For this particular inference or query about Stanislov's military ranks, your knowledge graph would have to already contain facts about militaries and military ranks.
It might even help if the knowledge base had facts about the titles of people and how people relate to occupations (jobs).
Perhaps you can see now how a base of knowledge helps a machine understand more about a statement than it could without that knowledge.
Without this base of knowledge, many of the facts in a simple statement like this will be "over the head" of your chatbot.
You might even say that questions about occupational rank would be "above the pay grade" of a bot that only knew how to classify documents according to randomly allocated topics. (See Chapter 4 if you've forgotten about how random topic allocation can be.)

It may not be obvious how big a deal this is, but it is a _BIG_ deal.
If you've ever interacted with a chatbot that doesn't understand "which way is up", literally, you'd understand.
One of the most daunting challenges in AI research is the challenge of compiling and efficiently querying a knowledge graph of common sense knowledge.
We take common-sense knowledge for granted in our everyday conversations.

Humans start acquiring much of their common sense knowledge even before they acquire language skills.
We don't spend our childhood writing about how a day begins with light and sleep usually follows sunset.
And we don't edit Wikipedia articles about how an empty belly should only be filled with food rather than dirt or rocks.
This makes it hard for machines to find a corpus of common sense knowledge to read and learn from.
No common-sense knowledge Wikipedia articles exist for your bot to do information extraction on.
And some of that knowledge is instinct, hard-coded into our DNA.footnote:[There are hard-coded common-sense knowledge bases out there for you to build on. Google Scholar is your friend in this knowledge graph search.]

All kinds of factual relationships exist between things and people, such as "kind-of", "is-used-for", "has-a", "is-famous-for", "was-born", and "has-profession."
NELL, the Carnegie Mellon Never Ending Language Learning bot is focused almost entirely on the task of extracting information about the `'kind-of'` relationship.

Most knowledge bases normalize the strings that define these relationships, so that "kind of" and "type of" would be assigned a normalized string or ID to represent that particular relation.
And some knowledge bases also normalize the nouns representing the objects in a knowledge base, using coreference resolution that we described before. 
So the bigram "Stanislav Petrov" might be assigned a particular ID.
Synonyms for "Stanislav Petrov", like "S. Petrov" and "Lt Col Petrov", would also be assigned to that same ID, if the NLP pipeline suspected they referred to the same person.

=== A large knowledge graph

If you've ever heard of a "mind map" they can give a pretty good mental model of what knowledge graphs are: connections between concepts in your mind.
To give you a more concrete mental model of the concept of knowledge graphs you probably want to explore the oldest public knowledge graph on the web: NELL (Never Ending Language Learning) graph, created by the bot we met in the last section.

The NLPiA2 Python package has several utilities for making the NELL knowledge graph a bit easier to wrap your head around.
Later in the chapter, you'll see the details about how these work so you can prettify whatever knowledge graph you are working with.

[source,python]
----
>>> import pandas as pd
>>> pd.options.display.max_colwidth = 20
>>> from nlpia2.nell import read_nell_tsv, simplify_names
>>> df = read_nell_tsv(nrows=1000)
>>> df[df.columns[:4]].head()
                entity            relation                value iteration
0  concept:biotechc...     generalizations  concept:biotechc...      1103
1  concept:company:...  concept:companyceo  concept:ceo:lesl...      1115
2  concept:company:...     generalizations  concept:retailstore      1097
3  concept:company:...     generalizations      concept:company      1104
4  concept:biotechc...     generalizations  concept:biotechc...      1095
----

The entity names are very precise and well-defined within a hierarchy, like paths for a file or name-spaced variable names in Python.
All of the entity and value names start with "concept:" so you can strip that from your name strings to make the data a bit easier to work with.
To simplify things further, you can eliminate the namespacing hierarchy and focus on just the last name in the hierarchy.

[source,python]
----
>>> pd.options.display.max_colwidth = 40
>>> df['entity'].str.split(':').str[1:].str.join(':')
0        biotechcompany:aspect_medical_systems
1                       company:limited_brands
2                       company:limited_brands
3                       company:limited_brands
4                biotechcompany:calavo_growers
                        ...
>>> df['entity'].str.split(':').str[-1]
0        aspect_medical_systems
1                limited_brands
2                limited_brands
3                limited_brands
4                calavo_growers
                 ...
----

The `nlpia2.nell` module simplifies the names of things even further.
This makes it easier to navigate the knowledge graph in a network diagram.
Otherwise, the names of entities can fill up the width of the plot and crowd each other out.

[source,python]
----
>>> df = simplify_names(df)  # <1>
>>> df[df.columns[[0, 1, 2, 4]]].head()
                   entity relation           value   prob
0  aspect_medical_systems     is_a  biotechcompany  0.924
1          limited_brands      ceo   leslie_wexner  0.938
2          limited_brands     is_a     retailstore  0.990
3          limited_brands     is_a         company  1.000
4          calavo_growers     is_a  biotechcompany  0.983
----
<1> Uses the `str.replace()` method to shorten the names of the entities, relations, and values

NELL scrapes text from Twitter, so the spelling and wording of facts can be quite varied.
In NELL the names of entities, relations and objects have been normalized by lowercasing them and removing all punctuation like apostrophes and hyphens.
Only proper names are allowed to retain their spaces, to help distinguish between names that contain spaces and those that are smashed together.
However, in NELL, just as in Word2vec token identifiers, proper names are joined with underscore ("\_") characters.

Entity and relation names are like variable names in Python.
You want to be able to query them like field names in a database, so they should not have ambiguous spellings.
The original NELL dataset contains one row per triple (fact).
Triples can be read like a terse, well-defined sentence.
Knowledge triples describe a single isolated fact about the world.
They give you one piece of information about an entity (object) in the world.

As a minimum, a knowledge triple consists of an entity, relation and value.
The first element of a knowledge triple gives you the name of the entity that the fact is about.
The second column, "relation," contains the relationship to some other quality (adjective) or object (noun) in the world called its value.
A relation is usually a verb phrase that starts with or implies words like "is" or "has."
The third column, "value," contains an identifier for some quality of that relation.
The "value" is the object of the relationship and is a named entity just as the subject ("entity") of the triple is.

Because NELL crowdsources the curation of the knowledge base, you also have a probability or confidence value that you can use to make inferences on conflicting pieces of information.
And NELL has 9 more columns of information about the fact.
It lists all the alternative phrases that were used to reference a particular entity, relation or value.
NELL also identifies the iteration (loop through Twitter) that the fact was created during.
The last column provides the source of the data - a list of all the texts that created the fact.

NELL contains facts about more than 800 unique relations and more than 2 million entities.
Because Twitter is mostly about people, places and businesses, it's a good knowledge base to use to augment a common sense knowledge base.
And it can be useful for doing fact-checking about famous people or businesses and places that are often the targets of misinformation campaigns.
There's even a "latitudelongitude" relation that you could use to verify any facts related to the location of things.

[source,python]
----
>>> islatlon = df['relation'] == 'latlon'
>>> df[islatlon].head()
               entity relation                 value
241          cheveron   latlon      40.4459,-79.9577
528        licancabur   latlon   -22.83333,-67.88333
1817             tacl   latlon     13.53333,37.48333
2967            okmok   latlon  53.448195,-168.15472
2975  redoubt_volcano   latlon   60.48528,-152.74306
----

Now you have learned how facts can be organized into a knowledge graph. 
But what do we do when we need to use this knowledge - for example, for answering questions? 
That's what we'll be dealing with in the last section of this chapter.

== Finding answers in a knowledge graph

Now that our facts are all organized in a graph database, how do we retrieve that knowledge?
As with any database, graph databases have special query languages to pull information from them. 
Just as SQL and its different dialects are used to query relational databases, a whole family of languages such as SPARQL (SPARQL Protocol and RDF Query Language), Cypher, and AQL exist to query graph databases. 
In this book, we'll focus on SPARQL, as it was adopted as a standard by the open-source communities. 
Other languages, such as Cypher or AQL, are used to query specific graph knowledge bases, such as Neo4j and ArangoDB.

As our knowledge base, we'll use an even bigger knowledge graph than NELL: Wikidata, the knowledge database version of Wikipedia.
It contains more than 100 million data items (entities and relations) and is maintained by volunteer editors and bots, just like all the other Wikimedia projects. 

In Wikidata, the relations between entities are called _properties_. 
There are more than 11,000 properties in Wikidata system, and each one has its "P-id", a unique identifier that is used to represent that property in queries. 
Similarly, every entity has its own unique "Q-id".
You can easily retrieve the Q-id of any Wikipedia article by using Wikidata's REST API:

[source,python]
----
>>> def get_wikidata_qid(wikiarticle, wikisite="enwiki"):
...     WIKIDATA_URL='https://www.wikidata.org/w/api.php'
...     resp = requests.get(WIKIDATA_URL, timeout=5, params={
...         'action': 'wbgetentities',
...         'titles': wikiarticle,
...         'sites': wikisite,
...         'props': '',
...         'format': 'json'
...     }).json()
...     return list(resp['entities'])[0]

>>> tg_qid = get_wikidata_qid('Timnit Gebru')
>>> tg_qid
'Q59753117'
----

You can confirm your findings by heading to (http://www.wikidata.org/entity/Q59753117) and finding there more properties of this entity, that link it to different entities. 
As you can see, this is a simple "GET" query that only works if we already have the entity's name and want to find the Q-id (or vice-versa).
For more complex queries, we will need to use SPARQL.
Let's write your first query then!

Let's say you want to find out who were Timnit Gebru's co-authors on her notable paper about Stochastic Parrots. 
If you don't remember the name of the paper exactly, you can actually find it with a simple query.
For this, you'll need a couple of property and entity IDs - for simplicity, we just list them in the code. 

[source,python]
----
>>> NOTABLE_WORK_PID = 'P800'     # <1>
>>> INSTANCE_OF_PID = 'P31'       # <2>
>>> SCH_ARTICLE_QID= 'Q13442814'  # <3>
>>> query = f"""
...     SELECT ?article WHERE {{
...         wd:{tg_qid} wdt:{NOTABLE_WORK_PID} ?article.
...         ?article wdt:{INSTANCE_OF_PID} wd:Q13442814.
...
...         SERVICE wikibase:label {{ bd:serviceParam
...                            wikibase:language "en". }}
...         }}
... """
----
<1> "notable work" property id
<2> "instance of" property id
<3> "scholarly article" entity id

[IMPORTANT]
====
Don't forget to double escape the curly braces in f-strings!
And you cannot use a backslash as an escape character in f-strings.
_WRONG_: f"\{"
Instead you must double the curly braces.
_RIGHT_: f"{{"

And if you are familiar with the `jinja2` package, be careful mixing using Python f-strings to populate jinja2 templates, you would need four curly braces to create a literal double curly brace.
====

Cryptic at the first sight, what this query means is "Find an entity A such that Timnit Gebru has A as notable work, and also A is an instance of an academic article".
You can see how each relational condition is codified in SPARQL, with operand `wd:` preceding entity Q-ids and the operand `wdt:` preceding property P-ids. 
Each relation constraint has a form of "ENTITY has-property ENTITY".

Let's now use WIKIDATA's SPARQL API to retrieve the results of our query. 
For this, we will use a dedicated `SPARQLWrapper` package that will simplify the process of querying for us. 
First, let's set up our wrapper:

[source,python]
----
>>> from SPARQLWrapper import SPARQLWrapper, JSON
>>>
>>> endpoint_url = "https://query.wikidata.org/sparql"
>>> sparql = SPARQLWrapper(endpoint_url)
>>> sparql.setReturnFormat(JSON)  # <1>
----
<1> Return query results as JSON string

Once that's set, you can execute your query and examine the response: 

[source,python]
----
>>> sparql.setQuery(query)
>>> result = sparql.queryAndConvert()
>>> result
{'head': {'vars': ['article', 'articleLabel']},
 'results': {'bindings': [{'article': {'type': 'uri',
     'value': 'http://www.wikidata.org/entity/Q105943036'},
    'articleLabel': {'xml:lang': 'en',
     'type': 'literal',
     'value': 'On the Dangers of Stochastic Parrots: 
     Can Language Models Be Too Big?🦜'}}]}}
----

This looks right! 
Now that you got the Q-id of the article - you can retrieve its authors by using the 'author' property of the article: 

[source,python]
----
>>> import re
>>> uri = result['results']['bindings'][0]['article']['value']
>>> match_id = re.search(r'entity/(Q\d+)', uri)
>>> article_qid = match_id.group(1)
>>> AUTHOR_PID = 'P50'
>>>
>>> query = f"""
...      SELECT ?author ?authorLabel WHERE {{
...      wd:{article_qid} wdt:{AUTHOR_PID} ?author.
...      SERVICE wikibase:label {{ bd:serviceParam wikibase:language "en". }}
...      }}
...      """
>>> sparql.setQuery(query)
>>> result = sparql.queryAndConvert()['results']['bindings']
>>> authors = [record['authorLabel']['value'] for record in result]
>>> authors
['Timnit Gebru', 'Margaret Mitchell', 'Emily M. Bender']
----

And here you have the answer to your question! 

Instead of doing two queries, we could have achieved the same result by nesting our queries, within each other, like this: 

[source,python]
----
>>> query = """
... SELECT ?author ?authorLabel WHERE {
...     {
...     SELECT ?article WHERE {
...         wd:Q59753117 wdt:P800 ?article.
...         ?article wdt:P31 wd:Q13442814.
...         }
...     }
...     ?article wdt:P50 ?author.
...     SERVICE wikibase:label {
...         bd:serviceParam wikibase:language "en".
...         }
... }
... """
----

SPARQL is a well-developed language whose functionality includes much more than just simple queries.
Wikidata itself has a pretty good manual on SPARQL.footnote:[Wikidata SPARQL tutorial: (https://www.wikidata.org/wiki/Wikidata:SPARQL_tutorial)]
The deeper you dig into Wikidata using SPARQL the more uses you will find for it in your NLP applications.
It is one of the only ways you can automatically evaluate the quality and correctness of the facts that your NLP pipeline asserts to your users.

=== From questions to queries 

So, you managed to find the answer to a pretty complex question in a knowledge database. 
That would have been pretty much impossible to do if your database was relational, or if all you had was unstructured text. 

However, looking for the answer took us quite a lot of work and two SPARQL queries. 
How do you transform a natural-language question into a query in a structured language like SPARQL?

You already did this kind of transformation before, back in Chapter 9.
Translating human language into machine language is a bit harder than translation between human languages, but it's still the same basic problem to a machine.
And now you know that transformers are good at transforming (pun intended) one language into another.
LLMs, being huge transformers, are especially good at it.
Sachin Charma created a great example of constructing a knowledge graph using another graph database, ArangoDB.
She used OpenAI's models to enable natural language question answering on the database he created.footnote[How to Build Knowledge Graph Enhanced Chatbot with ChatGPT and ArangoDB: (https://medium.datadriveninvestor.com/how-to-build-a-knowledge-graph-enhanced-chatbot-with-chatgpt-and-arangodb-f609be6073d5)]
// TODO MD: medium article links are paywalled (enshittified)

== Test yourself
* Give an example of a question that's easier to answer with a graph database than with a relational database.
* Convert a networkx directed graph to an edge list in a Pandas DataFrame with two columns `source_node` and `target_node`. How long does it take to retrieve all the target_node IDs for a single source node? What about all the target_nodes for those new source nodes? How would you speed up the Pandas graph query with an index?
* Create a Spacy Matcher that can more of Timnit Gebru's places of work out of the Wikipedia articles about her. How many could you retrieve?
* Is there anything a graph database can do that a relational database cannot? Can a relational database do anything that a graph database cannot?
* Use a Large Language Model to generate a SPARQL wikidata query from natural language. Did it work correctly without you editing the code? Will it work for a query that requires five relationship (edge) traversals in your knowledge graph?

== Summary

* A knowledge graph can be built to store relationships between entities.
* You can isolate and extract information from unstructured text using either rule-based methods (like regular expressions) or neural-based methods.
* Part-of-speech tagging allows you to extract relationships between entities mentioned in a sentence. 
