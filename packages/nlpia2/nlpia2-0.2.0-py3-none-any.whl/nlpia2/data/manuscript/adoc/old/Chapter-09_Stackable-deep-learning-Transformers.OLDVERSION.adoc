= Stackable deep learning (Transformers)
:chapter: 9
:part: 3
:imagesdir: .
:xrefstyle: short
:figure-caption: Figure {chapter}.
:listing-caption: Listing {chapter}.
:table-caption: Table {chapter}.
:stem: latexmath

This chapter covers

* Seeing how attention networks enable limitless "stacking" options for NLP
* Understanding what makes transformers so powerful
* Improving any NLU pipeline that relies on embeddings using sentence transformers
* Using transfer learning from popular transformers like BERT and GPT (GPT-2)
* Comparing GPT-Neo, GPT-J, BERT, PaLM, and T5
* Applying transformers to extractive and abstraction summarization of long documents
* Generating grammatically correct and interesting text with transformers
* Estimating the information capacity of a transformer network required for a particular problem

// References:
// - https://www.ibm.com/blogs/watson/2020/12/how-bert-and-gpt-models-change-the-game-for-nlp/
// - https://towardsdatascience.com/transformers-explained-understand-the-model-behind-gpt-3-bert-and-t5-cdbf3fc8a40a
// - https://arxiv.org/pdf/2104.08691.pdf
// - scaling laws for neural language models: https://arxiv.org/pdf/2001.08361.pdf
// - training compute (FLOPs) for 100s of models: https://arxiv.org/pdf/2202.05924.pdf
// - parameters and compute trends:  [Compute and Energy Consumption Trends in Deep Learning Inference](https://arxiv.org/pdf/2109.05472.pdf)

This book promised to use NLP to write some of the content of this book.
The First Edition attempted to do this with not-so-impressive results.
NLP technology has advanced a lot since we wrote the first edition and one of the most powerful advancements has been transformers.
Transformers now make it possible for us to make good on our promise to generate worthy content that can be incorporated into a book like this without you noticing.

== Problems with RNN

Encoder-decoder RNN (recurrent neural networks) don't work well for longer passages of text.
Even long sentences are a challenge for RNNs doing translation.footnote:[http://www.adeveloperdiary.com/data-science/deep-learning/nlp/machine-translation-using-attention-with-pytorch/]

// TODO: plot translation accuracy with sentence length using pretrained LSTM from huggingface and compare to BERT transformer


== Attention Revisited

In Chapter 10 you learned about sequence-to-sequence models for language translation where the encoders and decoders in the previous generation of state-of-the-art models are generally comprised of RNNs.
In such models, the encoder processes each element in the input sequence to distill the sentence into a fixed-length thought (or "context") vector which is then passed to the decoder.
This model architecture has challenges dealing with long sentences, as the context vector is not sufficient for encapsulating _all_ the important information of the sentence.
The Attention mechanism presented by Bahdanau et al footnote:[Neural Machine Translation by Jointly Learning to Align and Translate: https://arxiv.org/abs/1409.0473] to solve this issue is shown to improve sequence-to-sequence performance, particularly on long sentences, however it does not alleviate the time sequencing complexity of recurrent models.

The introduction of the Transformer model in the paper "Attention Is All You Need" footnote:[Attention Is All You Need: https://arxiv.org/abs/1706.03762], authored by researchers at Google Brain and Google Research  (Vaswani, Ashish et al. 2017), ushered in a new era of transduction models capable of achieving state-of-the-art performance on a variety of NLP tasks.
The Transformer heavily leverages a mechanism sometimes called _intra-attention_, or more commonly _self-attention_, to build global connections between input and output.
However, different from prior encoder-decoder architectures that use recurrence or convolution, the Transformer employs stacked blocks of attention layers and fully-connected feed forward layers for the encoder and decoder.
Without the time sequencing constraint of recurrent networks, many computations in Transformers can be parallelized and deep models can be trained comparatively more quickly than recurrent counterparts with impressive results.

In the next few sections, you'll walk through the fundamental concepts behind the transformer and take a look at the architecture of the model.
Then you will use the base pytorch implementation of the Transformer module to implement a language translation model, as this was the reference task in "Attention Is All You Need", to see how it is both powerful and elegant in design.

=== Self-Attention

Before the Transformer architecture, self-attention was used successfully in a multitude of tasks in including abstractive summarization, reading comprehension, and task independent sentence representations amongst others.
Self-attention takes the input embedding vectors and puts them through linear projections to create key, value and query vectors.
The query is used with the key to get a context vector for words related to the query.
This context vector is then used to get a weighted sum of values.
In practice, all these operations are done on sets of queries, keys, and values packed together in matrices, _Q_, _K_, and _V_, respectively.
Of the two types of attention functions commonly used, _additive_ and _dot-product_ attention, the one used in transformers is a scaled version of dot-production attention, in which the scalar products between the query _Q_ and the keys _K_, are scaled down by a factor related to the dimension of the model and the number of attention _heads_ in the multi-head mechanism.
We'll introduce the concept of multi-head attention shortly, but first let's look at the basic equation for computing the attention outputs over matrices _Q_, _K_, _V_.

.Equation 12.1 Self-attention outputs
[latexmath]
++++
Attention(Q, K, V ) = softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V
++++

To counter the effect of large dot products forcing small gradients in the softmax, the product of the query and key matrices is scaled by latexmath:[\frac{1}{\sqrt{d_{k}}}].
The softmax normalizes the resulting vectors so that they are all positive and sum to 1.
This "scoring" matrix is then multiplied with the values matrix to get a weighted values matrix as shown below.

.Self-Attention
image::../images/Ch12/transformer_attention.png[alt="Attention",width=100%,align="center",link="../images/ch12/transformer_attention.png"]

Unlike, RNNs where there is recurrence and shared weights, in self-attention all of the vectors used in the query, key, and value matrices come from the input sequences' embedding vectors.
The entire mechanism can be implemented with highly optimized matrix multiplication operations.


=== Multi-Head Self-Attention
Perhaps the most ground-breaking and novel feature of the transformer is the concept of multi-head self-attention.
The authors linearly project the query, key, and value matrices _n_ times ("heads") with different latexmath:[d_q] , latexmath:[d_k], and latexmath:[d_v] dimensions and compute the attention function on all in parallel.
The latexmath:[d_v]-dimensional outputs are concatenated and again projected with a latexmath:[W^o] matrix as shown in the next equation.

.Equation 12.2 Multi-Head Self-Attention outputs
[latexmath]
++++
MultiHeadAttention(Q, K, V ) = Concat(head_1, ..., head_n) W^o\\
                  where\ head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
++++

The multiple heads allow the model to focus on different positions, not just ones centered about the word at a given position, and to generalize to different subspaces.
In the paper, the model uses _n_=8 attention heads such that latexmath:[d_k = d_v = \frac{d_{model}}{n} = 64].
The reduced dimensionality in the multi-head setup is to ensure the computation and concatenation cost is nearly equivalent with size of a full-dimensional single attention head.

.Multi-Head Self-Attention
image::../images/Ch12/transformer_multihead_attention.png[alt="Multi-Head Self-Attention",width=80%,align="center",link="../images/ch12/transformer_multihead_attention.png"]

=== Positional Encodings
Word order in the input sentences matter, so another facility that is required is a way to bake in some positional meaning with the word embeddings.
A positional encoding is simply a function that adds information about the relative or absolute position of a word in a sequence to the input embeddings.
The encodings have the same dimension, latexmath:[d_{model}], as the input embeddings so they can be summed with the embedding vectors.
The paper discusses learned and fixed encodings and proposes a sinusoidal function of sin and cosine with different frequencies, defined as:

.Equation 12.3 Positional Encoding function
[latexmath]
++++
PE_{(pos, 2i)} = sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}})\\
PE_{(pos, 2i+1)} = cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})
++++

This mapping function was chosen because for any offset _k_, latexmath:[PE_{(pos+k)}] can be represented as a linear function of latexmath:[PE_{pos}].
In short, the model should be able to learn to attend to relative positions easily.

Let's look at how this can be coded in Pytorch.
The official Pytorch Sequence-to-Sequence Modeling with nn.Transformer tutorial footnote:[Pytorch Sequence-to-Sequence Modeling With nn.Transformer Tutorial: https://simpletransformers.ai/docs/multi-label-classification/] provides an implementation of a PositionEncoding nn.Module based on the previous function:

.Pytorch PositionalEncoding
[source,python]
----
>>> from torch import nn
>>> class PositionalEncoding(nn.Module):
...     def __init__(self, d_model, dropout=0.1, max_len=5000):
...         super(PositionalEncoding, self).__init__()
...         self.dropout = nn.Dropout(p=dropout)
...
...         pe = torch.zeros(max_len, d_model)
...         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
...         div_term = torch.exp(torch.arange(0, d_model, 2).float() *
...                              (-math.log(10000.0) / d_model))
...         pe[:, 0::2] = torch.sin(position * div_term)
...         pe[:, 1::2] = torch.cos(position * div_term)
...         pe = pe.unsqueeze(0).transpose(0, 1)
...         self.register_buffer('pe', pe)
...
...     def forward(self, x):
...         x = x + self.pe[:x.size(0), :]
...         return self.dropout(x)
----

You will use this module in the translation transformer you build.
However, first we need to fill in the remaining details of the model to complete your understanding of the architecture.

=== Bidirectional backpropagation and "BERT"
// HL: maybe move this to chapter 9
// SUM: BERT significantly improved the accuracy and efficiency of language models by backpropagating through time in both directions, reading the text backwards and forwards simultaneously, with equal care and weighting of the patterns it detected.

In addition to NLP, RNNs are useful for any sequence of numerical data, such as time series.
This ability to output something for each and every element in a sequence means you can create an RNN that outputs a prediction for each element in a time series.
This means that once you master backpropagation through time, you will be able to use RNNs to predict things such as:

* The next day's weather
* The next minute's web traffic volume
* The next second's Distributed Denial of Services (DDOS) web requests
* The action an automobile driver will take over the next 100 milliseconds
* The next image in a sequence of frames in a video clip

As soon as you have a prediction of the target variable you can measure the error - the difference between the model's output and the desired output.
This usually happens at the last time step in whatever sequence of events you are processing.
But natural language sequences of words are not like other time series.
A human reader or an NLP pipeline can start wherever they like.
And for NLP you always have a particular piece of text, with finite length, that you want process.
So you could start at the end of the text or the beginning... or _both_!

////
KM: The last sentence in the paragraph below is not a complete sentence. Please double check. 
HL: fixed
////

BERT is a bidirectional model that took the NLP world by storm by processing text from both directions at once.
The "B" is for "bidirectional."
BERT isn't named for a Sesame Street character it means "Bidirectional Encoder Representations from Transformers" - basically just a bidirectional transformer.
Bidirectional transformers were a huge leap forward for machine-kind.
In the next chapter, chapter 9, you'll learn about the three tricks that helped Transformers (souped up RNNs) reach the top of the leaderboard for many of the hardest NLP problems.
Giving RNNs the ability to read in both directions simultaneously was one of these innovative tricks that helped machines surpass humans at reading comprehension tasks.

If you're curious about bidirectional RNNs, all of the PyTorch RNN models (RNNs, GRUs, LSTMs, and even Transformers) include an option to turn on bidirectional recurrence.footnote:[PyTorch `RNNBase` class source code (https://github.com/pytorch/pytorch/blob/75451d3c81c88eebc878fb03aa5fcb89328989d9/torch/nn/modules/rnn.py#L44)]
For question answering models and other difficult problems you will often see a 5-10% improvement in the accuracy of bidrectional models relative to the default forward direction models.

=== Gluing it all Together
We've discussed embeddings, positional encodings, and the multi-head self-attention mechanism.
Using simple feed-forward layers, normalization and some residual layers, an encoder and decoder blocks of the transformer are constructed as shown in the following figure.

.Original Transformer Model Architecture
image::../images/ch09/transformer_original.png[alt="Original Transfomer from 'Attention Is All You Need'",width=100%,align="center",link="../images/ch12/transformer_original.png"]

In the original transformer, both the encoder and decoder are comprised of _N_ = 6 stacked identical encoder and decoder layers, respectively.

==== Encoder
The encoder is composed of two sub-layers, a multi-head attention layer and a position-wise fully connected feed-forward network.
A residual connection is made around each sub-layer coupled with layer normalization.
It's important to note that the outputs of all sub-layers in module along with all embeddings are of dimesion latexmath:[d_{model}].
Also notice that the input embedding sequences to the encoder are summed with the positional encodings previously described.

==== Decoder
The decoder is nearly identical to the encoder in the model, however it has a separate multi-head self-attention sub-layer that applies masking to the output sequences, which are shifted right by one position.
This ensures that predictions for position _i_ can depend only on previous outputs, for positions less than _i_.
i.e. The model cannot look forward in the sequence for making predictions.


=== Transformer Language Translation Example

Transformers are suited for many tasks.
"Attention Is All You Need" demonstrated ground-breaking results in English-German translation.
Using `torchtext`, you will prepare the Multi30k dataset for training a Transformer for German-English translation using the now standard `torch.nn.Transformer` module.
We'll take a look at the architecture model and you will customize the decoder to output the multi-head self-attention weights for each sublayer.
Finally you will train the model and use it for inference on a test set. .i.e. perform some translations.

==== Preparing the Data
First, load the spacy tokenizers for German and English and define functions for tokenizing sentence strings into a list of tokenized strings.

.Spacy tokenizers for German and English
[source,python]
----
>>> import spacy

>>> spacy_de = spacy.load('de')
>>> spacy_en = spacy.load('en')

>>> def tokenize_de(text):
...     return [tok.text for tok in spacy_de.tokenizer(text)]

>>> def tokenize_en(text):
...     return [tok.text for tok in spacy_en.tokenizer(text)]
----

[TIP]
====
You may need to download the spacy language models for "de" and "en" if you have not used them previously.
Simply run `python -m spacy download de` and/or `python -m spacy download en` to obtain the models.
====

// TODO: !!! The torchtext is undergoing a major rewrite/refactor. Need to redo this section once torchtext update is released.

Next import the `torchtext` utils that will help us to create the datasets for training, evaluation and testing.

.Load torchtext utils for generating datasets
[source,python]
----
>>> import torchtext
>>> from torchtext.datasets import Multi30k
>>> from torchtext.data import Field, BucketIterator

>>> SRC = Field(tokenize = tokenize_de,
...             init_token = '<sos>',  #<1>
...             eos_token = '<eos>',
...             lower = True,
...             batch_first = True)

>>> TRG = Field(tokenize = tokenize_en,
...             init_token = '<sos>',
...             eos_token = '<eos>',
...             lower = True,
...             batch_first = True)
----
<1> An init (start-of-sequence) token, '<sos>', is added to front of sequence and end-of-string token, '<eos>', is appended after last token in sequence.
You can choose any token any token for these, but ensure that they're not actual words in the vocabularly for the language.

// TODO: Again, this will need to be changed with torchtext refactoring. At that point add discussion for user to understand what they're doing and why.

.Generate datasets, build vocabularies, and initialize iterators for the datasets
[source,python]
----
>>> device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  #<1>

>>> train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),
...                                                     fields = (SRC, TRG))

>>> SRC.build_vocab(train_data, min_freq = 2)
>>> TRG.build_vocab(train_data, min_freq = 2)

>>> BATCH_SIZE = 128
>>> train_iterator, valid_iterator, test_iterator = BucketIterator.splits(
...     (train_data, valid_data, test_data),
...      batch_size = BATCH_SIZE,
...      device = device)
>>>
----
<1> Hopefully _device_ is "cuda". As always, access to a GPU is recommended.

==== TranslationTransformer Model

At this point you have tokenized the sentences in the Multi30k data, and converted to tensors consisting of indexes into the vocabularies for the source and target languages, German and English, respectively.
The dataset has been split it into separate training, validation and test sets, which you have wrapped with iterators for batch training.
Now that the data is prepared you turn your focus to setting up the model.
Pytorch provides an implementation of the model presented in "Attention Is All You Need", `torch.nn.Transformer`.
You will notice the constructor takes a number of parameters, familiar amongst them are `d_model=512`, `nhead=8`, `num_encoder_layers=6`, and `num_decoder_layers=6`.
The default values are set to the parameters employed in the paper.
Along with several other parameters for the feed-forward dimension, dropout, and activation, the model also provides support for a `custom_encoder` and `custom_decoder`.
To make things interesting, create a custom decoder that additionally outputs a list of attention weights from the multi-head self-attention layer in each sublayer of the decoder.
It might sound complicated, but it's actually fairly straight-forward if you simply subclass `torch.nn.TransformerDecoderLayer` and `torch.nn.TransformerDecoder` and augment the _forward()_ methods to return the auxillary outputs - the attention weights.


.Extend torch.nn.TransformerDecoderLayer to additionally return multi-head self-attention weights
[source,python]
----
>>> from torch import Tensor
>>> from typing import Optional, Any

>>> class CustomDecoderLayer(nn.TransformerDecoderLayer):
...     def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,
...                 memory_mask: Optional[Tensor] = None,
...                 tgt_key_padding_mask: Optional[Tensor] = None,
...                 mem_key_padding_mask: Optional[Tensor] = None) -> Tensor:
...         """Same as DecoderLayer but returns multi-head attention weights.
...         """
...         tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,
...                               key_padding_mask=tgt_key_padding_mask)[0]
...         tgt = tgt + self.dropout1(tgt2)
...         tgt = self.norm1(tgt)
...         tgt2, attention_weights = self.multihead_attn(tgt, memory, memory,  #<1>
...                                                       attn_mask=memory_mask,
...                                                       key_padding_mask=mem_key_padding_mask,
...                                                       need_weights=True)
...         tgt = tgt + self.dropout2(tgt2)
...         tgt = self.norm2(tgt)
...         tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))
...         tgt = tgt + self.dropout3(tgt2)
...         tgt = self.norm3(tgt)
...         return tgt, attention_weights  #<2>
----
<1> Save the weights from the mulithead_attn layer
<2> In addition to target outputs, return attention weights

.Extend torch.nn.TransformerDecoder to additionally return list of multi-head self-attention weights
[source,python]
----
>>> class CustomDecoder(nn.TransformerDecoder):
...     def __init__(self, decoder_layer, num_layers, norm=None):
...          super(CustomDecoder, self).__init__(decoder_layer, num_layers, norm)

...     def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,
...                 memory_mask: Optional[Tensor] = None,
...                 tgt_key_padding_mask: Optional[Tensor] = None,
...                 memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:
...         """Same as TransformerDecoder except caches the multi-head attention output
...            from each decoder layer.
...         """
...         self.attention_weights = []  #<1>
...         output = tgt
...         for mod in self.layers:
...             output, attention = mod(output, memory, tgt_mask=tgt_mask,
...                          memory_mask=memory_mask,
...                          tgt_key_padding_mask=tgt_key_padding_mask,
...                          memory_key_padding_mask=memory_key_padding_mask)

...             # save the attention weights from this decoder layer  #<2>
...             self.attention_weights.append(attention)

...         if self.norm is not None:
...             output = self.norm(output)

...         return output
----
<1> Reset the list of weights on each _forward()_ call.
<2> The only other change to _forward()_ from the parent's version is to cache weights in list member variable, `attention_weights`.

To recap, you have extended the `torch.nn.TransformerDecoder` and its sublayer component, `torch.nn.TransformerDecoderLayer`, mainly for exploratory purposes.
That is, you save the multi-head self-attention weights from the different decoder layers in the Transformer model you are about to configure and train.
The _forward()_ methods in each of these classes copy the one in the parent nearly verbatim, with the exception of the changes called out to save the attention weights.

The `torch.nn.Transformer` is a somewhat bare-bones version of the sequence-to-sequence model containing the main secret sauce, the multi-head self-attention in both the encoder and decoder.
If one looks at the source code for the module footnote:[Pytorch nn.Transformer source:https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/transformer.py], the model does not assume the use of embedding layers or positional encodings.
Now you will create your _TranslationTransformer_ model that uses the custom decoder components, by extending `torch.nn.Transformer` module.
Begin with defining the constructor, which takes parameters _src_vocab_size_ for a source embedding size, and _tgt_vocab_size_ for the target, and uses them to initialize a basic `torch.nn.Embedding` for each.
Notice a _PositionalEncoding_ member, _pos_enc_, is created in the constructor for adding the word location information.

.Extend torch.nn.Transformer for language translation using your CustomDecoder
[source,python]
----
>>> from einops import rearrange  #<1>

>>> class TranslationTransformer(nn.Transformer):  #<2>
>>>     def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,
...                  tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,
...                  d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,
...                  num_decoder_layers: int = 6, dim_feedforward: int = 2048,
...                  dropout: float = 0.1, activation: str = "relu"):
...
...         decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,  # <3>
...                                            dropout, activation)
...         decoder_norm = nn.LayerNorm(d_model)
...         decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)  # <4>
...
...         super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,
...             num_encoder_layers=num_encoder_layers,
...             num_decoder_layers=num_decoder_layers,
...             dim_feedforward=dim_feedforward,
...             dropout=dropout, custom_decoder=decoder)
...         self.src_pad_idx = src_pad_idx
...         self.tgt_pad_idx = tgt_pad_idx
...         self.device = device
...         self.src_emb = nn.Embedding(src_vocab_size, d_model)  #<5>
...         self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)
...         self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)  #<6>
...         self.linear = nn.Linear(d_model, tgt_vocab_size) #<7>
----
<1> einops is helpful utility for working with tensors
<2> TranslationTransformer extends torch.nn.Transformer
<3> Create instance of your CustomDecoderLayer for use in CustomDecoder.
<4> Create instance of your CustomDecoder which collects the attention weights from the CustomerDecoderLayer's, for use in the Transformer.
<5> Define individual embedding layers for the input and target sequences.
<6> PositionalEncoding for the source and target sequences.
<7> Final linear layer for target word probabilities.


Note the import of `rearrange` from the `einops` footnote:[einops:https://github.com/arogozhnikov/einops] package, which is a handy tensor manipulation utility for its ease of use and readability.
To see why you want to use `rearrange()` please refer to the `torch.nn.Transformer` documentation footnote:[Pytorch torch.nn.Transformer documentation:https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html].
It has clear requirements on the shapes it expects for tensors.

.torch.nn.Transformer "Shape" description (summarized)
[source]
----
    src: (S, N, E)
    tgt: (T, N, E)
    src_mask: (S, S)
    tgt_mask: (T, T)
    memory_mask: (T, S)
    src_key_padding_mask: (N, S)
    tgt_key_padding_mask: (N, T)
    memory_key_padding_mask: (N, S)

    output(T, N, E)

    where S is the source sequence length, T is the target sequence length, N is the batch size,
    E is the feature number
----

The datasets you created using torchtext are batch first.
So, borrowing the nomenclature in the Transformer documentation, your source and target tensors have shape _(N, S)_ and _(N, T)_, respectively.
To feed to them to the `torch.nn.Transformer` (i.e. call its _forward()_ method), the source and target must be reshaped.
Also, you want to apply the embeddings plus the positional encoding to the source and target sequences.
Additionally, a _padding key mask_ is needed for each and a _memory key mask_ is required for the target.
Note, you can manage the embeddings and positional encodings outside the class, in the training and inference sections of the pipeline.
However, since the model is specifically setup for translation, you make a stylistic/design choice to encapsulate the source and target sequence preparation within the class.
To this end you define _prepare_src()_ and _prepare_tgt()_ methods for preparing the sequences and generating the required masks.

.TranslationTransformer prepare_src()
[source]
----
>>>     def _make_key_padding_mask(self, t, pad_idx):
...         mask = (t == pad_idx).to(self.device)
...         return mask
>>>     def prepare_src(self, src, src_pad_idx):
...         src_key_padding_mask = self._make_key_padding_mask(src, src_pad_idx)
...         src = rearrange(src, 'N S -> S N')
...         src = self.pos_enc(self.src_emb(src) * math.sqrt(self.d_model))

...         return src, src_key_padding_mask
----

The _make_key_padding_mask()_ method returns a tensor set to 1's in the position of the padding token in the given tensor, and zero otherwise.
The _prepare_src()_ method generates the padding mask and then rearranges the _src_ to the requisite shape for the model.
It then applies the positional encoding to the source embedding multipled by the square root of the model's dimension.
This is taken directly from "Attention Is All You Need".
The method returns the _src_ with positional encoding applied, and the key padding mask for it.

The _prepare_tgt()_ method used for the target sequence is nearly identical to _prepare_src()_.
It returns the _tgt_ adjusted for positional encodings, and a target key padding mask.
However, it also returns a "subsequent" mask, _tgt_mask_, which is a triangular matrix for which columns (ones) in a row that are permitted to be observed.
To generate the subsequent mask you use the _Transformer.generate_square_subsequent_mask()_ method defined in the base class as shown in the following listing.

.TranslationTransformer prepare_tgt()
[source]
----
>>>     def prepare_tgt(self, tgt, tgt_pad_idx):
...         tgt_key_padding_mask = self._make_key_padding_mask(tgt, tgt_pad_idx)
...         tgt = rearrange(tgt, 'N T -> T N')
...         tgt_mask = self.generate_square_subsequent_mask(tgt.shape[0]).to(self.device)
...         tgt = self.pos_enc(self.tgt_emb(tgt) * math.sqrt(self.d_model))
...         return tgt, tgt_key_padding_mask, tgt_mask
----

You put _prepare_src()_ and _prepare_tgt()_ to use in the model's _forward()_ method.
After preparing the inputs, it simply invokes the parent's _forward()_ and feeds the outputs through a Linear reduction layer after transforming from (T, N, E) back to batch first (N, T, E).
We do this for consistency in our training and inference.

.TranslationTransformer forward()
[source]
----
>>>     def forward(self, src, tgt):
...         src, src_key_padding_mask = self.prepare_src(src, self.src_pad_idx)
...         tgt, tgt_key_padding_mask, tgt_mask = self.prepare_tgt(tgt, self.tgt_pad_idx)
...         memory_key_padding_mask = src_key_padding_mask.clone()
...
...         output = super(TranslationTransformer, self).forward(src, tgt, tgt_mask=tgt_mask,
...                      src_key_padding_mask=src_key_padding_mask,
...                      tgt_key_padding_mask=tgt_key_padding_mask,
...                      memory_key_padding_mask=memory_key_padding_mask)
...         output = rearrange(output, 'T N E -> N T E')
...         return self.linear(output)
----

Also, define an _init_weights()_ method that can be called to initialize the weights of all submodules of the Transformer.
Xavier initialization is commonly used for Transformers, so use it here.
The Pytorch _nn.Module_ documentation footnote:[Pytorch nn.Module documentation:https://pytorch.org/docs/stable/generated/torch.nn.Module.html] describes the _apply(fn)_ method that recursively applies `fn` to every submodule of the caller.

.TranslationTransformer init_weights()
[source]
----
>>>     def init_weights(self):
...         def _init_weights(m):
...             if hasattr(m, 'weight') and m.weight.dim() > 1:
...             nn.init.xavier_uniform_(m.weight.data)
...         self.apply(_init_weights);  #<1>
----
<1> Call the model's _apply()_ method. The semi-colon (";") at the end of the line suppresses output from _apply()_ in IPython and Jupyter notebooks, and is not required.

The individual components of the model have been defined and the complete model is shown in the next listing.


.TranslationTransformer complete model definition
[source,python]
----
>>> from einops import rearrange

>>> class TranslationTransformer(nn.Transformer):
>>>     def __init__(self, device: str, src_vocab_size: int, src_pad_idx: int,
...                  tgt_vocab_size: int, tgt_pad_idx: int, max_sequence_length: int = 100,
...                  d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,
...                  num_decoder_layers: int = 6, dim_feedforward: int = 2048,
...                  dropout: float = 0.1, activation: str = "relu"):
...
...         decoder_layer = CustomDecoderLayer(d_model, nhead, dim_feedforward,
...                                            dropout, activation)
...         decoder_norm = nn.LayerNorm(d_model)
...         decoder = CustomDecoder(decoder_layer, num_decoder_layers, decoder_norm)
...
...         super(TranslationTransformer, self).__init__(d_model=d_model, nhead=nhead,
...                                                      num_encoder_layers=num_encoder_layers,
...                                                      num_decoder_layers=num_decoder_layers,
...                                                      dim_feedforward=dim_feedforward,
...                                                      dropout=dropout, custom_decoder=decoder)
...
...         self.src_pad_idx = src_pad_idx
...         self.tgt_pad_idx = tgt_pad_idx
...         self.device = device
...
...         self.src_emb = nn.Embedding(src_vocab_size, d_model)
...         self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)
...
...         self.pos_enc = PositionalEncoding(d_model, dropout, max_sequence_length)
...         self.linear = nn.Linear(d_model, tgt_vocab_size)
...
>>>     def init_weights(self):
...         def _init_weights(m):
...             if hasattr(m, 'weight') and m.weight.dim() > 1:
...             nn.init.xavier_uniform_(m.weight.data)
...         self.apply(_init_weights);
...
>>>     def _make_key_padding_mask(self, t, pad_idx):
...         mask = (t == pad_idx).to(self.device)
...
...         return mask
...
...     def prepare_src(self, src, src_pad_idx):
...         src_key_padding_mask = self._make_key_padding_mask(src, src_pad_idx)
...         src = rearrange(src, 'N S -> S N')
...         src = self.pos_enc(self.src_emb(src) * math.sqrt(self.d_model))
...
...         return src, src_key_padding_mask
...
>>>     def prepare_tgt(self, tgt, tgt_pad_idx):
...         tgt_key_padding_mask = self._make_key_padding_mask(tgt, tgt_pad_idx)
...         tgt = rearrange(tgt, 'N T -> T N')
...         tgt_mask = self.generate_square_subsequent_mask(tgt.shape[0]).to(self.device)
...         tgt = self.pos_enc(self.tgt_emb(tgt) * math.sqrt(self.d_model))
...
...         return tgt, tgt_key_padding_mask, tgt_mask
...
>>>     def forward(self, src, tgt):
...         src, src_key_padding_mask = self.prepare_src(src, self.src_pad_idx)
...
...         tgt, tgt_key_padding_mask, tgt_mask = self.prepare_tgt(tgt, self.tgt_pad_idx)
...
...         memory_key_padding_mask = src_key_padding_mask.clone()
...
...         output = super(TranslationTransformer, self).forward(src, tgt, tgt_mask=tgt_mask,
...                      src_key_padding_mask=src_key_padding_mask,
...                      tgt_key_padding_mask=tgt_key_padding_mask,
...                      memory_key_padding_mask=memory_key_padding_mask)
...
...         output = rearrange(output, 'T N E -> N T E')
...
...         return self.linear(output)
----

==== Training the TranslationTransformer
Now create an instance of the model for our translation task and initialize the weights in preparation for training.
For the model's dimensions you use the defaults, which correlate to the sizes of the original "Attention Is All You Need" transformer.
Know that since the encoder and decoder building blocks comprise duplicate, stackable layers, you can configure the model with any number of these layers.

.Create TranslationTransformer instance
[source,python]
----
>>> SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]
>>> TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]

>>> model = TranslationTransformer(device=device,
...                            src_vocab_size=len(SRC.vocab), src_pad_idx=SRC_PAD_IDX,
...                            tgt_vocab_size=len(TRG.vocab), tgt_pad_idx=TRG_PAD_IDX).to(device)

>>> model.init_weights()
----

You note that you pass the calculated sizes of your source and target vocabularies in the constructor.
Also, you pass the indices for the source and target padding tokens for the model to use in preparing the source, targets, and associated masking sequences.
Now that you have the model defined, take a moment to do a quick sanity check to make sure there are no obvious coding errors before you setup the training and inference pipeline.
You can create "batches" of random integer tensors for the sources and targets and pass them to the model as demonstrated in the following listing.

.Quick model sanity check with random tensors
[source,python]
----
>>> src = torch.randint(1, 100, (10, 5)).to('cuda')  #<1>
>>> tgt = torch.randint(1, 100, (10, 7)).to('cuda')

>>> with torch.no_grad():
...     output = model(src, tgt)  #<2>

>>> print(output.shape)
torch.Size([10, 7, 5893])
----
<1> _torch.randint(low, high, size)_ where size is tuple for shape of the tensor
<2> A _forward_ pass of the model with _src_ and _tgt_.

We created two tensors, _src_ and _tgt_, each with random integers between 1 and 100 distributed uniformly.
Your model accepts tensors having batch-first shape, so we made sure that the batch sizes (10 in this case) were identical, otherwise we would have received a runtime error on the forward pass, that looks like this:
```
RuntimeError: the batch number of src and tgt must be equal
```
It may be obvious, the source and target sequence lengths do not have to match, which is confirmed by the successful call to _model(src, tgt)_.

[TIP]
====
When setting up a new sequence-to-sequence model for training, you may want to initially use smaller tunables in your setup.
This includes limiting max sequence lengths, reducing batch sizes, and specifying a smaller number of training loops or epochs.
This will make it easier to debug issues in your model and/or pipeline to get your program executing end-to-end more quickly.
Be cautioned not to draw any conclusions on the capabilities/accuracy of your model at this "bootstrapping" stage; the goal is simply to get the pipeline to run.
====

Now that you feel confident the model is ready for action, the next step is to define the optimizer and criterion for training.
"Attention Is All You Need" used Adam optimizer with a warmup period in which the learning rate is increased followed by a decreasing rate for the duration of training.
You will use static rate, 1e-4, which is smaller than the default rate 1e-2 for Adam.
This should provide for stable training as long as you are patient to run enough epochs.
You can play with learning rate scheduling as an exercise if you are interested.
Other Transformer based models you will look at later in this chapter use a static learning rate.
As is common for this type of task, you use `torch.nn.CrossEntropyLoss` for the criterion.

.Optimizer and Criterion
[source,python]
----
>>> LEARNING_RATE = 0.0001
>>> optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
>>> criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)  # <1>

...       ----
<1> Ignore padding in the input gradient calculation

Ben Trevett contributed much of the code for the Pytorch Transformer Beginner tutorial.
He, along with colleagues, have written an outstanding and informative Jupyter notebook series for their Pytorch Seq2Seq tutorial footnote:[Trevett,Ben - PyTorch Seq2Seq: https://github.com/bentrevett/pytorch-seq2seq] covering sequence-to-sequence models.
Their Attention Is All You Need footnote:[Trevett,Ben - Attention Is All You Need Jupyter notebook: https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb] notebook provides a from-scratch implementation of a basic transformer model.
To avoid re-inventing the wheel, the training and evaluation driver code in the next sections is borrowed from Ben's notebook, with minor changes.

The _train()_ function implements a training loop similar to others you have seen.
Remember to put the model into _train_ mode before the batch iteration.
Also, note that the last token in the target, which is the <eos> token, is stripped from _trg_ before passing it as input to the model.
We want the model to predict end of string.
The function returns the average loss per iteration.

.Model training function
[source,python]
----
>>> def train(model, iterator, optimizer, criterion, clip):
... 
...     model.train()  #<1>
...     epoch_loss = 0
... 
>>>     for i, batch in enumerate(iterator):
...         src = batch.src
...         trg = batch.trg
... 
...         optimizer.zero_grad()
...         output = model(src, trg[:,:-1])  #<2>
... 
...         output_dim = output.shape[-1]
...         output = output.contiguous().view(-1, output_dim)
...         trg = trg[:,1:].contiguous().view(-1)
... 
...         loss = criterion(output, trg)
... 
...         loss.backward()
... 
...         torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
... 
...         optimizer.step()
... 
...         epoch_loss += loss.item()
... 
...     return epoch_loss / len(iterator)
----
<1> Make sure model is in training mode
<2> The last token in _trg_ is the <eos> token. Slice it off so that it's not an input to the model.

The _evaluate()_ function is similar to _train()_.
You set the model to _eval_ mode and use the _with torch.no_grad()_ paradigm as usual for straight inference.

.Model evaluation function
[source,python]
----
>>> def evaluate(model, iterator, criterion):

...     model.eval()  #<1>
...     epoch_loss = 0

...     with torch.no_grad():  #<2>

...         for i, batch in enumerate(iterator):

...             src = batch.src
...             trg = batch.trg

...             output = model(src, trg[:,:-1])

...             output_dim = output.shape[-1]

...             output = output.contiguous().view(-1, output_dim)
...             trg = trg[:,1:].contiguous().view(-1)

...             loss = criterion(output, trg)

...             epoch_loss += loss.item()

...     return epoch_loss / len(iterator)
----
<1> Set the model to eval mode
<2> Disable gradient calculation for inference

Next a straight-forward utility function, _epoch_time()_, for calculating time elapsed during training is defined as follows.

.Utility function for elapsed time
[source,python]
----
>>> def epoch_time(start_time, end_time):
...     elapsed_time = end_time - start_time
...     elapsed_mins = int(elapsed_time / 60)
...     elapsed_secs = int(elapsed_time - (elapsed_mins * 60))
...     return elapsed_mins, elapsed_secs
----

Now, proceed to setup the training.
You set the number of epochs to 15, to give the model enough opportunities to train with the previously selected learning rate of 1e-4.
You can experiment with different learning rate and epoch combinations.
In a future example you will use an early stopping mechanism to avoid over-fitting and unnecessary training time.
Here you declare a filename for _BEST_MODEL_FILE_ and after each epoch, if the validation loss is an improvement over the previous best loss, the model is saved and best loss is updated as shown.

.Run the TranslationTransformer model training and save the *best* model to file
[source,python]
----
>>> N_EPOCHS = 15
>>> CLIP = 1
>>> BEST_MODEL_FILE = 'best_model.pytorch'

>>> best_valid_loss = float('inf')

>>> for epoch in range(N_EPOCHS):

...     start_time = time.time()

...     train_loss = train(model, train_iterator, optimizer, criterion, CLIP)
...     valid_loss = evaluate(model, valid_iterator, criterion)

...     end_time = time.time()

...     epoch_mins, epoch_secs = epoch_time(start_time, end_time)

...     if valid_loss < best_valid_loss:
...         best_valid_loss = valid_loss
...         torch.save(model.state_dict(), BEST_MODEL_FILE)

...    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')
...    print(f'\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')
...    print(f'\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')

Epoch: 01 | Time: 0m 55s
	Train Loss: 4.835 | Train PPL: 125.848
	 Val. Loss: 3.769 |  Val. PPL:  43.332
Epoch: 02 | Time: 0m 56s
	Train Loss: 3.617 | Train PPL:  37.242
	 Val. Loss: 3.214 |  Val. PPL:  24.874
Epoch: 03 | Time: 0m 56s
	Train Loss: 3.197 | Train PPL:  24.448
	 Val. Loss: 2.872 |  Val. PPL:  17.679

...
Epoch: 13 | Time: 0m 57s
	Train Loss: 1.242 | Train PPL:   3.463
	 Val. Loss: 1.570 |  Val. PPL:   4.805
Epoch: 14 | Time: 0m 57s
	Train Loss: 1.164 | Train PPL:   3.204
	 Val. Loss: 1.560 |  Val. PPL:   4.759
Epoch: 15 | Time: 0m 57s
	Train Loss: 1.094 | Train PPL:   2.985
	 Val. Loss: 1.545 |  Val. PPL:   4.689
----

Notice that we could have probably ran a few more epochs given that validation loss was still decreasing prior to exiting the loop.
Let's see how the model performs on a test set by loading the _best_ model and running the _evaluate()_ function on the test set.

.Load *best* model from file and perform evaluation on test data set
[source,python]
----
>>> model.load_state_dict(torch.load(BEST_MODEL_FILE))

>>> test_loss = evaluate(model, test_iterator, criterion)

>>> print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')
| Test Loss: 1.590 | Test PPL:   4.902 |
----

==== TranslationTransformer Inference
You are now convinced your model is ready to become your personal German-to-English interpreter.
Performing translation requires only slightly more work to setup, which you do in the _translate_sentence()_ function in the next listing.
In brief, start by tokenizing the source _sentence_ if it has not been tokenized already and end-capping it with the _<sos>_ and _<eos>_ tokens.
Next you call the _prepare_src()_ method of the model to transform the _src_ sequence and generate the source key padding mask as was done in training and evaluation.
Then run the prepared _src_ and _src_key_padding_mask_ through the model's encoder and save its output (in _enc_src_).
Now, here is the fun part, where the target sentence (the translation) is generated.
Start by initializing a list, _trg_indexes_, to the _<sos>_ token.
In a loop - while the generated sequence has not reached a maximum length - convert the current prediction, _trg_indexes_, to a tensor.
Use the model's _prepare_tgt()_ method to prepare the target sequence, creating the target key padding mask, and the target sentence mask.
Run the current decoder output, the encoder output, and the two masks through the decoder.
Get the latest predicted token from the decoder output and append it to _trg_indexes_.
Break out of the loop if the prediction was an _<eos>_ token (or if maximum sentence length is reached).
The function returns the target indexes converted to tokens (words) and the attention weights from the decoder in the model.

.Define _translate_sentence()_ for performing inference
[source,python]
----
>>> def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):

...     model.eval()

...     if isinstance(sentence, str):
...         nlp = spacy.load('de')
...         tokens = [token.text.lower() for token in nlp(sentence)]
...     else:
...         tokens = [token.lower() for token in sentence]

...     tokens = [src_field.init_token] + tokens + [src_field.eos_token]  #<1>

...     src_indexes = [src_field.vocab.stoi[token] for token in tokens]

...     src = torch.LongTensor(src_indexes).unsqueeze(0).to(device)

...     src, src_key_padding_mask = model.prepare_src(src, SRC_PAD_IDX)

...     with torch.no_grad():
...         enc_src = model.encoder(src, src_key_padding_mask=src_key_padding_mask)

...     trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]  #<2>

...     for i in range(max_len):

...         tgt = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)

...         tgt, tgt_key_padding_mask, tgt_mask = model.prepare_tgt(tgt, TRG_PAD_IDX)

...         with torch.no_grad():
...             output = model.decoder(tgt, enc_src, tgt_mask=tgt_mask,
...                               tgt_key_padding_mask=tgt_key_padding_mask,
...                               memory_key_padding_mask=src_key_padding_mask)

...             output = rearrange(output, 'T N E -> N T E')
...             output = model.linear(output)

...         pred_token = output.argmax(2)[:,-1].item()  #<3>

...         trg_indexes.append(pred_token)

...         if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:  #<4>
...             break

...     trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]
...     translation = trg_tokens[1:]

...     return translation, model.decoder.attention_weights
----
<1> Prepare the source string by encapsulating in _<sos>_ and _<eos>_ tokens.
<2> Start _trg_indexes_ (predictions) with index of _<sos>_ token.
<3> Each time through the loop retrieve the latest predicted token.
<4> Break out of the inference loop on _<eos>_ token.

==== TranslationTransformer Inference Example 1
Use _translate_sentence()_ on an example from the test data.

.Load sample at _test_data_ index 10
[source,python]
----
>>> example_idx = 10

>>> src = vars(test_data.examples[example_idx])['src']
>>> trg = vars(test_data.examples[example_idx])['trg']

>>> print(f'src = {src}')
>>> print(f'trg = {trg}')
src = ['eine', 'mutter', 'und', 'ihr', 'kleiner', 'sohn', 'genieen', 'einen', 'schnen', 'tag', 'im', 'freien', '.']
trg = ['a', 'mother', 'and', 'her', 'young', 'song', 'enjoying', 'a', 'beautiful', 'day', 'outside', '.']
----

Run the _src_ through your translator.

.Translate the test data sample
[source,python]
----
>>> translation, attention = translate_sentence(src, SRC, TRG, model, device)
>>> print(f'translation = {translation}')
translation = ['a', 'mother', 'and', 'her', 'little', 'son', 'enjoying', 'a', 'beautiful', 'day', 'outside', '.', '<eos>']
----

Interestingly, it appears there is a typo in the German word for "son" (sohn?) in the source sentence that has it being translated to "song" in English.
Based on context, it appears the model did well to infer that a mother is (probably) with her young "son".
The model gives us the adjective "little" instead of "young", which is acceptable, given that the direct translation of German word "kleiner" is "smaller".

Let's focus our attention on, um, _attention_.
In your model you defined a _CustomDecoder_ that save the average attention weights for each decoder layer on each forward pass.
You have the have the _attention_ weights from the translation.
Now write a function to visualize self-attention for each decoder layer using `matplotlib`.

.Function to visualize self-attention weights for decoder layers of the TranslationTransformer
[source,python]
----
>>> import matplotlib.pyplot as plt
>>> import matplotlib.ticker as ticker

>>> def display_attention(sentence, translation, attention_weights):
...     n_attention = len(attention_weights)

...     n_cols = 2
...     n_rows = n_attention // n_cols + n_attention % n_cols

...     fig = plt.figure(figsize=(15,25))

...     for i in range(n_attention):

...         ax = fig.add_subplot(n_rows, n_cols, i+1)

...         attention = attention_weights[i].squeeze(0).cpu().detach().numpy()

...         cax = ax.matshow(attention, cmap='gist_yarg')

...         ax.tick_params(labelsize=12)
...         ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'],
...                            rotation=45)
...         ax.set_yticklabels(['']+translation)

...         ax.xaxis.set_major_locator(ticker.MultipleLocator(1))
...         ax.yaxis.set_major_locator(ticker.MultipleLocator(1))

...     plt.show()
...     plt.close()
----

The function plots the attention values at each index in the sequence with the original sentence on the x-axis and the translation along the y-axis.
We use the _gist_yarg_ color map since it's a gray-scale scheme that is printer friendly.
Now you display the attention for the mother and son enjoying the beautiful day sentence.

.Visualize the self-attention weights for the test example translation
[source,python]
----
>>> display_attention(src, translation, attention_weights)
----

Looking at the plots for the initial two decoder layers we can see that an area of concentration is starting to develop along the diagonal.

.Test Translation Example: Decoder Self-Attention Layers 1 and 2
image::../images/Ch12/translation_attention_1_2.png[alt="TranlationTransformer Attention Layers 1 and 2",width=100%,align="center",link="../images/ch12/translation_attention_1_2.png"]

In the subsequent layers, three and four, the focus is appearing to become more refined.

.Test Translation Example: Decoder Self-Attention Layers 3 and 4
image::../images/Ch12/translation_attention_3_4.png[alt="TranlationTransformer Attention Layers 3 and 4",width=100%,align="center",link="../images/ch12/translation_attention_3_4.png"]

In the final two layers, we see the attention is strongly weighted where direct word-to-word translation is done, along the diagonal, which is what you likely would expect.
Notice the shaded clusters of article-noun and adjective-noun pairings.
For example, "son" is clearly weighted on the word "sohn", yet there is also attention given to "kleiner".

.Test Translation Example: Decoder Self-Attention Layers 5 and 6
image::../images/Ch12/translation_attention_5_6.png[alt="TranlationTransformer Attention Layers 5 and 6",width=100%,align="center",link="../images/ch12/translation_attention_5_6.png"]

You selected this example arbitrarily from the test set to get a sense of the translation capability of the model.
The attention plots appear to show that the model is picking up on relations in the sentence, but the word importance is still strongly positional in nature.
By that, we mean the German word at the current position in the original sentence is generally translated to the English version of the word at the same or similar position in the target output.

==== TranslationTransformer Inference Example 2
Have a look at another example, this time from the validation set, where the ordering of clauses in the input sequence and the output sequence are different, and see how the attention plays out.
Load and print the data for the validation sample at index 25 in the next listing.

.Load sample at _valid_data_ index 25
[source,python]
----
>>> example_idx = 25

>>> src = vars(valid_data.examples[example_idx])['src']
>>> trg = vars(valid_data.examples[example_idx])['trg']

>>> print(f'src = {src}')
>>> print(f'trg = {trg}')
src = ['zwei', 'hunde', 'spielen', 'im', 'hohen', 'gras', 'mit', 'einem', 'orangen', 'spielzeug', '.']
trg = ['two', 'dogs', 'play', 'with', 'an', 'orange', 'toy', 'in', 'tall', 'grass', '.']
----

Even if your German comprehension is not great, it seems fairly obvious that the _orange toy_ ("orangen spielzeug") is at the end of the source sentence, and the _in the tall grass_ is in the middle.
In the English sentence, however, "in tall grass" completes the sentence, while "with an orange toy" is the direct recipient of the "play" action, in the middle part of the sentence.
Translate the sentence with your model.

.Translate the validation data sample
[source,python]
----
>>> translation, attention = translate_sentence(src, SRC, TRG, model, device)
>>> print(f'translation = {translation}')
translation = ['two', 'dogs', 'are', 'playing', 'with', 'an', 'orange', 'toy', 'in', 'the', 'tall', 'grass', '.', '<eos>']
----

This is a pretty exciting result for a model that took about 15 minutes to train (depending on your computing power).
Again, plot the attention weights by calling the _display_attention()_ function with the _src_, _translation_ and _attention_.

.Visualize the self-attention weights for the validation example translation
[source,python]
----
>>> display_attention(src, translation, attention)
----

Here we show the plots for the last two layers (5 and 6).

.Validation Translation Example: Decoder Self-Attention Layers 5 and 6
image::../images/Ch12/translation_attention_validation_5_6.png[alt="TranlationTransformer Validation Self-Attention Layers 5 and 6",width=100%,align="center",link="../images/ch12/translation_attention_validation_5_6.png"]

This is sample excellentlly depicts how the attention weights can break from the position-in-sequence mold and actually attend to words later or earlier in the sentence.
It truly shows the uniqueness and power of the multi-head self-attention mechanism.

To wrap up the section, you will calculate the BLEU (bilingual evaluation understudy) score for the model.
The `torchtext` package supplies a function, _bleu_score_,  for doing the calculation.
You use the following function, again from Mr. Trevett's notebook, to do inference on a dataset and return the score.

[source,python]
----
>>> from torchtext.data.metrics import bleu_score

>>> def calculate_bleu(data, src_field, trg_field, model, device, max_len = 50):

...     trgs = []
...     pred_trgs = []

...     for datum in data:

...         src = vars(datum)['src']
...         trg = vars(datum)['trg']

...         pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len)

...         # strip <eos> token
...         pred_trg = pred_trg[:-1]

...         pred_trgs.append(pred_trg)
...         trgs.append([trg])

...     return bleu_score(pred_trgs, trgs)
----

Calculate the score for your test data.

[source,python]
----
>>> bleu_score = calculate_bleu(test_data, SRC, TRG, model, device)

>>> print(f'BLEU score = {bleu_score*100:.2f}')
BLEU score = 37.68
----

To compare to Ben Trevett's tutorial code, a convolutional sequence-to-sequence model footnote:[Trevett,Ben - Convolutional Sequence to Sequence Learning:https://github.com/bentrevett/pytorch-seq2seq/blob/master/5%20-%20Convolutional%20Sequence%20to%20Sequence%20Learning.ipynb] achieves a 33.3 BLEU and the smaller-scale Transformer scores about 35.
Your model uses the same dimensions of the original "Attention Is All You Need" Transformer, hence it is no surprise that it performs well.

== BERT

In 2018, researchers at Google AI unveiled a new language model they call BERT, for "Bi-directional Encoder Representations from Transformers" footnote:[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: https://arxiv.org/abs/1810.04805 (Devlin, Jacob et al. 2018)].
The model, which comes in two flavors (configurations) - BERT~BASE~ and BERT~LARGE~ - is comprised of a stack of encoder transformers with feed forward and attention layers.
Different from transformer models that preceded it, like OpenAI GPT, BERT uses masked language modeling (MLM) objective to train a deep bi-directional transformer.
MLM involves randomly masking tokens in the input sequence and then attempting to predict the actual tokens from context.
More powerful than typical left-to-right language model training, the MLM objective allows BERT to better generalize language representations by joining the left and right context of a token in all layers.
The BERT models were pre-trained in a semi-unsupervised fashion on the English Wikipedia sans tables and charts (2500M words), and the BooksCorpus (800M words and upon which GPT was also trained).
With simply some tweaks to inputs and the output layer, the models can be fine-tuned to achieve state of the art results on specific sentence-level and token-level tasks.


=== Tokenization and Pre-training
The input sequences to BERT can ambiguously represent a single sentence or a pair of sentences.
BERT uses WordPiece embeddings with the first token of each sequence always set as a special _[CLS]_ token.
Sentences are distinguished by a trailing separator token, _[SEP]_.
Tokens in a sequence are further distinguished by a separate segment embedding with either sentence A or B assigned to each token.
Additionally, a positional embedding is added to the sequence, such that each position the input representation of a token is formed by summation of the corresponding token, segment, and positional embeddings as shown in the figure below (from the published paper):

image::../images/Ch12/bert_inputs.png[alt="BERT input representation",width=100%,align="center",link="../images/Ch12/bert_inputs.png"]

During pre-training a percentage of input tokens are masked randomly (with a _[MASK]_ token) and the model the model predicts the actual token IDs for those masked tokens.
In practice, 15% of the WordPiece tokens were selected to be masked for training, however a downside of this is that during fine-tuning there is no _[MASK]_ token.
To work around this, the authors came up with a formula to replace the selected tokens for masking (the 15%) with the _[MASK]_ token 80% of the time.
For the other 20%, they replace the token with a random token 10% of the time and keep the original token 10% of the time.
In addition to this MLM objective pre-training, a secondary training is done for Next Sentence Prediction (NSP).
Many downstream tasks, such as Question Answering (QA), depend upon understanding the relationship between two sentences, and cannot be solved with language modeling alone.
For the NSP wave of training, the authors generated a simple binarized NSP task by selecting pairs of sentences A and B for each sample and labeling as _IsNext_ and _NotNext_.
Fifty percent of the samples for the pre-training had selections where sentence B followed sentence A in the corpus, and for the other half sentence B was chosen at random.
This plain solution shows that sometimes one need not overthink a problem.

=== Fine-tuning
For most BERT tasks, you will want to load the BERT~BASE~ or BERT~LARGE~ model with all its parameters initialized from the pre-training and fine-tune the model for your specific task.
The fine-tuning should typically be straight forward; one simply plugs in the task-specific inputs and outputs and then commence training all parameters end-to-end.
Compared to the initial pre-training, the fine-tuning of the model is much less expensive.
BERT is shown to be more than capable on a multitude of tasks.
For example, at the time its publication, BERT outperformed the current state-of-the-art OpenAI GPT model on the General Language Understanding Evaluation (GLUE) benchmark.
And BERT bested the top-performing systems (ensembles) on the Stanford Question Answering Dataset (SQuAD v1.1), where the task is to select the text span from a given Wikipedia passage that provides the answer to a given question.
Unsurprisingly, BERT was also best at a variation of this task, SQuAD v2.0, where it is allowed that a short answer for the problem question in the text might not exist.

=== Implementation
Borrowing from the discussion on the original transformer earlier in the chapter, for the BERT configurations, _L_ denotes the number of transformer layers.
The hidden size is _H_ and the number of self-attention heads is _A_.
BERT~BASE~ has dimensions _L_=12, _H_=768, and _A_=12, for a total of 110M parameters.
BERT~LARGE~ has _L_=24, _H_=1024, and _A_=16 for 340M total parameters!
The large model outperforms the base model on all tasks, however depending on hardware resources available to you, you may find working with the base model more than adequate.
There are are _cased_ and _uncased_ versions of the pretrained models for both, the base and large configurations.
The _uncased_ version had the text converted to all lowercase prior to pre-training WordPiece tokenization, while there were no changes made to the input text for the _cased_ model.

The original BERT implementation was open sourced as part of the TensorFlow _tensor2tensor_ library footnote:[tensor2tensor library:https://github.com/tensorflow/tensor2tensor].
A _Google Colab_ notebook footnote:[BERT Fine-tuning With Cloud TPUS:https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb] demonstrating how to fine-tune BERT for sentence-pair classification tasks was published by the TensorFlow Hub authors circa the time the BERT academic paper was released.
Running the notebook requires registering for access to Google Cloud Platform Compute Engine and acquiring a Google Cloud Storage bucket.
At the time of this writing it appears Google continues to offer monetary credits for first-time users, but generally you will have to pay for access to computing power once you have exhausted the initial trial offer credits.

[NOTE]
=====
As you go deeper into NLP models, literally with the use of models having deep stacks of transformers, you may find that your current computer hardware is insufficient for computationally expensive tasks of training and/or fine-tuning large models.
You will want to evaluate the costs of building out a personal computer to meet your workloads and weigh that against pay-per-use cloud and virtual computing offerings for AI.
We reference basic hardware requirements and compute options in this text, however discussion of the "right" PC setup or providing an exhaustive list of competitive computing options are outside the scope of this book.
In addition to the Google Compute Engine, just mentioned, the appendix has instructions for setting up Amazon Web Services (AWS) GPU.
=====

Accepted op-for-op Pytorch versions of BERT models were implemented as _pytorch-pretrained-bert_ footnote:[pytorch-pretrained-bert:https://pypi.org/project/pytorch-pretrained-bert] and then later incorporated in the indispensable HuggingFace _transformers_ library footnote:[HuggingFace transformers:https://huggingface.co/transformers/].
You would do well to spend some time to read the getting started documentation and the summaries of the transformer models and associated tasks on the site.
To install the transformers library, simple use `pip install transformers`.
Once installed, import the BertModel from transformers using the `BertModel.from_pretrained()` API to load one by name.
You print summary for the loaded "bert-base-uncased" model in the listing that follows, to get an idea of the architecture.

.Pytorch "bert-base-uncased" summary
[source,python]
----
>>> from transformers import BertModel
>>> model = BertModel.from_pretrained('bert-base-uncased')
>>> print(model)
BertModel(
  (embeddings): BertEmbeddings(
    (word_embeddings): Embedding(30522, 768, padding_idx=0)
    (position_embeddings): Embedding(512, 768)
    (token_type_embeddings): Embedding(2, 768)
    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): BertEncoder(
    (layer): ModuleList(
      (0): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )

      ... # BertEncoder layers 1-10 (not shown for brevity) identical to the other BertLayer's

      (11): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (pooler): BertPooler(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (activation): Tanh()
  )
)
----

=== Sample Task: Fine-tuning pretrained BERT for Text Classification
In 2018, the Conversation AI footnote:[Conversation AI:https://conversationai.github.io/] team (a joint venture between Jigsaw and Google) hosted a Kaggle footnote:[Kaggle:kaggle.com] competition to develop a model to detect various types of toxicity in Wikipedia page user's comments.
At the time, LSTM's and Convolutional Neural Networks were best of breed, with use of bi-directional LSTM with attention considered to be cutting edge.
The promise of BERT is that it can learn word context from both left and right, making it powerful on a wide range of tasks including sentiment analysis and of course sentence classification.
Additionally, because BERT is pretrained on a large corpus, we should be able to fine-tune it fairly easily for this toxic comment classification task, so let's get started.

First, you need to obtain the Toxic Comment Classification Challenge dataset, which is available for download under the Creative Commons CCO license, by downloading from the competition site, https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge.
Once you have downloaded the dataset, unzip it to find that it contains zip files for training set (_train.csv.zip_) and the test set (_test.csv.zip_).
You will work with the training data, so extract the _train.csv.zip_ to resultant file, _train.csv_.

Next, built on top of the wonderful HuggingFace `transformers`, is the `simpletransformers` library footnote:[simpletransformers:https://simpletransformers.ai] that makes it easy to quickly setup and execute common NLP tasks including text classisfication, named entity recognition, question answering, conversational AI, and others.
You will use the library to quickly fine-tune a pre-trained BERT model for classifying toxic comments.
After that you will make some adjustments to improve the model in your quest to combat bad behavior and rid the world of online trolls.
Please install the package now with `pip install simplestransformers`.

==== BERT Example 1

It is useful to get a feel for the data, to see how it is formatted and to gain insight on what some sample comments look like.
Begin by loading the toxic comment training data using pandas and take a look at the first few entries as shown in the next listing.

.Load the toxic comments training data set
[source,python]
----
>>> import pandas as pd
>>> df = pd.read_csv('data/train.csv')  # <1>
>>> df.head()
                 id                                       comment_text  toxic  severe_toxic  obscene  threat  insult  identity_hate
0  0000997932d777bf  Explanation\nWhy the edits made under my usern...      0             0        0       0       0              0
1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0             0        0       0       0              0
2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0             0        0       0       0              0
3  0001b41b1c6bb37e  "\nMore\nI can't make any real suggestions on ...      0             0        0       0       0              0
4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0             0        0       0       0              0
>>> df.shape
(159571, 8)
----
<1> We extracted the downloaded toxic comment csv files to a `data` dir.

Whew, luckily none of the first five comments are obscene, so they're fit to print in this book.

[TIP]
.Spend a little time with the data
====
Typically at this point you would explore and analyze the data, focusing on the qualities of the text samples and the accuracy of the labels and perhaps ask yourself questions about the data.
How long are the comments in general?
Does sentence length or comment length have any relation to toxicity?
Consider focusing on some of the _severe_toxic_ comments.
What sets them apart from the merely _toxic_ ones?
What is the the class distribution?
Do you need to potentially account for a class imbalance in your training techniques?
====

You want to get to the training, so let's split the data set into training and validation (evaluation) sets.
With almost 160,000 samples available for model tuning, we elect to use an 80-20 train-test split.

.Split data into training and validation sets
[source,python]
----
>>> from sklearn.model_selection import train_test_split
>>> random_state=42
>>> labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']

>>> X = df[['comment_text']]
>>> y = df[labels]

>>> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,
...                                                     random_state=random_state)  # <1>
----
<1> Use `random_state` so we can guarantee the same split each time we run this code.

The `simpletransformers` library provides models for various classification tasks.
Since each comment can be assigned multiple labels (zero or more),  you determine that the `simpletransformers.classification.MultiLabelClassificationModel` is best suited for this job.
According to the documentation, footnote:[simpletransformers Multi-Label Classification: https://simpletransformers.ai/docs/multi-label-classification/] the model expects training samples in format of `["text", [labels]]`.
The next listing shows how to construct the datasets for training and evaluation.

.Create datasets for model
[source,python]
----
>>> def get_dataset(X, y):
...     data = [[X.iloc[i][0], y.iloc[i].values.tolist()] for i in range(X.shape[0])]
...     return pd.DataFrame(data, columns=['text', 'labels'])
...
>>> train_df = get_dataset(X_train, y_train)
>>> eval_df = get_dataset(X_test, y_test)
>>> train_df.shape, eval_df.shape
((127656, 2), (31915, 2))

>>> train_df.head()  # <1>
                                                text              labels
0  Grandma Terri Should Burn in Trash \nGrandma T...  [1, 0, 0, 0, 0, 0]
1  , 9 May 2009 (UTC)\nIt would be easiest if you...  [0, 0, 0, 0, 0, 0]
2  "\n\nThe Objectivity of this Discussion is dou...  [0, 0, 0, 0, 0, 0]
3              Shelly Shock\nShelly Shock is. . .( )  [0, 0, 0, 0, 0, 0]
4  I do not care. Refer to Ong Teng Cheong talk p...  [0, 0, 0, 0, 0, 0]
----
<1> Check that the dataframe matches the format to feed to the model. (Oh, and we see our first toxic comment - poor Grandma Terri.)

You have prepared the (raw) data for training.
Next, you'll setup just a few basic parameters and then you will be ready to load a pretrained BERT for multi-label classification and kick-off the fine-tuning (training).

.Setup training parameters
[source,python]
----
>>> import logging
>>> logging.basicConfig(level=logging.INFO)  # <1>

>>> model_type = 'bert'  # <2>
>>> model_name = 'bert-base-cased'
>>> output_dir = f'{model_type}-example1-outputs'

>>> model_args = {
...     'output_dir': output_dir, # where to save results
...     'overwrite_output_dir': True, # allow re-run without having to manually clear output_dir
...     'manual_seed': random_state, # <3>
...     'no_cache': True,
... }
----
<1> Basic logging for model output during training.
<2> `model_type`, `model_name` will be used to load the base-cased BERT in the next code segment.
<3> For reproducible results. You recycled the same seed you used for `train_test_split()`, but you could have used a different one.

In the listing below you load the pretrained _bert-base-cased_ model configured to output the number of labels in our toxic comment data (6 total) and initialized for training with your `model_args` dictionary.footnote:[See "Configuring a Simple Transformers Model" section of the following webpage for full list of options and their defaults: https://simpletransformers.ai/docs/usage/]

// TODO: more discussion on model selection, hardware requirements, etc. e.g. For those of you who like power tools, you may be asking why we're using the "base" BERT and not BERT Large?

.Load pre-trained model and fine-tune
[source,python]
----
>>> from sklearn.metrics import roc_auc_score
>>> from simpletransformers.classification import MultiLabelClassificationModel

>>> model = MultiLabelClassificationModel(model_type, model_name, num_labels=len(labels),
...                                       args=model_args)  # <1>

>>> model.train_model(train_df=train_df)
----
<1> When the model is loaded you likely will see a message emitted that reads in part, "_You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference_". This is exactly what is done on the very next line!

The `train_model()` is doing the heavy lifting for you.
It loads the pretrained `BertTokenizer` for the pretrained _bert-base-cased_ model you selected and uses it to tokenize the `train_df['text']` to inputs for training the model.
The function combines these inputs with the `train_df[labels]` to generate a `TensorDataset` which it wraps with a pytorch `DataLoader`, that is then iterated over in batches to comprise the training loop.


In other words, with just a few lines of code you've fine-tuned a model (for one epoch) that has 12 Transformer blocks and 110 million parameters!
So what does that yield us?
Let's run inference on your evaluation set and check the results.

.Evaluation
[source,python]
----
>>> result, model_outputs, wrong_predictions = model.eval_model(eval_df, acc=roc_auc_score) # <1>
>>> result
{'LRAP': 0.9955934600588362,
 'acc': 0.9812396881786198,
 'eval_loss': 0.04415484298031397}
----
<1> Select `roc_auc_score` for accuracy metric because that was the one used in the Toxic Comment Challenge.

An roc_auc_score of 0.981 is not too bad out of the gate.
While it's not going to win you any accolades footnote:[Final leader board from the Kaggle Toxic Comment Classification Challenge:  https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/leaderboard], it does provide encouraging feedback that your training simulation and inference is setup correctly.

The implementations for `eval_model()` and `train_model()` are found in the base class for `MultiLabelClassificationModel`, `simpletransformers.classification.ClassificationModel`.
The evaluation code will look familiar to you, as it uses the `with torch.no_grad()` context manager for doing inference, as one would expect.
Taking the time to look at at the method implementations is suggested.
Particularly, `train_model()` is helpful for viewing exactly how the configuration options you select in the next section are employed during training and evaluation.

==== BERT Example 2

Building upon the training code you executed in the previous example, you'll work on improving your model's accuracy.
Cleaning the text a bit with some preprocessing is fairly straight-forward.
The book's example source code comes with a utility `TextPreprocessor` class we authored to replace common misspellings, expand contractions and perform other miscellaneous cleaning such as removing extra white-space characters.
Go ahead and rename the `comment_text` column to `original_text` in the loaded _train.csv_ dataframe.
Apply the preprocessor to the original text and store the refined text back to a `comment_text` column.

.Simple pre-processing on the comment_text
[source,python]
----
>>> from preprocessing.preprocessing import TextPreprocessor
>>> tp = TextPreprocessor()
loaded ./inc/preprocessing/json/contractions.json
loaded ./inc/preprocessing/json/misc_replacements.json
loaded ./inc/preprocessing/json/misspellings.json

>>> df = df.rename(columns={'comment_text':'original_text'})
>>> df['comment_text'] = df['original_text'].apply(lambda x: tp.preprocess(x)) # <1>

>>> pd.set_option('display.max_colwidth', 45)
>>> df[['original_text', 'comment_text']].head()
                                  original_text                                  comment_text
0  Explanation\nWhy the edits made under my ...  Explanation Why the edits made under my u...
1  D'aww! He matches this background colour ...  D'aww! He matches this background colour ...
2  Hey man, I'm really not trying to edit wa...  Hey man, i am really not trying to edit w...
3  "\nMore\nI can't make any real suggestion...  " More I cannot make any real suggestions...
4  You, sir, are my hero. Any chance you rem...  You, sir, are my hero. Any chance you rem...
----
<1> Save the original comment text in `df.original_text` for side-by-side comparison with the newly processed `df.comment_text`.

With the text cleaned, turn your focus to tuning the model initialization and training parameters.
In your first training run you accepted the default input sequence length (128) as an explicit value for `max_sequence_length` was not provided to the model.
The BERT-base model can handle sequences of maximum length 512.
As you increase `max_sequence_length` you may need to decrease `train_batch_size` and `eval_batch_size` to fit tensors into GPU memory, depending on the hardware available to you.
You can do some exploration on the lengths of the comment text to find an optimal max length.
Be mindful that at some point you'll get diminishing returns, where longer training and evaluation times incurred by using larger sequences do not yield a significant improvement in model accuracy.
For this example pick a `max_sequence_length` of 300, which is between the default of 128 and the model's capacity.
Also explicitly select `train_batch_size` and `eval_batch_size` to fit into GPU memory.

[WARNING]
================
You'll quickly realize your batch sizes are set too large if a GPU memory exception is displayed shortly after training or evaluation commences.
================

Recall that in your first fine-tuning run, the model trained for exactly one epoch.
Your hunch that the model could have trained longer to achieve better results is likely correct.
You want to find the sweet spot for the amount of training to do before the model overfits on the training samples.
Configure options to enable evaluation during training so you can also setup parameters for early stopping.
The evaluation scores during training are used to inform early stopping. So set `evaluation_during_training=True` to enable it, and set `use_early_stopping=True` also.
As the model learns to generalize, we expect oscillations in performance between evaluation steps, so you don't want to stop training just because the accuracy declined from previous value in the latest evaluation step.
Configure the _patience_ for early stopping, which is the number of consecutive evaluations without improvement (defined to be greater than some delta) at which to terminate the training.
You're going to set `early_stopping_patience=4` because you're somewhat patient but you have your limits. Use `early_stopping_delta=0` because no amount of improvement is too small.

Saving these transformers models to disk repeatedly during training (e.g. after each evaluation phase or after each epoch) takes time and disk space.
For this example you're looking to keep the _best_ model generated during training, so specify `best_model_dir` to save your best performing model.
It's convenient to save it to a location under the `output_dir` so all your training results are organized as you run more experiments on your own.

.Setup parameters for evaluation during training and early stopping
[source,python]
----
...
>>> model_type = 'bert'
>>> model_name = 'bert-base-cased'

>>> output_dir = f'{model_type}-example2-outputs'  # <1>
>>> best_model_dir = f'{output_dir}/best_model'

>>> model_args = {
...     'output_dir': output_dir,
...     'overwrite_output_dir': True,
...     'manual_seed': random_state,
...     'no_cache': True,
...
...     'best_model_dir': best_model_dir,
...
...     'max_seq_length': 300,
...     'train_batch_size': 24,
...     'eval_batch_size': 24,
...
...     'gradient_accumulation_steps': 1,
...     'learning_rate': 5e-5,
...
...     'evaluate_during_training': True,
...     'evaluate_during_training_steps': 1000,
...     'save_eval_checkpoints': False,
...     "save_model_every_epoch": False,
...     'save_steps': -1,  # saving model unnecessarily takes time during training
...     'reprocess_input_data': True,
...
...     'num_train_epochs': 5,
...     'use_early_stopping': True,
...     'early_stopping_patience': 4,
...     'early_stopping_delta': 0,
... }
----
<1> Take note that the `output_dir` path is changed for "example2" so the results from "example1" are not clobbered.

Train the model by calling `model.train_train_model()`, as you did previously.
You notice however, that you now must pass an `eval_df` to the API because you updated `model_args` with parameters to describe evaluation during training.

.Load pre-trained model and fine-tune with early stopping
[source,python]
----
>>> model = MultiLabelClassificationModel(model_type, model_name, num_labels=len(labels),
...                                       args=model_args)

>>> model.train_model(train_df=train_df, eval_df=eval_df, acc=roc_auc_score,
...                   show_running_loss=False, verbose=False)
----

Your _best_ model was saved during training in the `best_model_dir`.
It should go without saying that this is the model you want to use for inference.
The evaluation code segment is updated to load the model by passing `best_model_dir` for the `model_name` parameter in the model class' constructor.

.Evaluation with *best* model
[source,python]
----
>>> best_model = MultiLabelClassificationModel(model_type, best_model_dir,
...                                            num_labels=len(labels), args=model_args)

>>> result, model_outputs, wrong_predictions = best_model.eval_model(eval_df, acc=roc_auc_score)

>>> result
{'LRAP': 0.996060542761153,
 'acc': 0.9893854727083252,
 'eval_loss': 0.040633044850540305}
----
Now that's looking better. A 0.989 accuracy puts us in contention with the top challenge solutions of early 2018.

// TODO: Reference complete source code for these BERT examples (notebooks)?

// TODO: this just provides a small taste of fine-tuning with a transformer model...next steps, other options availale (wandb/tensordboard dashboards) for reader to experiment with.


== In the real world

Transformers have taken off in popularity for a variety of real world applications:

* Replika uses GPT-3 to generate more than 20% of its replies
* Qary uses BERT to generate open domain question answers
* Search engines use BERT to improve search results
* `nboost` uses transformers to create a semantic search proxy for ElasticSearch
* aidungeon.io uses GPT-3 to generate an endless variety of rooms

== Test Yourself

1. How is the input and output dimensionality of a transformer layer different from any other deep learning layer like CNN, RNN, or LSTM layers?
2. How could you expand the information capacity of an transformer network like BERT or GPT-2?
3. What is a rule of thumb for estimating the information capacity required to get high accuracy on a particular labeled dataset?
4. What is a good measure of the relative information capacity of 2 deep learning networks?
5. What are some techniques for reducing the amount of labeled data required to train a transformer for a problem like summarization?
6. How do you measure the accuracy or loss of a summarizer?

== Summary

* By combining multiple proven approaches, you can build an intelligent dialog engine.
* Breaking "ties" between the replies generated by the four main chatbot approaches is one key to intelligence.
* Good chatbots may help save the world.
* You can teach machines a lifetime of knowledge without spending a lifetime programming them.

