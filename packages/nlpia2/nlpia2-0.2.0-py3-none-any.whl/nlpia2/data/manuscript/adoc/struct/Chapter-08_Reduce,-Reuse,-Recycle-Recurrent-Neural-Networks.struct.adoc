
:toc: left
:toclevels: 6

++++
  <style>
  .first-sentence {
    text-align: left;
    margin-left: 0%;
    margin-right: auto;
    width: 66%;
    background: Beige;
  }
  .last-sentence {
    text-align: right;
    margin-left: auto;
    margin-right: 0%;
    width: 66%;
    background: AliceBlue;
  }
  </style>
++++
= Natural Language Processing in Action, Second Edition
== Reduce, Reuse, Recycle Your Words (RNNs and LSTMs)
[.first-sentence]
This chapter covers

[.last-sentence]
This chapter covers

[.first-sentence]
An _RNN_ (Recurrent Neural Network) recycles tokens.

[.last-sentence]
This sustainability or regenerative ability of RNNs is their superpower.

[.first-sentence]
It turns out that your NLP pipeline can predict the next tokens in a sentence much better if it remembers what it has already read and understood.

[.last-sentence]
This makes your machine reader much more sustainable, it can keep reading and reading and reading...for as long as you like.

[.first-sentence]
But wait, isn't recursion dangerous?

[.last-sentence]
For your NLP RNN, this comes naturally as you _pop_ (remove) a token off of the _stack_ (the text string) before you feed that input back into your network.

[.first-sentence]
Technically "recurrence" and "recursion" are two different things. footnote:[Mathematics forum StackExchange question about recurrence and recursion (https://math.stackexchange.com/questions/931035/recurrence-vs-recursive)]

[.last-sentence]
The `.forward(x)` method is called in a `for` loop that is outside of the RNN itself.

[.first-sentence]
RNNs are _neuromorphic_.

[.last-sentence]
So recurrence must be a smart, efficient way to use your brain resources to understand text.

[.first-sentence]
As you read this text you are recycling what you already know about the previous words before updating your prediction of what's going to happen next.

[.last-sentence]
So RNNs are great not only for reading text data but also for tagging and writing text.

[.first-sentence]
RNNs are a game changer for NLP.

[.last-sentence]
They have spawned an explosion of practical applications and advancements in deep learning and AI.

=== What are RNNs good for?
[.first-sentence]
The previous deep learning architectures you've learned about are great for processing short bits of text - usually individual sentences.

[.last-sentence]
RNNs open up a whole new range of applications like generative conversational chatbots and text summarizers that combine concepts from many different places within your documents.

[.first-sentence]
This is the superpower of RNNs, they process sequences of tokens or vectors.

[.last-sentence]
Your code can dynamically decide when enough is enough.

.Recycling tokens creates endless options

[.first-sentence]
You can use RNNs to achieve state-of-the-art performance on many of the tasks you're already familiar with, even when your text is shorter than infinity `;)`.

[.last-sentence]
You can use RNNs to achieve state-of-the-art performance on many of the tasks you're already familiar with, even when your text is shorter than infinity `;)`.

[.first-sentence]
And RNNs are one of the most efficient and accurate ways to accomplish some new NLP tasks that you will learn about in this chapter:

[.last-sentence]
And RNNs are one of the most efficient and accurate ways to accomplish some new NLP tasks that you will learn about in this chapter:

[.first-sentence]
If you read through the RNNs that are at the top of the leader board on Papers with Code footnote:[Papers with Code query for RNN applications (https://proai.org/pwc-rnn)] you can see that RNNs are the most efficient approach for many applications.

[.last-sentence]
If you read through the RNNs that are at the top of the leader board on Papers with Code footnote:[Papers with Code query for RNN applications (https://proai.org/pwc-rnn)] you can see that RNNs are the most efficient approach for many applications.

[.first-sentence]
RNNs aren't just for researchers and academics.

[.last-sentence]
In the real world, people are using RNNs to:

[.first-sentence]
You can probably guess what most of those applications are about, but you're probably curious about that last one (subdomain prediction).

[.last-sentence]
Once you know a subdomain, a hacker or pentester can scan the domain to find vulnerabilities in the server security.

[.first-sentence]
And once you will soon be comfortable using RNNs to generate completely new words, phrases, sentences, paragraphs and even entire pages of text.

[.last-sentence]
It can be so much fun playing around with RNNs that you could find yourself accidentally creating applications that open up opportunities for completely new businesses.

==== RNNs can handle any sequence
[.first-sentence]
In addition to NLP, RNNs are useful for any sequence of numerical data, such as time series.

[.last-sentence]
And often they will want to predict all of this simultaneously in one vector.

[.first-sentence]
Because RNNs can output something for each and every element in a sequence, you can create an RNN that outputs a prediction for "tomorrow" -- the next sequence element after the one you currently know.

[.last-sentence]
This means that once you master backpropagation through time, you will be able to use RNNs to predict things such as:

[.first-sentence]
As soon as you have a prediction of the target variable you can measure the error - the difference between the model's output and the desired output.

[.last-sentence]
This usually happens at the last time step in whatever sequence of events you are processing.

==== RNNs remember everything you tell them
[.first-sentence]
Have you ever accidentally touched wet paint and found yourself "reusing" that paint whenever you touched something?

[.last-sentence]
Well now, instead of sliding a word stencil across the words in a sentence you are going to roll a paint roller across them... while they're still wet!

[.first-sentence]
Imagine painting the letters of a sentence with slow-drying paint and laying it on thick.

[.last-sentence]
Maybe you're even supporting LBGTQ pride week by painting the crosswalks and bike lanes in North Park.

.A rainbow of meaning

[.first-sentence]
Now, pick up a clean paint roller and roll it across the letters of the sentence from the beginning of the sentence to the end.

[.last-sentence]
All the letters after the first one would be smeared together to create a smudgy stripe that only vaguely resembles the original sentence.

.Pot of gold at the end of the rainbow

[.first-sentence]
The smudge gathers up all the paint from the previous letters into a single compact representation of the original text.

[.last-sentence]
And for a machine, it is certainly much more dense and compact than the original sequence of characters.

[.first-sentence]
In NLP we want to create compact, dense vector representations of text.

[.last-sentence]
You could keep rolling the roller forever across more and more text, if you like, squeezing more and more text into the compact representation.

[.first-sentence]
In previous chapters, your tokens were mostly words or word n-grams.

[.last-sentence]
Now does it make more sense how this smudge at the end of the "Wet Paint!" lettering represents an embedding of all the letters of the text?

[.first-sentence]
One last imaginary step might help you bring out the hidden meaning in this thought experiment.

[.last-sentence]
In fact, this vector representation of your text is stored in a variable called `hidden` in many implementations of RNNs.

[.first-sentence]
RNN embeddings are different from the word and document embeddings you learned about in Chapters 6 and 7.

[.last-sentence]
This new vocabulary made it a lot easier for his team to develop a shared mental model of the NLP pipeline.

[.first-sentence]
Keep your eye out for the hidden layer later in this chapter.

[.last-sentence]
In figure <<ch8_best_figure>> you can see how this blending of meaning in an embedding vector is much more compact and blurry than the original text.

.Gather up meaning into one spot

[.first-sentence]
You could read into the paint smudge something of the meaning of the original text, just like in a Rorschach inkblot test.

[.last-sentence]
Soon you'll see how each of these steps is analogous to the actual mathematical operations going on in an RNN layer of neurons.

[.first-sentence]
Your paint roller has smeared many of the letters at the end of the sentence so that the last exclamation point at the end is almost completely unintelligible.

[.last-sentence]
And if you want to see the message embedded in your paint roller, you just roll it out onto a clean piece of paper.

[.first-sentence]
In your RNN you can accomplish this by outputting the hidden layer activations after you've rolled your RNN over the tokens of some text.

[.last-sentence]
We even use the word "gather" to express understanding of something someone says, as in "I gather from what you just said, that rolling paint rollers over wet paint are analogous to RNNs."

[.first-sentence]
Your paint roller has compressed, or encoded, the entire sentence of letters into a short smudgy impressionistic stripe of paint.

[.last-sentence]
And then you reused all those smudges on your roller to create a new impression of the entire sentence.

==== RNNs hide their understanding
[.first-sentence]
The key change for an RNN is that it maintains a hidden embedding by recycling the meaning of each token as it reads them one at a time.

[.last-sentence]
An RNN needs to read your text one token at a time.

[.first-sentence]
An ordinary feedforward neuron just multiplies the input vector by a bunch of weights to create an output.

[.last-sentence]
Feedforward network layers transform can only transform one vector into another.

.Ordinary feedforward neuron

[.first-sentence]
With RNNs, your neuron never gets to see the vector for the entire text.

[.last-sentence]
RNNs enable machines to finally learn Turing complete programs rather than just isolated functions.footnote:["The unreasonable effectiveness of RNNs" (https://karpathy.github.io/2015/05/21/rnn-effectiveness)]

.A neuron with recurrence

[.first-sentence]
If you unroll your RNN it begins to look a lot like a chain... a Markov Chain, in fact.

[.last-sentence]
Fortunately, you started doing something similar to this when you slid the CNN window or kernel across the text in chapter 7.

[.first-sentence]
How can you implement neural network recurrence in Python?

[.last-sentence]
<<listing-recurrence-pytorch>> implements a minimal RNN from scratch, without using PyTorch's `RNNBase` class.

.Recurrence in PyTorch

[.first-sentence]
You can see how this new RNN neuron now outputs more than one thing.

[.last-sentence]
Not only do you need to return the output or prediction, but you also need to output the hidden state tensor to be reused by the "future self" neuron.

[.first-sentence]
Of course, the PyTorch implementation has many more features.

[.last-sentence]
This is simply because their embeddings of a bidirectional language model are more balanced, forgetting as much about the beginning of the text as they forget about the end of the text.

==== RNNs remember everything you tell them
[.first-sentence]
To see how RNNs retain a memory of all the tokens of a document you can unroll the neuron diagram in Figure 8.7.

[.last-sentence]
This is like unrolling a `for` loop, when you just copy and paste the lines of code within the loop the appropriate number of times.

.Unroll an RNN to reveal its hidden secrets

[.first-sentence]
Figure 8.7 shows an RNN passes the hidden state along to the next "future self" neuron, sort of like Olympic relay runners passing the baton.

[.last-sentence]
You can see how the tensors for the input tokens are modified many, many times before the RNN finally sees the last token in the text.

[.first-sentence]
Another nice feature of RNNs is that you can tap into an output tensor anywhere along the way.

[.last-sentence]
This means you can tackle challenges like machine translation, named entity recognition, anonymization and deanonymization of text, and even unredaction of government documents.footnote:[Portland Python User Group presentation on unredacting the Meuller Report (https://proai.org/unredact)]

[.first-sentence]
These two features are what make RNNs unique.

[.last-sentence]
These two features are what make RNNs unique.

[.first-sentence]
That first feature is not such a big deal.

[.last-sentence]
In fact, the most advanced NLP models to date, _transformers_, create a max length limit and pad the text just like CNNs.

[.first-sentence]
However, that second feature of RNNs is a really big deal.

[.last-sentence]
Just look at some of the linguistic features that SpaCy can identify for each word in some example "hello world" text in listing <<figure-spacy-tags-tokens>>.

.SpaCy tags tokens with RNNs

[.first-sentence]
It's all well and good to have all that information - all that output whenever you want it.

[.last-sentence]
And you're probably excited to try out RNNs on really long text to see how much it can actually remember.

=== Predict someone&#8217;s nationality from only their last name
[.first-sentence]
To get you up to speed quickly on recycling, you'll start with the simplest possible token -- the lowly character (letter or punctuation).

[.last-sentence]
You might even be worried that it could be used to harm individuals from particular cultures.

[.first-sentence]
Like you, the authors' LinkedIn followers were suspicious when we mentioned we were training a model to predict the demographic characteristics of names.

[.last-sentence]
Volunteers and open-source contributors can then train NLP models from these anonymized conversation datasets to identify healthcare or education content that can be helpful for users, while simultaneously protecting user privacy.

[.first-sentence]
This multilingual dataset will give you a chance to learn how to deal with diacritics and other embellishments that are common for non-English words.

[.last-sentence]
You will also need to remove the cedilla embellishment that is often added to the letter "C" in Turkish, Kurdish, Romance and other alphabets.

[.first-sentence]
Now that you have a pipeline that "normalizes" the alphabet for a broad range of languages, your model will generalize better.

[.last-sentence]
You just need to label a few dozen examples in each language you are interested in "solving" for.

[.first-sentence]
Now let's see if you've created a _solvable problem_.

[.last-sentence]
A solvable machine learning problem is one where:

[.first-sentence]
Think about this problem of predicting the country or dialect associated with a surname.

[.last-sentence]
Is it solvable?

[.first-sentence]
Start with the first question above.

[.last-sentence]
This is where the concept of AI comes from, if a machine or algorithm can do intelligent things, we call it AI.

[.first-sentence]
Think about what makes this problem hard.

[.last-sentence]
And if you want to change your name, this model can help you craft it so that it invokes the nationality that you want people (and machines) to perceive of you.

[.first-sentence]
Take a look at some random names from this dataset to see if you can find any character patterns that are reused in multiple countries.

[.last-sentence]
Take a look at some random names from this dataset to see if you can find any character patterns that are reused in multiple countries.

.Load the

[.first-sentence]
Take a quick look at the data before diving in.

[.last-sentence]
Don't expect to achieve 90% accuracy on a classifier.

[.first-sentence]
You also want to count up the unique categories in your dataset so you know how many options your model will have to choose from.

[.last-sentence]
You also want to count up the unique categories in your dataset so you know how many options your model will have to choose from.

.Unique nationalities in the dataset

[.first-sentence]
In listing <<listing-unique-nationalities-in-the-dataset>> you can see the thirty-seven unique nationalities and language categories that were collected from multiple sources.

[.last-sentence]
It can only try to return the right answer as often as possible.

[.first-sentence]
The diversity of nationalities and data sources helped us do name substitution to anonymize messages exchanged within our multilingual chatbots.

[.last-sentence]
To build this dataset we augmented the PyTorch RNN tutorial dataset with names scraped from public APIs that contained data for underrepresented countries in Africa, South and Central America, and Oceania.

[.first-sentence]
When we were building this dataset during our weekly mob programming on Manning's Twitch channel, Rochdi Khalid pointed out that his last name is Arabic.

[.last-sentence]
This dataset is a mashup of data from a variety of sources.footnote:[There's more info and data scraping code in the nlpia2 package (https://proai.org/nlpia-ch08-surnames)] some of which create labels based on broad language labels such as "Arabic" and others are labeled with their specific nationality or dialect, such as Moroccan, Algerian, Palestinian, or Malaysian.

[.first-sentence]
Dataset bias is one of the most difficult biases to compensate for unless you can find data for the groups you want to elevate.

[.last-sentence]
You can use this dataset for your own projects where you need a truly global slice of names from a variety of cultures.

[.first-sentence]
Diversity has its challenges.

[.last-sentence]
Most multilingual deep learning pipelines utilize the Latin character set (Romance script alphabet) to represent words in all languages.

[.first-sentence]
Transliteration is when you translate the characters and spellings of words from one language's alphabet to another, making it possible to represent words using the Latin character set (Romance script alphabet) used in Europe and the Americas.

[.last-sentence]
Transliteration is a lot harder for non-Latin alphabets such as Nepalese.

[.first-sentence]
Here's how you can calculate just how much overlap there is within each of your categories (nationalities).

[.last-sentence]
Here's how you can calculate just how much overlap there is within each of your categories (nationalities).

[.first-sentence]
In addition to the overlap _across_ nationalities, the PyTorch tutorial dataset contained many duplicated names within nationalities.

[.last-sentence]
This technique is sometimes referred to as "oversampling the minority class" because it boosts the frequency and accuracy of underrepresented classes in your dataset.

[.first-sentence]
If you're curious about the original surname data check out the PyTorch "RNN Classification Tutorial".footnote:[PyTorch RNN Tutorial by Sean Robertson (https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)]

[.last-sentence]
There were only 108 unique Arabic surnames among the 2000 Arabic examples in Arabic.txt.footnote:[The original PyTorch RNN Tutorial surname dataset with duplicates (https://download.pytorch.org/tutorial/data.zip)]

.Surname oversampling

[.first-sentence]
This means that even a relatively simple model (like the one shown in the PyTorch tutorial) should be able to correctly label popular names like Abadi and Zogby as Arabic.

[.last-sentence]
And you can anticipate your model's confusion matrix statistics by counting up the number of nationalities associated with each name in the dataset.

[.first-sentence]
You are going to use a deduplicated dataset that you loaded in listing <<listing-surname-oversampling>>.

[.last-sentence]
You'll need to keep this in mind if you intend to use this model in the real world on a more random sample of names.

.Name nationality overlap

[.first-sentence]
To help diversify this dataset and make it a little more representative of real-world statistics, we added some names from India and Africa.

[.last-sentence]
This allows us to _default to open_ with both NLP datasets as well as software.footnote:[Qary (https://docs.qary.ai) combines technology and data from all our multilingual chatbots (https://tangibleai.com/our-work)]

[.first-sentence]
A great way to find out if a machine learning pipeline has a chance of solving your problem, pretend you are the machine.

[.last-sentence]
And in some cases, you might find machines are much better than you because they can balance many patterns in their head more accurately than you can.

[.first-sentence]
By computing the most popular nationality for each name in the dataset, it is possible to create a confusion matrix, using the most common nationality as the "true" label for a particular name.

[.last-sentence]
This is because there are thousands of English and Irish names, but only 100 Scottish names in the original PyTorch tutorial dataset.

.The dataset is confused even before training

[.first-sentence]
We've added 26 more nationalities to the original PyTorch dataset.

[.last-sentence]
An RNN can deal with this ambiguity quite well, using the statistics of patterns in the character sequences to guide its classification decisions.

==== Build an RNN from scratch
[.first-sentence]
Here's the heart of your `RNN` class in <<listing-heart-rnn>>

[.last-sentence]
For an RNN you can use the `__init__()` method to set the hyperparameters that control the number of neurons in the hidden vector as well as the size of the input and output vectors.

[.first-sentence]
For an NLP application that relies on tokenizers, it's a good idea to include the tokenizer parameters within the init method to make it easier to instantiate again from data saved to disk.

[.last-sentence]
Keeping all those models and tokenizers connected is a challenge if they aren't stored together in one object.

[.first-sentence]
The same goes for the vectorizers in your NLP pipeline.

[.last-sentence]
If you store your vectorizers in your model class (see listing <<listing-heart-rnn>>), it will know exactly which category labels it wants to apply to your data.

.Heart of an RNN

[.first-sentence]
Technically, your model doesn't need the full `char2i` vocabulary.

[.last-sentence]
But by including the category labels within your model you can print them to the console whenever you want to debug the internals of your model.

==== Training an RNN, one token at a time
[.first-sentence]
The dataset of 30000+ surnames for 37+ countries in the `nlpia2` project is manageable, even on a modest laptop.

[.last-sentence]
And if you limit yourself to only 10 countries, 10000 surnames, and get lucky (or smart) with your choice of the learning rate, you can train a good model in two minutes.

[.first-sentence]
Rather than using the built-in `torch.nn.RNN` layer you can build your first RNN from scratch using plain old `Linear` layers.

[.last-sentence]
This will generalize your understanding so you can design your own RNNs for almost any application.

.Training on a single sample must loop through the characters

[.first-sentence]
The `nlpia2` package contains a script to orchestrate the training process and allow you to experiment with different hyperparameters.

[.last-sentence]
The `nlpia2` package contains a script to orchestrate the training process and allow you to experiment with different hyperparameters.

[.first-sentence]
You want to use the `%run` magic command within the iPython console rather than running your machine learning scripts in the terminal using the `python` interpreter.

[.last-sentence]
And if you cancel the run or if there is an error that halts the script, you will still be able to examine the global variables without having to start over from scratch.

[.first-sentence]
Once you launch the `classify_name_nationality.py` script it will prompt you with several questions about the model's hyperparameters.

[.last-sentence]
This allows you to try many different hyperparameter combinations and fine tune your intuitions about NLP while fine tuning your model.

[.first-sentence]
Listing <<listing-interactive-prompts-hyperparameters>> shows some hyperparameter choices that will give you pretty good results.

[.last-sentence]
Can you find a set of hyperparameters that can identify a broader set of nationalities with better accuracy?

.Interactive prompts so you can play with hyperparameters

[.first-sentence]
Even this simplified RNN model with only 128 neurons and 1500 epochs takes several minutes to converge to a decent accuracy.

[.last-sentence]
So you don't expect to get very high accuracy, especially when you give the model many nationalities (categories) to choose from.

.Training output log

[.first-sentence]
Looks like the RNN achieved 57% accuracy on the training set and 29% accuracy on the validation set.

[.last-sentence]
They don't want to confuse you.

[.first-sentence]
Now that you understand the ambiguity in the dataset you can see how hard the problem is and that this RNN does a really good job of generalizing from the patterns it found in the character sequences.

[.last-sentence]
Random guesses would have achieved 4% accuracy on 25 categories (`1/25 == .04`) even if there was no ambiguity in the nationality associated with each name.

[.first-sentence]
Let's try it on some common surnames that are used in many countries.

[.last-sentence]
Even though Morocco isn't the top prediction for "Khalid", Morocco is in second place!

[.first-sentence]
The top 3 predictions are all for Arabic-speaking countries.

[.last-sentence]
I don't think there are expert linguists that could do this prediction as fast or as accurately as this RNN model did.

[.first-sentence]
Now it's time to dig deeper and examine some more predictions to see if you can figure out how only 128 neurons can predict someone's nationality so well.

[.last-sentence]
Now it's time to dig deeper and examine some more predictions to see if you can figure out how only 128 neurons can predict someone's nationality so well.

==== Understanding the results
[.first-sentence]
To use a model like this in the real world you will need to be able to explain how it works to your boss.

[.last-sentence]
Once you understand how this RNN works you'll be able to use that knowledge to trick algorithms into doing what's right, elevating rather than discriminating against historically disadvantaged groups and cultures.

[.first-sentence]
Perhaps the most important piece of an AI algorithm is the metric you used to train it.

[.last-sentence]
As with most ML algorithms, `log` means natural log, sometimes written as _ln_ or _log to the base e_.

[.first-sentence]
This means that the model is only 31% confident that Rochdi is Algerian.

[.last-sentence]
These probabilities (likelihoods) can be used to explain how confident your model is to your boss or teammates or even your users.

[.first-sentence]
If you're a fan of "debug by print" you can modify your model to print out anything you're interested in about the math the model uses to make predictions.

[.last-sentence]
If you do decide to use this approach, you only need to `.detach()` the tensors from the GPU or CPU where they are located to bring them back into your working RAM for recording in your model class.

[.first-sentence]
A nice feature of RNNs is that the predictions are built up step by step as your `forward()` method is run on each successive token.

[.last-sentence]
Instead, you can just make predictions of the hidden and output tensors for parts of the input text.

[.first-sentence]
You may want to add some `predict_*` convenience functions for your model class to make it easier to explore and explain the model's predictions.

[.last-sentence]
So you can create a `predict_hidden` method to output the 128-D hidden tensor and a `predict_proba` to show you the predicted probabilities for each of the target categories (nationalities).

[.first-sentence]
This `predict_hidden` convenience method converts the text (surname) into a tensor before iterating through the one-hot tensors to run the forward method (or just the model's `self`).

[.last-sentence]
This `predict_hidden` convenience method converts the text (surname) into a tensor before iterating through the one-hot tensors to run the forward method (or just the model's `self`).

[.first-sentence]
This `predict_hidden` method gives you access to the most interesting part of the model where the "logic" of the predictions is taking place.

[.last-sentence]
The hidden layer evolves as it learns more and more about the nationality of a name with each character.

[.first-sentence]
Finally, you can use a `predict_category` convenience method to run the model's forward pass predictions to predict the nationality of a name.

[.last-sentence]
Finally, you can use a `predict_category` convenience method to run the model's forward pass predictions to predict the nationality of a name.

[.first-sentence]
The key thing to recognize is that for all of these methods, you don't necessarily have to input the entire string for the surname.

[.last-sentence]
It is perfectly fine to reevaluate the first part of the surname text over and over again, as long as you reset the hidden layer each time.

[.first-sentence]
If you input an expanding window of text you can see how the predictions and hidden layer evolve in their understanding of the surname.

[.last-sentence]
This is perhaps because so many Chinese surnames contain 4 (or fewer) characters.footnote:[Thank you Tiffany Kho for pointing this out.]

[.first-sentence]
Now that you have helper functions you can use them to record the hidden and category predictions as the RNN is run on each letter in a name.

[.last-sentence]
Now that you have helper functions you can use them to record the hidden and category predictions as the RNN is run on each letter in a name.

[.first-sentence]
And you can create a 128 x 6 matrix of all the hidden layer values in a 6-letter name.

[.last-sentence]
The list of PyTorch tensors can be converted to a list of lists and then a DataFrame to make it easier to manipulate and explore.

[.first-sentence]
This wall of numbers contains everything your RNN "thinks" about the name as it is reading through it.

[.last-sentence]
This wall of numbers contains everything your RNN "thinks" about the name as it is reading through it.

[.first-sentence]
There are some Pandas display options that will help you get a feel for the numbers in a large DataFrame without TMI ("too much information").

[.last-sentence]
Here are some of the settings that helped improve the printouts of tables in this book

[.first-sentence]
To display only 2 decimal places of precision for floating point values try: `pd.options.display.float_format = '{:.2f}'`.

[.last-sentence]
To display only 2 decimal places of precision for floating point values try: `pd.options.display.float_format = '{:.2f}'`.

[.first-sentence]
To display a maximum of 12 columns and 7 rows of data from your DataFrame: `pd.options.display.max_columns = 12` and `pd.options.display.max_rows = 7`

[.last-sentence]
To display a maximum of 12 columns and 7 rows of data from your DataFrame: `pd.options.display.max_columns = 12` and `pd.options.display.max_rows = 7`

[.first-sentence]
These only affect the displayed representation of your data, not the internal values used when you do addition or multiplication.

[.last-sentence]
These only affect the displayed representation of your data, not the internal values used when you do addition or multiplication.

[.first-sentence]
As you've probably done with other large tables of numbers, it's often helpful to find patterns by correlating it with other numbers that are interesting to you.

[.last-sentence]
For example, you may want to find out if any of the hidden weights are keeping track of the RNN's position within the text - how many characters it is from the beginning or end of the text.

[.first-sentence]
Interestingly our hidden layer has room in its hidden memory to record the position in many different places.

[.last-sentence]
Andrej Karpathy experimented with several more ways to glean insight from the weights of your RNN model in his blog post "The unreasonable effectiveness of RNNs" in the early days of discovering RNNs. footnote:[footnote:["The unreasonable effectiveness of RNNs" by Andrej Karpathy (https://karpathy.github.io/2015/05/21/rnn-effectiveness)]]

==== Multiclass classifiers vs multi-label taggers
[.first-sentence]
How can you deal with the ambiguity of multiple different correct nationalities for surnames?

[.last-sentence]
And if you're looking for the `sklearn` models suited to this kind of problem you want to search for "multi-output classification."

[.first-sentence]
Multi-label taggers are made for ambiguity.

[.last-sentence]
We're talking about a kind of machine learning model that can assign multiple discrete labels to an object in your dataset.

[.first-sentence]
A multiclass classifier has multiple different categorical labels that are matched to objects, one label for each object.

[.last-sentence]
But if you want to label a name with all the relevant nationalities and genders that are appropriate, then you would need a tagging model.

[.first-sentence]
This may seem like splitting hairs to you, but it's much more than just semantics.

[.last-sentence]
Ultimately he ended up building a tagger, which gave RTD advertisers more effective placements for their ads and gave developers reading documentation more relevant advertisements.

[.first-sentence]
To turn any multi-class classifier into a multi-label tagger you must change your activation function from `softmax` to an element-wise `sigmoid` function.

[.last-sentence]
A sigmoid function allows every value to take on any value between zero and one, such that each dimension in your multi-label tagging output represents the independent binary probability of that particular label applying to that instance.

=== Backpropagation through time
[.first-sentence]
Backpropagation for RNNs is a lot more work than for CNNs.

[.last-sentence]
RNNs do forward and backward propagation in time, from one token in the sequence to the next.

[.first-sentence]
But you can see in the unrolled RNN in Figure 8.7 that your training must propagate the error back through all the weight matrix multiplications.

[.last-sentence]
So your training loop will need to loop through all the tokens backward to ensure that the error at each step of the way is used to adjust the weights.

[.first-sentence]
The initial error value is the distance between the final output vector and the "true" vector for the label appropriate for that sample of text.

[.last-sentence]
PyTorch calculates the gradients it needs during forward propagation and then multiplies those gradients by the error for each token to decide how much to adjust the weights and improve the predictions.

[.first-sentence]
And once you've adjusted the weights for all the tokens in one layer you do the same thing again for all the tokens on the next layer.

[.last-sentence]
Unlike backpropagation through a linear layer or CNN layer, the backpropagation on an RNN must happen serially, one token at a time.

[.first-sentence]
An RNN is just a normal feedforward neural network "rolled up" so that the Linear weights are multiplied again and again for each token in your text.

[.last-sentence]
At each time step, it is the _same_ neural network, just processing a different input and output at that location in the text.

[.first-sentence]
In all of these examples, you have been passing in a single training example, the _forward pass_, and then backpropagating the error.

[.last-sentence]
But for now, think of these processes in terms of just single data samples, single sentences, or documents.

[.first-sentence]
In chapter 7 you learned how to process a string all at once with a CNN.

[.last-sentence]
CNNs accomplish this with overlapping windows of weights that can detect almost any pattern of meaning in text.

.1D convolution with embeddings

[.first-sentence]
In chapter 7 you imagined striding the kernel window over your text, one step at a time.

[.last-sentence]
In fact, on a GPU these matrix multiplications (dot products) are all happening _in parallel_ at approximately the _same_ time.

[.first-sentence]
But an RNN is different.

[.last-sentence]
You need one matrix of weights for the hidden vector and another for the output vector.

[.first-sentence]
If you've done any signal processing or financial modeling you may have used an RNN without knowing it.

[.last-sentence]
An  _auto-regressive moving average_ (ARMA) model is an RNN in disguise.footnote:[ARMA model explanation (https://en.wikipedia.org/wiki/Autoregressive_model)]

[.first-sentence]
In this chapter, you are learning about a new way to structure the input data.

[.last-sentence]
The variable `t` is just another name for the index variable in your sequence of tokens.

[.first-sentence]
You will even see places where you use the integer value of `t` to retrieve a particular token in the sequence of tokens with an expression such as `token = tokens[t]`.

[.last-sentence]
In past chapters, you may have seen that we sometimes used `i` for this index value.

[.first-sentence]
Now you will use multiple different indexes to keep track of what has been passed into the network and is being output by the network:

[.last-sentence]
Now you will use multiple different indexes to keep track of what has been passed into the network and is being output by the network:

.Data fed into a recurrent network

[.first-sentence]
This 2-D tensor representation of a document is similar to the "player piano" representation of text in chapter 2.

[.last-sentence]
Only this time you are creating a dense representation of each token using word embeddings.

[.first-sentence]
For an RNN you no longer need to process each text sample all at once.

[.last-sentence]
Instead, you process text one token at a time.

[.first-sentence]
In your recurrent neural net, you pass in the word vector for the first token and get the network's output.

[.last-sentence]
The network has a concept of before and after, cause and effect - some vague notion of time (see Figure 8.8).

==== Initializing the hidden layer in an RNN
[.first-sentence]
There's a chicken-and-egg problem with the hidden layer when you restart the training of an RNN on each new document.

[.last-sentence]
Your model's `forward()` method needs a vector to concatenate with the input vector so that it will be the right size for multiplying by `W_c2h` and `W_c2o`.

[.first-sentence]
The most obvious approach is to set the initial hidden state to all zeroes and allow the biases and weights to quickly ramp up to the best values during the training on each sample.

[.last-sentence]
Better yet you can use some gradient or pattern of values between zero and 1 which is your particular "secret sauce", based on your experience with similar problems.

[.first-sentence]
Getting creative and being consistent with your initialization of deep learning networks has the added benefit of creating more "explainable" AI.

[.last-sentence]
For example, you will know which positions in the hidden state vector are keeping track of position (time) within the text.

[.first-sentence]
To get the full benefit of this consistency in your initialization values you will also need to be consistent with the ordering of your samples used during training.

[.last-sentence]
So don't pursue this advanced seeding approach until you've fully mastered the random sampling and shuffling that has proven so effective.

[.first-sentence]
As long as you are consistent throughout the training process, your network will learn the biases and weights that your network needs to layer on top of these initial values.

[.last-sentence]
And that can create a recognizable structure in your neural network weights.

[.first-sentence]
In some cases, it can help to seed your neural networks with an initial hidden state other than all zeros.

[.last-sentence]
Their approach is to initialize all weights and biases using a random seed that can be reused in subsequent trainings.

[.first-sentence]
Now your network is remembering something! Well, sort of. A few things remain for you to figure out. For one, how does backpropagation even work in a structure like this?

[.last-sentence]
Now your network is remembering something! Well, sort of. A few things remain for you to figure out. For one, how does backpropagation even work in a structure like this?

[.first-sentence]
Another approach that is popular in the Keras community is to retain the hidden layer from a previous batch of documents.

[.last-sentence]
You do this when you want your model to work equally well at making predictions "cold" without any priming by reading similar documents or nearby passages of text.

[.first-sentence]
So unless you are trying to squeeze out every last bit of accuracy you can for a really difficult problem you should probably just reset it to zeros every time to start feeding a new document into your model.

[.last-sentence]
And make sure you prepare your documents in a consistent order and can reproduce this document ordering for a new set of documents that you need to do prediction on with your model.

=== Remembering with recurrent networks
[.first-sentence]
An RNN remembers previous words in the text they are processing and can keep adding more and more patterns to its memory as it processes a theoretically limitless amount of text.

[.last-sentence]
This can help it understand patterns that span the entire text and recognize the difference between two texts that have dramatically different meanings depending on where words occur.

[.first-sentence]
_I apologize for the lengthy letter. I didn't have time to write a shorter one._

[.last-sentence]
_I apologize for the lengthy letter. I didn't have time to write a shorter one._

[.first-sentence]
_I apologize for the short letter. I didn't have time to write a lengthy one._

[.last-sentence]
_I apologize for the short letter. I didn't have time to write a lengthy one._

[.first-sentence]
Swapping the words "short" and "lengthy", flips the meaning of this Mark Twain quote.

[.last-sentence]
It's something that smart humans can still do better than even the smartest AI.

[.first-sentence]
The CNNs you learned about in Chapter 7 would have a hard time making the connection between these two sentences about lengthy and short letters, whereas RNNs make this connection easily.

[.last-sentence]
This makes them better at summarizing lengthy Mark Twain letters and makes them better at understanding his long sophisticated jokes.

[.first-sentence]
Mark Twain was right.

[.last-sentence]
In chapter 9 you'll see this attention mechanism at work, as well as the other tricks that make the transformer approach to RNNs the most successful and versatile deep learning architecture so far.

[.first-sentence]
Summarization of lengthy text is still an unsolved problem in NLP.

[.last-sentence]
And it's hard to measure how well you've done it.

[.first-sentence]
You will have to develop generally intelligent machines that understand common sense logic and can organize and manipulate memories and symbolic representations of those memories.

[.last-sentence]
This would give it a working memory that it could then store in long-term memory whenever it ran across a concept that was important to remember.

==== Word-level Language Models
[.first-sentence]
All the most impressive language models that you've read about use words as their tokens, rather than individual characters.

[.last-sentence]
And you're going to have to deal with much longer documents than just surnames, so you will want to `batchify` your dataset to speed it up.

[.first-sentence]
Take a look at the Wikitext-2 dataset and think about how you will preprocess it to create a sequence of token IDs (integers).

[.last-sentence]
Take a look at the Wikitext-2 dataset and think about how you will preprocess it to create a sequence of token IDs (integers).

[.first-sentence]
Oh wow, this is going to be an interesting dataset.

[.last-sentence]
If you use your tokenization and vocabulary-building skills from previous chapters you should be able to create a Corpus class like the one used in the RNN examples coming up.footnote:[The full source code is in the nlpia2 package (https://gitlab.com/tangibleai/nlpia2/-/blob/main/src/nlpia2/ch08/rnn_word/data.py)]

[.first-sentence]
And you always want to make sure that your vocabulary has all the info you need to generate the correct words from the sequence of word IDs:

[.last-sentence]
And you always want to make sure that your vocabulary has all the info you need to generate the correct words from the sequence of word IDs:

[.first-sentence]
Now, during training your RNN will have to read each token one at a time.

[.last-sentence]
These batches can each become columns or rows in a matrix that PyTorch can more efficiently perform math on within a _GPU_ (Graphics Processing Unit).

[.first-sentence]
In the `nlpia2.ch08.data` module you'll find some functions for batchifying long texts.

[.last-sentence]
In the `nlpia2.ch08.data` module you'll find some functions for batchifying long texts.

[.first-sentence]
One last step and your data is ready for training.

[.last-sentence]
You need to `stack` the tensors within this list so that you have one large tensor to iterate through during your training.

==== Gated Recurrent Units (GRUs)
[.first-sentence]
For short text, ordinary RNNs with a single activation function for each neuron works well.

[.last-sentence]
That's the problem that GRU (Gated Recurrent Unit) and LSTM (Long and Short Term Memory) neural networks aim to fix.

[.first-sentence]
How do you think you could counteract fading memory of early tokens in a text string?

[.last-sentence]
GRUs add `if` statements, called _logic gates_ (or just "gates"), to RNN neurons.

[.first-sentence]
The magic of machine learning and backpropagation will take care of the if statement conditions for you, so you don't have to adjust logic gate thresholds manually.

[.last-sentence]
And the magic of back-propagation in time will train the LSTM gates to let important signals (aspects of token meaning) pass through and get recorded in the hidden vector and cell state vector.

[.first-sentence]
But wait, you probably thought we already had if statements in our network.

[.last-sentence]
And that structure was intentionally designed with a purpose, reflecting what researchers thing would help RNN neurons deal with this long-term memory problem.

[.first-sentence]
In addition to the original RNN output gate, GRUs add two new logic gates or activation functions within your recurrent unit.

[.last-sentence]
In addition to the original RNN output gate, GRUs add two new logic gates or activation functions within your recurrent unit.

[.first-sentence]
You already had an activation function on the output of your RNN layer.

[.last-sentence]
This output logic gate is called the "new" logic gate in a GRU.

[.first-sentence]
So when you are thinking about how many units to add to your neural network to solve a particular problem, each LSTM or GRU unit gives your network a capacity similar to 2 "normal" RNN neurons or hidden vector dimensions.

[.last-sentence]
A unit is just a more complicated, higher-capacity neuron, and you can see this if you count up the number of "learned parameters" in your LSTM model and compare it to those of an equivalent RNN.

[.first-sentence]
You're probably wondering why we started using the word "unit" rather than "neuron" for the elements of this neural net.

[.last-sentence]
This gives your GRU or LSTM units more capacity for learning and understanding text, so you will probably need fewer of them to achieve the same performance as an ordinary RNN.

[.first-sentence]
The _reset_, _update_, and _new_ logic gates are implemented with the fully-connected linear matrix multiplications and nonlinear activation functions you are familiar with from Chapter 5.

[.last-sentence]
Figure 8.12 shows how the input vector and hidden vector for a single token flow through the logic gates and output the prediction and hidden state tensors.

.GRUs add capacity with logic gates

[.first-sentence]
If you have gotten good at reading data flow diagrams like Figure 8.12 you may be able to see that the GRU _update_ and _relevance_ logic gates are implementing the following two functions: footnote:[PyTorch docs for GRU layers (https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU)]

[.last-sentence]
If you have gotten good at reading data flow diagrams like Figure 8.12 you may be able to see that the GRU _update_ and _relevance_ logic gates are implementing the following two functions: footnote:[PyTorch docs for GRU layers (https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU)]

[.first-sentence]
Looking at these two lines of code you can see that inputs to the formula are exactly the same.

[.last-sentence]
And you may notice in the block diagram (figure 8.12) that the input and hidden tensors are concatenated together before the matrix multiplication by W_reset, the reset weight matrix.

[.first-sentence]
Once you add GRUs to your mix of RNN model architectures, you'll find that they are much more efficient.

[.last-sentence]
)]

[.first-sentence]
The more weights or learned parameters there are, the greater the capacity of your model to learn more things about the data.

[.last-sentence]
By choosing the right combination of algorithms, sizes and types of layers, you can reduce the number of weights or parameters your model must learn while simultaneously creating smarter models with greater capacity to make good predictions.

[.first-sentence]
If you experiment with a variety of GRU hyperparameters using the `nlpia2/ch08/rnn_word/hypertune.py` script you can aggregate all the results with your RNN results to compare them all together.

[.last-sentence]
If you experiment with a variety of GRU hyperparameters using the `nlpia2/ch08/rnn_word/hypertune.py` script you can aggregate all the results with your RNN results to compare them all together.

[.first-sentence]
You can see from these experiments that GRUs are your best bet for creating language models that understand text well enough to predict the next word.

[.last-sentence]
And they take less time to train than RNNs to achieve comparable accuracy.

==== Long and Short-Term Memory (LSTM)
[.first-sentence]
An LSTM neuron adds two more internal gates in an attempt to improve both the long-term and the short-term memory capacity of an RNN.

[.last-sentence]
The first one is just the normal activation function that you are familiar with.

[.first-sentence]
But what about that unlabeled `tanh` activation function at the upper right of Figure 8.12?

[.last-sentence]
The cell state vector holds a representation of the meaning of the text over the long term, since the beginning of a document.

[.first-sentence]
In Figure 8.13 you can see how these four logic gates fit together.

[.last-sentence]
But the new cell state tensor is where the long and short-term memories of past patterns are encoded and stored to be reused on the next token.

.LSTMs add a forgetting gate and a cell output

[.first-sentence]
One thing in this diagram that you'll probably only see in the smartest blog posts is the explicit linear weight matrix needed to compute the output tensor.footnote:[Thank you Rian Dolphin for your rigorous explanation (https://towardsdatascience.com/lstm-networks-a-detailed-explanation-8fae6aefc7f9)]

[.last-sentence]
You'll need to add this fully connected linear layer yourself at whichever layer you are planning to compute predictions based on your hidden state tensor.

[.first-sentence]
You're probably saying to yourself "Wait, I thought all hidden states (encodings) were the same, why do we have this new _cell state_ thing?"

[.last-sentence]
It's designed to be selective in the things it retrains to keep room for things it learns about the text long before it reaches the end of the string.

[.first-sentence]
The formulas for computing the LSTM logic gates and outputs are very similar to those for the GRU.

[.last-sentence]
Because an LSTM cell contains more nonlinear activation functions and weights it has more information processing capacity.

==== Give your RNN a tuneup
[.first-sentence]
As you learned in Chapter 7, hyperparameter tuning becomes more and more important as your neural networks get more and more complicated.

[.last-sentence]
You can explore all the hyperparameters that you are curious about using the code in `nlpia2/ch08`.footnote:[The `hypertune.py` script in the `ch08/rnn_word` module within the `nlpia2` Python package https://gitlab.com/tangibleai/nlpia2/-/blob/main/src/nlpia2/ch08/rnn_word/hypertune.py]

[.first-sentence]
It's a really exciting thing to explore the hyperspace of options like this and discover surprising tricks for building accurate models.

[.last-sentence]
And the fewer layers you have the faster the model will train.

[.first-sentence]
Experiment often, and always document what things you tried and how well the model worked.

[.last-sentence]
Your lifelong goal is to train your mental model to predict which hyperparameter values will produce the best results in any given situation.

[.first-sentence]
If you feel the model is overfitting the training data but you can't find a way to make your model simpler, you can always try increasing the `Dropout(percentage)`.

[.last-sentence]
But 20% to 50% is a pretty safe range for a lot of RNNs and most NLP problems.

[.first-sentence]
If you're like Cole and I when we were getting started in NLP, you're probably wondering what a "unit" is.

[.last-sentence]
A unit is just a more complicated, higher-capacity neuron, and you can see this if you count up the number of "learned parameters" in your LSTM model and compare it to those of an equivalent RNN.

=== Predicting
[.first-sentence]
The word-based RNN language model you trained for this chapter used the `WikiText-2` corpus.footnote:[PyTorch `torchtext` dataset (https://pytorch.org/text/0.8.1/datasets.html#wikitext-2)]

[.last-sentence]
Also, the uninteresting sections such as the References at the end of the articles have been removed.

[.first-sentence]
Unfortunately, the PyTorch version of the WikiText-2 includes "<unk>" tokens that randomly replace, or mask, 2.7% of the tokens.

[.last-sentence]
And you can compare the accuracy of your LSTM and GRU models to those of the experts that use this benchmark data.footnote:[AI researchers(https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/)]

[.first-sentence]
Here is an example paragraph at the end of the masked training dataset `train.txt`.

[.last-sentence]
Here is an example paragraph at the end of the masked training dataset `train.txt`.

[.first-sentence]
It seems that the last Wikipedia article in the WikiText-2 benchmark corpus is about the common starling (a small bird in Europe).

[.last-sentence]
And from the article, it seems that the starling appears to be good at mimicking human speech, just as your RNN can.

[.first-sentence]
What about those "<unk>" tokens?

[.last-sentence]
Because you have a pretty good English language model in your brain you can probably predict the tokens that have been masked out with all those "<unk>" tokens.

[.first-sentence]
But if the machine learning model you are training thinks these are normal English words, you may confuse it.

[.last-sentence]
The RNN you are training in this chapter is trying to discern the _meaning_ of the meaningless "<unk>" token, and this will reduce its understanding of all other words in the corpus.

[.first-sentence]
If you want to avoid this additional source of error and confusion, you can try training your RNN on the unofficial raw text for the wikitext-2 benchmark.

[.last-sentence]
There is a one-to-one correspondence between the tokens of the official wikitext-2 corpus and the unofficial raw version in the nlpia2 repository. footnote:[`nlpia2` package with code and data for the rnn_word model code and datasets used in this chapter (https://gitlab.com/tangibleai/nlpia2/-/tree/main/src/nlpia2/ch08/rnn_word/data/wikitext-2)]

[.first-sentence]
So how many "<eos>" and "<unk>" tokens are there in this training set?

[.last-sentence]
So how many "<eos>" and "<unk>" tokens are there in this training set?

[.first-sentence]
So 2.6% of the tokens have been replaced with the meaningless "<unk>" token.

[.last-sentence]
And the "<eos>" token marks the newlines in the original text, which is typically the end of a paragraph in a Wikipedia article.

[.first-sentence]
So let's see how well it does at writing new sentences similar to those in the WikiText-2 dataset, including the "<unk>" tokens.

[.last-sentence]
We'll prompt the model to start writing with the word "The" to find out what's on the top of its "mind".

[.first-sentence]
The first line in the training set is "= Valkyria Chronicles III =" and the last article in the training corpus is titled "= Common Starling =".

[.last-sentence]
And it certainly isn't going to do any sense-making any time soon.

[.first-sentence]
Sense-making is the way people give meaning to the experiences that they share.

[.last-sentence]
Startups like DAOStack are experimenting with chatbots that bubble up the best ideas of a community and use them for building knowledge bases and making decisions. footnote:[DAOStack platform for decentralized governance (https://daostack.io/deck/DAOstack-Deck-ru.pdf)]

[.first-sentence]
You now know how to train a versatile NLP language model that you can use on word-level or character-level tokens.

[.last-sentence]
And you didn't have to go crazy on expensive GPUs and servers.

=== Test yourself
=== Summary
