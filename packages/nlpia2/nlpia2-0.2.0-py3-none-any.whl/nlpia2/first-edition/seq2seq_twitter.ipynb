{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# install Keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "tweets = []\n",
    "responses = []\n",
    "\n",
    "with io.open(\"/Users/hannes/Downloads/twitter_corpus/test.txt\", \"rb\") as twitter_data:\n",
    "    for i, text in enumerate(twitter_data.readlines()):\n",
    "        text = text.decode('ascii', 'ignore').strip().replace('\\n', '')  # this will remove emojis\n",
    "        if not i%2:  # line 1, 3, 5, ...\n",
    "            tweets.append(text)\n",
    "        else:\n",
    "            responses.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: psych is being taken off of netflix tomorrow and i have 8 episodes left and i work tonight kill me \n",
      "Reply: before you go to work open each episode up in its own tab\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "i = 3\n",
    "print(\"Tweet: {} \\nReply: {}\".format(tweets[i], responses[i]))\n",
    "dataset = tweets + responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', \"'\", '-', '.', '8', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y']\n",
      "Longest tweet found has 132 characters\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "chars = list(set(''.join(dataset)))\n",
    "max_len = max([len(i) for i in dataset])\n",
    "print(sorted(chars))\n",
    "print(\"Longest tweet found has {} characters\".format(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def convert_data(words, max_len, chars, char_indices):\n",
    "    num_words = len(words)\n",
    "    X = np.zeros((num_words, max_len, len(chars)), dtype=np.bool)\n",
    "    for i in range(num_words):\n",
    "        word = words[i]\n",
    "        for j, c in enumerate(word):\n",
    "            X[i, j, char_indices[c]] = 1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = convert_data(tweets, max_len, chars, char_indices)\n",
    "Y = convert_data(responses, max_len, chars, char_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-184ca172be73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurrent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimpleRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGRU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Dropout, TimeDistributed, Bidirectional, Input, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import SimpleRNN, GRU, LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(None, len(chars)), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128, input_shape=(None, len(chars)), return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(RepeatVector(max_len))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(TimeDistributed(Dense(len(chars))))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, Y, batch_size=1, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''An implementation of sequence to sequence learning for performing addition\n",
    "Input: \"535+61\"\n",
    "Output: \"596\"\n",
    "Padding is handled by using a repeated sentinel character (space)\n",
    "Input may optionally be inverted, shown to increase performance in many tasks in:\n",
    "\"Learning to Execute\"\n",
    "http://arxiv.org/abs/1410.4615\n",
    "and\n",
    "\"Sequence to Sequence Learning with Neural Networks\"\n",
    "http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n",
    "Theoretically it introduces shorter term dependencies between source and target.\n",
    "Two digits inverted:\n",
    "+ One layer LSTM (128 HN), 5k training examples = 99% train/test accuracy in 55 epochs\n",
    "Three digits inverted:\n",
    "+ One layer LSTM (128 HN), 50k training examples = 99% train/test accuracy in 100 epochs\n",
    "Four digits inverted:\n",
    "+ One layer LSTM (128 HN), 400k training examples = 99% train/test accuracy in 20 epochs\n",
    "Five digits inverted:\n",
    "+ One layer LSTM (128 HN), 550k training examples = 99% train/test accuracy in 30 epochs\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, TimeDistributed, Dense, RepeatVector, recurrent\n",
    "import numpy as np\n",
    "from six.moves import range\n",
    "from keras.engine.training import _slice_arrays\n",
    "\n",
    "class CharacterTable(object):\n",
    "    '''\n",
    "    Given a set of characters:\n",
    "    + Encode them to a one hot integer representation\n",
    "    + Decode the one hot integer representation to their character output\n",
    "    + Decode a vector of probabilities to their character output\n",
    "    '''\n",
    "    def __init__(self, chars, maxlen):\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def encode(self, C, maxlen=None):\n",
    "        maxlen = maxlen if maxlen else self.maxlen\n",
    "        X = np.zeros((maxlen, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            X[i, self.char_indices[c]] = 1\n",
    "        return X\n",
    "\n",
    "    def decode(self, X, calc_argmax=True):\n",
    "        if calc_argmax:\n",
    "            X = X.argmax(axis=-1)\n",
    "        return ''.join(self.indices_char[x] for x in X)\n",
    "\n",
    "\n",
    "class colors:\n",
    "    ok = '\\033[92m'\n",
    "    fail = '\\033[91m'\n",
    "    close = '\\033[0m'\n",
    "\n",
    "# Parameters for the model and dataset\n",
    "TRAINING_SIZE = 5000\n",
    "DIGITS = 3\n",
    "INVERT = True\n",
    "# Try replacing GRU, or SimpleRNN\n",
    "RNN = recurrent.LSTM\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "LAYERS = 1\n",
    "MAXLEN = DIGITS + 1 + DIGITS\n",
    "\n",
    "chars = '0123456789+ '\n",
    "ctable = CharacterTable(chars, MAXLEN)\n",
    "\n",
    "questions = []\n",
    "expected = []\n",
    "seen = set()\n",
    "print('Generating data...')\n",
    "while len(questions) < TRAINING_SIZE:\n",
    "    f = lambda: int(''.join(np.random.choice(list('0123456789')) for i in range(np.random.randint(1, DIGITS + 1))))\n",
    "    a, b = f(), f()\n",
    "    # Skip any addition questions we've already seen\n",
    "    # Also skip any such that X+Y == Y+X (hence the sorting)\n",
    "    key = tuple(sorted((a, b)))\n",
    "    if key in seen:\n",
    "        continue\n",
    "    seen.add(key)\n",
    "    # Pad the data with spaces such that it is always MAXLEN\n",
    "    q = '{}+{}'.format(a, b)\n",
    "    query = q + ' ' * (MAXLEN - len(q))\n",
    "    ans = str(a + b)\n",
    "    # Answers can be of maximum size DIGITS + 1\n",
    "    ans += ' ' * (DIGITS + 1 - len(ans))\n",
    "    if INVERT:\n",
    "        query = query[::-1]\n",
    "    questions.append(query)\n",
    "    expected.append(ans)\n",
    "print('Total addition questions:', len(questions))\n",
    "\n",
    "print('Vectorization...')\n",
    "X = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(questions):\n",
    "    X[i] = ctable.encode(sentence, maxlen=MAXLEN)\n",
    "for i, sentence in enumerate(expected):\n",
    "    y[i] = ctable.encode(sentence, maxlen=DIGITS + 1)\n",
    "\n",
    "# Shuffle (X, y) in unison as the later parts of X will almost all be larger digits\n",
    "indices = np.arange(len(y))\n",
    "np.random.shuffle(indices)\n",
    "X = X[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# Explicitly set apart 10% for validation data that we never train over\n",
    "split_at = len(X) - len(X) / 10\n",
    "(X_train, X_val) = (_slice_arrays(X, 0, split_at), _slice_arrays(X, split_at))\n",
    "(y_train, y_val) = (y[:split_at], y[split_at:])\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "# \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE\n",
    "# note: in a situation where your input sequences have a variable length,\n",
    "# use input_shape=(None, nb_feature).\n",
    "model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars))))\n",
    "# For the decoder's input, we repeat the encoded input for each time step\n",
    "model.add(RepeatVector(DIGITS + 1))\n",
    "# The decoder RNN could be multiple layers stacked or a single layer\n",
    "for _ in range(LAYERS):\n",
    "    model.add(RNN(HIDDEN_SIZE, return_sequences=True))\n",
    "\n",
    "# For each of step of the output sequence, decide which character should be chosen\n",
    "model.add(TimeDistributed(Dense(len(chars))))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model each generation and show predictions against the validation dataset\n",
    "for iteration in range(1, 2):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(X_train, y_train, batch_size=BATCH_SIZE, nb_epoch=1,\n",
    "              validation_data=(X_val, y_val))\n",
    "    ###\n",
    "    # Select 10 samples from the validation set at random so we can visualize errors\n",
    "    for i in range(10):\n",
    "        ind = np.random.randint(0, len(X_val))\n",
    "        rowX, rowy = X_val[np.array([ind])], y_val[np.array([ind])]\n",
    "        preds = model.predict_classes(rowX, verbose=0)\n",
    "        q = ctable.decode(rowX[0])\n",
    "        correct = ctable.decode(rowy[0])\n",
    "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
    "        print('Q', q[::-1] if INVERT else q)\n",
    "        print('T', correct)\n",
    "        print('X', guess)\n",
    "        print(colors.ok + '☑' + colors.close if correct == guess else colors.fail + '☒' + colors.close, guess)\n",
    "        print('---')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "from keras.layers.recurrent import SimpleRNN, GRU, LSTM\n",
    "from keras.layers import TimeDistributed, Activation\n",
    "\n",
    "n_in_out = 1\n",
    "n_hidden = 100\n",
    "n_samples = 2297\n",
    "n_timesteps = 400\n",
    "\n",
    "model = Sequential()\n",
    "# `return_sequences` controls whether to copy the input automatically\n",
    "model.add(GRU( n_hidden, input_dim = n_in_out, return_sequences=True))\n",
    "model.add(TimeDistributed(n_in_out))\n",
    "model.compile(loss='mse', optimizer='rmsprop')\n",
    "\n",
    "X = np.random.random((n_samples, n_timesteps, n_in))\n",
    "Y = np.random.random((n_samples, n_timesteps, n_out))\n",
    "\n",
    "# learning the hidden states from source sentences\n",
    "\n",
    "Xp = model._predict(X)\n",
    "print(Xp.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "model.fit(X, Y, nb_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from keras.preprocessing.text import Tokenizer,base_filter\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation,Dropout,Embedding\n",
    "from keras.layers import LSTM\n",
    "\n",
    "\n",
    "# def shift(seq, n):\n",
    "#     n = n % len(seq)\n",
    "#     return seq[n:] + seq[:n]\n",
    "\n",
    "# txt=\"abcdefghijklmn\"*100\n",
    "\n",
    "# tk = Tokenizer(nb_words=2000, filters=base_filter(), lower=True, split=\" \")\n",
    "# tk.fit_on_texts(txt)\n",
    "# x = tk.texts_to_sequences(txt)\n",
    "# #shifing to left\n",
    "# y = shift(x,1)\n",
    "\n",
    "X_train = np.asarray([map(lambda x: ord(x), i) for i in X])\n",
    "Y_train = np.asarray([map(lambda y: ord(y), i) for i in Y])\n",
    "\n",
    "# max_features=max_len\n",
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "Y_train = pad_sequences(Y_train, maxlen=max_len)\n",
    "\n",
    "#lstm model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, len(max_features), input_length=max_len, dropout=0.2))\n",
    "model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2))\n",
    "model.add(Dense(max_len))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "model.fit(X, Y, batch_size=200, nb_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "max_features = set(itertools.chain(*X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(max_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, TimeDistributed, Bidirectional, Input, Activation\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "from keras.layers.recurrent import SimpleRNN, GRU, LSTM\n",
    "\n",
    "def convert_data(words, max_len, chars, char_indices):\n",
    "    num_words = len(words)\n",
    "    X = np.zeros((num_words, max_len, len(chars)), dtype=np.bool)\n",
    "    for i in xrange(num_words):\n",
    "        word = words[i]\n",
    "        for j, c in enumerate(word):\n",
    "            X[i, j, char_indices[c]] = 1\n",
    "    return X\n",
    "\n",
    "\n",
    "def decode(matrix, indices_char):\n",
    "    rv = []\n",
    "    for v in matrix:\n",
    "        _m = v.max(keepdims=True)\n",
    "        if _m:\n",
    "            _v = (v == v.max(keepdims=True)).astype(int)\n",
    "            rv.extend(''.join(indices_char[i] for i, x in enumerate(_v) if x))\n",
    "    return rv\n",
    "\n",
    "with open('missp.dat') as file:\n",
    "    words = map(lambda x: x.replace('\\n', ''), file.readlines())\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "cor_word = ''\n",
    "for w in words:\n",
    "    if w.startswith('$'):\n",
    "        cor_word = w.replace('$', '')\n",
    "    else:\n",
    "        Y.append(list(cor_word))\n",
    "        X.append(list(w))\n",
    "assert len(X) == len(Y)\n",
    "print(\"Detected {} training pairs\".format(len(X)))\n",
    "\n",
    "index = 0\n",
    "print(\"Misspelled: {}, Correctly Spelled: {}\".format(X[index], Y[index]))\n",
    "chars = list(set(''.join(words))).remove('$')\n",
    "max_len = max([len(i) for i in X+Y])\n",
    "print(\"Longest Word found has {} characters\".format(max_len))\n",
    "\n",
    "incorrect_words = [''.join(i).replace('_', ' ') for i in X[:100]]\n",
    "correct_words = [''.join(i).replace('_', ' ') for i in Y[:100]]\n",
    "\n",
    "CHARS = list(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ .'-\")\n",
    "chars = CHARS\n",
    "chars = sorted(set(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "X = convert_data(incorrect_words, max_len, chars, char_indices)\n",
    "Y = convert_data(correct_words, max_len, chars, char_indices)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(None, len(chars)), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128, input_shape=(None, len(chars)), return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(RepeatVector(max_len))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(TimeDistributed(Dense(len(chars))))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, Y, batch_size=1, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = model.predict(X[1].reshape(1, 18, 56))  # correct spelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"predicted: \\t\", \"\".join(decode(a[0], indices_char)))\n",
    "print(\"Output: \\t\", \"\".join(decode(Y[1], indices_char)))\n",
    "print(\"Input: \\t\\t\", \"\".join(decode(X[1], indices_char)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(incorrect_words)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def decode(matrix, indices_char):\n",
    "    rv = []\n",
    "    for v in matrix:\n",
    "        _m = v.max(keepdims=True)\n",
    "        if _m:\n",
    "            _v = (v == v.max(keepdims=True)).astype(int)\n",
    "            rv.extend(''.join(indices_char[i] for i, x in enumerate(_v) if x))\n",
    "    return rv\n",
    "decode(a[0], indices_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
