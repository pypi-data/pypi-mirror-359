{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bffa5397",
   "metadata": {},
   "source": [
    "#### [CH04_lane3](/home/hobs/code/hobs/nlpia-manuscript/manuscript/adoc/CH04_lane3.adoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a666d923",
   "metadata": {},
   "source": [
    "#### .Sample weights for your topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3af3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "topic = {}\n",
    "tfidf = dict(list(zip('cat dog apple lion NYC love'.split(),\n",
    "    np.random.rand(6))))  # <1>\n",
    "topic['petness'] = (.3 * tfidf['cat'] +\\\n",
    "                    .3 * tfidf['dog'] +\\\n",
    "                     0 * tfidf['apple'] +\\\n",
    "                     0 * tfidf['lion'] -\\\n",
    "                    .2 * tfidf['NYC'] +\\\n",
    "                    .2 * tfidf['love'])  # <2>\n",
    "topic['animalness']  = (.1 * tfidf['cat']  +\\\n",
    "                        .1 * tfidf['dog'] -\\\n",
    "                        .1 * tfidf['apple'] +\\\n",
    "                        .5 * tfidf['lion'] +\\\n",
    "                        .1 * tfidf['NYC'] -\\\n",
    "                        .1 * tfidf['love'])\n",
    "topic['cityness']    = ( 0 * tfidf['cat']  -\\\n",
    "                        .1 * tfidf['dog'] +\\\n",
    "                        .2 * tfidf['apple'] -\\\n",
    "                        .1 * tfidf['lion'] +\\\n",
    "                        .5 * tfidf['NYC'] +\\\n",
    "                        .1 * tfidf['love'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1142ec9c",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d23812e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector = {}\n",
    "word_vector['cat']  =  .3 * topic['petness'] + \\\n",
    "                       .1 * topic['animalness'] + \\\n",
    "                        0 * topic['cityness']\n",
    "word_vector['dog']  =  .3 * topic['petness'] \\\n",
    "                      -.1 * topic['animalness'] + \\\n",
    "                       .1 * topic['cityness']\n",
    "word_vector['apple']=   0 * topic['petness'] \\\n",
    "                      -.1 * topic['animalness'] + \\\n",
    "                       .2 * topic['cityness']\n",
    "word_vector['lion'] =   0 * topic['petness'] + \\\n",
    "                       .5 * topic['animalness'] \\\n",
    "                      -.1 * topic['cityness']\n",
    "word_vector['NYC']  = -.2 * topic['petness'] + \\\n",
    "                       .1 * topic['animalness'] + \\\n",
    "                       .5 * topic['cityness']\n",
    "word_vector['love'] =  .2 * topic['petness'] \\\n",
    "                      -.1 * topic['animalness'] + \\\n",
    "                       .1 * topic['cityness']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d1a4bc",
   "metadata": {},
   "source": [
    "#### .The toxic comment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a35a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.width = 120  # <1>\n",
    "DATA_DIR = ('https://gitlab.com/tangibleai/nlpia/-/raw/master/'\n",
    "            'src/nlpia/data')\n",
    "url= DATA_DIR + '/toxic_comment_small.csv'\n",
    "comments = pd.read_csv(url)\n",
    "index = ['comment{}{}'.format(i, '!'*j) for (i,j) in\n",
    "         zip(range(len(comments)), comments.toxic)\n",
    "        ]  # <2>\n",
    "comments = pd.DataFrame(\n",
    "    comments.values, columns=comments.columns, index=index)\n",
    "mask = comments.toxic.astype(bool).values\n",
    "comments['toxic'] = comments.toxic.astype(int)\n",
    "len(comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce482bb",
   "metadata": {},
   "source": [
    "#### .The toxic comment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad8ca4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.toxic.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd14150",
   "metadata": {},
   "source": [
    "#### .The toxic comment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cfd522",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb7919e",
   "metadata": {},
   "source": [
    "#### .The toxic comment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24a7d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def spacy_tokenize(sentence):\n",
    "   return [token.text for token in nlp(sentence.lower())]\n",
    "tfidf_model = TfidfVectorizer(tokenizer=spacy_tokenize)\n",
    "tfidf_docs = tfidf_model.fit_transform(\\\n",
    "    raw_documents=comments.text).toarray()\n",
    "tfidf_docs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243f4844",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efd380a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = comments.toxic.astype(bool).values  # <1>\n",
    "toxic_centroid = tfidf_docs[mask].mean(axis=0)  # <2>\n",
    "nontoxic_centroid = tfidf_docs[~mask].mean(axis=0)  # <3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54633a06",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bee124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_axis = toxic_centroid - nontoxic_centroid\n",
    "toxicity_score = tfidf_docs.dot(centroid_axis)  # <1>\n",
    "toxicity_score.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2170b6b6",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ad10a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "comments['manual_score'] = MinMaxScaler().fit_transform(\\\n",
    "    toxicity_score.reshape(-1,1))\n",
    "comments['manual_predict'] = (comments.manual_score > .5).astype(int)\n",
    "comments['toxic manual_predict manual_score'.split()].round(2).head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8e6452",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c026a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "(1 - (comments.toxic - comments.manual_predict).abs().sum()\n",
    "    / len(comments))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a81bde",
   "metadata": {},
   "source": [
    "#### .LDA performance on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16a35db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import \\\n",
    "    LinearDiscriminantAnalysis \n",
    "lda_tfidf = LinearDiscriminantAnalysis(n_components=1)\n",
    "lda_tfidf = lda_tfidf.fit(tfidf_docs, comments['toxic'])\n",
    "comments['tfidf_predict'] = lda_tfidf.predict(tfidf_docs)\n",
    "float(lda_tfidf.score(tfidf_docs, comments['toxic']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77d9fb4",
   "metadata": {},
   "source": [
    "#### .LDA performance on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9fa4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_docs,\\\n",
    "    comments.toxic.values, test_size=0.5, random_state=271828)\n",
    "lda_tfidf = LinearDiscriminantAnalysis(n_components=1)\n",
    "lda = lda_tfidf.fit(X_train, y_train)  # <1>\n",
    "round(float(lda.score(X_train, y_train)), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdbd835",
   "metadata": {},
   "source": [
    "#### .LDA performance on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397ea277",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(float(lda.score(X_test, y_test)), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b8b4ba",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b6e626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, lda.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93cb040",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df07754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(lda,X_test, y_test, cmap=\"Greys\",\n",
    "               display_labels=['non-toxic', 'toxic'], colorbar=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f261ed",
   "metadata": {},
   "source": [
    "#### .PCA magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98480fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn\n",
    "from matplotlib import pyplot as plt\n",
    "DATA_DIR = ('https://gitlab.com/tangibleai/nlpia/'\n",
    "            '-/raw/master/src/nlpia/data')\n",
    "df = pd.read_csv(DATA_DIR + '/pointcloud.csv.gz', index_col=0)\n",
    "pca = PCA(n_components=2)  # <1>\n",
    "df2d = pd.DataFrame(pca.fit_transform(df), columns=list('xy'))\n",
    "df2d.plot(kind='scatter', x='x', y='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefef2cc",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5328bdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_docs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15bd72d",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c90b3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=16, n_iter=100)  # <1>\n",
    "columns = ['topic{}'.format(i) for i in range(svd.n_components)]\n",
    "svd_topic_vectors = svd.fit_transform(tfidf_docs)  # <2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d880775e",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed40455",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(tfidf_model.vocabulary_.items())[:5]  # <1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a87c1d",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b391441",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_nums, terms = zip(*sorted(zip(tfidf.vocabulary_.values(),\n",
    "    tfidf.vocabulary_.keys())))  # <2>\n",
    "terms[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630411a7",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b85a119",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_term_matrix = pd.DataFrame(\n",
    "    svd.components_, columns=terms,\n",
    "    index=['topic{}'.format(i) for i in range(16)])\n",
    "pd.options.display.max_columns = 8\n",
    "topic_term_matrix.sample(5, axis='columns',\n",
    "    random_state=271828).head(4)  # <1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b050b2",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ce203f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 8\n",
    "toxic_terms = topic_term_matrix[\n",
    "    'pathetic crazy stupid idiot lazy hate die kill'.split()\n",
    "    ].round(3) * 100  # <1>\n",
    "toxic_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6965db4a",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af005692",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_terms.T.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9017f5b",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b3c8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_docs = tfidf_docs - tfidf_docs.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a8a4f8",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6976b714",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_16d, X_test_16d, y_train_16d, y_test_16d = train_test_split(\n",
    "    svd_topic_vectors, comments.toxic.values, test_size=0.5,\n",
    "    random_state=271828)\n",
    "lda_lsa = LinearDiscriminantAnalysis(n_components=1)\n",
    "lda_lsa = lda_lsa.fit(X_train_16d, y_train_16d)\n",
    "round(float(lda_lsa.score(X_train_16d, y_train_16d)), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b992130",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68740b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(float(lda_lsa.score(X_test_16d, y_test_16d)), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05629bb",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c2a817",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test_16d, lda_lsa.predict(X_test_16d).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b51ca86",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcabda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparam_table = pd.DataFrame()\n",
    "tfidf_performance = {'classifier': 'LDA',\n",
    "                     'features': 'TF–IDF (spacy tokenizer)',\n",
    "                     'train_accuracy': 0.99 ,\n",
    "                     'test_accuracy': 0.554,\n",
    "                     'test_precision': 0.383 ,\n",
    "                     'test_recall': 0.12,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80c4ce7",
   "metadata": {},
   "source": [
    "#### .A function that creates a record in the hyperparameter table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aae592e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hparam_rec(model, X_train, y_train, X_test, y_test,\n",
    "               model_name, features):\n",
    "    return {\n",
    "        'classifier': model_name,\n",
    "        'features': features,\n",
    "        'train_accuracy': float(model.score(X_train, y_train)),\n",
    "        'test_accuracy': float(model.score(X_test, y_test)),\n",
    "        'test_precision':\n",
    "            precision_score(y_test, model.predict(X_test)),\n",
    "        'test_recall':\n",
    "            recall_score(y_test, model.predict(X_test)),\n",
    "        'test_f1': f1_score(y_test, model.predict(X_test))\n",
    "        }\n",
    "lsa_performance = hparam_rec(lda_lsa, X_train_16d, y_train_16d,\n",
    "       X_test_16d,y_test_16d, 'LDA', 'LSA (16 components)'))\n",
    "hparam_table = hparam_table.append(lsa_performance)\n",
    "hparam_table.T  # <1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6301a6e",
   "metadata": {},
   "source": [
    "#### .A function that creates a record in the hyperparameter table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3956afb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(X,y, classifier, classifier_name, features):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.5, random_state=271828)\n",
    "    classifier = classifier.fit(X_train, y_train)\n",
    "    return hparam_rec(classifier, X_train, y_train, X_test,y_test,\n",
    "                      classifier_name, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20158ce8",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f0799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_corpus_len = 0\n",
    "for document_text in comments.text:\n",
    "    total_corpus_len += len(spacy_tokenize(document_text))\n",
    "mean_document_len = total_corpus_len / len(sms)\n",
    "round(mean_document_len, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6907bacb",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97283735",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([len(spacy_tokenize(t)) for t in comments.text]\n",
    "    ) * 1. / len(comments.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a89fc1",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649e29f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "counter = CountVectorizer(tokenizer=spacy_tokenize)\n",
    "bow_docs = pd.DataFrame(\n",
    "    counter.fit_transform(\n",
    "        raw_documents=comments.text).toarray(), index=index)\n",
    "column_nums, terms = zip(*sorted(zip(counter.vocabulary_.values(),\n",
    "    counter.vocabulary_.keys())))\n",
    "bow_docs.columns = terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3741b46a",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cfc2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.loc['comment0'].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b733c1d6",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be271733",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_docs.loc['comment0'][bow_docs.loc['comment0'] > 0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618e7e1f",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6271bab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation as LDiA\n",
    "ldia = LDiA(n_components=16, learning_method='batch')\n",
    "ldia = ldia.fit(bow_docs)  # <1>\n",
    "ldia.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc808680",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99c9499",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.width', 75)\n",
    "term_topic_matrix = pd.DataFrame(ldia.components_, index=terms,\\\n",
    "    columns=columns)  # <1>\n",
    "term_topic_matrix.round(2).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e4fc58",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fc1819",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_terms= components.loc['pathetic crazy stupid lazy idiot hate die kill'.split()].round(2)\n",
    "toxic_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0096838f",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe8571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_trivial_terms = [term for term in components.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebead2c1",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4451d0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldia16_topic_vectors = ldia.transform(bow_docs)\n",
    "ldia16_topic_vectors = pd.DataFrame(ldia16_topic_vectors,\\\n",
    "    index=index, columns=columns)\n",
    "ldia16_topic_vectors.round(2).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25df4381",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445e16cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ldia16 = LinearDiscriminantAnalysis()\n",
    "ldia16_performance=evaluate_model(ldia16_topic_vectors,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4795b33",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce00ec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparam_table = hparam_table.append(ldia16_performance,\n",
    "   ignore_index = True)\n",
    "hparam_table.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a18da3",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2463bba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldia32 = LDiA(n_components=32, learning_method='batch')\n",
    "ldia32 = ldia32.fit(bow_docs)\n",
    "model_ldia32 = LinearDiscriminantAnalysis()\n",
    "ldia32_performance =evaluate_model(ldia32_topic_vectors,\n",
    "         comments.toxic, model_ldia32, 'LDA', 'LDIA (32d)')\n",
    "hparam_table = hparam_table.append(ldia32_performance,\n",
    "          ignore_index = True)\n",
    "hparam_table.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af97051",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf843e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__file__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8766d08",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1654b8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis\\\n",
    "    import LinearDiscriminantAnalysis as LDA\n",
    "LDA??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11156308",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d8e03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = 1. / (1. + distance)\n",
    "distance = (1. / similarity) - 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a41463",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cfd353",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = 1. - distance\n",
    "distance = 1. - similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1f3377",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6e5f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "angular_distance = math.acos(cosine_similarity) / math.pi\n",
    "distance = 1. / similarity - 1.\n",
    "similarity = 1. - distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fed9b7",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bde101",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_URL = 'https://gitlab.com/tangibleai/qary/-/raw/master'\n",
    "FAQ_DIR = 'src/qary/data/faq'\n",
    "FAQ_FILENAME = 'short-faqs.csv'\n",
    "DS_FAQ_URL = '/'.join([REPO_URL, FAQ_DIR, FAQ_FILENAME])\n",
    "df = pd.read_csv(DS_FAQ_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96f2211",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e806f92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(df['question'])\n",
    "tfidfvectors = vectorizer.transform(df['question'])\n",
    "svd = TruncatedSVD(n_components=16, n_iterations=100)\n",
    "tfidfvectors_16d = svd.fit_transform(tfidfvectors)\n",
    "def bot_reply(question):\n",
    "      question_tfidf = vectorizer.transform([question]).todense()\n",
    "      question_16d = svd.transform(question_tfidf)\n",
    "      idx = question_16d.dot(tfidfvectors_16d.T).argmax()\n",
    "      print(\n",
    "           f\"Your question:\\n  {question}\\n\\n\"\n",
    "           f\"Most similar FAQ question:\\n  {df['question'][idx]}\\n\\n\"\n",
    "           f\"Answer to that FAQ question:\\n  {df['answer'][idx]}\\n\\n\"\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb0f158",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4c8af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_reply(\"What's overfitting a model?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea7d4f0",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2f3e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_reply(\"How do I decrease overfitting for Logistic Regression?\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
