{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57d0bdc6",
   "metadata": {},
   "source": [
    "#### [CH07_lane3](/home/hobs/code/hobs/nlpia-manuscript/manuscript/adoc/CH07_lane3.adoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950cc9c3",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9c14b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')  # <1>\n",
    "text = 'right ones in the right order you can nudge the world'\n",
    "doc = nlp(text)\n",
    "df = pd.DataFrame([\n",
    "   {k: getattr(t, k) for k in 'text pos_'.split()}\n",
    "   for t in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696d8198",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8313f30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df, columns=['pos_'], prefix='', prefix_sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ce5b35",
   "metadata": {},
   "source": [
    "#### .A function to compute Pierson correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36b24a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr(a, b):\n",
    "   \"\"\" Compute the Pearson correlation coefficient R \"\"\"\n",
    "   a = a - np.mean(a)\n",
    "   b = b - np.mean(b)\n",
    "   return sum(a * b) / np.sqrt(sum(a*a) * sum(b*b))\n",
    "a = np.array([0, 1, 2, 0, 1, 2, 0, 1, 2])\n",
    "b = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
    "corr(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e679b91",
   "metadata": {},
   "source": [
    "#### .A function to compute Pierson correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2515d944",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr(a, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d6bf24",
   "metadata": {},
   "source": [
    "#### .Tagging a quote with parts of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2cd0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "quote = \"The right word may be effective, but no word was ever\" \\\n",
    "   \" as effective as a rightly timed pause.\"\n",
    "tagged_words = {\n",
    "   t.text: [t.pos_, int(t.pos_ == 'ADV')]  # <1>\n",
    "   for t in nlp(quote)}\n",
    "df_quote = pd.DataFrame(tagged_words, index=['POS', 'ADV'])\n",
    "print(df_quote)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e65361",
   "metadata": {},
   "source": [
    "#### .Tagging a quote with parts of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96d395d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt = list(df_quote.loc['ADV'])\n",
    "print(inpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28db2c17",
   "metadata": {},
   "source": [
    "#### .Tagging a quote with parts of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f30194b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = [.5, .5]  # <1>\n",
    "output = []\n",
    "for i in range(len(inpt) - 1):  # <2>\n",
    "   z = 0\n",
    "   for k, weight in enumerate(kernel):  # <3>\n",
    "       z = z + weight * inpt[i + k]\n",
    "   output.append(z)\n",
    "print(f'inpt:\\n{inpt}')\n",
    "print(f'len(inpt): {len(inpt)}')\n",
    "print(f'output:\\n{[int(o) if int(o)==o else o for o in output]}')\n",
    "print(f'len(output): {len(output)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea4587f",
   "metadata": {},
   "source": [
    "#### .A line plot of input (is_adv) and output (adverbiness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fa9a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 120  # <1>\n",
    "import seaborn as sns\n",
    "sns.set_theme('paper')  # <2>\n",
    "df = pd.DataFrame([inpt, output], index=['inpt', 'output']).T\n",
    "ax = df.plot(style=['+-', 'o:'], linewidth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de48bc3",
   "metadata": {},
   "source": [
    "#### .A line plot of input (is_adv) and output (adverbiness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391faa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve(inpt, kernel):\n",
    "   output = []\n",
    "   for i in range(len(inpt) - len(kernel) + 1):  # <1>\n",
    "       output.append(\n",
    "           sum(\n",
    "               [\n",
    "                   inpt[i + k] * kernel[k]\n",
    "                   for k in range(len(kernel))  # <2>\n",
    "               ]\n",
    "           )\n",
    "       )\n",
    "   return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd0f037",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2bfca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = 'ADV ADJ VERB NOUN'.split()\n",
    "tagged_words = [\n",
    "   [tok.text] + [int(tok.pos_ == tag) for tag in tags]  # <1>\n",
    "   for tok in nlp(quote)]  # <2>\n",
    "df = pd.DataFrame(tagged_words, columns=['token'] + tags).T\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a6050a",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1400fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.tensor(\n",
    "    df.iloc[1:].astype(float).values,\n",
    "    dtype=torch.float32)  # <1>\n",
    "x = x.unsqueeze(0) # <2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a25684e",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb2b0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = pd.DataFrame(\n",
    "          [[1, 0, 0.],\n",
    "           [0, 0, 0.],\n",
    "           [0, 1, 0.],\n",
    "           [0, 0, 1.]], index=tags)\n",
    "print(kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35913f9f",
   "metadata": {},
   "source": [
    "#### .Running a single example through a convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a78bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(conv.forward(x).detach()).squeeze()\n",
    "df.loc['y'] = pd.Series(y)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1074c7e4",
   "metadata": {},
   "source": [
    "#### .Download secret message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde0f5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlpia2.init import maybe_download\n",
    "url = 'https://upload.wikimedia.org/wikipedia/' \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2856a45c",
   "metadata": {},
   "source": [
    "#### .Download secret message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fcbfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = maybe_download(url)  # <1>\n",
    "filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f82189e",
   "metadata": {},
   "source": [
    "#### .Download secret message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f257689a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "sample_rate, audio = wavfile.read(filepath)\n",
    "print(f'sample_rate: {sample_rate}')\n",
    "print(f'audio:\\n{audio}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e54193a",
   "metadata": {},
   "source": [
    "#### .Download secret message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a094332e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 7\n",
    "audio = audio[:sample_rate * 2]  # <1>\n",
    "audio = np.abs(audio - audio.max() / 2) - .5  # <2>\n",
    "audio = audio / audio.max()  # <3>\n",
    "audio = audio[::sample_rate // 400]  # <4>\n",
    "audio = pd.Series(audio, name='audio')\n",
    "audio.index = 1000 * audio.index / sample_rate  # <5>\n",
    "audio.index.name = 'time (ms)'\n",
    "print(f'audio:\\n{audio}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eeda32a",
   "metadata": {},
   "source": [
    "#### .A dot detecting kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d8ca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = [-1] * 24 + [1] * 24 + [-1] * 24  # <1>\n",
    "kernel = pd.Series(kernel, index=2.5 * np.arange(len(kernel)))\n",
    "kernel.index.name = 'Time (ms)'\n",
    "ax = kernel.plot(linewidth=3, ylabel='Kernel weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ed65f7",
   "metadata": {},
   "source": [
    "#### .A dot detecting kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016a5fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.array(kernel) / sum(np.abs(kernel))  # <1>\n",
    "pad = [0] * (len(kernel) // 2)  # <2>\n",
    "isdot = convolve(audio.values, kernel)\n",
    "isdot =  np.array(pad[:-1] + list(isdot) + pad)  # <3>\n",
    "df = pd.DataFrame()\n",
    "df['audio'] = audio\n",
    "df['isdot'] = isdot - isdot.min()\n",
    "ax = df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a78e70e",
   "metadata": {},
   "source": [
    "#### .A dot detecting kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cb02ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "isdot = np.convolve(audio.values, kernel, mode='same')  # <1>\n",
    "df['isdot'] = isdot - isdot.min()\n",
    "ax = df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc966a04",
   "metadata": {},
   "source": [
    "#### .Loading news posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ad9c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(HOME_DATA_DIR / 'news.csv')\n",
    "df = df[['text', 'target']]  # <1>\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43db8d5",
   "metadata": {},
   "source": [
    "#### .Learning token embeddings from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352b4c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "embedding = nn.Embedding(\n",
    "    num_embeddings=2000,  # <1>\n",
    "    embedding_dim=64,  # <2>\n",
    "    padding_idx=0)\n",
    "----\n",
    "<1> Your vocabulary must be the same as in your tokenizer.\n",
    "<2> A total of 50–100 dimensions is fine for small vocabularies and corpora.\n",
    "\n",
    "The embedding layer will be the first layer in your CNN.\n",
    "That will convert your token IDs into their own unique 64D word vectors.\n",
    "And backpropagation during training will adjust the weights in each dimension for each word to match 64 different ways words can be used to talk about newsworthy disasters.\n",
    "These embeddings won’t represent the complete meanings of words the way the fastText and GloVe vectors did in chapter 6; in fact, they are good for only one thing—determining whether a post contains newsworthy disaster information or not.\n",
    "\n",
    "Finally, you can train your CNN to see how well it will do on an extremely narrow dataset, like the Kaggle disaster posts dataset.\n",
    "Those hours of work crafting a CNN will pay off with a super-fast training time and impressive accuracy.\n",
    "\n",
    "[id=learn-your-embeddings, reftext={chapter}.{counter:listing}]\n",
    ".Learning your embeddings from scratch\n",
    "[source,python]\n",
    "----\n",
    "from nlpia2.ch07.cnn.train79 import Pipeline  # <1>\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    vocab_size=2000,\n",
    "    embeddings=(2000, 64),\n",
    "    epochs=7,\n",
    "    torch_random_state=433994,  # <2>\n",
    "    split_random_state=1460940,\n",
    ")\n",
    "\n",
    "pipeline = pipeline.train()\n",
    "----\n",
    "<1> nlpia2/src/nlpia2/ch07/cnn/train79.py\n",
    "<2> Sets random seeds, so others can reproduce your results\n",
    "\n",
    "[source,text]\n",
    "----\n",
    "Epoch: 1, loss: 0.66147, Train accuracy: 0.61392, Test accuracy: 0.63648\n",
    "Epoch: 2, loss: 0.64491, Train accuracy: 0.69712, Test accuracy: 0.70735\n",
    "Epoch: 3, loss: 0.55865, Train accuracy: 0.73391, Test accuracy: 0.74278\n",
    "Epoch: 4, loss: 0.38538, Train accuracy: 0.76558, Test accuracy: 0.77165\n",
    "Epoch: 5, loss: 0.27227, Train accuracy: 0.79288, Test accuracy: 0.77690\n",
    "Epoch: 6, loss: 0.29682, Train accuracy: 0.82119, Test accuracy: 0.78609\n",
    "Epoch: 7, loss: 0.23429, Train accuracy: 0.82951, Test accuracy: 0.79003\n",
    "----\n",
    "\n",
    "After only seven passes through your training dataset, you achieved 79% accuracy on your test set.\n",
    "On a modern laptop CPU, this should take less than a minute, and you kept the overfitting to a minimum by minimizing the total parameters in your model.\n",
    "The CNN uses very few parameters compared to the embedding layer.\n",
    "\n",
    "What happens if you continue the training for a bit longer?\n",
    "\n",
    "[id=continue-training, reftext={chapter}.{counter:listing}]\n",
    ".Continuing the training\n",
    "[source,python]\n",
    "----\n",
    "pipeline.epochs = 13  # <1>\n",
    "pipeline = pipeline.train()\n",
    "----\n",
    "<1> 7 + 13 will give you 20 total epochs of training.\n",
    "\n",
    "[source,python]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98327938",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251abb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_model(model):  # <1>\n",
    "    state = model.state_dict()\n",
    "    names = state.keys()\n",
    "    weights = state.values()\n",
    "    params = model.parameters()\n",
    "    df = pd.DataFrame()\n",
    "    df['name'] = list(state.keys())\n",
    "    df['all'] = p.numel(),\n",
    "    df['learned'] = [\n",
    "        p.requires_grad  # <2>\n",
    "        for p in params],  # <3>\n",
    "    size=p.size(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352b48a7",
   "metadata": {},
   "source": [
    "#### .Making room for GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbe5165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "embedding = nn.Embedding(\n",
    "    num_embeddings=2000,  # <1>\n",
    "    embedding_dim=50,  # <2>\n",
    "    padding_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6efefa",
   "metadata": {},
   "source": [
    "#### .Making room for GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c713bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nessvec.files import load_vecs_df\n",
    "glove = load_vecs_df(HOME_DATA_DIR / 'glove.6B.50d.txt')\n",
    "zeroes = [0.] * 50\n",
    "embed = []\n",
    "for tok in vocab:  # <1>\n",
    "    if tok in glove.index:\n",
    "        embed.append(glove.loc[tok])\n",
    "    else:\n",
    "        embed.append(zeros.copy())  # <2>\n",
    "embed = np.array(embed)\n",
    "embed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b543e1d1",
   "metadata": {},
   "source": [
    "#### .Making room for GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82904fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb437b9",
   "metadata": {},
   "source": [
    "#### .Initializing your embedding layer with GloVE vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54787c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = torch.Tensor(embed)  # <1>\n",
    "print(f'embed.size(): {embed.size()}')\n",
    "embed = nn.Embedding.from_pretrained(embed, freeze=False)  # <2>\n",
    "print(embed)\n",
    "----\n",
    "<1> Converts the pandas DataFrame to a torch.Tensor\n",
    "<2> freeze=False allows your Embedding layer to fine tune your embeddings\n",
    "\n",
    "==== Detecting meaningful patterns\n",
    "\n",
    "How you say something—the order of the words—makes a big difference.\n",
    "You combine words to create patterns that mean something significant to you, so you can convey that meaning to someone else.\n",
    "\n",
    "If you want your machine to be a meaningful natural language processor, it will need to be able to detect more than just the presence or absence of particular tokens.\n",
    "You want your machine to detect meaningful patterns hidden within word sequences.footnote:[_The International Association of Facilitators Handbook of Group Facilitation: Best Practices from the Leading Organization in Facilitation_,  Sandy Schuman (Ed.) (https://books.google.com/books?id=TgWsY7oSgtsC&lpg=PT35&dq=%22beneath%20the%20words%22%20empathy%20listening&pg=PT35#v=onepage&q=%22beneath%20the%20words%22%20empathy%20listening&f=false).]\n",
    "\n",
    "Convolutions are the filters that bring out meaningful patterns from words.\n",
    "And the best part is, you no longer have to hardcode these patterns into the convolutional kernel.\n",
    "The training process will search for the best possible pattern-matching convolutions for your problem.\n",
    "Each time you propagate the error from your labeled dataset back through the network (backpropagation), the optimizer will adjust the weights in each of your filters so that they get better and better at detecting meaning and classifying your text examples.\n",
    "\n",
    "=== Robustifying your CNN with dropout\n",
    "\n",
    "Most neural networks are susceptible to adversarial examples that trick them into outputting incorrect classifications or text.\n",
    "And sometimes, neural networks are susceptible to changes as straightforward as synonym substitution, misspellings, or insertion of slang.\n",
    "In certain cases, all it takes is a little “word salad”—nonsensical random words—to distract and confuse an NLP algorithm.\n",
    "Humans know how to ignore noise and filter out distractors, but machines sometimes have trouble with this.\n",
    "\n",
    "_Robust NLP_ is the study of approaches and techniques for building machines that are smart enough to handle unusual text from diverse sources.footnote:[See Robin Jia’s thesis on Robust NLP (https://robinjia.github.io/assets/pdf/robinjia_thesis.pdf) and his presentation with Kai-Wei Chang, He He, and Sameer Singh (https://robustnlp-tutorial.github.io).]\n",
    "In fact, research on robust NLP may uncover paths toward artificial general intelligence.\n",
    "Humans are able to learn new words and concepts from just a few examples, and we generalize well—not too much and not too little.\n",
    "Machines need a little help.\n",
    "And if you can figure out the “secret sauce” that makes us humans good at this, then you can encode it into your NLP pipelines.\n",
    "\n",
    "One popular technique for increasing the robustness of neural networks is _random dropout_.\n",
    "_Random dropout_, or just _dropout_, has become popular because of its ease and effectiveness.\n",
    "Your neural networks will almost always benefit from a dropout layer.\n",
    "A dropout layer randomly hides some of the neuron outputs from the neurons listening to them.\n",
    "This causes that pathway in your artificial brain to go quiet and forces the other neurons to learn from the examples in front of it during that dropout.\n",
    "\n",
    "It’s counterintuitive, but dropout helps your neural network to spread the learning around.\n",
    "Without a dropout layer, your network will focus on the words, patterns, and convolutional filters that helped it achieve the greatest accuracy boost.\n",
    "But you need your neurons to diversify their patterns so that your network can be robust against common variations on natural language text.\n",
    "\n",
    "The best place in your neural network to install a dropout layer is close to the end, just before you run the fully connected linear layer that computes the predictions on a batch of data.\n",
    "This vector of weights passing into your linear layer are the outputs from your CNN and pooling layers.\n",
    "Each of these values represents a sequence of words, or patterns of meaning and syntax.\n",
    "Hiding some of these patterns from your prediction layer forces your prediction layer to diversify its “thinking.”\n",
    "Though your software isn’t really thinking about anything, it’s okay to anthropomorphize it a bit, if it helps you develop intuitions about why techniques like random dropout can improve your model’s accuracy.\n",
    "\n",
    "== PyTorch CNN to process disaster toots\n",
    "\n",
    "Now comes the fun part.\n",
    "You are going to build a real-world CNN that can distinguish real-world news from sensationalism.\n",
    "Your model can help you filter out culture war tweets, so you can focus on news from real war zones.\n",
    "\n",
    "First, you will see where your new convolution layers fit into the pipeline.\n",
    "Then, you’ll assemble all the pieces to train a CNN on a dataset of disaster tweets.\n",
    "And if doom scrolling and disaster are not your thing, the CNN is easily adaptable to any labeled dataset of tweets.\n",
    "You can even pick a hashtag that you like and use that as you target label.\n",
    "Then, you can find tweets that match that hashtag topic even when the person who posted it doesn’t use hashtags.\n",
    "\n",
    "=== Network architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26eaabf",
   "metadata": {},
   "source": [
    "#### .CNN hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80f70f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNTextClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, embeddings):\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq_len = 40  # <1>\n",
    "        self.vocab_size = 10000  # <2>\n",
    "        self.embedding_size = 50  # <3>\n",
    "        self.out_channels = 5  # <4>\n",
    "        self.kernel_lengths = [2, 3, 4, 5, 6]  # <5>\n",
    "        self.stride = 1  # <6>\n",
    "        self.dropout = nn.Dropout(0)  # <7>\n",
    "        self.pool_stride = self.stride  # <8>\n",
    "        self.conv_out_seq_len = calc_out_seq_len(  # <9>\n",
    "            seq_len=self.seq_len,\n",
    "            kernel_lengths=self.kernel_lengths,\n",
    "            stride=self.stride,\n",
    "            )\n",
    "----\n",
    "<1> N_: Assumes a maximum text length of 40 tokens\n",
    "<2> V: The number of unique tokens (words) in your vocabulary\n",
    "<3> E: The number of word embedding dimensions (kernel input channels)\n",
    "<4> F: The number of filters (kernel output channels)\n",
    "<5> K: The number of columns of weights in each kernel\n",
    "<6> S: The number of time steps (tokens) to slide the kernel forward with each step\n",
    "<7> D: The portion of convolution output to ignore, where 0 dropout increases overfitting\n",
    "<8> P: Pooling strides greater than 1 will increase feature reduction\n",
    "<9> C: Total convolutional output size based on kernel and pooling hyperparameters\n",
    "\n",
    "As with your handcrafted convolutions earlier in this chapter, the sequence length is reduced by each convolutional operation, and the amount of shortening depends on the size of the kernel and the stride.\n",
    "The PyTorch documentationfootnote:[https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html.] for a `Conv1d` layer provides this formula and a detailed explanation of the terms:\n",
    "\n",
    "[source,python]\n",
    "----\n",
    "def calc_conv_out_seq_len(seq_len, kernel_len,\n",
    "                          stride=1, dilation=1, padding=0):\n",
    "    \"\"\"\n",
    "    L_out =     (L_in + 2 * padding - dilation * (kernel_size - 1) - 1)\n",
    "            1 + _______________________________________________________\n",
    "                                        stride\n",
    "    \"\"\"\n",
    "    return (\n",
    "        1 + (seq_len +\n",
    "             2 * padding - dilation * (kernel_len - 1) - 1\n",
    "            ) //\n",
    "        stride\n",
    "        )\n",
    "----\n",
    "\n",
    "As shown in listing <<cnn-embedding>>, our first CNN layer is an `nn.Embedding` layer that converts a sequence of word ID integers into a sequence of embedding vectors.\n",
    "It has as many rows as you have unique tokens in your vocabulary, including the new padding token.\n",
    "It also has a column for each dimension of the embedding vectors.\n",
    "You can load these embedding vectors from GloVe or any other pretrained embeddings.\n",
    "\n",
    "[id=cnn-embedding, reftext={chapter}.{counter:listing}]\n",
    ".Initializing the CNN embedding\n",
    "[source,python]\n",
    "----\n",
    "self.embed = nn.Embedding(\n",
    "    self.vocab_size,  # <1>\n",
    "    self.embedding_size,  # <2>\n",
    "    padding_idx=0)\n",
    "state = self.embed.state_dict()\n",
    "state['weight'] = embeddings  # <3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6405c3",
   "metadata": {},
   "source": [
    "#### .Constructing convolution and pooling layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92313afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.convolvers = []\n",
    "self.poolers = []\n",
    "total_out_len = 0\n",
    "for i, kernel_len in enumerate(self.kernel_lengths):\n",
    "    self.convolvers.append(\n",
    "        nn.Conv1d(in_channels=self.embedding_size,\n",
    "                  out_channels=self.out_channels,\n",
    "                  kernel_size=kernel_len,\n",
    "                  stride=self.stride))\n",
    "    print(f'conv[{i}].weight.shape: {self.convolvers[-1].weight.shape}')\n",
    "    conv_output_len = calc_conv_out_seq_len(\n",
    "        seq_len=self.seq_len, kernel_len=kernel_len, stride=self.stride)\n",
    "    print(f'conv_output_len: {conv_output_len}')\n",
    "    self.poolers.append(\n",
    "        nn.MaxPool1d(kernel_size=conv_output_len, stride=self.stride))\n",
    "    total_out_len += calc_conv_out_seq_len(\n",
    "        seq_len=conv_output_len, kernel_len=conv_output_len,\n",
    "        stride=self.stride)\n",
    "    print(f'total_out_len: {total_out_len}')\n",
    "    print(f'poolers[{i}]: {self.poolers[-1]}')\n",
    "print(f'total_out_len: {total_out_len}')\n",
    "self.linear_layer = nn.Linear(self.out_channels * total_out_len, 1)\n",
    "print(f'linear_layer: {self.linear_layer}')\n",
    "----\n",
    "\n",
    "Unlike in the previous examples, this time you’ll create multiple convolution and pooling layers.\n",
    "You won’t layer them up, as is often done in computer vision; instead, you will concatenate the convolution and pooling outputs together.\n",
    "This is effective because you’ve limited the dimensionality of your convolution and pooling output by performing global max pooling and keeping the number of output channels much smaller than the number of embedding dimensions.\n",
    "\n",
    "You can use print statements to help debug mismatching matrix shapes for each layer of your CNN.\n",
    "You want to make sure you don’t unintentionally create too many trainable parameters that cause more overfitting than you’d like.\n",
    "Your pooling outputs each contain a sequence length of `1`, but they also contain `5` channels for the embedding dimensions combined during convolution, as shown in the following listing.\n",
    "Therefore, the concatenated and pooled convolution output is a 5x5 tensor, which produces a 25D linear layer for the output tensor that encodes the meaning of each text.\n",
    "\n",
    "[id=cnn-layer, reftext={chapter}.{counter:listing}]\n",
    ".CNN layer shapes\n",
    "[source,python]\n",
    "----\n",
    "conv[0].weight.shape: torch.Size([5, 50, 2])\n",
    "conv_output_len: 39\n",
    "total_pool_out_len: 1\n",
    "poolers[0]: MaxPool1d(kernel_size=39, stride=1, padding=0, dilation=1,\n",
    "    ceil_mode=False)\n",
    "conv[1].weight.shape: torch.Size([5, 50, 3])\n",
    "conv_output_len: 38\n",
    "total_pool_out_len: 2\n",
    "poolers[1]: MaxPool1d(kernel_size=38, stride=1, padding=0, dilation=1,\n",
    "    ceil_mode=False)\n",
    "conv[2].weight.shape: torch.Size([5, 50, 4])\n",
    "conv_output_len: 37\n",
    "total_pool_out_len: 3\n",
    "poolers[2]: MaxPool1d(kernel_size=37, stride=1, padding=0, dilation=1,\n",
    "    ceil_mode=False)\n",
    "conv[3].weight.shape: torch.Size([5, 50, 5])\n",
    "conv_output_len: 36\n",
    "total_pool_out_len: 4\n",
    "poolers[3]: MaxPool1d(kernel_size=36, stride=1, padding=0, dilation=1,\n",
    "    ceil_mode=False)\n",
    "conv[4].weight.shape: torch.Size([5, 50, 6])\n",
    "conv_output_len: 35\n",
    "total_pool_out_len: 5\n",
    "poolers[4]: MaxPool1d(kernel_size=35, stride=1, padding=0, dilation=1,\n",
    "     ceil_mode=False)\n",
    "total_out_len: 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378d8844",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272f2702",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_pool_total = 0  # <1>\n",
    "for kernel_len, stride in zip(kernel_lengths, strides):\n",
    "    out_conv = (\n",
    "        (in_seq_len - dilation * (kernel_len - 1) - 1)\n",
    "        // stride\n",
    "        ) + 1\n",
    "    out_pool_total +=  (\n",
    "        (out_conv - dilation * (kernel_len - 1) - 1)\n",
    "        // stride"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca751cea",
   "metadata": {},
   "source": [
    "#### .CNN hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94433d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "             learning  seq  case vocab           training      test\n",
    " kernel_sizes    rate  len  sens  size dropout  accuracy  accuracy\n",
    "          [2]  0.0010   32 False  2000     NaN    0.5790    0.5459\n",
    "[1 2 3 4 5 6]  0.0010   40 False  2000     NaN    0.7919    0.7100\n",
    "    [2 3 4 5]  0.0015   40 False  2000     NaN    0.8038    0.7152\n",
    "[1 2 3 4 5 6]  0.0010   40  True  2000     NaN    0.7685    0.7520\n",
    "          [2]  0.0010   32  True  2000     0.2    0.8472    0.7533\n",
    "    [2 3 4 5]  0.0010   32  True  2000     0.2    0.8727    0.7900\n",
    "----\n",
    "\n",
    "Can you find a better combination of hyperparameters to improve this model’s accuracy?\n",
    "This is quite a difficult problem, so don’t expect to achieve much better than 80% test set accuracy.\n",
    "Even human readers can’t reliably determine whether a tweet represents a factual newsworthy disaster or not.\n",
    "After all, many humans (and bots) are composing these tweets in an attempt to fool readers.\n",
    "This is an adversarial problem.\n",
    "Even a small, one-layer CNN does a decent job.\n",
    "\n",
    "[id=learning-curve, reftext={chapter}.{counter:figure}]\n",
    ".The learning curve for the best hyperparameters we found\n",
    "image::../images/ch07/learning-curve-87-79.png[alt=\"Training and test set accuracy for 10 epochs of training using the best hyperparameters found\", width=80%, link=\"../images/ch07/learning-curve-87-79.png\"]\n",
    "\n",
    "The key to hyperparameter tuning is to conscientiously record each experiment and make thoughtful hyperparameter adjustments for the next experiment.\n",
    "You can automate this decision-making with a Bayesian optimizer, but in most cases, using your own biological neural network—the intuition you develop with practice—for the Bayesian optimization will lead to faster hyperparameter tuning.\n",
    "And if you are curious about the effect of the transpose operation on the embedding layer, you can try it both ways to see which works best on your problem, but you probably want to follow the experts if your goal is achieving state-of-the-art solutions to difficult problems.\n",
    "Don’t believe everything you read on the internet, especially when it comes to CNNs for NLP.\n",
    "\n",
    "== Test yourself\n",
    "\n",
    ". For a length `3` kernel and an input array of length `8`, what is the length of the output?\n",
    ". What is the kernel for detecting an SOS distress signalfootnote:[This initialism is short for _save our souls_ or _save our ship_.] within the secret message audio file in this chapter?\n",
    ". What is the best training set accuracy you can achieve after tuning the hyperparameters for the newsworthiness microblog post problem?\n",
    ". How would you extend the model to accommodate an additional class? The news.csv file, provided in the nlpia2 package on GitLab, contains famous quotes to give you another level of profundity to attempt to classify with your CNN.\n",
    ". Write three kernels, one each for detecting dots, dashes, and pauses. Write a pooling function that counts unique occurrences of these symbols. BONUS: Create a system of functions that _translates_ the secret message audio file into the symbols `\".\"`, `\"-\"`, and `\" \"`.\n",
    ". Find some hyperparameters (don’t forget about random seeds) that achieve better than 80% accuracy on the test set for the disaster tweets dataset.\n",
    ". Create a sarcasm detector using word-based CNN with datasets and examples on Hugging Face (huggingface.co). Do you find the claim of detecting sarcasm with 91% accuracy, made in several published papers,footnote:[Ivan Helin makes the claim of 92% accuracy for their model on Hugging Face (https://huggingface.co/helinivan/english-sarcasm-detector).]footnote:[Soujanya Poria et al. claim an accuracy rate of 91% in “A Deeper Look into Sarcastic Tweets Using a CNN” (https://arxiv.org/abs/1610.08815).] from a single, context-free tweet to be credible? \n",
    "\n",
    "== Summary\n",
    "\n",
    "* A convolution is a windowed filter that slides over your sequence of words to compress its meaning into an encoding vector.\n",
    "* Handcrafted convolutional filters work great on predictable signals, such as Morse code, but you will need CNNs that learn their own filters for NLP.\n",
    "* Neural networks can extract patterns in a sequence of words that other NLP approaches would miss.\n",
    "* During training, if you sandbag your model a bit with a dropout layer, you can keep it from overachieving (over fitting) on your training data.\n",
    "* Hyperparameter tuning for neural networks gives you more room to exercise your creativity than conventional machine learning models."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
