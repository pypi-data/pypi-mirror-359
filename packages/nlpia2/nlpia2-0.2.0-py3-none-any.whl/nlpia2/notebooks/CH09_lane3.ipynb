{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d81dd71a",
   "metadata": {},
   "source": [
    "#### [CH09_lane3](/home/hobs/code/hobs/nlpia-manuscript/manuscript/adoc/CH09_lane3.adoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7db39b",
   "metadata": {},
   "source": [
    "#### .Loading a translation dataset in Hugging Face format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99150b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset  # <1>\n",
    "opus = load_dataset('opus_books', 'de-en')\n",
    "opus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80e03d6",
   "metadata": {},
   "source": [
    "#### .Loading a translation dataset in Hugging Face format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483ee6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = opus['train'].train_test_split(test_size=.1)\n",
    "sents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a9968e",
   "metadata": {},
   "source": [
    "#### .Loading a translation dataset in Hugging Face format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de39f074",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(sents['test']))  # <1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953a84aa",
   "metadata": {},
   "source": [
    "#### .Enabling any available GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107d1b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\n",
    "    'cuda' if torch.cuda.is_available()\n",
    "    else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb871a47",
   "metadata": {},
   "source": [
    "#### .Enabling any available GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ba8d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = 'en'  # <1>\n",
    "TGT = 'de'  # <2>\n",
    "SOS, EOS = '<s>', '</s>'\n",
    "PAD, UNK, MASK = '<pad>', '<unk>', '<mask>'\n",
    "SPECIAL_TOKS = [SOS, PAD, EOS, UNK, MASK]\n",
    "VOCAB_SIZE = 10_000\n",
    "from tokenizers import ByteLevelBPETokenizer  # <3>\n",
    "tokenize_src = ByteLevelBPETokenizer()\n",
    "tokenize_src.train_from_iterator(\n",
    "    [x[SRC] for x in sents['train']['translation']],\n",
    "    vocab_size=10000, min_frequency=2,\n",
    "    special_tokens=SPECIAL_TOKS)\n",
    "PAD_IDX = tokenize_src.token_to_id(PAD)\n",
    "tokenize_tgt = ByteLevelBPETokenizer()\n",
    "tokenize_tgt.train_from_iterator(\n",
    "    [x[TGT] for x in sents['train']['translation']],\n",
    "    vocab_size=10000, min_frequency=2,\n",
    "    special_tokens=SPECIAL_TOKS)\n",
    "assert PAD_IDX == tokenize_tgt.token_to_id(PAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9af93a",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b64fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from typing import Optional, Any\n",
    "class CustomDecoderLayer(nn.TransformerDecoderLayer):\n",
    "    def forward(self, tgt: Tensor, memory: Tensor,\n",
    "            tgt_mask: Optional[Tensor] = None,\n",
    "            memory_mask: Optional[Tensor] = None,\n",
    "            tgt_key_padding_mask: Optional[Tensor] = None\n",
    "            ) -> Tensor:\n",
    "        \"\"\"Like decode but returns multi-head attention weights.\"\"\"\n",
    "        tgt2 = self.self_attn(\n",
    "            tgt, tgt, tgt, attn_mask=tgt_mask,\n",
    "            key_padding_mask=tgt_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "        tgt2, attention_weights = self.multihead_attn(\n",
    "            tgt, memory, memory,  # <1>\n",
    "            attn_mask=memory_mask,\n",
    "            key_padding_mask=mem_key_padding_mask,\n",
    "            need_weights=True)\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        tgt2 = self.linear2(\n",
    "            self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt, attention_weights  # <2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d3f733",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db323c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDecoder(nn.TransformerDecoder):\n",
    "    def __init__(self, decoder_layer, num_layers, norm=None):\n",
    "        super().__init__(\n",
    "            decoder_layer, num_layers, norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab366910",
   "metadata": {},
   "source": [
    "#### .A quick model sanity check with random tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4930d3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = torch.randint(1, 100, (10, 5)).to(DEVICE)  # <1>\n",
    "tgt = torch.randint(1, 100, (10, 7)).to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    output = model(src, tgt)  # <2>\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4f7592",
   "metadata": {},
   "source": [
    "#### .A quick model sanity check with random tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219e6c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)  # <1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146c38a7",
   "metadata": {},
   "source": [
    "#### .A quick model sanity check with random tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9b76ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cec7ee0",
   "metadata": {},
   "source": [
    "#### .The model evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38581fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()  # <1>\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():  # <2>\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "            output = model(src, trg[:,:-1])\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3f390b",
   "metadata": {},
   "source": [
    "#### .The model evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20d9c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99033818",
   "metadata": {},
   "source": [
    "#### .The model evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abf4afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "N_EPOCHS = 15\n",
    "CLIP = 1\n",
    "BEST_MODEL_FILE = 'best_model.pytorch'\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss = train(\n",
    "        model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), BEST_MODEL_FILE)\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    train_ppl = f'{math.exp(train_loss):7.3f}'\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {train_ppl}')\n",
    "    valid_ppl = f'{math.exp(valid_loss):7.3f}'\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {valid_ppl}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa7b5b9",
   "metadata": {},
   "source": [
    "#### .Loading the _best_ model from the file and performing evaluation on the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9d8f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(BEST_MODEL_FILE))\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a22abe",
   "metadata": {},
   "source": [
    "#### .Loading the _best_ model from the file and performing evaluation on the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d956bc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, src_field, trg_field,\n",
    "        model, device=DEVICE, max_len=50):\n",
    "    model.eval()\n",
    "    if isinstance(sentence, str):\n",
    "        nlp = spacy.load('de')\n",
    "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "    tokens = ([src_field.init_token] + tokens\n",
    "        + [src_field.eos_token])  # <1>\n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "    src = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "    src, src_key_padding_mask = model.prepare_src(src, SRC_PAD_IDX)\n",
    "    with torch.no_grad():\n",
    "        enc_src = model.encoder(src,\n",
    "            src_key_padding_mask=src_key_padding_mask)\n",
    "    trg_indexes = [\n",
    "        trg_field.vocab.stoi[trg_field.init_token]]  # <2>\n",
    "\n",
    "    for i in range(max_len):\n",
    "        tgt = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "        tgt, tgt_key_padding_mask, tgt_mask = model.prepare_tgt(\n",
    "            tgt, TRG_PAD_IDX)\n",
    "        with torch.no_grad():\n",
    "            output = model.decoder(\n",
    "                tgt, enc_src, tgt_mask=tgt_mask,\n",
    "                tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "            output = rearrange(output, 'T N E -> N T E')\n",
    "            output = model.linear(output)\n",
    "\n",
    "        pred_token = output.argmax(2)[:,-1].item()  # <3>\n",
    "        trg_indexes.append(pred_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29026a5",
   "metadata": {},
   "source": [
    "#### .Translating the test data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4124c05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
    "print(f'translation = {translation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e862904e",
   "metadata": {},
   "source": [
    "#### .Translating the test data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e45038e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "def display_attention(sentence, translation, attention_weights):\n",
    "    n_attention = len(attention_weights)\n",
    "\n",
    "    n_cols = 2\n",
    "    n_rows = n_attention // n_cols + n_attention % n_cols\n",
    "\n",
    "    fig = plt.figure(figsize=(15,25))\n",
    "\n",
    "    for i in range(n_attention):\n",
    "\n",
    "        attention = attention_weights[i].squeeze(0)\n",
    "        attention = attention.cpu().detach().numpy()\n",
    "        cax = ax.matshow(attention, cmap='gist_yarg')\n",
    "\n",
    "        ax = fig.add_subplot(n_rows, n_cols, i+1)\n",
    "        ax.tick_params(labelsize=12)\n",
    "        ax.set_xticklabels([''] + ['<sos>'] +\n",
    "            [t.lower() for t in sentence]+['<eos>'],\n",
    "            rotation=45)\n",
    "        ax.set_yticklabels(['']+translation)\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cdfd10",
   "metadata": {},
   "source": [
    "#### .Translating the test data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf0956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_attention(src, translation, attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8625f529",
   "metadata": {},
   "source": [
    "#### .Translating the validation data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23c9439",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
    "print(f'translation = {translation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024cf874",
   "metadata": {},
   "source": [
    "#### .Translating the validation data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ac9ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_attention(src, translation, attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5580b850",
   "metadata": {},
   "source": [
    "#### .Translating the validation data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd71a8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "def calculate_bleu(data, src_field, trg_field, model, device,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568d17b2",
   "metadata": {},
   "source": [
    "#### .Translating the validation data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3f325f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_score = calculate_bleu(test_data, SRC, TRG, model, device)\n",
    "print(f'BLEU score = {bleu_score*100:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961ecc5c",
   "metadata": {},
   "source": [
    "#### .A PyTorch summary of BERT architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7cdc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "model = BertModel.from_pre-trained('bert-base-uncased')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09bbc22",
   "metadata": {},
   "source": [
    "#### .Loading the toxic comments dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3470da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/train.csv')  # <1>\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1c2212",
   "metadata": {},
   "source": [
    "#### .Loading the toxic comments dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f152b65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319e2bac",
   "metadata": {},
   "source": [
    "#### .Loading the toxic comments dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4de192",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "random_state=42\n",
    "labels = ['toxic', 'severe', 'obscene', 'threat', 'insult', 'hate']\n",
    "X = df[['comment_text']]\n",
    "y = df[labels]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2,\n",
    "    random_state=random_state)  # <1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2d211b",
   "metadata": {},
   "source": [
    "#### .Creating datasets for your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08f828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(X, y):\n",
    "    data = [[X.iloc[i][0], y.iloc[i].values.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2b6293",
   "metadata": {},
   "source": [
    "#### .Creating datasets for your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214c6b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = get_dataset(X_train, y_train)\n",
    "eval_df = get_dataset(X_test, y_test)\n",
    "train_df.shape, eval_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292672bd",
   "metadata": {},
   "source": [
    "#### .Creating datasets for your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd87d4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()  # <1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80633acf",
   "metadata": {},
   "source": [
    "#### .Creating datasets for your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a41f42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)  # <1>\n",
    "model_type = 'bert'  # <2>\n",
    "model_name = 'bert-base-cased'\n",
    "output_dir = f'{model_type}-example1-outputs'\n",
    "model_args = {\n",
    "    'output_dir': output_dir, # where to save results\n",
    "    'overwrite_output_dir': True, # allow re-run without having to manually clear output_dir\n",
    "    'manual_seed': random_state, # <3>\n",
    "    'no_cache': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593890e0",
   "metadata": {},
   "source": [
    "#### .Creating datasets for your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dba1b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from simpletransformers.classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858ae0bf",
   "metadata": {},
   "source": [
    "#### .Creating datasets for your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a151f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLabelClassificationModel(\n",
    "    model_type, model_name, num_labels=len(labels),\n",
    "    args=model_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba272da",
   "metadata": {},
   "source": [
    "#### .Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06277d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "result, model_outputs, wrong_predictions = model.eval_model(eval_df,\n",
    "    acc=roc_auc_score)  # <1>\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f72f0a",
   "metadata": {},
   "source": [
    "#### .Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e215d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.preprocessing import TextPreprocessor\n",
    "tp = TextPreprocessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320aa38f",
   "metadata": {},
   "source": [
    "#### .Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed96ba0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'comment_text':'original_text'})\n",
    "df['comment_text'] = df['original_text'].apply(\n",
    "    lambda x: tp.preprocess(x))  # <1>\n",
    "pd.set_option('display.max_colwidth', 45)\n",
    "df[['original_text', 'comment_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cffa1d",
   "metadata": {},
   "source": [
    "#### .Setting up parameters for evaluation during training and early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86319a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'bert'\n",
    "model_name = 'bert-base-cased'\n",
    "output_dir = f'{model_type}-example2-outputs'  # <1>\n",
    "best_model_dir = f'{output_dir}/best_model'\n",
    "model_args = {\n",
    "    'output_dir': output_dir,\n",
    "    'overwrite_output_dir': True,\n",
    "    'manual_seed': random_state,\n",
    "    'no_cache': True,\n",
    "    'best_model_dir': best_model_dir,\n",
    "    'max_seq_length': 300,\n",
    "    'train_batch_size': 24,\n",
    "    'eval_batch_size': 24,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'learning_rate': 5e-5,\n",
    "    'evaluate_during_training': True,\n",
    "    'evaluate_during_training_steps': 1000,\n",
    "    'save_eval_checkpoints': False,\n",
    "    \"save_model_every_epoch\": False,\n",
    "    'save_steps': -1,  # saving model unnecessarily takes time during training\n",
    "    'reprocess_input_data': True,\n",
    "    'num_train_epochs': 5,  # <2>\n",
    "    'use_early_stopping': True,\n",
    "    'early_stopping_patience': 4,  # <3>\n",
    "    'early_stopping_delta': 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef61f65",
   "metadata": {},
   "source": [
    "#### .Setting up parameters for evaluation during training and early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf2f029",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLabelClassificationModel(\n",
    "    model_type, model_name, num_labels=len(labels),\n",
    "    args=model_args)\n",
    "model.train_model(\n",
    "    train_df=train_df, eval_df=eval_df, acc=roc_auc_score,\n",
    "    show_running_loss=False, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bb020d",
   "metadata": {},
   "source": [
    "#### .Setting up parameters for evaluation during training and early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498280da",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = MultiLabelClassificationModel(\n",
    "    model_type, best_model_dir,\n",
    "    num_labels=len(labels), args=model_args)\n",
    "result, model_outputs, wrong_predictions = best_model.eval_model(\n",
    "    eval_df, acc=roc_auc_score)\n",
    "result"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
