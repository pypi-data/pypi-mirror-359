Chapter 1:
  - Why is NLP considered to be a core enabling feature for AGI?
  - Why do advanced NLP models tend to show significant discriminatory biases?
  - How is it possible to create a prosocial chatbot using training data from sources that include antisocial examples?
  - What are 4 different approaches or architectures for building a chatbot?
  - How is NLP used within a search engine?
  - Write a regular expression to recognize your name and all the variations on its spelling (including nicknames) that you've seen.
  - Write a regular expression to try to recognize a sentence boundary (usually a period ("."), question mark "?", or exclamation mark "!")
Chapter 2:
  - 
    Q: What is the difference between a lemmatizer and a stemmer? Which one is better (in most cases)?
    exercise: Run an NLTK stemmer example words with weird spellings (such as "corpora" or "lemmatizer"). Then compare the results to the lemma tags that spaCy gives you. And experiment with context, providing complete sentences to spaCy so it is able to guess the part of speech to use within the lemmatizer section of pipe. 
    A: A stemmer uses fixed rules to try to remove the suffix of a word to cluster words together that are spelled similarly. A lemmatizer uses a dictionary of word definitions and synonyms (such as `WordNet`) to try to find a root word that represents the meaning of the word, and it takes into account the part-of-speech. For example, a stemmer would leave the word "corpora" unchanged, but a lemmatizer would correctly convert it to the word "corpus." A lemmatizer is usually better, if you can find a good one for your language. But a lemmatizer needs to know the part of speech of a word, so you want to use spaCy as your lemmatizer rather than relying on NLTK. Otherwise you would need to build a custom NLP pipeline to detect part of speech of the words you are lemmatizing.
  - 
    Q: How does a lemmatizer increase the likelihood that a search engine (such as You.com) returns search results that contain what you are looking for?
    exercise: Use the `nlpia_lines.csv` file in `nlpia2/data/` and a `CountVectorizer` to build an index of the lines in this book. Then rebuild it using a lemmatizer. How many lines contain the word "corpus"? How many contain _either_ the word "corpus" or "corpora"?
    A: By clustering similar tokens together into one token in a smaller vocabulary, the search engine index will contain more relevant documents for each possible form of a particular word. This increases the "recall" of a search engine, the number of relevant documents listed. For example, the lemma for "better" is "good", so if you were search for "a better search engine" a lemmatizer would help you find "good" search engines, perhaps search engines that are attempting to produce a better society with less clickbait because they mention "social good" on their landing page. Pro tip - most search engines allow you to quote any phrases or words that you don't want to be lemmatized.
  - 
    Q_adoc: "Will case folding, lemmatizing or stopword removal improve the accuracy of your typical NLP pipeline? What about for a problem like detecting misleading news article titles (clickbait)?footnote:[Hint: When in doubt do an experiment. This is called _hyperparameter tuning_. Here's a fake news dataset you can experiment with: (https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset/download)]"
    Q: Will case folding, lemmatizing or stopword removal improve the accuracy of your typical NLP pipeline? What about for a problem like detecting misleading news article titles (clickbait)?
    hint: Hint: When in doubt do an experiment. This is called _hyperparameter tuning_. Here's [a fake news dataset](https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset/download) you can experiment with.
    exercise: Download the "Fake and Real News Dataset" from Kaggle and train a binary classifier using a LogisticRegression with a CountVectorizer and spaCy tokenizer. Now, instead of using `Token.text` for your tokens, use `Token.lemma`. Which pipeline gives you better accuracy?
    A: No, unless you are doing keyword detection, or building a search engine with a small corpus. Most modern machine learning models utilize regularization to reduce the dimensionality of your feature vectors (token count vectors) better than a preplanned algorithm such as a lemmatizer. The machine learning model in your NLP pipeline will do a better job of correlating related words together that belong together, without relying on a precomputed dictionary of word meanings and roots.
  - 
    Q: Are there statistics in your token counts that you can use to decide what `n` to use for your n-grams in an NLP pipeline?
    A: Yes, you want to look at the counts of the usages of your n-grams in your documents and how they are distributed among the documents. If an n-gram only occurs once in one of your documents, it cannot help your NLU pipeline understand what that document is saying. So you want to set a min and max frequency for all your n-grams (including 1-grams), that allows your NLP pipeline to extract the _important_ tokens from your documents.
  - 
    Q_adoc: "Is there a website where you can download the token frequencies for most of the words and n-grams ever published? footnote:[Hint: A company that once aspired to 'do no evil', but now does, created this massive NLP corpus.]"
    Q: Is there a website where you can download the token frequencies for most of the words and n-grams ever published
    hint: A company that aspired to "do no evil", but now does, created this massive NLP corpus.
    A: Fortunately, when Larry Page was in charge and he enforced his "Do No Evil" philosophy, he had his lawyers fight several lawsuits to allow him to build the Project Gutenberg corpus and count up the n-grams in millions of books. And now the evil-doing Google is required by law to maintain that dataset and make it publicly available. Use `google-ngram-downloader` to retrieve the n-gram statistics you need for your application. 
  - 
    Q: How could your find out the best sizes for the word pieces or sentence pieces for your tokenizer?
    A: Byte-pair encoding counts up the statistics of character-grams to build a vocabulary of word or sentence pieces. The tokens it produces can be combined to represent any token in your entire corpus. And this vocabulary of sentence pieces is optimal for your machine learning pipeline.
    exercise: Repeat the `nlpia_lines.csv` search engine lemmatizer exercise, but this time use a byte-pair encoder as your tokenizer, instead of spaCy. Did the accuracy of your model improve or get worse? What happens when you increase or decrease the vocabulary size? 
Chapter 3:
  - 
    Q: What is the most memory efficient data structure for storing BOW and TF-IDF vectors?
  - 
    Q: Is it possible to store a million by million array of TF-IDF values in a laptop with only 8 GB of RAM? One million times one million is a thousand billion floating point TF-IDF values.
    A: On a 64-bit laptop that's typically 8,000 GB if you just stored it all in a naive data structure. You could reduct that a bit to 4,000 GB if you reduced the floating point precision from 64-bit to 32-bit. But the real trick is to store the data in a sparse array so that all the values that are the same. Usually 99.99% of a TF-IDF matrix are zeros or some small fixed value. So a sparse array for a TF-IDF matrix can usually take up 10,000 times less space or maybe 3,000 times less space if you take into account the 2 integers required to keep track of the indexes for the values. 
Chapter 4:
  - 
    Q: What is the context, or neighborhood of words, that LSA (PCA on BOW or TF-IDF vectors) pays attention to when creating topics?
    A: PCA and T-SNE pay attention to the coocurrence of words within the same document, since this is what makes up the TF-IDF and BOW word vectors in the rows of the matrix it is transforming. This is different from what other Deep Learning transformations like Word2Vec and GLoVE do. Word2Vec and GLoVe limit the neighborhood to only those words that occur within 5 words of the _target_ word.
  -
    Q: What does PCA optimize as it is composing linear combinations of words into topics? How is this different from what T-SNE does?
    A: PCA maximizes the variance of the topic vectors in the lower dimensional space. Variance is the average euclidian distance between vectors and the origin. PCA keeps vectors far apart in the low dimensional space that were far apart in the high dimensional space. This is essentially what you would do if you rotated a 3-D object in front of a camera to maximize the amount of area the object occupies in the 2-D picture you take. T-SNE minimizes the distance between pairs of vectors in the low dimensional space that were close to each other in the high dimensional space. 
Chapter 5:
  - 
    Q: What are the advantages and disadvantages of using the `torch.nn.functional` interface to PyTorch rather than the `torch.nn.Module` class?
    A: The PyTorch `nn.functional` API is easier to understand and debug for many people. But the declarative `torch.nn.Module` allows you to create an unlimitted array of new architectures that utilize the internal model _state_ to produce more complex behavior. 
  - 
    Q: What is the simple AI logic "problem" that Rosenblatt's artifical neurons couldn't?
    A: Rosenblatt's perceptron couldn't handle the XOR logic gate problem and this led to the first AI Winter when most researchers stopped pursuing AI and neural networks.
  - 
    Q: What minor change to Rosenblatt's architecture "fixed" perceptrons and ended the first "AI Winter"?
    A: Adding a second layer of neurons allowed just 3 neurons to solve the XOR logic gate problem. Multi-layer peceptrons eventually evolved into the deep learning architectures we use today to solve AI problems that were previously unsolvable.
  - 
    Q: What is the equivalent of a PyTorch `model.forward()` function in SciKit-Learn models?
    A: The `model.predict()` function is what accomplishes inference or prediction in the same way that `model.forward()` does within PyTorch models.
  -
    Q: Imagine you want to solve a binary classification problem to predict one particular disease such as Parkinsons disease and your only input features are the genetic sequence from the patients with 10s of thousands of gene SNIPs for each patient. Your training set has fewer than 10 thousand patients and only a small fraction ever contracted the disease. Would you use a SciKit-Learn LogisticRegression model or an equivalent PyTorch neural network model with a single linear layer from the input to the output? Why? How would you deal with the imbalance problem? What about overfitting?
    A: Your fastest, simplest most accurate approach would be to use a `LogisticRegression`. You could use the `class_weight='balanced'` argument to deal with the imblance. And you could use a small value of `alpha` such as 0.001 in order to regularize your model and deal with overfitting. If you used a PyTorch neural network this process would be more complicated and you would have to use a batch size of about 128 or larger and a high random dropout percentage to deal with overfitting. You would need to do oversampling or use a loss function with class imbalance compensation.

Chapter 6:
  - 
    Q: What is the context, or neighborhood of words, that Word2Vec and GloVE pays attention to when creating semantic embeddings for words?
    A: Word2Vec and GloVE do. Word2Vec and GloVe limit the neighborhood to only those words that occur within 5 words of the _target_ word.
  - 
    Q: Create the same graph data structure as was done in this chapter, but use lemmatization rather than noun phrases as labels for your nodes.
    A: SEE examples in the [`nessvec` Python package](https://gitlab.com/tangibleai/nessvec/-/tree/main/examples)
  -
    Q: Create a graph data structure of another chapter in this book.
    A: SEE examples in the [`nessvec` Python package](https://gitlab.com/tangibleai/nessvec/-/tree/main/examples)
  -
    Q: Create a graph data structure of the full book using sentiment analysis for each chapter.
    A: SEE examples in the [`nessvec` Python package](https://gitlab.com/tangibleai/nessvec/-/tree/main/examples)
  -
    Q: Give examples for how word vectors enable at least two of Hofstadter's eight elements of intelligence
    A: Word analogies ("similarities and differnces in situations") - for example, "embeddings are to words as character sheets are to _____". Another example is the ability for a chatbot to "respond flexibly" by recognizing the meanings of words such as the word "SUP" even if no one has ever used that word as a greeting before. And if my friend Nick said SUP to your bot it could recognize when he was talking about his "Standup Paddleboard" that he paddles to work... when it's not too cold in Portland. 
Chapter 10:
  -
    Q: What are the risks and possible benefits of pair coding AI assistants built on LLMs? What sort of organizations and algorithms do you trust with your mind and your craft (help you write code and create NLP pipelines)?
    A: One proven danger is that LLMs sometimes fabricate the names of packages that it is recommending you use in your code. And these packages are then obvious risk is that most code completion snippets produced by LLMs contain subtle errors that are not obvious unless you are an expert in the particular kind of software you are trying to write.