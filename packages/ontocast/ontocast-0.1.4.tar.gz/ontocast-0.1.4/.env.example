# Domain configuration (used for URI generation)
CURRENT_DOMAIN=https://example.com
PORT=8999
LLM_TEMPERATURE=0.1

# openai flavor
# OpenAI API Key (required for LLM functionality)
# LLM_PROVIDER=openai
OPENAI_API_KEY=your-api-key-here

# ollama flavor
# BASE URL (if using ollama)
LLM_BASE_URL=ollama-base-url
LLM_PROVIDER=ollama
LLM_MODEL_NAME=granite3.3


# neo4j 
NEO4J_PORT=7476
NEO4J_BOLT_PORT=7689

RECURSION_LIMIT=1000
ESTIMATED_CHUNKS=30


# -------------------------------
# OntoCast Environment Configuration
# Copy this file to .env and update with your values

# Required: OpenAI API Key
OPENAI_API_KEY=your_openai_api_key_here

# Optional: LLM Configuration
LLM_PROVIDER=openai
LLM_MODEL_NAME=gpt-4o-mini
LLM_TEMPERATURE=0.0
LLM_BASE_URL=

# Optional: Server Configuration
PORT=8999
RECURSION_LIMIT=1000
ESTIMATED_CHUNKS=30

# Optional: Triple Store Configuration
# Fuseki is preferred over Neo4j when both are configured

# Fuseki Configuration (Recommended)
FUSEKI_URI=http://localhost:3030/test
FUSEKI_AUTH=admin/admin

# Neo4j Configuration (Alternative)
NEO4J_URI=bolt://localhost:7687
NEO4J_AUTH=neo4j/test

# Note: If no triple store is configured, OntoCast will use filesystem storage 