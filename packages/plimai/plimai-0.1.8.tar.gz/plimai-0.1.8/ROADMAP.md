# üöß Plimai Roadmap

This roadmap outlines the planned features, improvements, and vision for the plimai project. We welcome community feedback and contributions‚Äîopen an issue or join the discussion to suggest new ideas!

---

## ‚úÖ Current Features
- Modular Vision Transformer (ViT) backbone
- Plug-and-play LoRA adapters for efficient fine-tuning
- Unified model zoo for open-source visual models
- Easy configuration and extensible codebase
- CLI and Python API for training and inference
- Comprehensive documentation and tests

---

## ü•á Near-Term Goals (Q2 2025)
- [ ] Add support for more vision datasets (e.g., ImageNet, custom formats)
- [ ] Expand model zoo (ConvNeXT, Swin Transformer, ResNet with LoRA)
- [ ] Improve CLI with more options and better error messages
- [ ] Add more training callbacks (early stopping, LR schedulers)
- [ ] Integrate TensorBoard and Weights & Biases logging
- [ ] Add more tutorials and example notebooks

---

## ü•à Mid-Term Goals (Q3 2025)
- [ ] Multi-modal support (vision+text, VQA)
- [ ] Distributed and mixed-precision training
- [ ] Model export to ONNX and TorchScript
- [ ] Web-based demo and interactive visualization tools
- [ ] Advanced data augmentation and auto-augmentation
- [ ] Community-contributed model zoo entries

---

## ü•â Long-Term Vision
- [ ] Support for video and sequential data
- [ ] Integration with HuggingFace Hub for model sharing
- [ ] Automated hyperparameter tuning
- [ ] Real-time inference and deployment tools
- [ ] Research collaborations and academic partnerships

---

## ü§ù How to Contribute or Suggest Features
- Open an issue with your idea or feature request
- Join [GitHub Discussions](https://github.com/plim-ai/plim/discussions) to ask questions, share ideas, or get help
- Submit a pull request for new features or improvements
- Help review and test new releases

---

<p align="center"><b>We build plimai in the open‚Äîyour feedback shapes the future!</b></p> 