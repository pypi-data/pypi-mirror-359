{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7fd87ba-ea23-4392-8cd7-3ab6e493a46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import arviz\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import csr_matrix, lil_matrix, csr_array\n",
    "\n",
    "#Import Sparse Barrier Random Walks\n",
    "from polytopewalk.sparse import SparseDikinWalk, SparseVaidyaWalk, SparseJohnWalk\n",
    "\n",
    "#Import Dense Barrier Random Walks\n",
    "from polytopewalk.dense import DikinWalk, VaidyaWalk, JohnWalk\n",
    "\n",
    "def generate_simplex(d):\n",
    "    return np.array([1/d] * d), np.array([[1] * d]), np.array([1]), d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc57cb5b-d491-43a5-b716-3dabcb6c7946",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create 5d simplex polytope for sparse testing\n",
    "x1, A1, b1, k1 = generate_simplex(5)\n",
    "\n",
    "k = 5\n",
    "cov_simplex = np.full((k,k), (-1.0 / (k*k)) / (k + 1.0))\n",
    "for i in range(k):\n",
    "    cov_simplex[i,i] = (1 - (1.0 / (k))) * (1.0/k) / (k + 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "790ab037-b775-4a59-8e94-dffde3e0f96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run 5 chains on 100,000 iterations with burnin parameter of 1000 and thin parameter of 10\n",
    "nchains = 5\n",
    "niter = 100_000\n",
    "burn = 1000\n",
    "thin = 10\n",
    "\n",
    "seed = [1000, 2000, 3000, 4000, 5000]\n",
    "sparse_dikin = SparseDikinWalk(r = 0.5)\n",
    "sparse_vaidya = SparseVaidyaWalk(r = 0.7)\n",
    "sparse_john = SparseJohnWalk(r = 0.7)\n",
    "\n",
    "walks_all_dikin = np.empty([nchains, (niter-burn)//thin, k])\n",
    "walks_all_vaidya = np.empty([nchains, (niter-burn)//thin, k])\n",
    "walks_all_john = np.empty([nchains, (niter-burn)//thin, k])\n",
    "for c in range(nchains):\n",
    "    walks_all_dikin[c, :] = sparse_dikin.generateCompleteWalk(niter, x1, A1, b1, k1, burnin = burn, thin = thin, seed = seed[c])\n",
    "    walks_all_vaidya[c, :] = sparse_vaidya.generateCompleteWalk(niter, x1, A1, b1, k1, burnin = burn, thin = thin, seed = seed[c])\n",
    "    walks_all_john[c, :] = sparse_john.generateCompleteWalk(niter, x1, A1, b1, k1, burnin = burn, thin = thin, seed = seed[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe064ddd-8a8f-4c0c-8234-4925c8027b08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0070817737224627, 1.0051494205272644, 1.0071241574972754)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compute rhat to see if there is convergence to a distribution\n",
    "\n",
    "walks_dikin = arviz.convert_to_dataset(walks_all_dikin)\n",
    "max_rhat_dikin = arviz.rhat(walks_dikin).to_array().values.max().item()\n",
    "\n",
    "walks_vaidya = arviz.convert_to_dataset(walks_all_vaidya)\n",
    "max_rhat_vaidya = arviz.rhat(walks_vaidya).to_array().values.max().item()\n",
    "\n",
    "walks_john = arviz.convert_to_dataset(walks_all_john)\n",
    "max_rhat_john = arviz.rhat(walks_john).to_array().values.max().item()\n",
    "\n",
    "max_rhat_dikin, max_rhat_vaidya, max_rhat_john"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0c480b2-efad-4e13-a5dd-f6f559a0f40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0012444532007629282 0.0017318853821796472 0.0013303631926015384\n",
      "0.005223340170087404 0.0046792622976124035 0.005903213095929759\n"
     ]
    }
   ],
   "source": [
    "#Computing the difference between empirical mean/covariance and known mean/covariance\n",
    "\n",
    "walks_all_dikin_np = np.concatenate([walks_all_dikin[i,:,:] for i in range(nchains)])\n",
    "dikin_cov = np.cov(walks_all_dikin_np, rowvar = False)\n",
    "\n",
    "walks_all_vaidya_np = np.concatenate([walks_all_vaidya[i,:,:] for i in range(nchains)])\n",
    "vaidya_cov = np.cov(walks_all_vaidya_np, rowvar = False)\n",
    "\n",
    "walks_all_john_np = np.concatenate([walks_all_john[i,:,:] for i in range(nchains)])\n",
    "john_cov = np.cov(walks_all_john_np, rowvar = False)\n",
    "\n",
    "dikin_cov_diff = np.linalg.norm(dikin_cov - cov_simplex, ord='fro')\n",
    "vaidya_cov_diff = np.linalg.norm(vaidya_cov - cov_simplex, ord='fro')\n",
    "john_cov_diff = np.linalg.norm(john_cov - cov_simplex, ord='fro')\n",
    "\n",
    "dikin_mean_diff = np.linalg.norm(walks_all_dikin_np.mean(axis = 0) - [0.2] * 5)\n",
    "vaidya_mean_diff = np.linalg.norm(walks_all_vaidya_np.mean(axis = 0) - [0.2] * 5)\n",
    "john_mean_diff = np.linalg.norm(walks_all_john_np.mean(axis = 0) - [0.2] * 5)\n",
    "\n",
    "print(dikin_cov_diff, vaidya_cov_diff, john_cov_diff)\n",
    "print(dikin_mean_diff, vaidya_mean_diff, john_mean_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b22e99d3-1bf3-4e9b-bd0d-34e0aab59ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create 3d hypercube polytope for dense testing\n",
    "x2 = np.array([0] * 3)\n",
    "A2 = np.array([[1, 0, 0], [-1, 0, 0], [0, 1, 0], [0, -1, 0], [0, 0, 1], [0, 0, -1]])\n",
    "b2 = np.array([1] * 6)\n",
    "cov_hc = 1/3 * np.eye(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40c31872-c03e-4fea-853e-66dbef8c3e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run 5 chains on 300,000 iterations with burnin parameter of 1_000 and thin parameter of 10\n",
    "nchains = 5\n",
    "niter = 300_000\n",
    "burn = 1_000\n",
    "thin = 10\n",
    "\n",
    "seed = [1000, 2000, 3000, 4000, 5000]\n",
    "dikin = DikinWalk(r = 0.8)\n",
    "vaidya = VaidyaWalk(r = 0.9)\n",
    "john = JohnWalk(r = 0.9)\n",
    "\n",
    "walks_all_dikin = np.empty([nchains, (niter-burn)//thin, 3])\n",
    "walks_all_vaidya = np.empty([nchains, (niter-burn)//thin, 3])\n",
    "walks_all_john = np.empty([nchains, (niter-burn)//thin, 3])\n",
    "for c in range(nchains):\n",
    "    walks_all_dikin[c, :] = dikin.generateCompleteWalk(niter, x2, A2, b2, burnin = burn, thin = thin, seed = seed[c])\n",
    "    walks_all_vaidya[c, :] = vaidya.generateCompleteWalk(niter, x2, A2, b2, burnin = burn, thin = thin, seed = seed[c])\n",
    "    walks_all_john[c, :] = john.generateCompleteWalk(niter, x2, A2, b2, burnin = burn, thin = thin, seed = seed[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "153f166c-90dd-4b2b-aabd-682b4a45749a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0008391082946915, 1.0011157493535179, 1.0010260939235482)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compute rhat to see if there is convergence to a distribution\n",
    "\n",
    "walks_dikin = arviz.convert_to_dataset(walks_all_dikin)\n",
    "max_rhat_dikin = arviz.rhat(walks_dikin).to_array().values.max().item()\n",
    "\n",
    "walks_vaidya = arviz.convert_to_dataset(walks_all_vaidya)\n",
    "max_rhat_vaidya = arviz.rhat(walks_vaidya).to_array().values.max().item()\n",
    "\n",
    "walks_john = arviz.convert_to_dataset(walks_all_john)\n",
    "max_rhat_john = arviz.rhat(walks_john).to_array().values.max().item()\n",
    "\n",
    "max_rhat_dikin, max_rhat_vaidya, max_rhat_john"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ea1194e-e02c-4529-bc7a-058e8075ba6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006754593574308272 0.007232981472577084 0.007729710269585108\n",
      "0.006344973743305176 0.011318646710829806 0.00969524865630466\n"
     ]
    }
   ],
   "source": [
    "#Computing the difference between empirical mean/covariance and known mean/covariance\n",
    "\n",
    "walks_all_dikin_np = np.concatenate([walks_all_dikin[i,:,:] for i in range(nchains)])\n",
    "dikin_cov = np.cov(walks_all_dikin_np, rowvar = False)\n",
    "\n",
    "walks_all_vaidya_np = np.concatenate([walks_all_vaidya[i,:,:] for i in range(nchains)])\n",
    "vaidya_cov = np.cov(walks_all_vaidya_np, rowvar = False)\n",
    "\n",
    "walks_all_john_np = np.concatenate([walks_all_john[i,:,:] for i in range(nchains)])\n",
    "john_cov = np.cov(walks_all_john_np, rowvar = False)\n",
    "\n",
    "dikin_cov_diff = np.linalg.norm(dikin_cov - cov_hc, ord='fro')\n",
    "vaidya_cov_diff = np.linalg.norm(vaidya_cov - cov_hc, ord='fro')\n",
    "john_cov_diff = np.linalg.norm(john_cov - cov_hc, ord='fro')\n",
    "\n",
    "dikin_mean_diff = np.linalg.norm(walks_all_dikin_np.mean(axis = 0) - [0.0] * 3)\n",
    "vaidya_mean_diff = np.linalg.norm(walks_all_vaidya_np.mean(axis = 0) - [0.0] * 3)\n",
    "john_mean_diff = np.linalg.norm(walks_all_john_np.mean(axis = 0) - [0.0] * 3)\n",
    "\n",
    "print(dikin_cov_diff, vaidya_cov_diff, john_cov_diff)\n",
    "print(dikin_mean_diff, vaidya_mean_diff, john_mean_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28007157-94ae-48ae-9208-ab8f21ad361d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4655e2-7d1d-4c26-bbe1-dd4aa039c279",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df075ef-f4cb-4858-a9f1-a77e4e1463ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
