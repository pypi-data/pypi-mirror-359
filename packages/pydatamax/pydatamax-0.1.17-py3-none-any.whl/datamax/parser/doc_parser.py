import html
import os
import re
import shutil
import subprocess
import tempfile
from pathlib import Path
from typing import Union

import chardet
from loguru import logger

from datamax.parser.base import BaseLife, MarkdownOutputVo
from datamax.utils.lifecycle_types import LifeType

# å°è¯•å¯¼å…¥OLEç›¸å…³åº“ï¼ˆç”¨äºè¯»å–DOCå†…éƒ¨ç»“æ„ï¼‰
try:
    import olefile

    HAS_OLEFILE = True
except ImportError:
    HAS_OLEFILE = False
    logger.warning("âš ï¸ olefileåº“æœªå®‰è£…ï¼Œæ— æ³•è¿›è¡Œé«˜çº§DOCè§£æ")

# å°è¯•å¯¼å…¥UNOå¤„ç†å™¨
try:
    from datamax.utils.uno_handler import HAS_UNO, convert_with_uno
except ImportError:
    HAS_UNO = False
    logger.error(
        "âŒ UNOå¤„ç†å™¨å¯¼å…¥å¤±è´¥ï¼\n"
        "ğŸ”§ è§£å†³æ–¹æ¡ˆï¼š\n"
        "1. å®‰è£…LibreOfficeå’Œpython-unoï¼š\n"
        "   - Ubuntu/Debian: sudo apt-get install libreoffice python3-uno\n"
        "   - CentOS/RHEL: sudo yum install libreoffice python3-uno\n"
        "   - macOS: brew install libreoffice\n"
        "   - Windows: ä¸‹è½½å¹¶å®‰è£…LibreOffice\n"
        "2. ç¡®ä¿Pythonå¯ä»¥è®¿é—®unoæ¨¡å—ï¼š\n"
        "   - Linux: export PYTHONPATH=/usr/lib/libreoffice/program:$PYTHONPATH\n"
        "   - Windows: æ·»åŠ LibreOffice\\programåˆ°ç³»ç»ŸPATH\n"
        "3. éªŒè¯å®‰è£…ï¼špython -c 'import uno'\n"
        "4. å¦‚æœä»æœ‰é—®é¢˜ï¼Œè¯·æŸ¥çœ‹å®Œæ•´æ–‡æ¡£ï¼š\n"
        "   https://wiki.documentfoundation.org/Documentation/DevGuide/Installing_the_SDK"
    )


class DocParser(BaseLife):
    def __init__(
        self,
        file_path: Union[str, list],
        to_markdown: bool = False,
        use_uno: bool = True,
        domain: str = "Technology"
    ):
        super().__init__(domain=domain)
        self.file_path = file_path
        self.to_markdown = to_markdown

        # ä¼˜å…ˆä½¿ç”¨UNOï¼ˆé™¤éæ˜ç¡®ç¦ç”¨ï¼‰
        if use_uno and HAS_UNO:
            self.use_uno = True
            logger.info(f"ğŸš€ DocParseråˆå§‹åŒ–å®Œæˆ - ä½¿ç”¨UNO APIè¿›è¡Œå•çº¿ç¨‹é«˜æ•ˆå¤„ç†")
        else:
            self.use_uno = False
            if use_uno and not HAS_UNO:
                logger.warning(
                    f"âš ï¸ UNOä¸å¯ç”¨ï¼Œå›é€€åˆ°ä¼ ç»Ÿå‘½ä»¤è¡Œæ–¹å¼\n"
                    f"ğŸ’¡ æç¤ºï¼šUNOè½¬æ¢æ›´å¿«æ›´ç¨³å®šï¼Œå¼ºçƒˆå»ºè®®å®‰è£…å’Œé…ç½®UNO\n"
                    f"ğŸ“– è¯·å‚è€ƒä¸Šè¿°é”™è¯¯ä¿¡æ¯ä¸­çš„å®‰è£…æŒ‡å—"
                )
            else:
                logger.info(f"ğŸš€ DocParseråˆå§‹åŒ–å®Œæˆ - ä½¿ç”¨ä¼ ç»Ÿå‘½ä»¤è¡Œæ–¹å¼")

        logger.info(f"ğŸ“„ æ–‡ä»¶è·¯å¾„: {file_path}, è½¬æ¢ä¸ºmarkdown: {to_markdown}")

    def extract_all_content(self, doc_path: str) -> str:
        """
        ç»¼åˆæå–DOCæ–‡ä»¶çš„æ‰€æœ‰å†…å®¹
        æ”¯æŒå¤šç§DOCå†…éƒ¨æ ¼å¼å’Œå­˜å‚¨æ–¹å¼
        """
        logger.info(f"ğŸ” å¼€å§‹ç»¼åˆå†…å®¹æå–: {doc_path}")

        all_content = []

        try:
            # 1. å°è¯•ä½¿ç”¨OLEè§£ææå–å†…å®¹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if HAS_OLEFILE:
                ole_content = self._extract_ole_content(doc_path)
                if ole_content:
                    all_content.append(("ole", ole_content))

            # 2. å°è¯•æå–åµŒå…¥å¯¹è±¡
            embedded_content = self._extract_embedded_objects(doc_path)
            if embedded_content:
                all_content.append(("embedded", embedded_content))

            # 3. å¦‚æœä¸Šè¿°æ–¹æ³•éƒ½æ²¡æœ‰æå–åˆ°å†…å®¹ï¼Œä½¿ç”¨ä¼ ç»Ÿè½¬æ¢
            if not all_content:
                logger.info("ğŸ”„ ä½¿ç”¨ä¼ ç»Ÿè½¬æ¢æ–¹å¼æå–å†…å®¹")
                return ""  # è¿”å›ç©ºï¼Œè®©è°ƒç”¨è€…ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼

            # æ£€æŸ¥å†…å®¹è´¨é‡ï¼Œç‰¹åˆ«æ˜¯å¯¹äºWPSæ–‡ä»¶
            for content_type, content in all_content:
                if content and self._check_content_quality(content):
                    logger.info(f"âœ… ä½¿ç”¨ {content_type} å†…å®¹æå–æˆåŠŸ")
                    return content

            # å¦‚æœæ‰€æœ‰å†…å®¹è´¨é‡éƒ½ä¸ä½³ï¼Œè¿”å›ç©º
            logger.warning("âš ï¸ æ‰€æœ‰æå–æ–¹å¼çš„å†…å®¹è´¨é‡éƒ½ä¸ä½³")
            return ""

        except Exception as e:
            logger.error(f"ğŸ’¥ ç»¼åˆå†…å®¹æå–å¤±è´¥: {str(e)}")
            return ""

    def _extract_ole_content(self, doc_path: str) -> str:
        """ä½¿ç”¨OLEè§£ææå–DOCå†…å®¹"""
        try:
            ole = olefile.OleFileIO(doc_path)
            logger.info(f"ğŸ“‚ æˆåŠŸæ‰“å¼€OLEæ–‡ä»¶: {doc_path}")

            # åˆ—å‡ºæ‰€æœ‰æµ
            streams = ole.listdir()
            logger.debug(f"ğŸ“‹ å¯ç”¨çš„OLEæµ: {streams}")

            # æ£€æŸ¥æ˜¯å¦æ˜¯WPSç”Ÿæˆçš„æ–‡ä»¶
            is_wps = any("WpsCustomData" in str(stream) for stream in streams)
            if is_wps:
                logger.info("ğŸ“ æ£€æµ‹åˆ°WPS DOCæ–‡ä»¶ï¼Œå»ºè®®ä½¿ç”¨ä¼ ç»Ÿè½¬æ¢æ–¹å¼")
                # å¯¹äºWPSæ–‡ä»¶ï¼ŒOLEè§£æå¯èƒ½ä¸å¯é ï¼Œè¿”å›ç©ºè®©å…¶ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼
                ole.close()
                return ""

            all_texts = []

            # å°è¯•æå–WordDocumentæµ
            if ole.exists("WordDocument"):
                try:
                    word_stream = ole.openstream("WordDocument").read()
                    logger.info(f"ğŸ“„ WordDocumentæµå¤§å°: {len(word_stream)} å­—èŠ‚")
                    text = self._parse_word_stream(word_stream)
                    if text:
                        all_texts.append(text)
                except Exception as e:
                    logger.error(f"ğŸ’¥ è§£æWordDocumentæµå¤±è´¥: {str(e)}")

            # å°è¯•è¯»å–å…¶ä»–å¯èƒ½åŒ…å«æ–‡æœ¬çš„æµ
            text_content = []
            for entry in ole.listdir():
                if any(name in str(entry) for name in ["Text", "Content", "Body"]):
                    try:
                        stream = ole.openstream(entry)
                        data = stream.read()
                        # å°è¯•è§£ç 
                        decoded = self._try_decode_bytes(data)
                        if decoded and len(decoded.strip()) > 10:
                            text_content.append(decoded)
                    except:
                        continue

            if text_content:
                combined = "\n".join(text_content)
                logger.info(f"ğŸ“„ ä»OLEæµä¸­æå–æ–‡æœ¬: {len(combined)} å­—ç¬¦")
                return self._clean_extracted_text(combined)

            ole.close()

            return ""

        except Exception as e:
            logger.warning(f"âš ï¸ OLEè§£æå¤±è´¥: {str(e)}")

        return ""

    def _parse_word_stream(self, data: bytes) -> str:
        """è§£æWordDocumentæµä¸­çš„æ–‡æœ¬"""
        try:
            # DOCæ–‡ä»¶æ ¼å¼å¤æ‚ï¼Œè¿™é‡Œæä¾›åŸºç¡€çš„æ–‡æœ¬æå–
            # æŸ¥æ‰¾æ–‡æœ¬ç‰‡æ®µ
            text_parts = []

            # å°è¯•å¤šç§ç¼–ç ï¼Œç‰¹åˆ«æ³¨æ„ä¸­æ–‡ç¼–ç 
            for encoding in [
                "utf-16-le",
                "utf-8",
                "gbk",
                "gb18030",
                "gb2312",
                "big5",
                "cp936",
                "cp1252",
            ]:
                try:
                    decoded = data.decode(encoding, errors="ignore")
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«åˆç†çš„ä¸­æ–‡å­—ç¬¦
                    chinese_chars = len(
                        [c for c in decoded if "\u4e00" <= c <= "\u9fff"]
                    )
                    if chinese_chars > 10 or (decoded and len(decoded.strip()) > 50):
                        # è¿‡æ»¤å‡ºå¯æ‰“å°å­—ç¬¦ï¼Œä½†ä¿ç•™ä¸­æ–‡
                        cleaned = self._filter_printable_text(decoded)
                        if cleaned and len(cleaned.strip()) > 20:
                            text_parts.append(cleaned)
                            logger.debug(
                                f"ğŸ“ ä½¿ç”¨ç¼–ç  {encoding} æˆåŠŸè§£ç ï¼ŒåŒ…å« {chinese_chars} ä¸ªä¸­æ–‡å­—ç¬¦"
                            )
                            break
                except:
                    continue

            return "\n".join(text_parts) if text_parts else ""

        except Exception as e:
            logger.error(f"ğŸ’¥ è§£æWordæµå¤±è´¥: {str(e)}")
            return ""

    def _filter_printable_text(self, text: str) -> str:
        """è¿‡æ»¤æ–‡æœ¬ï¼Œä¿ç•™å¯æ‰“å°å­—ç¬¦å’Œä¸­æ–‡"""
        result = []
        for char in text:
            # ä¿ç•™ä¸­æ–‡å­—ç¬¦
            if "\u4e00" <= char <= "\u9fff":
                result.append(char)
            # ä¿ç•™æ—¥æ–‡å­—ç¬¦
            elif "\u3040" <= char <= "\u30ff":
                result.append(char)
            # ä¿ç•™éŸ©æ–‡å­—ç¬¦
            elif "\uac00" <= char <= "\ud7af":
                result.append(char)
            # ä¿ç•™ASCIIå¯æ‰“å°å­—ç¬¦å’Œç©ºç™½å­—ç¬¦
            elif char.isprintable() or char.isspace():
                result.append(char)
            # ä¿ç•™å¸¸ç”¨æ ‡ç‚¹ç¬¦å·
            elif char in 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""' "ï¼ˆï¼‰ã€ã€‘ã€Šã€‹ã€Â·â€¦â€”":
                result.append(char)

        return "".join(result)

    def _try_decode_bytes(self, data: bytes) -> str:
        """å°è¯•ä½¿ç”¨å¤šç§ç¼–ç è§£ç å­—èŠ‚æ•°æ®"""
        # ä¼˜å…ˆå°è¯•ä¸­æ–‡ç¼–ç 
        encodings = [
            "utf-8",
            "gbk",
            "gb18030",
            "gb2312",
            "big5",
            "utf-16-le",
            "utf-16-be",
            "cp936",
            "cp1252",
            "latin-1",
        ]

        # é¦–å…ˆå°è¯•ä½¿ç”¨chardetæ£€æµ‹ç¼–ç 
        try:
            import chardet

            detected = chardet.detect(data)
            if detected["encoding"] and detected["confidence"] > 0.7:
                encodings.insert(0, detected["encoding"])
                logger.debug(
                    f"ğŸ” æ£€æµ‹åˆ°ç¼–ç : {detected['encoding']} (ç½®ä¿¡åº¦: {detected['confidence']})"
                )
        except:
            pass

        for encoding in encodings:
            try:
                decoded = data.decode(encoding, errors="ignore")
                # æ£€æŸ¥æ˜¯å¦åŒ…å«æœ‰æ„ä¹‰çš„æ–‡æœ¬ï¼ˆåŒ…æ‹¬ä¸­æ–‡ï¼‰
                if decoded and (
                    any(c.isalnum() for c in decoded)
                    or any("\u4e00" <= c <= "\u9fff" for c in decoded)
                ):
                    # è¿›ä¸€æ­¥æ¸…ç†æ–‡æœ¬
                    cleaned = self._filter_printable_text(decoded)
                    if cleaned and len(cleaned.strip()) > 10:
                        return cleaned
            except:
                continue

        return ""

    def _extract_embedded_objects(self, doc_path: str) -> str:
        """æå–DOCæ–‡ä»¶ä¸­çš„åµŒå…¥å¯¹è±¡"""
        try:
            if not HAS_OLEFILE:
                return ""

            embedded_content = []

            with olefile.OleFileIO(doc_path) as ole:
                # æŸ¥æ‰¾åµŒå…¥çš„å¯¹è±¡
                for entry in ole.listdir():
                    entry_name = "/".join(entry)

                    # æ£€æŸ¥æ˜¯å¦æ˜¯åµŒå…¥å¯¹è±¡
                    if any(
                        pattern in entry_name.lower()
                        for pattern in ["object", "embed", "package"]
                    ):
                        logger.info(f"ğŸ“ æ‰¾åˆ°åµŒå…¥å¯¹è±¡: {entry_name}")
                        try:
                            stream = ole.openstream(entry)
                            data = stream.read()

                            # å°è¯•æå–æ–‡æœ¬å†…å®¹
                            text = self._try_decode_bytes(data)
                            if text and len(text.strip()) > 20:
                                embedded_content.append(text.strip())
                        except:
                            continue

            return "\n\n".join(embedded_content) if embedded_content else ""

        except Exception as e:
            logger.warning(f"âš ï¸ æå–åµŒå…¥å¯¹è±¡å¤±è´¥: {str(e)}")
            return ""

    def _clean_extracted_text(self, text: str) -> str:
        """æ¸…ç†æå–çš„æ–‡æœ¬ï¼Œå½»åº•ç§»é™¤æ‰€æœ‰XMLæ ‡ç­¾å’Œæ§åˆ¶å­—ç¬¦ï¼Œåªä¿ç•™çº¯æ–‡æœ¬"""
        try:
            # 1. è§£ç HTML/XMLå®ä½“
            text = html.unescape(text)

            # 2. ç§»é™¤æ‰€æœ‰XML/HTMLæ ‡ç­¾
            text = re.sub(r"<[^>]+>", "", text)

            # 3. ç§»é™¤XMLå‘½åç©ºé—´å‰ç¼€
            text = re.sub(r"\b\w+:", "", text)

            # 4. ç§»é™¤NULLå­—ç¬¦å’Œå…¶ä»–æ§åˆ¶å­—ç¬¦
            text = re.sub(r"[\x00-\x08\x0b-\x0c\x0e-\x1f\x7f]", "", text)

            # 5. ç§»é™¤ç‰¹æ®Šçš„XMLå­—ç¬¦åºåˆ—
            text = re.sub(r"&[a-zA-Z]+;", "", text)
            text = re.sub(r"&#\d+;", "", text)
            text = re.sub(r"&#x[0-9a-fA-F]+;", "", text)

            # 6. ä¿ç•™æœ‰æ„ä¹‰çš„å­—ç¬¦ï¼Œç§»é™¤å…¶ä»–ç‰¹æ®Šå­—ç¬¦
            # ä¿ç•™ï¼šä¸­æ–‡ã€æ—¥æ–‡ã€éŸ©æ–‡ã€è‹±æ–‡ã€æ•°å­—ã€å¸¸ç”¨æ ‡ç‚¹å’Œç©ºç™½
            allowed_chars = (
                r"\w\s"  # å­—æ¯æ•°å­—å’Œç©ºç™½
                r"\u4e00-\u9fff"  # ä¸­æ–‡
                r"\u3040-\u30ff"  # æ—¥æ–‡
                r"\uac00-\ud7af"  # éŸ©æ–‡
                r'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""'
                "ï¼ˆï¼‰ã€ã€‘ã€Šã€‹ã€Â·â€¦â€”"  # ä¸­æ–‡æ ‡ç‚¹
                r'.,!?;:()[\]{}"\'`~@#$%^&*+=\-_/\\'  # è‹±æ–‡æ ‡ç‚¹å’Œå¸¸ç”¨ç¬¦å·
            )

            # ä½¿ç”¨æ›´ä¸¥æ ¼çš„è¿‡æ»¤ï¼Œä½†ä¿ç•™æ‰€æœ‰æœ‰æ„ä¹‰çš„å­—ç¬¦
            cleaned_text = "".join(
                char for char in text if re.match(f"[{allowed_chars}]", char)
            )

            # 7. ç§»é™¤è¿‡é•¿çš„æ— æ„ä¹‰å­—ç¬¦åºåˆ—ï¼ˆé€šå¸¸æ˜¯äºŒè¿›åˆ¶åƒåœ¾ï¼‰
            cleaned_text = re.sub(r"([^\s\u4e00-\u9fff])\1{5,}", r"\1", cleaned_text)

            # 8. æ¸…ç†å¤šä½™çš„ç©ºç™½ï¼Œä½†ä¿ç•™æ®µè½ç»“æ„
            cleaned_text = re.sub(
                r"[ \t]+", " ", cleaned_text
            )  # å¤šä¸ªç©ºæ ¼/åˆ¶è¡¨ç¬¦å˜ä¸ºå•ä¸ªç©ºæ ¼
            cleaned_text = re.sub(
                r"\n\s*\n\s*\n+", "\n\n", cleaned_text
            )  # å¤šä¸ªç©ºè¡Œå˜ä¸ºåŒç©ºè¡Œ
            cleaned_text = re.sub(
                r"^\s+|\s+$", "", cleaned_text, flags=re.MULTILINE
            )  # ç§»é™¤è¡Œé¦–è¡Œå°¾ç©ºç™½

            # 9. è¿›ä¸€æ­¥æ¸…ç†ï¼šç§»é™¤ç‹¬ç«‹çš„æ ‡ç‚¹ç¬¦å·è¡Œ
            lines = cleaned_text.split("\n")
            cleaned_lines = []

            for line in lines:
                line = line.strip()
                if line:
                    # æ£€æŸ¥è¡Œæ˜¯å¦ä¸»è¦æ˜¯æœ‰æ„ä¹‰çš„å†…å®¹
                    # è®¡ç®—ä¸­æ–‡ã€è‹±æ–‡å­—æ¯å’Œæ•°å­—çš„æ¯”ä¾‹
                    meaningful_chars = sum(
                        1 for c in line if (c.isalnum() or "\u4e00" <= c <= "\u9fff")
                    )

                    # å¦‚æœæœ‰æ„ä¹‰å­—ç¬¦å æ¯”è¶…è¿‡30%ï¼Œæˆ–è€…è¡Œé•¿åº¦å°äº5ï¼ˆå¯èƒ½æ˜¯æ ‡é¢˜ï¼‰ï¼Œåˆ™ä¿ç•™
                    if len(line) < 5 or (
                        meaningful_chars > 0 and meaningful_chars / len(line) > 0.3
                    ):
                        cleaned_lines.append(line)
                elif cleaned_lines and cleaned_lines[-1]:  # ä¿ç•™æ®µè½åˆ†éš”
                    cleaned_lines.append("")

            result = "\n".join(cleaned_lines).strip()

            # 10. æœ€ç»ˆæ£€æŸ¥
            if len(result) < 10:
                logger.warning("âš ï¸ æ¸…ç†åçš„æ–‡æœ¬è¿‡çŸ­ï¼Œå¯èƒ½å­˜åœ¨é—®é¢˜")
                return ""

            # æ£€æŸ¥æ˜¯å¦è¿˜åŒ…å«XMLæ ‡ç­¾
            if re.search(r"<[^>]+>", result):
                logger.warning("âš ï¸ æ¸…ç†åä»åŒ…å«XMLæ ‡ç­¾ï¼Œè¿›è¡ŒäºŒæ¬¡æ¸…ç†")
                result = re.sub(r"<[^>]+>", "", result)

            return result

        except Exception as e:
            logger.error(f"ğŸ’¥ æ¸…ç†æ–‡æœ¬å¤±è´¥: {str(e)}")
            return text

    def _combine_extracted_content(self, content_list: list) -> str:
        """åˆå¹¶æå–åˆ°çš„å„ç§å†…å®¹"""
        combined = []

        # æŒ‰ä¼˜å…ˆçº§æ’åºå†…å®¹
        priority_order = ["ole", "embedded", "converted", "fallback"]

        for content_type in priority_order:
            for item_type, content in content_list:
                if item_type == content_type and content.strip():
                    combined.append(content.strip())

        # æ·»åŠ å…¶ä»–æœªåˆ†ç±»çš„å†…å®¹
        for item_type, content in content_list:
            if item_type not in priority_order and content.strip():
                combined.append(content.strip())

        return "\n\n".join(combined) if combined else ""

    def doc_to_txt(self, doc_path: str, dir_path: str) -> str:
        """å°†.docæ–‡ä»¶è½¬æ¢ä¸º.txtæ–‡ä»¶"""
        logger.info(
            f"ğŸ”„ å¼€å§‹è½¬æ¢DOCæ–‡ä»¶ä¸ºTXT - æºæ–‡ä»¶: {doc_path}, è¾“å‡ºç›®å½•: {dir_path}"
        )

        if self.use_uno:
            # ä½¿ç”¨UNO APIè¿›è¡Œè½¬æ¢
            try:
                logger.info("ğŸ¯ ä½¿ç”¨UNO APIè¿›è¡Œæ–‡æ¡£è½¬æ¢...")
                txt_path = convert_with_uno(doc_path, "txt", dir_path)

                if not os.path.exists(txt_path):
                    logger.error(f"âŒ è½¬æ¢åçš„TXTæ–‡ä»¶ä¸å­˜åœ¨: {txt_path}")
                    raise Exception(f"æ–‡ä»¶è½¬æ¢å¤±è´¥ {doc_path} ==> {txt_path}")
                else:
                    logger.info(f"ğŸ‰ TXTæ–‡ä»¶è½¬æ¢æˆåŠŸï¼Œæ–‡ä»¶è·¯å¾„: {txt_path}")
                    return txt_path

            except Exception as e:
                logger.error(
                    f"ğŸ’¥ UNOè½¬æ¢å¤±è´¥: {str(e)}\n"
                    f"ğŸ” è¯Šæ–­ä¿¡æ¯ï¼š\n"
                    f"   - é”™è¯¯ç±»å‹: {type(e).__name__}\n"
                    f"   - LibreOfficeæ˜¯å¦å·²å®‰è£…ï¼Ÿå°è¯•è¿è¡Œ: soffice --version\n"
                    f"   - Python UNOæ¨¡å—æ˜¯å¦å¯ç”¨ï¼Ÿå°è¯•: python -c 'import uno'\n"
                    f"   - æ˜¯å¦æœ‰å…¶ä»–LibreOfficeå®ä¾‹åœ¨è¿è¡Œï¼Ÿ\n"
                    f"   - æ–‡ä»¶æƒé™æ˜¯å¦æ­£ç¡®ï¼Ÿ\n"
                    f"ğŸ”§ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆï¼š\n"
                    f"   1. ç¡®ä¿LibreOfficeæ­£ç¡®å®‰è£…\n"
                    f"   2. å…³é—­æ‰€æœ‰LibreOfficeè¿›ç¨‹\n"
                    f"   3. æ£€æŸ¥æ–‡ä»¶æƒé™å’Œè·¯å¾„\n"
                    f'   4. å°è¯•æ‰‹åŠ¨è¿è¡Œ: soffice --headless --convert-to txt "{doc_path}"'
                )
                logger.warning("âš ï¸ è‡ªåŠ¨å›é€€åˆ°ä¼ ç»Ÿå‘½ä»¤è¡Œæ–¹å¼...")
                return self._doc_to_txt_subprocess(doc_path, dir_path)
        else:
            # ä½¿ç”¨ä¼ ç»Ÿçš„subprocessæ–¹å¼
            return self._doc_to_txt_subprocess(doc_path, dir_path)

    def _doc_to_txt_subprocess(self, doc_path: str, dir_path: str) -> str:
        """ä½¿ç”¨subprocesså°†.docæ–‡ä»¶è½¬æ¢ä¸º.txtæ–‡ä»¶ï¼ˆä¼ ç»Ÿæ–¹å¼ï¼‰"""
        try:
            cmd = f'soffice --headless --convert-to txt "{doc_path}" --outdir "{dir_path}"'
            logger.debug(f"âš¡ æ‰§è¡Œè½¬æ¢å‘½ä»¤: {cmd}")

            process = subprocess.Popen(
                cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE
            )
            stdout, stderr = process.communicate()
            exit_code = process.returncode

            if exit_code == 0:
                logger.info(f"âœ… DOCåˆ°TXTè½¬æ¢æˆåŠŸ - é€€å‡ºç : {exit_code}")
                if stdout:
                    logger.debug(
                        f"ğŸ“„ è½¬æ¢è¾“å‡º: {stdout.decode('utf-8', errors='replace')}"
                    )
            else:
                encoding = chardet.detect(stderr)["encoding"]
                if encoding is None:
                    encoding = "utf-8"
                error_msg = stderr.decode(encoding, errors="replace")
                logger.error(
                    f"âŒ DOCåˆ°TXTè½¬æ¢å¤±è´¥ - é€€å‡ºç : {exit_code}, é”™è¯¯ä¿¡æ¯: {error_msg}"
                )
                raise Exception(
                    f"Error Output (detected encoding: {encoding}): {error_msg}"
                )

            fname = str(Path(doc_path).stem)
            txt_path = os.path.join(dir_path, f"{fname}.txt")

            if not os.path.exists(txt_path):
                logger.error(f"âŒ è½¬æ¢åçš„TXTæ–‡ä»¶ä¸å­˜åœ¨: {txt_path}")
                raise Exception(f"æ–‡ä»¶è½¬æ¢å¤±è´¥ {doc_path} ==> {txt_path}")
            else:
                logger.info(f"ğŸ‰ TXTæ–‡ä»¶è½¬æ¢æˆåŠŸï¼Œæ–‡ä»¶è·¯å¾„: {txt_path}")
                return txt_path

        except subprocess.SubprocessError as e:
            logger.error(f"ğŸ’¥ subprocessæ‰§è¡Œå¤±è´¥: {str(e)}")
            raise Exception(f"æ‰§è¡Œè½¬æ¢å‘½ä»¤æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        except Exception as e:
            logger.error(f"ğŸ’¥ DOCåˆ°TXTè½¬æ¢è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}")
            raise

    def read_txt_file(self, txt_path: str) -> str:
        """è¯»å–txtæ–‡ä»¶å†…å®¹"""
        logger.info(f"ğŸ“– å¼€å§‹è¯»å–TXTæ–‡ä»¶: {txt_path}")

        try:
            # æ£€æµ‹æ–‡ä»¶ç¼–ç 
            with open(txt_path, "rb") as f:
                raw_data = f.read()
                encoding = chardet.detect(raw_data)["encoding"]
                if encoding is None:
                    encoding = "utf-8"
                logger.debug(f"ğŸ” æ£€æµ‹åˆ°æ–‡ä»¶ç¼–ç : {encoding}")

            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(txt_path, "r", encoding=encoding, errors="replace") as f:
                content = f.read()

            logger.info(f"ğŸ“„ TXTæ–‡ä»¶è¯»å–å®Œæˆ - å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
            logger.debug(f"ğŸ‘€ å‰100å­—ç¬¦é¢„è§ˆ: {content[:100]}...")

            return content

        except FileNotFoundError as e:
            logger.error(f"ğŸš« TXTæ–‡ä»¶æœªæ‰¾åˆ°: {str(e)}")
            raise Exception(f"æ–‡ä»¶æœªæ‰¾åˆ°: {txt_path}")
        except Exception as e:
            logger.error(f"ğŸ’¥ è¯»å–TXTæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            raise

    def read_doc_file(self, doc_path: str) -> str:
        """è¯»å–docæ–‡ä»¶å¹¶è½¬æ¢ä¸ºæ–‡æœ¬"""
        logger.info(f"ğŸ“– å¼€å§‹è¯»å–DOCæ–‡ä»¶ - æ–‡ä»¶: {doc_path}")

        try:
            # é¦–å…ˆå°è¯•ç»¼åˆæå–ï¼ˆå¦‚æœæœ‰é«˜çº§è§£æåŠŸèƒ½ï¼‰
            if HAS_OLEFILE:
                comprehensive_content = self.extract_all_content(doc_path)
                if comprehensive_content and comprehensive_content.strip():
                    # æ£€æŸ¥å†…å®¹è´¨é‡
                    if self._check_content_quality(comprehensive_content):
                        logger.info(
                            f"âœ¨ ä½¿ç”¨ç»¼åˆæå–æ–¹å¼æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(comprehensive_content)} å­—ç¬¦"
                        )
                        return comprehensive_content
                    else:
                        logger.warning("âš ï¸ ç»¼åˆæå–çš„å†…å®¹è´¨é‡ä¸ä½³ï¼Œå°è¯•å…¶ä»–æ–¹å¼")

            # é™çº§åˆ°ä¼ ç»Ÿè½¬æ¢æ–¹å¼
            logger.info("ğŸ”„ ä½¿ç”¨ä¼ ç»Ÿè½¬æ¢æ–¹å¼")

            with tempfile.TemporaryDirectory() as temp_path:
                logger.debug(f"ğŸ“ åˆ›å»ºä¸´æ—¶ç›®å½•: {temp_path}")

                temp_dir = Path(temp_path)

                file_path = temp_dir / "tmp.doc"
                shutil.copy(doc_path, file_path)
                logger.debug(f"ğŸ“‹ å¤åˆ¶æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•: {doc_path} -> {file_path}")

                # è½¬æ¢DOCä¸ºTXT
                txt_file_path = self.doc_to_txt(str(file_path), str(temp_path))
                logger.info(f"ğŸ¯ DOCè½¬TXTå®Œæˆ: {txt_file_path}")

                # è¯»å–TXTæ–‡ä»¶å†…å®¹
                content = self.read_txt_file(txt_file_path)
                logger.info(f"âœ¨ TXTæ–‡ä»¶å†…å®¹è¯»å–å®Œæˆï¼Œå†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")

                return content

        except FileNotFoundError as e:
            logger.error(f"ğŸš« æ–‡ä»¶æœªæ‰¾åˆ°: {str(e)}")
            raise Exception(f"æ–‡ä»¶æœªæ‰¾åˆ°: {doc_path}")
        except PermissionError as e:
            logger.error(f"ğŸ”’ æ–‡ä»¶æƒé™é”™è¯¯: {str(e)}")
            raise Exception(f"æ— æƒé™è®¿é—®æ–‡ä»¶: {doc_path}")
        except Exception as e:
            logger.error(f"ğŸ’¥ è¯»å–DOCæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            raise

    def _check_content_quality(self, content: str) -> bool:
        """æ£€æŸ¥æå–å†…å®¹çš„è´¨é‡"""
        if not content or len(content) < 50:
            return False

        # è®¡ç®—ä¹±ç å­—ç¬¦æ¯”ä¾‹
        total_chars = len(content)
        # å¯è¯†åˆ«å­—ç¬¦ï¼šASCIIã€ä¸­æ–‡ã€æ—¥æ–‡ã€éŸ©æ–‡ã€å¸¸ç”¨æ ‡ç‚¹
        recognizable = sum(
            1
            for c in content
            if (
                c.isascii()
                or "\u4e00" <= c <= "\u9fff"  # ä¸­æ–‡
                or "\u3040" <= c <= "\u30ff"  # æ—¥æ–‡
                or "\uac00" <= c <= "\ud7af"  # éŸ©æ–‡
                or c in 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""' "ï¼ˆï¼‰ã€ã€‘ã€Šã€‹ã€Â·â€¦â€”\n\r\t "
            )
        )

        # å¦‚æœå¯è¯†åˆ«å­—ç¬¦å æ¯”ä½äº70%ï¼Œè®¤ä¸ºè´¨é‡ä¸ä½³
        if recognizable / total_chars < 0.7:
            logger.warning(
                f"âš ï¸ å†…å®¹è´¨é‡æ£€æŸ¥å¤±è´¥ï¼šå¯è¯†åˆ«å­—ç¬¦æ¯”ä¾‹ {recognizable}/{total_chars} = {recognizable/total_chars:.2%}"
            )
            return False

        return True

    def parse(self, file_path: str):
        """è§£æDOCæ–‡ä»¶"""
        logger.info(f"ğŸ¬ å¼€å§‹è§£æDOCæ–‡ä»¶: {file_path}")

        try:
            # éªŒè¯æ–‡ä»¶å­˜åœ¨
            if not os.path.exists(file_path):
                logger.error(f"ğŸš« æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

            # éªŒè¯æ–‡ä»¶æ‰©å±•å
            if not file_path.lower().endswith(".doc"):
                logger.warning(f"âš ï¸ æ–‡ä»¶æ‰©å±•åä¸æ˜¯.doc: {file_path}")

            # éªŒè¯æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(file_path)
            logger.info(f"ğŸ“ æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")

            if file_size == 0:
                logger.warning(f"âš ï¸ æ–‡ä»¶å¤§å°ä¸º0å­—èŠ‚: {file_path}")
            # ç”Ÿå‘½å‘¨æœŸï¼šData Processing å¼€å§‹
            lc_start = self.generate_lifecycle(
                source_file=file_path,
                domain=self.domain,
                life_type=LifeType.DATA_PROCESSING,
                usage_purpose="Documentation",
            )

            # ğŸ·ï¸ æå–æ–‡ä»¶æ‰©å±•å
            extension = self.get_file_extension(file_path)
            logger.debug(f"ğŸ·ï¸ æå–æ–‡ä»¶æ‰©å±•å: {extension}")

            # è¯»å–æ–‡ä»¶å†…å®¹
            logger.info("ğŸ“ è¯»å–DOCæ–‡ä»¶å†…å®¹")
            content = self.read_doc_file(doc_path=file_path)

            # æ ¹æ®to_markdownå‚æ•°å†³å®šæ˜¯å¦ä¿æŒåŸæ ¼å¼è¿˜æ˜¯å¤„ç†ä¸ºmarkdownæ ¼å¼
            if self.to_markdown:
                # ç®€å•çš„æ–‡æœ¬åˆ°markdownè½¬æ¢ï¼ˆä¿æŒæ®µè½ç»“æ„ï¼‰
                mk_content = self.format_as_markdown(content)
                logger.info("ğŸ¨ å†…å®¹å·²æ ¼å¼åŒ–ä¸ºmarkdownæ ¼å¼")
            else:
                mk_content = content
                logger.info("ğŸ“ ä¿æŒåŸå§‹æ–‡æœ¬æ ¼å¼")
            # 3) ç”Ÿå‘½å‘¨æœŸï¼šData Processed or Failed
            lc_end = self.generate_lifecycle(
                source_file=file_path,
                domain=self.domain,
                life_type=(
                    LifeType.DATA_PROCESSED
                    if mk_content.strip()
                    else LifeType.DATA_PROCESS_FAILED
                ),
                usage_purpose="Documentation",
            )

            logger.info(f"ğŸŠ æ–‡ä»¶å†…å®¹è§£æå®Œæˆï¼Œæœ€ç»ˆå†…å®¹é•¿åº¦: {len(mk_content)} å­—ç¬¦")

            # æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºç©º
            if not mk_content.strip():
                logger.warning(f"âš ï¸ è§£æå‡ºçš„å†…å®¹ä¸ºç©º: {file_path}")

            lifecycle = self.generate_lifecycle(
                source_file=file_path,
                domain=self.domain,
                usage_purpose="Documentation",
                life_type="LLM_ORIGIN",
            )
            logger.debug("âš™ï¸ ç”Ÿæˆlifecycleä¿¡æ¯å®Œæˆ")

            output_vo = MarkdownOutputVo(extension, mk_content)
            output_vo.add_lifecycle(lc_start)
            output_vo.add_lifecycle(lc_end)
            # output_vo.add_lifecycle(lc_origin)

            result = output_vo.to_dict()
            logger.info(f"ğŸ† DOCæ–‡ä»¶è§£æå®Œæˆ: {file_path}")
            logger.debug(f"ğŸ”‘ è¿”å›ç»“æœé”®: {list(result.keys())}")

            return result

        except FileNotFoundError as e:
            logger.error(f"ğŸš« æ–‡ä»¶ä¸å­˜åœ¨é”™è¯¯: {str(e)}")
            raise
        except PermissionError as e:
            logger.error(f"ğŸ”’ æ–‡ä»¶æƒé™é”™è¯¯: {str(e)}")
            raise Exception(f"æ— æƒé™è®¿é—®æ–‡ä»¶: {file_path}")
        except Exception as e:
            logger.error(
                f"ğŸ’€ è§£æDOCæ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯ç±»å‹: {type(e).__name__}, é”™è¯¯ä¿¡æ¯: {str(e)}"
            )
            raise

    def format_as_markdown(self, content: str) -> str:
        """å°†çº¯æ–‡æœ¬æ ¼å¼åŒ–ä¸ºç®€å•çš„markdownæ ¼å¼"""
        if not content.strip():
            return content

        lines = content.split("\n")
        formatted_lines = []

        for line in lines:
            line = line.strip()
            if not line:
                formatted_lines.append("")
                continue

            # ç®€å•çš„markdownæ ¼å¼åŒ–è§„åˆ™
            # å¯ä»¥æ ¹æ®éœ€è¦æ‰©å±•æ›´å¤šè§„åˆ™
            formatted_lines.append(line)

        return "\n".join(formatted_lines)

    def _extract_text_from_wps_stream(self, data: bytes) -> str:
        """ä»WPSçš„WordDocumentæµä¸­æå–æ–‡æœ¬ï¼ˆä½¿ç”¨æ›´å®½æ¾çš„ç­–ç•¥ï¼‰"""
        try:
            text_parts = []

            # WPSæ–‡ä»¶å¯èƒ½ä½¿ç”¨ä¸åŒçš„ç¼–ç å’Œç»“æ„
            # å°è¯•å¤šç§ç­–ç•¥æå–æ–‡æœ¬

            # ç­–ç•¥1ï¼šå°è¯•æ‰¾åˆ°è¿ç»­çš„æ–‡æœ¬å—
            # æŸ¥æ‰¾çœ‹èµ·æ¥åƒæ–‡æœ¬çš„å­—èŠ‚åºåˆ—
            i = 0
            while i < len(data):
                # æŸ¥æ‰¾å¯èƒ½çš„æ–‡æœ¬å¼€å§‹ä½ç½®
                if i + 2 < len(data):
                    # æ£€æŸ¥æ˜¯å¦æ˜¯Unicodeæ–‡æœ¬ï¼ˆå°ç«¯åºï¼‰
                    if data[i + 1] == 0 and 32 <= data[i] <= 126:
                        # å¯èƒ½æ˜¯ASCIIå­—ç¬¦çš„Unicodeç¼–ç 
                        text_block = bytearray()
                        j = i
                        while (
                            j + 1 < len(data)
                            and data[j + 1] == 0
                            and 32 <= data[j] <= 126
                        ):
                            text_block.append(data[j])
                            j += 2
                        if len(text_block) > 10:
                            text_parts.append(
                                text_block.decode("ascii", errors="ignore")
                            )
                        i = j
                    # æ£€æŸ¥æ˜¯å¦æ˜¯UTF-8æˆ–GBKä¸­æ–‡
                    elif 0xE0 <= data[i] <= 0xEF or 0x81 <= data[i] <= 0xFE:
                        # å¯èƒ½æ˜¯å¤šå­—èŠ‚å­—ç¬¦
                        text_block = bytearray()
                        j = i
                        while j < len(data):
                            if data[j] < 32 and data[j] not in [9, 10, 13]:
                                break
                            text_block.append(data[j])
                            j += 1
                        if len(text_block) > 20:
                            # å°è¯•è§£ç 
                            for encoding in ["utf-8", "gbk", "gb18030", "gb2312"]:
                                try:
                                    decoded = text_block.decode(
                                        encoding, errors="ignore"
                                    )
                                    if decoded and len(decoded.strip()) > 10:
                                        text_parts.append(decoded)
                                        break
                                except:
                                    continue
                        i = j
                    else:
                        i += 1
                else:
                    i += 1

            # åˆå¹¶æ–‡æœ¬éƒ¨åˆ†
            if text_parts:
                combined = "\n".join(text_parts)
                return self._clean_extracted_text(combined)

            # å¦‚æœä¸Šè¿°æ–¹æ³•å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹æ–¹æ³•
            return self._parse_word_stream(data)

        except Exception as e:
            logger.error(f"ğŸ’¥ è§£æWPSæµå¤±è´¥: {str(e)}")
            return ""
