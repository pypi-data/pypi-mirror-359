import html
import os
import re
import shutil
import subprocess
import tempfile
import zipfile
from pathlib import Path
from typing import Optional, Union

import chardet
from loguru import logger

from datamax.parser.base import BaseLife, MarkdownOutputVo
from datamax.utils.lifecycle_types import LifeType

# å°è¯•å¯¼å…¥UNOå¤„ç†å™¨
try:
    from datamax.utils.uno_handler import HAS_UNO, convert_with_uno
except ImportError:
    HAS_UNO = False
    logger.error(
        "âŒ UNOå¤„ç†å™¨å¯¼å…¥å¤±è´¥ï¼\n"
        "ğŸ”§ è§£å†³æ–¹æ¡ˆï¼š\n"
        "1. å®‰è£…LibreOfficeå’Œpython-unoï¼š\n"
        "   - Ubuntu/Debian: sudo apt-get install libreoffice python3-uno\n"
        "   - CentOS/RHEL: sudo yum install libreoffice python3-uno\n"
        "   - macOS: brew install libreoffice\n"
        "   - Windows: ä¸‹è½½å¹¶å®‰è£…LibreOffice\n"
        "2. ç¡®ä¿Pythonå¯ä»¥è®¿é—®unoæ¨¡å—ï¼š\n"
        "   - Linux: export PYTHONPATH=/usr/lib/libreoffice/program:$PYTHONPATH\n"
        "   - Windows: æ·»åŠ LibreOffice\\programåˆ°ç³»ç»ŸPATH\n"
        "3. éªŒè¯å®‰è£…ï¼špython -c 'import uno'\n"
        "4. å¦‚æœä»æœ‰é—®é¢˜ï¼Œè¯·æŸ¥çœ‹å®Œæ•´æ–‡æ¡£ï¼š\n"
        "   https://wiki.documentfoundation.org/Documentation/DevGuide/Installing_the_SDK"
    )


class DocxParser(BaseLife):
    def __init__(
        self,
        file_path: Union[str, list],
        to_markdown: bool = False,
        use_uno: bool = True,
        domain: str = "Technology",
    ):
        super().__init__(domain=domain)
        self.file_path = file_path
        self.to_markdown = to_markdown

        # ä¼˜å…ˆä½¿ç”¨UNOï¼ˆé™¤éæ˜ç¡®ç¦ç”¨ï¼‰
        if use_uno and HAS_UNO:
            self.use_uno = True
            logger.info(f"ğŸš€ DocxParseråˆå§‹åŒ–å®Œæˆ - ä½¿ç”¨UNO APIè¿›è¡Œå•çº¿ç¨‹é«˜æ•ˆå¤„ç†")
        else:
            self.use_uno = False
            if use_uno and not HAS_UNO:
                logger.warning(
                    f"âš ï¸ UNOä¸å¯ç”¨ï¼Œå›é€€åˆ°ä¼ ç»Ÿå‘½ä»¤è¡Œæ–¹å¼\n"
                    f"ğŸ’¡ æç¤ºï¼šUNOè½¬æ¢æ›´å¿«æ›´ç¨³å®šï¼Œå¼ºçƒˆå»ºè®®å®‰è£…å’Œé…ç½®UNO\n"
                    f"ğŸ“– è¯·å‚è€ƒä¸Šè¿°é”™è¯¯ä¿¡æ¯ä¸­çš„å®‰è£…æŒ‡å—"
                )
            else:
                logger.info(f"ğŸš€ DocxParseråˆå§‹åŒ–å®Œæˆ - ä½¿ç”¨ä¼ ç»Ÿå‘½ä»¤è¡Œæ–¹å¼")

        logger.info(f"ğŸ“„ æ–‡ä»¶è·¯å¾„: {file_path}, è½¬æ¢ä¸ºmarkdown: {to_markdown}")

    def docx_to_txt(self, docx_path: str, dir_path: str) -> str:
        """å°†.docxæ–‡ä»¶è½¬æ¢ä¸º.txtæ–‡ä»¶"""
        logger.info(
            f"ğŸ”„ å¼€å§‹è½¬æ¢DOCXæ–‡ä»¶ä¸ºTXT - æºæ–‡ä»¶: {docx_path}, è¾“å‡ºç›®å½•: {dir_path}"
        )

        if self.use_uno:
            # ä½¿ç”¨UNO APIè¿›è¡Œè½¬æ¢
            try:
                logger.info("ğŸ¯ ä½¿ç”¨UNO APIè¿›è¡Œæ–‡æ¡£è½¬æ¢...")
                txt_path = convert_with_uno(docx_path, "txt", dir_path)

                if not os.path.exists(txt_path):
                    logger.error(f"âŒ è½¬æ¢åçš„TXTæ–‡ä»¶ä¸å­˜åœ¨: {txt_path}")
                    raise Exception(f"æ–‡ä»¶è½¬æ¢å¤±è´¥ {docx_path} ==> {txt_path}")
                else:
                    logger.info(f"ğŸ‰ TXTæ–‡ä»¶è½¬æ¢æˆåŠŸï¼Œæ–‡ä»¶è·¯å¾„: {txt_path}")
                    return txt_path

            except Exception as e:
                logger.error(
                    f"ğŸ’¥ UNOè½¬æ¢å¤±è´¥: {str(e)}\n"
                    f"ğŸ” è¯Šæ–­ä¿¡æ¯ï¼š\n"
                    f"   - é”™è¯¯ç±»å‹: {type(e).__name__}\n"
                    f"   - LibreOfficeæ˜¯å¦å·²å®‰è£…ï¼Ÿå°è¯•è¿è¡Œ: soffice --version\n"
                    f"   - Python UNOæ¨¡å—æ˜¯å¦å¯ç”¨ï¼Ÿå°è¯•: python -c 'import uno'\n"
                    f"   - æ˜¯å¦æœ‰å…¶ä»–LibreOfficeå®ä¾‹åœ¨è¿è¡Œï¼Ÿ\n"
                    f"   - æ–‡ä»¶æƒé™æ˜¯å¦æ­£ç¡®ï¼Ÿ\n"
                    f"ğŸ”§ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆï¼š\n"
                    f"   1. ç¡®ä¿LibreOfficeæ­£ç¡®å®‰è£…\n"
                    f"   2. å…³é—­æ‰€æœ‰LibreOfficeè¿›ç¨‹\n"
                    f"   3. æ£€æŸ¥æ–‡ä»¶æƒé™å’Œè·¯å¾„\n"
                    f'   4. å°è¯•æ‰‹åŠ¨è¿è¡Œ: soffice --headless --convert-to txt "{docx_path}"'
                )
                logger.warning("âš ï¸ è‡ªåŠ¨å›é€€åˆ°ä¼ ç»Ÿå‘½ä»¤è¡Œæ–¹å¼...")
                return self._docx_to_txt_subprocess(docx_path, dir_path)
        else:
            # ä½¿ç”¨ä¼ ç»Ÿçš„subprocessæ–¹å¼
            return self._docx_to_txt_subprocess(docx_path, dir_path)

    def _docx_to_txt_subprocess(self, docx_path: str, dir_path: str) -> str:
        """ä½¿ç”¨subprocesså°†.docxæ–‡ä»¶è½¬æ¢ä¸º.txtæ–‡ä»¶ï¼ˆä¼ ç»Ÿæ–¹å¼ï¼‰"""
        try:
            cmd = f'soffice --headless --convert-to txt "{docx_path}" --outdir "{dir_path}"'
            logger.debug(f"âš¡ æ‰§è¡Œè½¬æ¢å‘½ä»¤: {cmd}")

            process = subprocess.Popen(
                cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE
            )
            stdout, stderr = process.communicate()
            exit_code = process.returncode

            if exit_code == 0:
                logger.info(f"âœ… DOCXåˆ°TXTè½¬æ¢æˆåŠŸ - é€€å‡ºç : {exit_code}")
                if stdout:
                    logger.debug(
                        f"ğŸ“„ è½¬æ¢è¾“å‡º: {stdout.decode('utf-8', errors='replace')}"
                    )
            else:
                encoding = chardet.detect(stderr)["encoding"]
                if encoding is None:
                    encoding = "utf-8"
                error_msg = stderr.decode(encoding, errors="replace")
                logger.error(
                    f"âŒ DOCXåˆ°TXTè½¬æ¢å¤±è´¥ - é€€å‡ºç : {exit_code}, é”™è¯¯ä¿¡æ¯: {error_msg}"
                )
                raise Exception(
                    f"Error Output (detected encoding: {encoding}): {error_msg}"
                )

            fname = str(Path(docx_path).stem)
            txt_path = os.path.join(dir_path, f"{fname}.txt")

            if not os.path.exists(txt_path):
                logger.error(f"âŒ è½¬æ¢åçš„TXTæ–‡ä»¶ä¸å­˜åœ¨: {txt_path}")
                raise Exception(f"æ–‡ä»¶è½¬æ¢å¤±è´¥ {docx_path} ==> {txt_path}")
            else:
                logger.info(f"ğŸ‰ TXTæ–‡ä»¶è½¬æ¢æˆåŠŸï¼Œæ–‡ä»¶è·¯å¾„: {txt_path}")
                return txt_path

        except subprocess.SubprocessError as e:
            logger.error(f"ğŸ’¥ subprocessæ‰§è¡Œå¤±è´¥: {str(e)}")
            raise Exception(f"æ‰§è¡Œè½¬æ¢å‘½ä»¤æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        except Exception as e:
            logger.error(f"ğŸ’¥ DOCXåˆ°TXTè½¬æ¢è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}")
            raise

    def read_txt_file(self, txt_path: str) -> str:
        """è¯»å–txtæ–‡ä»¶å†…å®¹"""
        logger.info(f"ğŸ“– å¼€å§‹è¯»å–TXTæ–‡ä»¶: {txt_path}")

        try:
            # æ£€æµ‹æ–‡ä»¶ç¼–ç 
            with open(txt_path, "rb") as f:
                raw_data = f.read()
                encoding = chardet.detect(raw_data)["encoding"]
                if encoding is None:
                    encoding = "utf-8"
                logger.debug(f"ğŸ” æ£€æµ‹åˆ°æ–‡ä»¶ç¼–ç : {encoding}")

            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(txt_path, "r", encoding=encoding, errors="replace") as f:
                content = f.read()

            logger.info(f"ğŸ“„ TXTæ–‡ä»¶è¯»å–å®Œæˆ - å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
            logger.debug(f"ğŸ‘€ å‰100å­—ç¬¦é¢„è§ˆ: {content[:100]}...")

            return content

        except FileNotFoundError as e:
            logger.error(f"ğŸš« TXTæ–‡ä»¶æœªæ‰¾åˆ°: {str(e)}")
            raise Exception(f"æ–‡ä»¶æœªæ‰¾åˆ°: {txt_path}")
        except Exception as e:
            logger.error(f"ğŸ’¥ è¯»å–TXTæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            raise

    def extract_all_content(self, docx_path: str) -> str:
        """
        ç»¼åˆæå–DOCXæ–‡ä»¶çš„æ‰€æœ‰å†…å®¹
        æ”¯æŒå¤šç§DOCXå†…éƒ¨æ ¼å¼å’Œå­˜å‚¨æ–¹å¼
        """
        logger.info(f"ğŸ” å¼€å§‹ç»¼åˆå†…å®¹æå–: {docx_path}")

        all_content = []

        try:
            with zipfile.ZipFile(docx_path, "r") as docx:
                # 1. æ£€æŸ¥å¹¶æå–altChunkå†…å®¹ (HTML/MHTåµŒå…¥)
                altchunk_content = self._extract_altchunk_content_internal(docx)
                if altchunk_content:
                    all_content.append(("altChunk", altchunk_content))

                # 2. æå–æ ‡å‡†document.xmlå†…å®¹
                standard_content = self._extract_standard_document_content(docx)
                if standard_content:
                    all_content.append(("standard", standard_content))

                # 3. æå–åµŒå…¥å¯¹è±¡å†…å®¹ (embeddings)
                embedded_content = self._extract_embedded_objects(docx)
                if embedded_content:
                    all_content.append(("embedded", embedded_content))

                # 4. æå–å¤´éƒ¨å’Œè„šéƒ¨å†…å®¹
                header_footer_content = self._extract_headers_footers(docx)
                if header_footer_content:
                    all_content.append(("header_footer", header_footer_content))

                # 5. æå–æ³¨é‡Šå’Œæ‰¹æ³¨
                comments_content = self._extract_comments(docx)
                if comments_content:
                    all_content.append(("comments", comments_content))

                # 6. æå–æ–‡æœ¬æ¡†å’Œå›¾å½¢å¯¹è±¡ä¸­çš„æ–‡æœ¬
                textbox_content = self._extract_textbox_content(docx)
                if textbox_content:
                    all_content.append(("textboxes", textbox_content))

        except Exception as e:
            logger.error(f"ğŸ’¥ ç»¼åˆå†…å®¹æå–å¤±è´¥: {str(e)}")
            return ""

        # åˆå¹¶æ‰€æœ‰å†…å®¹
        if all_content:
            combined_content = self._combine_extracted_content(all_content)
            logger.info(f"âœ… ç»¼åˆæå–å®Œæˆï¼Œæ€»å†…å®¹é•¿åº¦: {len(combined_content)} å­—ç¬¦")
            logger.debug(f"ğŸ“Š æå–åˆ°çš„å†…å®¹ç±»å‹: {[item[0] for item in all_content]}")
            return combined_content

        return ""

    def _extract_altchunk_content_internal(self, docx_zip: zipfile.ZipFile) -> str:
        """å†…éƒ¨æ–¹æ³•ï¼šæå–altChunkå†…å®¹ï¼Œä¼˜å…ˆä½¿ç”¨MHTæ–¹å¼"""
        try:
            # æ£€æŸ¥document.xmlä¸­çš„altChunkå¼•ç”¨
            if "word/document.xml" in docx_zip.namelist():
                doc_xml = docx_zip.read("word/document.xml").decode(
                    "utf-8", errors="replace"
                )
                if "altChunk" in doc_xml:
                    logger.info("ğŸ” æ£€æµ‹åˆ°altChunkæ ¼å¼")

                    # ä¼˜å…ˆæŸ¥æ‰¾MHTæ–‡ä»¶ï¼ˆæ›´ç®€æ´çš„å¤„ç†æ–¹å¼ï¼‰
                    mht_files = [
                        f
                        for f in docx_zip.namelist()
                        if f.endswith(".mht") and "word/" in f
                    ]
                    html_files = [
                        f
                        for f in docx_zip.namelist()
                        if f.endswith(".html") and "word/" in f
                    ]

                    # ä¼˜å…ˆå¤„ç†MHTæ–‡ä»¶
                    for filename in mht_files:
                        logger.info(f"ğŸ“„ ä¼˜å…ˆå¤„ç†MHTæ–‡ä»¶: {filename}")
                        content = docx_zip.read(filename).decode(
                            "utf-8", errors="replace"
                        )
                        return self._extract_html_from_mht(content)

                    # å¦‚æœæ²¡æœ‰MHTæ–‡ä»¶ï¼Œå†å¤„ç†HTMLæ–‡ä»¶
                    for filename in html_files:
                        logger.info(f"ğŸ“„ å¤„ç†HTMLæ–‡ä»¶: {filename}")
                        content = docx_zip.read(filename).decode(
                            "utf-8", errors="replace"
                        )
                        return self._html_to_clean_text(content)

            return ""
        except Exception as e:
            logger.error(f"ğŸ’¥ æå–altChunkå†…å®¹å¤±è´¥: {str(e)}")
            return ""

    def _extract_standard_document_content(self, docx_zip: zipfile.ZipFile) -> str:
        """æå–æ ‡å‡†document.xmlå†…å®¹ - åªæå–çº¯æ–‡æœ¬"""
        try:
            if "word/document.xml" in docx_zip.namelist():
                doc_xml = docx_zip.read("word/document.xml").decode(
                    "utf-8", errors="replace"
                )

                # è§£ç XMLå®ä½“
                doc_xml = html.unescape(doc_xml)

                # æå–æ‰€æœ‰<w:t>æ ‡ç­¾ä¸­çš„æ–‡æœ¬ï¼ˆåŒ…æ‹¬å„ç§å‘½åç©ºé—´å‰ç¼€ï¼‰
                # ä½¿ç”¨æ›´å®½æ¾çš„æ­£åˆ™è¡¨è¾¾å¼æ¥åŒ¹é…ä»»ä½•å‘½åç©ºé—´å‰ç¼€
                text_pattern = r"<[^:>]*:t[^>]*>([^<]*)</[^:>]*:t>"
                text_matches = re.findall(text_pattern, doc_xml)

                # é¢å¤–æå–å¯èƒ½å­˜åœ¨çš„æ— å‘½åç©ºé—´çš„<t>æ ‡ç­¾
                text_matches.extend(re.findall(r"<t[^>]*>([^<]*)</t>", doc_xml))

                if text_matches:
                    # æ¸…ç†å’Œç»„åˆæ–‡æœ¬
                    cleaned_texts = []
                    for text in text_matches:
                        # è§£ç XMLå®ä½“
                        text = html.unescape(text)
                        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦ï¼Œä½†ä¿ç•™å•ä¸ªç©ºæ ¼
                        text = re.sub(r"\s+", " ", text.strip())
                        if text:
                            cleaned_texts.append(text)

                    # æ™ºèƒ½è¿æ¥æ–‡æœ¬ç‰‡æ®µ
                    content = ""
                    for i, text in enumerate(cleaned_texts):
                        if i == 0:
                            content = text
                        else:
                            # å¦‚æœå‰ä¸€ä¸ªæ–‡æœ¬ç‰‡æ®µä¸æ˜¯ä»¥æ ‡ç‚¹ç»“æŸï¼Œä¸”å½“å‰æ–‡æœ¬ä¸æ˜¯ä»¥å¤§å†™å¼€å¤´ï¼Œåˆ™ä¸åŠ ç©ºæ ¼
                            prev_char = content[-1] if content else ""
                            curr_char = text[0] if text else ""

                            if (
                                prev_char in ".!?ã€‚ï¼ï¼Ÿ\n"
                                or curr_char.isupper()
                                or curr_char in "ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š"
                            ):
                                content += " " + text
                            else:
                                content += text

                    # æœ€ç»ˆæ¸…ç†
                    content = re.sub(r"\s+", " ", content)
                    content = content.strip()

                    logger.info(f"ğŸ“ ä»document.xmlæå–çº¯æ–‡æœ¬: {len(content)} å­—ç¬¦")
                    return content
            return ""
        except Exception as e:
            logger.error(f"ğŸ’¥ æå–æ ‡å‡†æ–‡æ¡£å†…å®¹å¤±è´¥: {str(e)}")
            return ""

    def _extract_embedded_objects(self, docx_zip: zipfile.ZipFile) -> str:
        """æå–åµŒå…¥å¯¹è±¡å†…å®¹"""
        try:
            embedded_content = []

            # æŸ¥æ‰¾åµŒå…¥çš„æ–‡æ¡£å¯¹è±¡
            for filename in docx_zip.namelist():
                if "word/embeddings/" in filename:
                    logger.info(f"ğŸ“ æ‰¾åˆ°åµŒå…¥å¯¹è±¡: {filename}")
                    # è¿™é‡Œå¯ä»¥æ ¹æ®æ–‡ä»¶ç±»å‹è¿›ä¸€æ­¥å¤„ç†
                    # ä¾‹å¦‚ï¼š.docx, .xlsx, .txtç­‰

            return " ".join(embedded_content) if embedded_content else ""
        except Exception as e:
            logger.error(f"ğŸ’¥ æå–åµŒå…¥å¯¹è±¡å¤±è´¥: {str(e)}")
            return ""

    def _extract_headers_footers(self, docx_zip: zipfile.ZipFile) -> str:
        """æå–é¡µçœ‰é¡µè„šå†…å®¹ - åªæå–çº¯æ–‡æœ¬"""
        try:
            header_footer_content = []

            for filename in docx_zip.namelist():
                if (
                    "word/header" in filename or "word/footer" in filename
                ) and filename.endswith(".xml"):
                    logger.debug(f"ğŸ“„ å¤„ç†é¡µçœ‰é¡µè„š: {filename}")
                    content = docx_zip.read(filename).decode("utf-8", errors="replace")

                    # è§£ç XMLå®ä½“
                    content = html.unescape(content)

                    # æå–æ–‡æœ¬å†…å®¹ - ä½¿ç”¨æ›´å®½æ¾çš„æ¨¡å¼
                    text_pattern = r"<[^:>]*:t[^>]*>([^<]*)</[^:>]*:t>"
                    text_matches = re.findall(text_pattern, content)
                    text_matches.extend(re.findall(r"<t[^>]*>([^<]*)</t>", content))

                    if text_matches:
                        # æ¸…ç†å’Œç»„åˆæ–‡æœ¬
                        cleaned_texts = []
                        for text in text_matches:
                            text = html.unescape(text)
                            text = re.sub(r"\s+", " ", text.strip())
                            if text:
                                cleaned_texts.append(text)

                        if cleaned_texts:
                            # åˆå¹¶æ–‡æœ¬ç‰‡æ®µ
                            header_footer_text = " ".join(cleaned_texts)
                            header_footer_text = re.sub(
                                r"\s+", " ", header_footer_text.strip()
                            )
                            if header_footer_text:
                                header_footer_content.append(header_footer_text)

            if header_footer_content:
                logger.info(f"ğŸ“‘ æå–é¡µçœ‰é¡µè„šçº¯æ–‡æœ¬: {len(header_footer_content)} ä¸ª")

            return "\n".join(header_footer_content) if header_footer_content else ""
        except Exception as e:
            logger.error(f"ğŸ’¥ æå–é¡µçœ‰é¡µè„šå¤±è´¥: {str(e)}")
            return ""

    def _extract_comments(self, docx_zip: zipfile.ZipFile) -> str:
        """æå–æ³¨é‡Šå’Œæ‰¹æ³¨å†…å®¹ - åªæå–çº¯æ–‡æœ¬"""
        try:
            if "word/comments.xml" in docx_zip.namelist():
                comments_xml = docx_zip.read("word/comments.xml").decode(
                    "utf-8", errors="replace"
                )

                # è§£ç XMLå®ä½“
                comments_xml = html.unescape(comments_xml)

                # æå–æ³¨é‡Šæ–‡æœ¬ - ä½¿ç”¨æ›´å®½æ¾çš„æ¨¡å¼
                text_pattern = r"<[^:>]*:t[^>]*>([^<]*)</[^:>]*:t>"
                text_matches = re.findall(text_pattern, comments_xml)
                text_matches.extend(re.findall(r"<t[^>]*>([^<]*)</t>", comments_xml))

                if text_matches:
                    # æ¸…ç†å’Œç»„åˆæ–‡æœ¬
                    cleaned_texts = []
                    for text in text_matches:
                        text = html.unescape(text)
                        text = re.sub(r"\s+", " ", text.strip())
                        if text:
                            cleaned_texts.append(text)

                    if cleaned_texts:
                        comments_text = " ".join(cleaned_texts)
                        comments_text = re.sub(r"\s+", " ", comments_text.strip())
                        logger.info(f"ğŸ’¬ æå–æ³¨é‡Šçº¯æ–‡æœ¬: {len(comments_text)} å­—ç¬¦")
                        return comments_text

            return ""
        except Exception as e:
            logger.error(f"ğŸ’¥ æå–æ³¨é‡Šå¤±è´¥: {str(e)}")
            return ""

    def _extract_textbox_content(self, docx_zip: zipfile.ZipFile) -> str:
        """æå–æ–‡æœ¬æ¡†å’Œå›¾å½¢å¯¹è±¡ä¸­çš„æ–‡æœ¬ - åªæå–çº¯æ–‡æœ¬"""
        try:
            textbox_content = []

            # æŸ¥æ‰¾å¯èƒ½åŒ…å«æ–‡æœ¬æ¡†çš„æ–‡ä»¶
            for filename in docx_zip.namelist():
                if "word/" in filename and filename.endswith(".xml"):
                    content = docx_zip.read(filename).decode("utf-8", errors="replace")

                    # è§£ç XMLå®ä½“
                    content = html.unescape(content)

                    # æŸ¥æ‰¾æ–‡æœ¬æ¡†å†…å®¹ (w:txbxContent)
                    textbox_matches = re.findall(
                        r"<[^:>]*:txbxContent[^>]*>(.*?)</[^:>]*:txbxContent>",
                        content,
                        re.DOTALL,
                    )

                    for match in textbox_matches:
                        # ä»æ–‡æœ¬æ¡†å†…å®¹ä¸­æå–æ–‡æœ¬
                        text_pattern = r"<[^:>]*:t[^>]*>([^<]*)</[^:>]*:t>"
                        text_matches = re.findall(text_pattern, match)
                        text_matches.extend(re.findall(r"<t[^>]*>([^<]*)</t>", match))

                        if text_matches:
                            # æ¸…ç†å’Œç»„åˆæ–‡æœ¬
                            cleaned_texts = []
                            for text in text_matches:
                                text = html.unescape(text)
                                text = re.sub(r"\s+", " ", text.strip())
                                if text:
                                    cleaned_texts.append(text)

                            if cleaned_texts:
                                textbox_text = " ".join(cleaned_texts)
                                textbox_text = re.sub(r"\s+", " ", textbox_text.strip())
                                if textbox_text:
                                    textbox_content.append(textbox_text)

            if textbox_content:
                logger.info(f"ğŸ“¦ æå–æ–‡æœ¬æ¡†çº¯æ–‡æœ¬: {len(textbox_content)} ä¸ª")

            return "\n".join(textbox_content) if textbox_content else ""
        except Exception as e:
            logger.error(f"ğŸ’¥ æå–æ–‡æœ¬æ¡†å†…å®¹å¤±è´¥: {str(e)}")
            return ""

    def _combine_extracted_content(self, content_list: list) -> str:
        """åˆå¹¶æå–åˆ°çš„å„ç§å†…å®¹ - è¾“å‡ºæ¸…æ™°çš„çº¯æ–‡æœ¬"""
        combined = []

        # æŒ‰é‡è¦æ€§æ’åºå†…å®¹
        priority_order = [
            "altChunk",
            "standard",
            "header_footer",
            "textboxes",
            "comments",
            "embedded",
        ]

        for content_type in priority_order:
            for item_type, content in content_list:
                if item_type == content_type and content.strip():
                    # æ¸…ç†å†…å®¹ä¸­çš„å¤šä½™ç©ºç™½
                    cleaned_content = re.sub(r"\s+", " ", content.strip())
                    cleaned_content = re.sub(r"\n\s*\n", "\n\n", cleaned_content)

                    if cleaned_content:
                        # æ ¹æ®å†…å®¹ç±»å‹æ·»åŠ ç®€å•çš„æ ‡è®°ï¼ˆä»…åœ¨æœ‰å¤šç§å†…å®¹ç±»å‹æ—¶ï¼‰
                        if len([1 for t, c in content_list if c.strip()]) > 1:
                            if item_type == "header_footer":
                                combined.append(f"[é¡µçœ‰é¡µè„š]\n{cleaned_content}")
                            elif item_type == "comments":
                                combined.append(f"[æ‰¹æ³¨]\n{cleaned_content}")
                            elif item_type == "textboxes":
                                combined.append(f"[æ–‡æœ¬æ¡†]\n{cleaned_content}")
                            else:
                                combined.append(cleaned_content)
                        else:
                            combined.append(cleaned_content)

        # æ·»åŠ å…¶ä»–æœªåˆ†ç±»çš„å†…å®¹
        for item_type, content in content_list:
            if item_type not in priority_order and content.strip():
                cleaned_content = re.sub(r"\s+", " ", content.strip())
                cleaned_content = re.sub(r"\n\s*\n", "\n\n", cleaned_content)
                if cleaned_content:
                    combined.append(cleaned_content)

        # åˆå¹¶æ‰€æœ‰å†…å®¹ï¼Œä½¿ç”¨åŒæ¢è¡Œåˆ†éš”ä¸åŒéƒ¨åˆ†
        final_content = "\n\n".join(combined) if combined else ""

        # æœ€ç»ˆæ¸…ç†ï¼šç¡®ä¿æ²¡æœ‰è¿‡å¤šçš„ç©ºè¡Œ
        final_content = re.sub(r"\n{3,}", "\n\n", final_content)
        final_content = final_content.strip()

        return final_content

    def _extract_html_from_mht(self, mht_content: str) -> str:
        """ä»MHTå†…å®¹ä¸­æå–HTMLéƒ¨åˆ†å¹¶è½¬æ¢ä¸ºç®€æ´æ–‡æœ¬"""
        try:
            # MHTæ–‡ä»¶ä½¿ç”¨MIMEæ ¼å¼ï¼Œå¯»æ‰¾HTMLéƒ¨åˆ†
            lines = mht_content.split("\n")
            in_html_section = False
            html_lines = []
            skip_headers = True

            for line in lines:
                # æ£€æµ‹HTMLéƒ¨åˆ†å¼€å§‹
                if "Content-Type: text/html" in line:
                    in_html_section = True
                    skip_headers = True
                    continue

                # åœ¨HTMLéƒ¨åˆ†ä¸­
                if in_html_section:
                    # è·³è¿‡Content-*å¤´éƒ¨
                    if (
                        skip_headers
                        and line.strip()
                        and not line.startswith("Content-")
                    ):
                        skip_headers = False

                    # ç©ºè¡Œè¡¨ç¤ºå¤´éƒ¨ç»“æŸï¼Œå†…å®¹å¼€å§‹
                    if skip_headers and not line.strip():
                        skip_headers = False
                        continue

                    # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾ä¸‹ä¸€ä¸ªMIMEéƒ¨åˆ†
                    if line.startswith("------=") and len(html_lines) > 0:
                        # HTMLéƒ¨åˆ†ç»“æŸ
                        break

                    # æ”¶é›†HTMLå†…å®¹
                    if not skip_headers:
                        html_lines.append(line)

            # åˆå¹¶æ‰€æœ‰HTMLè¡Œ
            html_content = "\n".join(html_lines)

            # è§£ç quoted-printableç¼–ç 
            if "=3D" in html_content or "=\n" in html_content:
                try:
                    import quopri

                    html_content = quopri.decodestring(html_content.encode()).decode(
                        "utf-8", errors="replace"
                    )
                    logger.info("ğŸ“§ è§£ç quoted-printableç¼–ç ")
                except Exception as e:
                    logger.warning(f"âš ï¸ quoted-printableè§£ç å¤±è´¥: {str(e)}")

            logger.debug(f"ğŸ“„ æå–çš„HTMLå†…å®¹é•¿åº¦: {len(html_content)} å­—ç¬¦")

            # è½¬æ¢ä¸ºç®€æ´æ–‡æœ¬
            return self._html_to_clean_text(html_content)

        except Exception as e:
            logger.error(f"ğŸ’¥ ä»MHTæå–HTMLå¤±è´¥: {str(e)}")
            return ""

    def _html_to_clean_text(self, html_content: str) -> str:
        """å°†HTMLå†…å®¹è½¬æ¢ä¸ºç®€æ´çš„çº¯æ–‡æœ¬ï¼Œä¸“é—¨ä¼˜åŒ–MHTå†…å®¹"""
        try:
            # é¦–å…ˆè§£ç HTMLå®ä½“
            text = html.unescape(html_content)

            # å…ˆå°è¯•æå–<body>æ ‡ç­¾å†…çš„æ‰€æœ‰å†…å®¹
            body_match = re.search(
                r"<body[^>]*>(.*?)</body>", text, re.DOTALL | re.IGNORECASE
            )
            if body_match:
                main_content = body_match.group(1)
                logger.info("ğŸ“„ æå–<body>æ ‡ç­¾å†…å®¹")
            else:
                main_content = text
                logger.info("ğŸ“„ ä½¿ç”¨å…¨éƒ¨å†…å®¹ï¼ˆæœªæ‰¾åˆ°bodyæ ‡ç­¾ï¼‰")

            # ç‰¹æ®Šå¤„ç†<pre><code>æ ‡ç­¾ï¼Œä¿æŒå…¶å†…éƒ¨çš„æ ¼å¼
            pre_code_blocks = []

            def preserve_pre_code(match):
                idx = len(pre_code_blocks)
                pre_code_blocks.append(match.group(1))
                return f"__PRE_CODE_{idx}__"

            main_content = re.sub(
                r"<pre[^>]*>\s*<code[^>]*>(.*?)</code>\s*</pre>",
                preserve_pre_code,
                main_content,
                flags=re.DOTALL | re.IGNORECASE,
            )

            # å¤„ç†å…¶ä»–HTMLç»“æ„
            # 1. å…ˆè½¬æ¢éœ€è¦ä¿ç•™æ¢è¡Œçš„æ ‡ç­¾
            main_content = re.sub(r"<br\s*/?>", "\n", main_content, flags=re.IGNORECASE)
            main_content = re.sub(r"</p>", "\n", main_content, flags=re.IGNORECASE)
            main_content = re.sub(r"<p[^>]*>", "", main_content, flags=re.IGNORECASE)
            main_content = re.sub(r"</div>", "\n", main_content, flags=re.IGNORECASE)
            main_content = re.sub(r"<div[^>]*>", "", main_content, flags=re.IGNORECASE)
            main_content = re.sub(
                r"</h[1-6]>", "\n\n", main_content, flags=re.IGNORECASE
            )
            main_content = re.sub(
                r"<h[1-6][^>]*>", "", main_content, flags=re.IGNORECASE
            )
            main_content = re.sub(r"</li>", "\n", main_content, flags=re.IGNORECASE)
            main_content = re.sub(r"<li[^>]*>", "â€¢ ", main_content, flags=re.IGNORECASE)
            main_content = re.sub(r"</tr>", "\n", main_content, flags=re.IGNORECASE)
            main_content = re.sub(r"</td>", " | ", main_content, flags=re.IGNORECASE)
            main_content = re.sub(r"</th>", " | ", main_content, flags=re.IGNORECASE)

            # 2. ç§»é™¤styleå’Œscriptæ ‡ç­¾åŠå…¶å†…å®¹
            main_content = re.sub(
                r"<style[^>]*>.*?</style>",
                "",
                main_content,
                flags=re.DOTALL | re.IGNORECASE,
            )
            main_content = re.sub(
                r"<script[^>]*>.*?</script>",
                "",
                main_content,
                flags=re.DOTALL | re.IGNORECASE,
            )

            # 3. ç§»é™¤æ‰€æœ‰å‰©ä½™çš„HTMLæ ‡ç­¾
            main_content = re.sub(r"<[^>]+>", "", main_content)

            # 4. è§£ç HTMLå®ä½“ï¼ˆç¬¬äºŒæ¬¡ï¼Œç¡®ä¿å®Œå…¨è§£ç ï¼‰
            main_content = html.unescape(main_content)

            # 5. æ¢å¤<pre><code>å—çš„å†…å®¹
            for idx, pre_code_content in enumerate(pre_code_blocks):
                # æ¸…ç†pre_codeå†…å®¹
                cleaned_pre_code = html.unescape(pre_code_content)
                main_content = main_content.replace(
                    f"__PRE_CODE_{idx}__", cleaned_pre_code
                )

            # 6. æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦ï¼Œä½†ä¿æŒæ®µè½ç»“æ„
            lines = main_content.split("\n")
            cleaned_lines = []

            for line in lines:
                # æ¸…ç†æ¯è¡Œçš„é¦–å°¾ç©ºæ ¼
                line = line.strip()
                # ä¿ç•™éç©ºè¡Œ
                if line:
                    # æ¸…ç†è¡Œå†…å¤šä½™ç©ºæ ¼
                    line = re.sub(r"[ \t]+", " ", line)
                    # æ¸…ç†è¡¨æ ¼åˆ†éš”ç¬¦å¤šä½™çš„ç©ºæ ¼
                    line = re.sub(r"\s*\|\s*", " | ", line)
                    cleaned_lines.append(line)
                else:
                    # ä¿ç•™ç©ºè¡Œä½œä¸ºæ®µè½åˆ†éš”
                    if cleaned_lines and cleaned_lines[-1] != "":
                        cleaned_lines.append("")

            # 7. åˆå¹¶æ¸…ç†åçš„è¡Œ
            main_content = "\n".join(cleaned_lines)

            # 8. æœ€ç»ˆæ¸…ç†ï¼šç§»é™¤å¤šä½™çš„ç©ºè¡Œ
            main_content = re.sub(r"\n{3,}", "\n\n", main_content)
            main_content = main_content.strip()

            logger.info(f"ğŸ“ HTMLå†…å®¹è½¬æ¢ä¸ºç®€æ´æ–‡æœ¬: {len(main_content)} å­—ç¬¦")

            return main_content

        except Exception as e:
            logger.error(f"ğŸ’¥ HTMLè½¬ç®€æ´æ–‡æœ¬å¤±è´¥: {str(e)}")
            # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œè¿”å›åŸå§‹æ–‡æœ¬çš„åŸºç¡€æ¸…ç†ç‰ˆæœ¬
            return re.sub(r"<[^>]+>", "", html_content)

    def _html_to_text(self, html_content: str) -> str:
        """å°†HTMLå†…å®¹è½¬æ¢ä¸ºçº¯æ–‡æœ¬ï¼ˆä¿ç•™æ­¤æ–¹æ³•ç”¨äºå…¶ä»–HTMLå†…å®¹ï¼‰"""
        # å¯¹äºéMHTçš„HTMLå†…å®¹ï¼Œä½¿ç”¨è¿™ä¸ªæ›´é€šç”¨çš„æ–¹æ³•
        return self._html_to_clean_text(html_content)

    def extract_altchunk_content(self, docx_path: str) -> Optional[str]:
        """
        æå–åŒ…å«altChunkçš„DOCXæ–‡ä»¶å†…å®¹ (ä¿æŒå‘åå…¼å®¹)
        """
        try:
            with zipfile.ZipFile(docx_path, "r") as docx:
                return self._extract_altchunk_content_internal(docx)
        except Exception as e:
            logger.error(f"ğŸ’¥ æå–altChunkå†…å®¹å¤±è´¥: {str(e)}")
            return None

    def read_docx_file(self, docx_path: str) -> str:
        """è¯»å–docxæ–‡ä»¶å¹¶è½¬æ¢ä¸ºæ–‡æœ¬"""
        logger.info(f"ğŸ“– å¼€å§‹è¯»å–DOCXæ–‡ä»¶ - æ–‡ä»¶: {docx_path}")

        try:
            # é¦–å…ˆå°è¯•ç»¼åˆæå–æ‰€æœ‰å†…å®¹
            comprehensive_content = self.extract_all_content(docx_path)
            if comprehensive_content and comprehensive_content.strip():
                logger.info(
                    f"âœ¨ ä½¿ç”¨ç»¼åˆæå–æ–¹å¼æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(comprehensive_content)} å­—ç¬¦"
                )
                return comprehensive_content

            # å¦‚æœç»¼åˆæå–å¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿè½¬æ¢æ–¹å¼
            logger.info("ğŸ”„ ç»¼åˆæå–å¤±è´¥æˆ–å†…å®¹ä¸ºç©ºï¼Œä½¿ç”¨ä¼ ç»Ÿè½¬æ¢æ–¹å¼")

            with tempfile.TemporaryDirectory() as temp_path:
                logger.debug(f"ğŸ“ åˆ›å»ºä¸´æ—¶ç›®å½•: {temp_path}")

                temp_dir = Path(temp_path)

                file_path = temp_dir / "tmp.docx"
                shutil.copy(docx_path, file_path)
                logger.debug(f"ğŸ“‹ å¤åˆ¶æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•: {docx_path} -> {file_path}")

                # è½¬æ¢DOCXä¸ºTXT
                txt_file_path = self.docx_to_txt(str(file_path), str(temp_path))
                logger.info(f"ğŸ¯ DOCXè½¬TXTå®Œæˆ: {txt_file_path}")

                # è¯»å–TXTæ–‡ä»¶å†…å®¹
                content = self.read_txt_file(txt_file_path)
                logger.info(f"âœ¨ TXTæ–‡ä»¶å†…å®¹è¯»å–å®Œæˆï¼Œå†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")

                return content

        except FileNotFoundError as e:
            logger.error(f"ğŸš« æ–‡ä»¶æœªæ‰¾åˆ°: {str(e)}")
            raise Exception(f"æ–‡ä»¶æœªæ‰¾åˆ°: {docx_path}")
        except PermissionError as e:
            logger.error(f"ğŸ”’ æ–‡ä»¶æƒé™é”™è¯¯: {str(e)}")
            raise Exception(f"æ— æƒé™è®¿é—®æ–‡ä»¶: {docx_path}")
        except Exception as e:
            logger.error(f"ğŸ’¥ è¯»å–DOCXæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            raise

    def parse(self, file_path: str):
        """è§£æDOCXæ–‡ä»¶"""
        logger.info(f"ğŸ¬ å¼€å§‹è§£æDOCXæ–‡ä»¶: {file_path}")

        try:
            # éªŒè¯æ–‡ä»¶å­˜åœ¨
            if not os.path.exists(file_path):
                logger.error(f"ğŸš« æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

            # éªŒè¯æ–‡ä»¶æ‰©å±•å
            if not file_path.lower().endswith(".docx"):
                logger.warning(f"âš ï¸ æ–‡ä»¶æ‰©å±•åä¸æ˜¯.docx: {file_path}")

            # éªŒè¯æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(file_path)
            logger.info(f"ğŸ“ æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")

            if file_size == 0:
                logger.warning(f"âš ï¸ æ–‡ä»¶å¤§å°ä¸º0å­—èŠ‚: {file_path}")

            # ğŸ·ï¸ æå–æ–‡ä»¶æ‰©å±•å
            extension = self.get_file_extension(file_path)
            logger.debug(f"ğŸ·ï¸ æå–æ–‡ä»¶æ‰©å±•å: {extension}")
            # 1) å¤„ç†å¼€å§‹ï¼šç”Ÿæˆ DATA_PROCESSING äº‹ä»¶
            lc_start = self.generate_lifecycle(
                source_file=file_path,
                domain=self.domain,
                life_type=LifeType.DATA_PROCESSING,
                usage_purpose="Parsing",
            )
            # ä½¿ç”¨sofficeè½¬æ¢ä¸ºtxtåè¯»å–å†…å®¹
            logger.info("ğŸ“ ä½¿ç”¨sofficeè½¬æ¢DOCXä¸ºTXTå¹¶è¯»å–å†…å®¹")
            content = self.read_docx_file(docx_path=file_path)

            # æ ¹æ®to_markdownå‚æ•°å†³å®šæ˜¯å¦ä¿æŒåŸæ ¼å¼è¿˜æ˜¯å¤„ç†ä¸ºmarkdownæ ¼å¼
            if self.to_markdown:
                # ç®€å•çš„æ–‡æœ¬åˆ°markdownè½¬æ¢ï¼ˆä¿æŒæ®µè½ç»“æ„ï¼‰
                mk_content = self.format_as_markdown(content)
                logger.info("ğŸ¨ å†…å®¹å·²æ ¼å¼åŒ–ä¸ºmarkdownæ ¼å¼")
            else:
                mk_content = content
                logger.info("ğŸ“ ä¿æŒåŸå§‹æ–‡æœ¬æ ¼å¼")

            logger.info(f"ğŸŠ æ–‡ä»¶å†…å®¹è§£æå®Œæˆï¼Œæœ€ç»ˆå†…å®¹é•¿åº¦: {len(mk_content)} å­—ç¬¦")

            # æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºç©º
            if not mk_content.strip():
                logger.warning(f"âš ï¸ è§£æå‡ºçš„å†…å®¹ä¸ºç©º: {file_path}")

            # 2) å¤„ç†ç»“æŸï¼šæ ¹æ®å†…å®¹æ˜¯å¦éç©ºç”Ÿæˆ DATA_PROCESSED æˆ– DATA_PROCESS_FAILED äº‹ä»¶
            lc_end = self.generate_lifecycle(
                source_file=file_path,
                domain=self.domain,
                life_type=(
                    LifeType.DATA_PROCESSED
                    if mk_content.strip()
                    else LifeType.DATA_PROCESS_FAILED
                ),
                usage_purpose="Parsing",
            )
            logger.debug("âš™ï¸ ç”Ÿæˆç”Ÿå‘½å‘¨æœŸäº‹ä»¶å®Œæˆ")

            # 3) å°è£…è¾“å‡ºå¹¶æ·»åŠ ç”Ÿå‘½å‘¨æœŸ
            output_vo = MarkdownOutputVo(extension, mk_content)
            output_vo.add_lifecycle(lc_start)
            output_vo.add_lifecycle(lc_end)

            result = output_vo.to_dict()
            logger.info(f"ğŸ† DOCXæ–‡ä»¶è§£æå®Œæˆ: {file_path}")
            logger.debug(f"ğŸ”‘ è¿”å›ç»“æœé”®: {list(result.keys())}")

            return result

        except FileNotFoundError as e:
            logger.error(f"ğŸš« æ–‡ä»¶ä¸å­˜åœ¨é”™è¯¯: {str(e)}")
            raise
        except PermissionError as e:
            logger.error(f"ğŸ”’ æ–‡ä»¶æƒé™é”™è¯¯: {str(e)}")
            raise Exception(f"æ— æƒé™è®¿é—®æ–‡ä»¶: {file_path}")
        except Exception as e:
            logger.error(
                f"ğŸ’€ è§£æDOCXæ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯ç±»å‹: {type(e).__name__}, é”™è¯¯ä¿¡æ¯: {str(e)}"
            )
            raise

    def format_as_markdown(self, content: str) -> str:
        """å°†çº¯æ–‡æœ¬æ ¼å¼åŒ–ä¸ºç®€å•çš„markdownæ ¼å¼"""
        if not content.strip():
            return content

        lines = content.split("\n")
        formatted_lines = []

        for line in lines:
            line = line.strip()
            if not line:
                formatted_lines.append("")
                continue

            # ç®€å•çš„markdownæ ¼å¼åŒ–è§„åˆ™
            # å¯ä»¥æ ¹æ®éœ€è¦æ‰©å±•æ›´å¤šè§„åˆ™
            formatted_lines.append(line)

        return "\n".join(formatted_lines)
