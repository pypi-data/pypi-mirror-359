import multiprocessing
import os
import time
import warnings
from multiprocessing import Queue

import pandas as pd
from loguru import logger

from datamax.parser.base import BaseLife, MarkdownOutputVo
from datamax.utils.lifecycle_types import LifeType

warnings.filterwarnings("ignore")


class XlsxParser(BaseLife):
    """XLSXè§£æå™¨ - ä½¿ç”¨pandasè¯»å–å¹¶è½¬æ¢ä¸ºmarkdownï¼Œæ”¯æŒå¤šè¿›ç¨‹å¤„ç†"""

    def __init__(self, file_path, domain: str = "Technology"):
        super().__init__(domain=domain)
        self.file_path = file_path
        logger.info(f"ğŸš€ XlsxParseråˆå§‹åŒ–å®Œæˆ - æ–‡ä»¶è·¯å¾„: {file_path}")

    def _parse_with_pandas(self, file_path: str) -> str:
        """ä½¿ç”¨pandasè¯»å–Excelå¹¶è½¬æ¢ä¸ºmarkdown"""
        logger.info(f"ğŸ¼ å¼€å§‹ä½¿ç”¨pandasè¯»å–Excelæ–‡ä»¶: {file_path}")

        try:
            # éªŒè¯æ–‡ä»¶å­˜åœ¨
            if not os.path.exists(file_path):
                logger.error(f"ğŸš« Excelæ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

            # éªŒè¯æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(file_path)
            logger.info(f"ğŸ“ æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")

            if file_size == 0:
                logger.warning(f"âš ï¸ æ–‡ä»¶å¤§å°ä¸º0å­—èŠ‚: {file_path}")
                return "*æ–‡ä»¶ä¸ºç©º*"

            # ä½¿ç”¨pandasè¯»å–Excelæ–‡ä»¶
            logger.debug("ğŸ“Š æ­£åœ¨è¯»å–Excelæ•°æ®...")
            df = pd.read_excel(file_path, sheet_name=None)  # è¯»å–æ‰€æœ‰å·¥ä½œè¡¨

            markdown_content = ""

            if isinstance(df, dict):
                # å¤šä¸ªå·¥ä½œè¡¨
                logger.info(f"ğŸ“‘ æ£€æµ‹åˆ°å¤šä¸ªå·¥ä½œè¡¨ï¼Œå…± {len(df)} ä¸ª")
                for sheet_name, sheet_df in df.items():
                    logger.debug(f"ğŸ“‹ å¤„ç†å·¥ä½œè¡¨: {sheet_name}, å½¢çŠ¶: {sheet_df.shape}")
                    markdown_content += f"## å·¥ä½œè¡¨: {sheet_name}\n\n"

                    if not sheet_df.empty:
                        # æ¸…ç†æ•°æ®ï¼šç§»é™¤å®Œå…¨ä¸ºç©ºçš„è¡Œå’Œåˆ—
                        sheet_df = sheet_df.dropna(how="all").dropna(axis=1, how="all")

                        if not sheet_df.empty:
                            sheet_markdown = sheet_df.to_markdown(index=False)
                            markdown_content += sheet_markdown + "\n\n"
                            logger.debug(
                                f"âœ… å·¥ä½œè¡¨ {sheet_name} è½¬æ¢å®Œæˆï¼Œæœ‰æ•ˆæ•°æ®å½¢çŠ¶: {sheet_df.shape}"
                            )
                        else:
                            markdown_content += "*è¯¥å·¥ä½œè¡¨æ— æœ‰æ•ˆæ•°æ®*\n\n"
                            logger.warning(f"âš ï¸ å·¥ä½œè¡¨ {sheet_name} æ¸…ç†åæ— æœ‰æ•ˆæ•°æ®")
                    else:
                        markdown_content += "*è¯¥å·¥ä½œè¡¨ä¸ºç©º*\n\n"
                        logger.warning(f"âš ï¸ å·¥ä½œè¡¨ {sheet_name} ä¸ºç©º")
            else:
                # å•ä¸ªå·¥ä½œè¡¨
                logger.info(f"ğŸ“„ å•ä¸ªå·¥ä½œè¡¨ï¼Œå½¢çŠ¶: {df.shape}")
                if not df.empty:
                    # æ¸…ç†æ•°æ®ï¼šç§»é™¤å®Œå…¨ä¸ºç©ºçš„è¡Œå’Œåˆ—
                    df = df.dropna(how="all").dropna(axis=1, how="all")

                    if not df.empty:
                        markdown_content = df.to_markdown(index=False)
                        logger.info(f"âœ… å·¥ä½œè¡¨è½¬æ¢å®Œæˆï¼Œæœ‰æ•ˆæ•°æ®å½¢çŠ¶: {df.shape}")
                    else:
                        markdown_content = "*å·¥ä½œè¡¨æ— æœ‰æ•ˆæ•°æ®*"
                        logger.warning("âš ï¸ å·¥ä½œè¡¨æ¸…ç†åæ— æœ‰æ•ˆæ•°æ®")
                else:
                    markdown_content = "*å·¥ä½œè¡¨ä¸ºç©º*"
                    logger.warning("âš ï¸ å·¥ä½œè¡¨ä¸ºç©º")

            logger.info(
                f"ğŸŠ pandasè½¬æ¢å®Œæˆï¼Œmarkdownå†…å®¹é•¿åº¦: {len(markdown_content)} å­—ç¬¦"
            )
            logger.debug(f"ğŸ‘€ å‰200å­—ç¬¦é¢„è§ˆ: {markdown_content[:200]}...")

            return markdown_content

        except FileNotFoundError as e:
            logger.error(f"ğŸš« æ–‡ä»¶æœªæ‰¾åˆ°: {str(e)}")
            raise
        except PermissionError as e:
            logger.error(f"ğŸ”’ æ–‡ä»¶æƒé™é”™è¯¯: {str(e)}")
            raise Exception(f"æ— æƒé™è®¿é—®æ–‡ä»¶: {file_path}")
        except pd.errors.EmptyDataError as e:
            logger.error(f"ğŸ“­ Excelæ–‡ä»¶ä¸ºç©º: {str(e)}")
            raise Exception(f"Excelæ–‡ä»¶ä¸ºç©ºæˆ–æ— æ³•è¯»å–: {file_path}")
        except Exception as e:
            logger.error(f"ğŸ’¥ pandasè¯»å–Excelå¤±è´¥: {str(e)}")
            raise

    def _parse(self, file_path: str, result_queue: Queue) -> dict:
        """è§£æExcelæ–‡ä»¶çš„æ ¸å¿ƒæ–¹æ³•"""
        logger.info(f"ğŸ¬ å¼€å§‹è§£æExcelæ–‡ä»¶: {file_path}")

        # â€”â€” ç”Ÿå‘½å‘¨æœŸï¼šå¼€å§‹å¤„ç† â€”â€” #
        lc_start = self.generate_lifecycle(
            source_file=file_path,
            domain=self.domain,
            usage_purpose="Documentation",
            life_type=LifeType.DATA_PROCESSING,
        )
        logger.debug("âš™ï¸ DATA_PROCESSING ç”Ÿå‘½å‘¨æœŸå·²ç”Ÿæˆ")

        try:
            # ä½¿ç”¨pandasè§£æExcel
            logger.info("ğŸ¼ ä½¿ç”¨pandasæ¨¡å¼è§£æExcel")
            mk_content = self._parse_with_pandas(file_path)

            # æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºç©º
            if not mk_content.strip():
                logger.warning(f"âš ï¸ è§£æå‡ºçš„å†…å®¹ä¸ºç©º: {file_path}")
                mk_content = "*æ— æ³•è§£ææ–‡ä»¶å†…å®¹*"

            logger.info(f"ğŸŠ æ–‡ä»¶å†…å®¹è§£æå®Œæˆï¼Œæœ€ç»ˆå†…å®¹é•¿åº¦: {len(mk_content)} å­—ç¬¦")

            # â€”â€” ç”Ÿå‘½å‘¨æœŸï¼šå¤„ç†å®Œæˆ â€”â€” #
            lc_end = self.generate_lifecycle(
                source_file=file_path,
                domain=self.domain,
                usage_purpose="Documentation",
                life_type=LifeType.DATA_PROCESSED,
            )
            logger.debug("âš™ï¸ DATA_PROCESSED ç”Ÿå‘½å‘¨æœŸå·²ç”Ÿæˆ")

            # åˆ›å»ºè¾“å‡ºå¯¹è±¡å¹¶æ·»åŠ ä¸¤ä¸ªç”Ÿå‘½å‘¨æœŸ
            extension = self.get_file_extension(file_path)
            output_vo = MarkdownOutputVo(extension, mk_content)
            output_vo.add_lifecycle(lc_start)
            output_vo.add_lifecycle(lc_end)

            result = output_vo.to_dict()
            result_queue.put(result)
            logger.info(f"ğŸ† Excelæ–‡ä»¶è§£æå®Œæˆ: {file_path}")
            logger.debug(f"ğŸ”‘ è¿”å›ç»“æœé”®: {list(result.keys())}")

            time.sleep(0.5)  # ç»™é˜Ÿåˆ—ä¸€ç‚¹æ—¶é—´
            return result

        except Exception as e:
            # â€”â€” ç”Ÿå‘½å‘¨æœŸï¼šå¤„ç†å¤±è´¥ â€”â€” #
            try:
                lc_fail = self.generate_lifecycle(
                    source_file=file_path,
                    domain=self.domain,
                    usage_purpose="Documentation",
                    life_type=LifeType.DATA_PROCESS_FAILED,
                )
                logger.debug("âš™ï¸ DATA_PROCESS_FAILED ç”Ÿå‘½å‘¨æœŸå·²ç”Ÿæˆ")
                # å¦‚æœéœ€è¦ï¼Œä¹Ÿå¯ä»¥æŠŠå®ƒåŠ åˆ° error_result é‡Œï¼š
                # error_result = {"error": str(e), "file_path": file_path, "lifecycle":[lc_fail.to_dict()]}
            except Exception:
                pass

            # â€”â€” ç”Ÿå‘½å‘¨æœŸï¼šå¤„ç†å¤±è´¥ â€”â€” #
            try:
                lc_fail = self.generate_lifecycle(
                    source_file=file_path,
                    domain=self.domain,
                    usage_purpose="Documentation",
                    life_type=LifeType.DATA_PROCESS_FAILED,
                )
                logger.debug("âš™ï¸ DATA_PROCESS_FAILED ç”Ÿå‘½å‘¨æœŸå·²ç”Ÿæˆ")
            except Exception:
                pass

            logger.error(f"ğŸ’€ è§£æExcelæ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {str(e)}")
            # å°†é”™è¯¯ä¹Ÿæ”¾å…¥é˜Ÿåˆ—
            error_result = {
                "error": str(e),
                "file_path": file_path,
                # é¢å¤–æŠŠå¤±è´¥çš„ lifecycle ä¹Ÿä¸€èµ·è¿”å›ï¼Œæµ‹è¯•ä¸­å¯é€‰æ ¡éªŒ
                "lifecycle": [lc_fail.to_dict()] if "lc_fail" in locals() else [],
            }
            result_queue.put(error_result)
            raise

    def parse(self, file_path: str) -> dict:
        """è§£æExcelæ–‡ä»¶ - æ”¯æŒå¤šè¿›ç¨‹å’Œè¶…æ—¶æ§åˆ¶"""
        logger.info(f"ğŸš€ å¯åŠ¨Excelè§£æè¿›ç¨‹ - æ–‡ä»¶: {file_path}")

        try:
            # éªŒè¯æ–‡ä»¶å­˜åœ¨
            if not os.path.exists(file_path):
                logger.error(f"ğŸš« æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

            # éªŒè¯æ–‡ä»¶æ‰©å±•å
            if not file_path.lower().endswith((".xlsx", ".xls")):
                logger.warning(f"âš ï¸ æ–‡ä»¶æ‰©å±•åä¸æ˜¯Excelæ ¼å¼: {file_path}")

            result_queue = Queue()
            process = multiprocessing.Process(
                target=self._parse, args=(file_path, result_queue)
            )
            process.start()
            logger.debug(f"âš¡ å¯åŠ¨å­è¿›ç¨‹ï¼ŒPID: {process.pid}")

        except Exception as e:
            logger.error(
                f"ğŸ’€ Excelè§£æå¤±è´¥: {file_path}, é”™è¯¯ç±»å‹: {type(e).__name__}, é”™è¯¯ä¿¡æ¯: {str(e)}"
            )
            raise
