{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ed30dc9-056e-4da8-a4c3-4063202a2fb0",
   "metadata": {},
   "source": [
    "# TPZ: Trees for Photo-Z's\n",
    "\n",
    "Author: Sam Schmidt <br>\n",
    "Last successfully run: March 24, 2025 <br>\n",
    "\n",
    "TPZ is one of the codes implemented in the MLZ (Machine Learning PhotoZ) package by Matias Carraso-Kind, some documentation for the algorithm is included in Matias' website for the package:\n",
    "http://matias-ck.com/mlz/\n",
    "However, the code is no longer actively maintained, and Matias' original code was written for Python 2.  This code is based on a fork by Erfan Nourbakhsh for a DESC project which is itself a fork that updated the code to be python3 compatible):\n",
    "Erfan's fork: https://github.com/enourbakhsh/MLZ\n",
    "\n",
    "This RAIL-wrapped version of the code does not include the SOM-based MLZ code, nor the implementation of BPZ, it only includes an implementation of the decision-tree-based code.  \n",
    "\n",
    "Initially, we have only implemented the regression-tree versionof the code, though the classification-tree method may be re-implemented at a future time.  Furthermore, the original code had options for out-of-bag (oob) error estimates and variational importance sampling, those have not been included initially, but will hopefully be added as options in the near future.\n",
    "\n",
    "For a quick summary of how the code operates, given a set of galaxy observables (usually magnitudes, and optional uncertainties), TPZ builds a set of decision trees where it splits a training set by some of the included parameters in a way that best differentiates the parameter of interest (in our case redshift).  It then performs repeated splits on parameters in each leaf branch of each tree that best differentiate the remaining data, thus building up a decision tree.  It creates multiple trees in two ways: 1) by creating N bootstrap realizations of the initial training set; 2) if uncertainties are provided (e.g. magnitude uncertainties) it creates M alternative training set realizations by adding Gaussian scatter to the training quantities galaxy-by-galaxy (Note: if an error is not supplied it assumes a very small error of 0.00005 for the Gaussian sigma).  Thus, it trains up a total of N x M total trees for its model, e.g. if you tell TPZ that you want 5 random realizations and 4 trees it will create 5 random datasets and bootstrap those 4 times to train a total of 20 trees.  \n",
    "To create a photo-z estimate, it then lets each test galaxy plinko down through the decision tree, and adds the redshifts of the training galaxies in the terminal leaf node to a histogram, building up the final PDF by looking at all N x M trees.\n",
    "\n",
    "## Running TPZ\n",
    "we'll start with a few basic imports, including the import of `TPZliteInformer` from RAIL, along with the `RAILDIR` path that will help us grab some basic test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22762394-6412-4db9-a2b1-bfa4b9bdfe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tables_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ac95af-256a-4a0b-bb89-ebe7a6a99cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rail.estimation.algos.tpz_lite import TPZliteInformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d18c2a4-38d8-4222-928e-8dd622a9d006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rail.utils.path_utils import RAILDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab87691-a368-443d-86d3-6292f8161e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAILDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5565338f-4d71-466b-a1a7-979f6dad3df8",
   "metadata": {},
   "source": [
    "A small set of ~10,000 training galaxies is include with the base rail repo, below we will point to that data and read it into the Data Store, where it is stored as an ordered dictionary of magnitudes, magnitude uncertainties, and redeshift (under a hdf5 group called `photometry`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d5174d-97c3-400e-99dd-720419b76e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = os.path.join(RAILDIR,\"rail/examples_data/testdata/test_dc2_training_9816.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3fd8e7-b48a-485b-9047-c2d81b15c9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rail\n",
    "import qp\n",
    "from rail.core.data import TableHandle\n",
    "from rail.core.stage import RailStage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b16c72-90d2-493f-9422-b99fb88e0a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = RailStage.data_store\n",
    "DS.__class__.allow_overwrite = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2e549d-cf56-429b-9a4d-b49a05e86b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = DS.read_file(\"training_data\", TableHandle, datafile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c29a5d-e44b-4139-8965-3150d6d58285",
   "metadata": {},
   "source": [
    "Next, we will create a dictionary with the configuration parameters that controld TPZ's behavior that we will feed to the `make_stage` method of `TPZliteInformer` to set up our training of the trees.  There are several configuration parameters available for TPZliteInformer.  A number of these are \"shared parameters\", including:\n",
    "`zmin`, `zmax`, `nzbins`, `mag_limits`, and `redshift_col`.\n",
    "\n",
    "Two shared parameter play an imporant role in TPZ:<br>\n",
    "- `bands` is a list that contains the names of the columns that will be used as features in the tree.  While we use the name `bands` to be consistent with other parts of RAIL, note that other meaningful quantities, e.g. size, shape, concentration, surface brightness, etc..., could also be used.\n",
    "-  `err_bands` contains a list of the column names that contain the 1-sigma uncertainties for the quantities listed in `bands`.  As TPZ creates mock data by sampling from the uncertainty distributions when making its forest of trees, these quantities are necessary for proper functioning of the code.\n",
    "\n",
    "Additionally, the configuration parameter `err_dict` must be a dictionary that contains the columns that will be used to predict as the keys and the errors associated with that column as the values, e.g. `errdict[\"mag_u_lsst\"] = \"mag_err_u_lsst\"`.  This dictionary is used by the code that generates random realizations of the galaxy by adding Gaussian scatter, it tells that bit of code which columns contain errors for each of the attributes that will have Gaussian scatter added. <br>\n",
    "\n",
    "The other configuration parameters for TPZ are:\n",
    "- `seed` (int): the random seed used by numpy for this stage <br>\n",
    "- `nrandom` (int): the number of random training catalogs with Gaussian scatter to create. <br>\n",
    "- `ntrees` (int): the number of bootstrap samples for a given random catalog to create. <br>\n",
    "REMINDER: the total number of trees trained will be `nrandom` * `ntrees`, and if `nrandom` is set to 1, then no random catalogs are created, only the original training sample is used.<br>\n",
    "- `minleaf` (int): the mininum number of galaxies in a terminal leaf. <br>\n",
    "- `natt` (int): the number of attributes to split. <br>\n",
    "- `sigmafactor` (float): Gaussian smoothing with kernel Sigma1*Resolution. <br>\n",
    "- `rmsfactor` (float): MS for zconf calculation. <br>\n",
    "- `tree_strategy` (string): see paragraph below.<br>\n",
    "\n",
    "`rail_tpz` uses a parameter, `tree_strategy` that which specific algorithm is used to construct the trees, and this choice can have a few important effects on the results.  The original TPZ code contains bespoke decision tree code custom written for the algorighm to perform the recursive data splits.  While it is functional, it can be somewhat slow.  As an alternative, we have implemented an alternative method that instead uses scikit-learn's `DecisionTreeRegressor`, which can result in training times for the random forest informer of more than 10,000 times faster than native.  The specifics for how the decisionas to where tree splits occur are slightly different between the \"native\" and \"sklearn\" methods, though resulting photo-z predictions are qualitatively similar.\n",
    "\n",
    "There is a notable difference in how the two methods handle the PDF construction that will affect results: both methods look at the input galaxy and use the decision tree to find the galaxies that are most similar to that input galaxy, splitting in the tree until they reach the \"terminal leaf\" where the last split occurs.  There are a small number of galaxies in this terminal leaf.  the \"native\" method takes this small number of galaxies and makes a histogram of their redshifts, and combines the MxN tree histograms to construct the final PDF.  The \"sklearn\" method, on the other hand, takes the mean of the small number of galaxies in the terminal leaf and returns a single float, so the final PDF estimate will be a histogram of single values from each of the MxN trees rather than #in leaf node xN xM entries for the \"native\" representation.  While results should mostly be qualitatively similar, the fact that TPZ uses bootstrap sampling when constructing the different trees means that some specz values can be repeated in some trees if they are drawn multiple times in the bootstrap.  In areas of photometric space with sparse coverage of spectroscopic galaxies, this can result in discrete values appearing multiple times in the histogram of neighbors in the PDF.  This can manifest as repeated values of the mode, for example, often seen at high redshift.  The \"sklearn\" strategy of averaging over the terminal leaf can somewhat mitigate this effect, as the discrete values are slightly smoothed by the averaging over the terminal leaf sample.   If you re-run this example notebook and switch the `tree_strategy` from \"sklearn\" to \"native\", you will likely see some discrete mode values in either a histogram of the zmode or plot of mode vs true redshift.  One method is not generally better than the other, it is simply a feature that users should be aware of, as it can impact a specific science case, particularly if point estimate are going to be employed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa1c5b1-d009-405e-b68d-a6b139513d96",
   "metadata": {},
   "source": [
    "We need to specify the attributes that TPZ will use to create its trees, we do this via a list passed to the `bands` parameter.  While the default list would work, we'll create and use it explicitly in this example.  Redshift, the parameter that we are trying to predict, should not be included in the attribute list (but needs to be included in the data file so the trees can be trained to split on it).\n",
    "\n",
    "TPZ generates addtional \"random\" realizations of a training set by adding Gaussian scatter in attributes with sigma values taken from a different column in the input file.  The corresponding uncertainty columns for each attribute are stored as a dictionary with the name of the attribute column as the key and the name of the uncertainty as the value, this configuration parameter is `err_dict`.  While the default values set by `tpz_lite` would work, we'll create the necessary dictionary explicity and use it for illustration.  As mentioned above, using \"sklearn\" for the `tree_strategy` is much faster, so we will use that option in this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683ade3b-4ec6-4d54-a00a-6917a07a453b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = [\"u\", \"g\", \"r\", \"i\", \"z\", \"y\"]\n",
    "new_err_dict = {}\n",
    "attribute_list = []\n",
    "error_list = []\n",
    "for band in bands:\n",
    "    attribute_list.append(f\"mag_{band}_lsst\")\n",
    "    error_list.append(f\"mag_err_{band}_lsst\")\n",
    "    new_err_dict[f\"mag_{band}_lsst\"] = f\"mag_err_{band}_lsst\"\n",
    "# redshift is also an attribute used in the training, but it does not have an associated\n",
    "# error its entry in the err_dict should be set to \"None\"\n",
    "new_err_dict[\"redshift\"] = None\n",
    "\n",
    "print(new_err_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fecba50-6ef5-4c0a-8632-ed4e9cca8e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpz_dict = dict(zmin=0.0, \n",
    "                zmax=3.0, \n",
    "                nzbins=301,\n",
    "                bands=attribute_list,\n",
    "                err_bands=error_list,\n",
    "                hdf5_groupname='photometry',\n",
    "                err_dict=new_err_dict,\n",
    "                nrandom=3, \n",
    "                ntrees=5,\n",
    "                #tree_strategy='native')  # uncomment this line and comment out the line below to switch to using \"native\" trees \n",
    "                tree_strategy='sklearn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638f6f40-e60d-47f4-b639-6bb4560b1631",
   "metadata": {},
   "source": [
    "Now, lets create our stage and run `inform`.  We specified `nrandom = 3` and `ntrees = 5`, so we will get 15 trained trees that constitute our model.  For our 10k training galaxy sample this takes about 0.5 seconds for \"sklearn\", or about 90 seconds using \"native\" on my Mac desktop for a rough guide for how long this should take to train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3696e1-5604-4283-8043-e39722aa0a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pz_train = TPZliteInformer.make_stage(name='inform_TPZ', model='demo_tpz.pkl', **tpz_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcc22c4-eae9-4f13-9e26-7133b4684051",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pz_train.inform(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f21022-6af9-4c9a-9690-e27fe12e8f1e",
   "metadata": {},
   "source": [
    "# Running the Estimate stage\n",
    "\n",
    "The model was created successfully, we now need to read in our test data, which consists of ~20,000 galaies drawn from the same cosmoDC2 simulated sample that was used to create our training sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e130988e-db5d-4d73-a9b3-2297b600bf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rail.estimation.algos.tpz_lite import TPZliteEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d39d9e4-a074-4032-935c-0adf49901482",
   "metadata": {},
   "outputs": [],
   "source": [
    "testfile = os.path.join(RAILDIR,\"rail/examples_data/testdata/test_dc2_validation_9816.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79934d1a-2685-41b8-90f3-705b9544881e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = DS.read_file(\"test_data\", TableHandle, testfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178ab60d-baa5-44d3-b56e-72d87a4055f4",
   "metadata": {},
   "source": [
    "We can now set up our `TPZliteEstimator` stage to actualy estimate our redshift PDFs.  There is only one configuration parameter for the stage: <br>\n",
    "    - `test_err_dict` (dict): this is a dictionary just like `err_dict` as described for `TPZliteInformer`, i.e. a dictionary with the attributes for keys and the associated errors as values. <br>\n",
    "\n",
    "The other parameters from the inform stage are carried within the model so that we do not accidentally use conflicting values for them.  We do need to supply the name of the model file to use, this can either be done directly as the file name, or as we do in the cell below, with the `get_handle` method from our inform stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6d578f-ef53-4cd6-a059-7c23f2c3b06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = dict(hdf5_groupname='photometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19eabc1-ee1c-4edc-9538-02033e386f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_runner = TPZliteEstimator.make_stage(name=\"test_tpz\", output=\"TPZ_demo_output.hdf5\",\n",
    "                                          model=pz_train.get_handle('model'), **test_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1ceaa9-a265-4ec0-9d4f-2cf54b7cf972",
   "metadata": {},
   "source": [
    "Now let's run the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbf0c9b-57fb-40de-b24a-ba8da6573850",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results = test_runner.estimate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc6688e-a3fc-4414-9050-fc7bdd56a25e",
   "metadata": {},
   "source": [
    "This took about 6.5 seconds on my Mac desktop, not the fastest photo-z code, but not unreasonable for 20,000 galaxies.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90e9ea2-b21e-4e85-af4c-3f1300cbba90",
   "metadata": {},
   "source": [
    "# Plotting point estimates and an example PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a0778e-6490-4cae-a489-db49c094c7bc",
   "metadata": {},
   "source": [
    "Now let's make a few diagnostic plots.  TPZ does calculate the PDF mode for each galaxy and stores this as ancillary data, so we can plot a point estimate vs the true redshift:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648e55e7-873e-40fb-8266-9aff921ae18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = test_data()['photometry']['redshift']\n",
    "zmode = results().ancil['zmode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22299c1c-6c28-4fa1-99eb-233e8d7184c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(sz,zmode, s=2,c='k')\n",
    "plt.plot([0,3],[0,3],'r--')\n",
    "plt.xlabel(\"redshift\", fontsize=15)\n",
    "plt.ylabel(\"TPZ mode\", fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24361040-d79c-4663-86e3-bd65cbfeff74",
   "metadata": {},
   "source": [
    "Not bad, a handful of outliers, no obvious biases.  Let's also plot an individual redshift PDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f338eb-83f0-42c4-be86-2ce1a218250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "which=5355\n",
    "fig, axs = plt.subplots()\n",
    "results().plot_native(key=which,axes=axs, label=f\"PDF for galaxy {which}\")\n",
    "axs.axvline(sz[which],c='r',ls='--', label=\"true redshift\")\n",
    "plt.legend(loc='upper right', fontsize=12)\n",
    "axs.set_xlabel(\"redshift\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffc6588-437b-4d4a-862a-28e8f8a54ddd",
   "metadata": {},
   "source": [
    "You can experiment by changing the integer value of `which` above and see some of the different PDF shapes, though in general you seill see peaks corresponding to the values in the terminal leaves of the trees with Gaussian scatter added on top.  For well constrained areas of parameter space, all will have similar redshifts and result in a nice unimodal peak, for others there will be multiple redshift bumps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8da100-e5d5-4a40-af6a-9cbaeedcc3dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
