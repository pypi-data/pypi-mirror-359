"""
Evaluator - Metrics Aggregation

ãƒ†ã‚¹ãƒˆçµæœã‚’é›†ç´„ã—ã€ã‚·ã‚¹ãƒ†ãƒ æ€§èƒ½ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—ã™ã‚‹DocumentProcessorã€‚
è¤‡æ•°ã®è©•ä¾¡æŒ‡æ¨™ã‚’çµ±åˆã—ã¦åŒ…æ‹¬çš„ãªè©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚
"""

from typing import List, Dict, Any, Optional, Union, Type
from pydantic import BaseModel, Field
from dataclasses import dataclass, field
import json
import os
import statistics
from pathlib import Path

from ..document_processor import DocumentProcessor, DocumentProcessorConfig
from ..models.document import Document


@dataclass
class MetricResult:
    """ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—çµæœ"""
    name: str
    value: float
    unit: str = ""
    description: str = ""
    threshold: Optional[float] = None
    passed: Optional[bool] = None


class EvaluationMetrics(BaseModel):
    """è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®šç¾©"""
    
    accuracy: float = 0.0
    precision: float = 0.0
    recall: float = 0.0
    f1_score: float = 0.0
    average_confidence: float = 0.0
    average_response_time: float = 0.0
    source_accuracy: float = 0.0
    coverage: float = 0.0
    consistency: float = 0.0
    user_satisfaction: float = 0.0


class CategoryMetrics(BaseModel):
    """ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    
    category: str
    total_queries: int
    successful_queries: int
    success_rate: float
    average_confidence: float
    average_response_time: float
    common_failures: List[str] = field(default_factory=list)


@dataclass
class EvaluatorConfig(DocumentProcessorConfig):
    """Evaluatorè¨­å®š"""
    
    include_category_analysis: bool = True
    include_temporal_analysis: bool = False
    include_failure_analysis: bool = True
    confidence_threshold: float = 0.7
    response_time_threshold: float = 2.0
    accuracy_threshold: float = 0.8
    output_format: str = "markdown"  # "markdown", "json", "html"
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹é‡ã¿
    metric_weights: Dict[str, float] = field(default_factory=lambda: {
        "accuracy": 0.3,
        "response_time": 0.2,
        "confidence": 0.2,
        "source_accuracy": 0.15,
        "coverage": 0.15
    })


class Evaluator(DocumentProcessor):
    """
    ãƒ¡ãƒˆãƒªã‚¯ã‚¹é›†ç´„è€…
    
    ãƒ†ã‚¹ãƒˆçµæœãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†ã—ã€ã‚·ã‚¹ãƒ†ãƒ æ€§èƒ½ã®åŒ…æ‹¬çš„ãªè©•ä¾¡ã‚’è¡Œã„ã¾ã™ã€‚
    """
    
    def __init__(self, config=None, **kwargs):
        """Initialize Evaluator processor
        
        Args:
            config: Optional EvaluatorConfig object (for backward compatibility)
            **kwargs: Configuration parameters, supports both individual parameters
                     and config dict, with environment variable fallback
        """
        # Handle backward compatibility with config object
        if config is not None and hasattr(config, 'include_category_analysis'):
            # Traditional config object passed
            super().__init__(config)
            self.include_category_analysis = config.include_category_analysis
            self.include_temporal_analysis = config.include_temporal_analysis
            self.include_failure_analysis = config.include_failure_analysis
            self.confidence_threshold = config.confidence_threshold
            self.response_time_threshold = config.response_time_threshold
            self.accuracy_threshold = config.accuracy_threshold
            self.output_format = config.output_format
            self.metric_weights = config.metric_weights
        else:
            # Extract config dict if provided
            config_dict = kwargs.get('config', {})
            
            # Environment variable fallback with priority: kwargs > config dict > env vars > defaults
            self.include_category_analysis = kwargs.get('include_category_analysis', 
                                                       config_dict.get('include_category_analysis', 
                                                                      os.getenv('REFINIRE_RAG_EVALUATOR_CATEGORY_ANALYSIS', 'true').lower() == 'true'))
            self.include_temporal_analysis = kwargs.get('include_temporal_analysis', 
                                                       config_dict.get('include_temporal_analysis', 
                                                                      os.getenv('REFINIRE_RAG_EVALUATOR_TEMPORAL_ANALYSIS', 'false').lower() == 'true'))
            self.include_failure_analysis = kwargs.get('include_failure_analysis', 
                                                      config_dict.get('include_failure_analysis', 
                                                                     os.getenv('REFINIRE_RAG_EVALUATOR_FAILURE_ANALYSIS', 'true').lower() == 'true'))
            self.confidence_threshold = kwargs.get('confidence_threshold', 
                                                  config_dict.get('confidence_threshold', 
                                                                 float(os.getenv('REFINIRE_RAG_EVALUATOR_CONFIDENCE_THRESHOLD', '0.7'))))
            self.response_time_threshold = kwargs.get('response_time_threshold', 
                                                     config_dict.get('response_time_threshold', 
                                                                    float(os.getenv('REFINIRE_RAG_EVALUATOR_RESPONSE_TIME_THRESHOLD', '2.0'))))
            self.accuracy_threshold = kwargs.get('accuracy_threshold', 
                                                config_dict.get('accuracy_threshold', 
                                                               float(os.getenv('REFINIRE_RAG_EVALUATOR_ACCURACY_THRESHOLD', '0.8'))))
            self.output_format = kwargs.get('output_format', 
                                           config_dict.get('output_format', 
                                                          os.getenv('REFINIRE_RAG_EVALUATOR_OUTPUT_FORMAT', 'markdown')))
            self.metric_weights = kwargs.get('metric_weights', 
                                            config_dict.get('metric_weights', {
                                                "accuracy": 0.3,
                                                "response_time": 0.2,
                                                "confidence": 0.2,
                                                "source_accuracy": 0.15,
                                                "coverage": 0.15
                                            }))
            
            # Create config object for backward compatibility
            config = EvaluatorConfig(
                include_category_analysis=self.include_category_analysis,
                include_temporal_analysis=self.include_temporal_analysis,
                include_failure_analysis=self.include_failure_analysis,
                confidence_threshold=self.confidence_threshold,
                response_time_threshold=self.response_time_threshold,
                accuracy_threshold=self.accuracy_threshold,
                output_format=self.output_format,
                metric_weights=self.metric_weights
            )
            
            super().__init__(config)
        
        self.evaluation_results: List[Dict[str, Any]] = []
        self.computed_metrics: Optional[EvaluationMetrics] = None
        self.category_metrics: Dict[str, CategoryMetrics] = {}
    
    def get_config(self) -> Dict[str, Any]:
        """Get current configuration as dictionary
        ç¾åœ¨ã®è¨­å®šã‚’è¾æ›¸ã¨ã—ã¦å–å¾—
        
        Returns:
            Dict[str, Any]: Current configuration dictionary
        """
        return {
            'include_category_analysis': self.include_category_analysis,
            'include_temporal_analysis': self.include_temporal_analysis,
            'include_failure_analysis': self.include_failure_analysis,
            'confidence_threshold': self.confidence_threshold,
            'response_time_threshold': self.response_time_threshold,
            'accuracy_threshold': self.accuracy_threshold,
            'output_format': self.output_format,
            'metric_weights': self.metric_weights
        }
    
    @classmethod
    def get_config_class(cls) -> Type[EvaluatorConfig]:
        """Get the configuration class for this processor (backward compatibility)
        ã“ã®ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã®è¨­å®šã‚¯ãƒ©ã‚¹ã‚’å–å¾—ï¼ˆä¸‹ä½äº’æ›æ€§ï¼‰
        """
        return EvaluatorConfig
    
    def process(self, document: Document) -> List[Document]:
        """
        ãƒ†ã‚¹ãƒˆçµæœãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«å¤‰æ›
        
        Args:
            document: TestSuiteã‹ã‚‰ã®çµæœãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
            
        Returns:
            List[Document]: è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
        """
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒãƒ†ã‚¹ãƒˆçµæœã‹ãƒã‚§ãƒƒã‚¯
        if not self._is_test_result_document(document):
            return [document]  # ãƒ†ã‚¹ãƒˆçµæœã§ãªã‘ã‚Œã°ãã®ã¾ã¾é€šã™
        
        # ãƒ†ã‚¹ãƒˆçµæœã‚’è§£æ
        test_results = self._parse_test_results(document)
        self.evaluation_results.extend(test_results)
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—ï¼ˆç´¯ç©ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ï¼‰
        metrics = self._compute_metrics(self.evaluation_results)
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ
        category_analysis = {}
        if self.include_category_analysis:
            category_analysis = self._analyze_by_category(test_results)
            self.category_metrics.update(category_analysis)
        
        # å¤±æ•—åˆ†æ
        failure_analysis = {}
        if self.include_failure_analysis:
            failure_analysis = self._analyze_failures(test_results)
        
        # è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆ
        report_doc = Document(
            id=f"evaluation_report_{document.id}",
            content=self._format_evaluation_report(metrics, category_analysis, failure_analysis),
            metadata={
                "processing_stage": "evaluation",
                "source_document_id": document.id,
                "metrics_computed": len(metrics.__dict__),
                "categories_analyzed": len(category_analysis) if category_analysis else 0,
                "overall_score": self._compute_overall_score(metrics),
                "recommendations": self._generate_recommendations(metrics)
            }
        )
        
        return [report_doc]
    
    def _is_test_result_document(self, document: Document) -> bool:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒãƒ†ã‚¹ãƒˆçµæœã‹ãƒã‚§ãƒƒã‚¯"""
        
        processing_stage = document.metadata.get("processing_stage", "")
        return processing_stage in ["test_execution", "test_results"]
    
    def _parse_test_results(self, document: Document) -> List[Dict[str, Any]]:
        """ãƒ†ã‚¹ãƒˆçµæœãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        
        results = []
        content = document.content
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰ãƒ†ã‚¹ãƒˆçµæœã‚’è§£æï¼ˆç°¡æ˜“å®Ÿè£…ï¼‰
        lines = content.split('\n')
        current_test = {}
        
        for line in lines:
            line = line.strip()
            
            if line.startswith("## âœ… PASS") or line.startswith("## âŒ FAIL"):
                if current_test:
                    results.append(current_test)
                    current_test = {}
                
                current_test["passed"] = "âœ… PASS" in line
                current_test["test_id"] = line.split()[-1] if len(line.split()) > 2 else "unknown"
            
            elif line.startswith("**Query**:"):
                current_test["query"] = line.replace("**Query**:", "").strip()
            
            elif line.startswith("**Confidence**:"):
                try:
                    current_test["confidence"] = float(line.replace("**Confidence**:", "").strip())
                except:
                    current_test["confidence"] = 0.0
            
            elif line.startswith("**Processing Time**:"):
                try:
                    time_str = line.replace("**Processing Time**:", "").replace("s", "").strip()
                    current_test["processing_time"] = float(time_str)
                except:
                    current_test["processing_time"] = 0.0
            
            elif line.startswith("**Sources Found**:"):
                try:
                    current_test["sources_found"] = int(line.replace("**Sources Found**:", "").strip())
                except:
                    current_test["sources_found"] = 0
        
        # æœ€å¾Œã®ãƒ†ã‚¹ãƒˆã‚’è¿½åŠ 
        if current_test:
            results.append(current_test)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰çµ±è¨ˆæƒ…å ±ã‚‚æŠ½å‡º
        doc_metadata = document.metadata
        
        # å¸¸ã«document_idã‚’è¿½åŠ 
        for result in results:
            result["document_id"] = doc_metadata.get("source_document_id", "unknown")
            
        if "tests_run" in doc_metadata:
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®çµ±è¨ˆã‚’çµæœã«çµ±åˆ
            for result in results:
                result["success_rate"] = doc_metadata.get("success_rate", 0.0)
        
        return results
    
    def _compute_metrics(self, test_results: List[Dict[str, Any]]) -> EvaluationMetrics:
        """ãƒ†ã‚¹ãƒˆçµæœã‹ã‚‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—"""
        
        if not test_results:
            return EvaluationMetrics()
        
        # åŸºæœ¬çµ±è¨ˆ
        total_tests = len(test_results)
        passed_tests = sum(1 for r in test_results if r.get("passed", False))
        
        # å„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—
        accuracy = passed_tests / total_tests if total_tests > 0 else 0.0
        
        confidences = [r.get("confidence", 0.0) for r in test_results]
        average_confidence = statistics.mean(confidences) if confidences else 0.0
        
        response_times = [r.get("processing_time", 0.0) for r in test_results]
        average_response_time = statistics.mean(response_times) if response_times else 0.0
        
        # é©åˆç‡ãƒ»å†ç¾ç‡ã®è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        true_positives = sum(1 for r in test_results 
                           if r.get("passed", False) and r.get("sources_found", 0) > 0)
        false_positives = sum(1 for r in test_results 
                            if not r.get("passed", False) and r.get("sources_found", 0) > 0)
        false_negatives = sum(1 for r in test_results 
                            if r.get("passed", False) and r.get("sources_found", 0) == 0)
        
        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0.0
        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0.0
        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0
        
        # ã‚½ãƒ¼ã‚¹ç²¾åº¦
        source_accuracy = sum(1 for r in test_results if r.get("sources_found", 0) > 0) / total_tests
        
        # ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼ˆãƒ†ã‚¹ãƒˆã®å¤šæ§˜æ€§ï¼‰
        unique_queries = len(set(r.get("query", "") for r in test_results))
        coverage = unique_queries / total_tests if total_tests > 0 else 0.0
        
        # ä¸€è²«æ€§ï¼ˆåŒæ§˜ã®ã‚¯ã‚¨ãƒªã§ã®æ€§èƒ½ã°ã‚‰ã¤ãï¼‰
        confidence_std = statistics.stdev(confidences) if len(confidences) > 1 else 0.0
        consistency = max(0.0, 1.0 - (confidence_std / average_confidence)) if average_confidence > 0 else 0.0
        
        metrics = EvaluationMetrics(
            accuracy=accuracy,
            precision=precision,
            recall=recall,
            f1_score=f1_score,
            average_confidence=average_confidence,
            average_response_time=average_response_time,
            source_accuracy=source_accuracy,
            coverage=coverage,
            consistency=consistency,
            user_satisfaction=self._estimate_user_satisfaction(test_results)
        )
        
        self.computed_metrics = metrics
        return metrics
    
    def _estimate_user_satisfaction(self, test_results: List[Dict[str, Any]]) -> float:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼æº€è¶³åº¦ã‚’æ¨å®š"""
        
        # è¤‡æ•°è¦å› ã‹ã‚‰æº€è¶³åº¦ã‚’æ¨å®š
        satisfaction_factors = []
        
        for result in test_results:
            # åŸºæœ¬çš„ãªæˆåŠŸ
            success_factor = 1.0 if result.get("passed", False) else 0.0
            
            # ä¿¡é ¼åº¦è¦å› 
            confidence = result.get("confidence", 0.0)
            confidence_factor = min(confidence / self.config.confidence_threshold, 1.0)
            
            # å¿œç­”æ™‚é–“è¦å› 
            response_time = result.get("processing_time", 0.0)
            time_factor = max(0.0, 1.0 - (response_time / self.config.response_time_threshold))
            
            # é‡ã¿ä»˜ãå¹³å‡
            overall_satisfaction = (
                success_factor * 0.5 + 
                confidence_factor * 0.3 + 
                time_factor * 0.2
            )
            
            satisfaction_factors.append(overall_satisfaction)
        
        return statistics.mean(satisfaction_factors) if satisfaction_factors else 0.0
    
    def _analyze_by_category(self, test_results: List[Dict[str, Any]]) -> Dict[str, CategoryMetrics]:
        """ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®åˆ†æã‚’å®Ÿè¡Œ"""
        
        category_data = {}
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«ãƒ†ã‚¹ãƒˆçµæœã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        for result in test_results:
            # ã‚«ãƒ†ã‚´ãƒªã®æ¨å®šï¼ˆã‚¯ã‚¨ãƒªå†…å®¹ã‹ã‚‰ï¼‰
            category = self._categorize_result(result)
            
            if category not in category_data:
                category_data[category] = []
            
            category_data[category].append(result)
        
        # å„ã‚«ãƒ†ã‚´ãƒªã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—
        category_metrics = {}
        
        for category, results in category_data.items():
            total = len(results)
            successful = sum(1 for r in results if r.get("passed", False))
            
            confidences = [r.get("confidence", 0.0) for r in results]
            times = [r.get("processing_time", 0.0) for r in results]
            
            # å…±é€šçš„ãªå¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç‰¹å®š
            failed_results = [r for r in results if not r.get("passed", False)]
            common_failures = self._identify_common_failures(failed_results)
            
            metrics = CategoryMetrics(
                category=category,
                total_queries=total,
                successful_queries=successful,
                success_rate=successful / total if total > 0 else 0.0,
                average_confidence=statistics.mean(confidences) if confidences else 0.0,
                average_response_time=statistics.mean(times) if times else 0.0,
                common_failures=common_failures
            )
            
            category_metrics[category] = metrics
        
        return category_metrics
    
    def _categorize_result(self, result: Dict[str, Any]) -> str:
        """ãƒ†ã‚¹ãƒˆçµæœã‚’ã‚«ãƒ†ã‚´ãƒªåˆ†é¡"""
        
        query = result.get("query", "").lower()
        
        if any(word in query for word in ["ã¨ã¯", "ä½•ã§ã™ã‹", "ã©ã®ã‚ˆã†ãª"]):
            return "definition"
        elif any(word in query for word in ["æ–¹æ³•", "æ‰‹é †", "ã‚„ã‚Šæ–¹"]):
            return "how_to"
        elif any(word in query for word in ["ãªãœ", "ç†ç”±", "åŸå› "]):
            return "why"
        elif any(word in query for word in ["æ¯”è¼ƒ", "é•ã„", "å·®"]):
            return "comparison"
        elif any(word in query for word in ["å¤©æ°—", "æ–™ç†", "ã‚¹ãƒãƒ¼ãƒ„"]):
            return "negative"
        else:
            return "general"
    
    def _identify_common_failures(self, failed_results: List[Dict[str, Any]]) -> List[str]:
        """å¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å…±é€šç‚¹ã‚’ç‰¹å®š"""
        
        failure_patterns = []
        
        if not failed_results:
            return failure_patterns
        
        # ä½ä¿¡é ¼åº¦
        low_confidence_count = sum(1 for r in failed_results if r.get("confidence", 1.0) < 0.3)
        if low_confidence_count > len(failed_results) * 0.5:
            failure_patterns.append("ä½ä¿¡é ¼åº¦")
        
        # é•·ã„å¿œç­”æ™‚é–“
        slow_response_count = sum(1 for r in failed_results if r.get("processing_time", 0.0) > 2.0)
        if slow_response_count > len(failed_results) * 0.3:
            failure_patterns.append("å¿œç­”æ™‚é–“é…å»¶")
        
        # ã‚½ãƒ¼ã‚¹ä¸è¶³
        no_source_count = sum(1 for r in failed_results if r.get("sources_found", 1) == 0)
        if no_source_count > len(failed_results) * 0.4:
            failure_patterns.append("é–¢é€£ã‚½ãƒ¼ã‚¹ä¸è¶³")
        
        return failure_patterns
    
    def _analyze_failures(self, test_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è©³ç´°ãªå¤±æ•—åˆ†æ"""
        
        failed_results = [r for r in test_results if not r.get("passed", False)]
        
        if not failed_results:
            return {"total_failures": 0, "failure_rate": 0.0}
        
        failure_analysis = {
            "total_failures": len(failed_results),
            "failure_rate": len(failed_results) / len(test_results),
            "common_patterns": self._identify_common_failures(failed_results),
            "failure_categories": {},
            "improvement_suggestions": []
        }
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥å¤±æ•—ç‡
        for category in ["definition", "how_to", "why", "comparison", "negative", "general"]:
            category_results = [r for r in test_results if self._categorize_result(r) == category]
            category_failures = [r for r in category_results if not r.get("passed", False)]
            
            if category_results:
                failure_analysis["failure_categories"][category] = {
                    "total": len(category_results),
                    "failures": len(category_failures),
                    "failure_rate": len(category_failures) / len(category_results)
                }
        
        # æ”¹å–„ææ¡ˆ
        if "ä½ä¿¡é ¼åº¦" in failure_analysis["common_patterns"]:
            failure_analysis["improvement_suggestions"].append("ãƒ¢ãƒ‡ãƒ«ã®ä¿¡é ¼åº¦æ ¡æ­£ã‚’æ”¹å–„")
        
        if "å¿œç­”æ™‚é–“é…å»¶" in failure_analysis["common_patterns"]:
            failure_analysis["improvement_suggestions"].append("æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æœ€é©åŒ–")
        
        if "é–¢é€£ã‚½ãƒ¼ã‚¹ä¸è¶³" in failure_analysis["common_patterns"]:
            failure_analysis["improvement_suggestions"].append("ã‚³ãƒ¼ãƒ‘ã‚¹ã®æ‹¡å……ã¾ãŸã¯æ¤œç´¢æˆ¦ç•¥ã®æ”¹å–„")
        
        return failure_analysis
    
    def _compute_overall_score(self, metrics: EvaluationMetrics) -> float:
        """é‡ã¿ä»˜ããƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰ç·åˆã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        
        weights = self.config.metric_weights
        
        score = (
            metrics.accuracy * weights.get("accuracy", 0.3) +
            (1.0 - min(metrics.average_response_time / self.config.response_time_threshold, 1.0)) * weights.get("response_time", 0.2) +
            metrics.average_confidence * weights.get("confidence", 0.2) +
            metrics.source_accuracy * weights.get("source_accuracy", 0.15) +
            metrics.coverage * weights.get("coverage", 0.15)
        )
        
        return min(max(score, 0.0), 1.0)  # 0-1ã®ç¯„å›²ã«æ­£è¦åŒ–
    
    def _generate_recommendations(self, metrics: EvaluationMetrics) -> List[str]:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«åŸºã¥ãæ”¹å–„ææ¡ˆã‚’ç”Ÿæˆ"""
        
        recommendations = []
        
        if metrics.accuracy < self.config.accuracy_threshold:
            recommendations.append("ã‚·ã‚¹ãƒ†ãƒ ç²¾åº¦ã®æ”¹å–„ãŒå¿…è¦ã§ã™")
        
        if metrics.average_response_time > self.config.response_time_threshold:
            recommendations.append("å¿œç­”æ™‚é–“ã®æœ€é©åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")
        
        if metrics.average_confidence < self.config.confidence_threshold:
            recommendations.append("ãƒ¢ãƒ‡ãƒ«ã®ä¿¡é ¼åº¦æ ¡æ­£ã‚’æ”¹å–„ã—ã¦ãã ã•ã„")
        
        if metrics.coverage < 0.7:
            recommendations.append("ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã®å¤šæ§˜æ€§ã‚’å¢—ã‚„ã—ã¦ãã ã•ã„")
        
        if metrics.consistency < 0.8:
            recommendations.append("çµæœã®ä¸€è²«æ€§å‘ä¸ŠãŒå¿…è¦ã§ã™")
        
        return recommendations
    
    def _format_evaluation_report(
        self, 
        metrics: EvaluationMetrics, 
        category_analysis: Dict[str, CategoryMetrics],
        failure_analysis: Dict[str, Any]
    ) -> str:
        """è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        
        if self.config.output_format == "json":
            return self._format_json_report(metrics, category_analysis, failure_analysis)
        else:
            return self._format_markdown_report(metrics, category_analysis, failure_analysis)
    
    def _format_markdown_report(
        self, 
        metrics: EvaluationMetrics, 
        category_analysis: Dict[str, CategoryMetrics],
        failure_analysis: Dict[str, Any]
    ) -> str:
        """Markdownå½¢å¼ã®è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ"""
        
        lines = ["# RAGã‚·ã‚¹ãƒ†ãƒ è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ\n"]
        
        # ç·åˆã‚¹ã‚³ã‚¢
        overall_score = self._compute_overall_score(metrics)
        lines.append(f"## ğŸ“Š ç·åˆè©•ä¾¡: {overall_score:.2f}/1.00")
        
        if overall_score >= 0.8:
            lines.append("ğŸŒŸ **è©•ä¾¡: å„ªç§€** - ã‚·ã‚¹ãƒ†ãƒ ã¯é«˜å“è³ªã§å®‰å®šã—ã¦ã„ã¾ã™")
        elif overall_score >= 0.6:
            lines.append("ğŸ‘ **è©•ä¾¡: è‰¯å¥½** - ä¸€éƒ¨æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™")
        elif overall_score >= 0.4:
            lines.append("ğŸ“ˆ **è©•ä¾¡: æ”¹å–„å¿…è¦** - é‡è¦ãªå•é¡ŒãŒã‚ã‚Šã¾ã™")
        else:
            lines.append("ğŸ”§ **è©•ä¾¡: è¦å¤§å¹…æ”¹å–„** - ã‚·ã‚¹ãƒ†ãƒ ã®æ ¹æœ¬çš„ãªè¦‹ç›´ã—ãŒå¿…è¦")
        
        lines.append("")
        
        # ä¸»è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        lines.append("## ğŸ“ˆ ä¸»è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹")
        lines.append(f"- **ç²¾åº¦ (Accuracy)**: {metrics.accuracy:.1%}")
        lines.append(f"- **é©åˆç‡ (Precision)**: {metrics.precision:.1%}")
        lines.append(f"- **å†ç¾ç‡ (Recall)**: {metrics.recall:.1%}")
        lines.append(f"- **F1ã‚¹ã‚³ã‚¢**: {metrics.f1_score:.3f}")
        lines.append(f"- **å¹³å‡ä¿¡é ¼åº¦**: {metrics.average_confidence:.3f}")
        lines.append(f"- **å¹³å‡å¿œç­”æ™‚é–“**: {metrics.average_response_time:.3f}ç§’")
        lines.append(f"- **ã‚½ãƒ¼ã‚¹ç²¾åº¦**: {metrics.source_accuracy:.1%}")
        lines.append(f"- **ã‚«ãƒãƒ¬ãƒƒã‚¸**: {metrics.coverage:.1%}")
        lines.append(f"- **ä¸€è²«æ€§**: {metrics.consistency:.1%}")
        lines.append(f"- **æ¨å®šãƒ¦ãƒ¼ã‚¶ãƒ¼æº€è¶³åº¦**: {metrics.user_satisfaction:.1%}")
        lines.append("")
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ
        if category_analysis:
            lines.append("## ğŸ“Š ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ")
            for category, cat_metrics in category_analysis.items():
                lines.append(f"### {category.title()}")
                lines.append(f"- ç·ã‚¯ã‚¨ãƒªæ•°: {cat_metrics.total_queries}")
                lines.append(f"- æˆåŠŸç‡: {cat_metrics.success_rate:.1%}")
                lines.append(f"- å¹³å‡ä¿¡é ¼åº¦: {cat_metrics.average_confidence:.3f}")
                lines.append(f"- å¹³å‡å¿œç­”æ™‚é–“: {cat_metrics.average_response_time:.3f}ç§’")
                if cat_metrics.common_failures:
                    lines.append(f"- å…±é€šå¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³: {', '.join(cat_metrics.common_failures)}")
                lines.append("")
        
        # å¤±æ•—åˆ†æ
        if failure_analysis.get("total_failures", 0) > 0:
            lines.append("## âš ï¸ å¤±æ•—åˆ†æ")
            lines.append(f"- ç·å¤±æ•—æ•°: {failure_analysis['total_failures']}")
            lines.append(f"- å¤±æ•—ç‡: {failure_analysis['failure_rate']:.1%}")
            
            if failure_analysis.get("common_patterns"):
                lines.append(f"- å…±é€šãƒ‘ã‚¿ãƒ¼ãƒ³: {', '.join(failure_analysis['common_patterns'])}")
            
            if failure_analysis.get("improvement_suggestions"):
                lines.append("### ğŸ”§ æ”¹å–„ææ¡ˆ")
                for suggestion in failure_analysis["improvement_suggestions"]:
                    lines.append(f"- {suggestion}")
                lines.append("")
        
        # é–¾å€¤ã¨ã®æ¯”è¼ƒ
        lines.append("## ğŸ¯ é–¾å€¤ã¨ã®æ¯”è¼ƒ")
        accuracy_status = "âœ…" if metrics.accuracy >= self.config.accuracy_threshold else "âŒ"
        response_status = "âœ…" if metrics.average_response_time <= self.config.response_time_threshold else "âŒ"
        confidence_status = "âœ…" if metrics.average_confidence >= self.config.confidence_threshold else "âŒ"
        
        lines.append(f"- ç²¾åº¦é–¾å€¤ ({self.config.accuracy_threshold:.1%}): {accuracy_status}")
        lines.append(f"- å¿œç­”æ™‚é–“é–¾å€¤ ({self.config.response_time_threshold}ç§’): {response_status}")
        lines.append(f"- ä¿¡é ¼åº¦é–¾å€¤ ({self.config.confidence_threshold:.1f}): {confidence_status}")
        
        return "\n".join(lines)
    
    def _format_json_report(
        self, 
        metrics: EvaluationMetrics, 
        category_analysis: Dict[str, CategoryMetrics],
        failure_analysis: Dict[str, Any]
    ) -> str:
        """JSONå½¢å¼ã®è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ"""
        
        report = {
            "overall_score": self._compute_overall_score(metrics),
            "metrics": metrics.model_dump(),
            "category_analysis": {k: v.model_dump() for k, v in category_analysis.items()},
            "failure_analysis": failure_analysis,
            "recommendations": self._generate_recommendations(metrics),
            "thresholds": {
                "accuracy": self.config.accuracy_threshold,
                "response_time": self.config.response_time_threshold,
                "confidence": self.config.confidence_threshold
            }
        }
        
        return json.dumps(report, ensure_ascii=False, indent=2)
    
    def get_summary_metrics(self) -> Dict[str, float]:
        """ã‚µãƒãƒªãƒ¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—"""
        
        if not self.computed_metrics:
            return {}
        
        return {
            "overall_score": self._compute_overall_score(self.computed_metrics),
            "accuracy": self.computed_metrics.accuracy,
            "f1_score": self.computed_metrics.f1_score,
            "average_confidence": self.computed_metrics.average_confidence,
            "average_response_time": self.computed_metrics.average_response_time,
            "user_satisfaction": self.computed_metrics.user_satisfaction
        }