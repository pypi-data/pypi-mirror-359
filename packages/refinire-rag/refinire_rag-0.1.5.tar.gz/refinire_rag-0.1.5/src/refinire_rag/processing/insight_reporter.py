"""
InsightReporter - Threshold-based Interpretation and Reporting

é–¾å€¤ãƒ™ãƒ¼ã‚¹ã®è§£é‡ˆã¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚’è¡Œã†DocumentProcessorã€‚
è©•ä¾¡çµæœã‚’åˆ†æã—ã€å®Ÿç”¨çš„ãªã‚¤ãƒ³ã‚µã‚¤ãƒˆã¨æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
"""

from typing import List, Dict, Any, Optional, Tuple, Type
from pydantic import BaseModel, Field
from dataclasses import field, dataclass
from enum import Enum
import statistics
import json
import os
from pathlib import Path
from datetime import datetime

from ..document_processor import DocumentProcessor, DocumentProcessorConfig
from ..models.document import Document


class InsightType(str, Enum):
    """ã‚¤ãƒ³ã‚µã‚¤ãƒˆã‚¿ã‚¤ãƒ—"""
    PERFORMANCE = "performance"        # æ€§èƒ½ã«é–¢ã™ã‚‹ã‚¤ãƒ³ã‚µã‚¤ãƒˆ
    QUALITY = "quality"               # å“è³ªã«é–¢ã™ã‚‹ã‚¤ãƒ³ã‚µã‚¤ãƒˆ
    CONSISTENCY = "consistency"       # ä¸€è²«æ€§ã«é–¢ã™ã‚‹ã‚¤ãƒ³ã‚µã‚¤ãƒˆ
    EFFICIENCY = "efficiency"         # åŠ¹ç‡æ€§ã«é–¢ã™ã‚‹ã‚¤ãƒ³ã‚µã‚¤ãƒˆ
    RELIABILITY = "reliability"       # ä¿¡é ¼æ€§ã«é–¢ã™ã‚‹ã‚¤ãƒ³ã‚µã‚¤ãƒˆ
    SCALABILITY = "scalability"       # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã«é–¢ã™ã‚‹ã‚¤ãƒ³ã‚µã‚¤ãƒˆ


class Insight(BaseModel):
    """ã‚¤ãƒ³ã‚µã‚¤ãƒˆãƒ¢ãƒ‡ãƒ«"""
    
    id: str
    insight_type: InsightType
    title: str
    description: str
    severity: str  # "critical", "high", "medium", "low", "info"
    confidence: float
    affected_metrics: List[str]
    recommendations: List[str]
    supporting_data: Dict[str, Any] = Field(default_factory=dict)
    metadata: Dict[str, Any] = Field(default_factory=dict)


class Threshold(BaseModel):
    """é–¾å€¤è¨­å®š"""
    
    metric_name: str
    critical_threshold: Optional[float] = None
    warning_threshold: Optional[float] = None
    target_threshold: Optional[float] = None
    comparison_operator: str = "greater_than"  # "greater_than", "less_than", "equals"


@dataclass
class InsightReporterConfig(DocumentProcessorConfig):
    """InsightReporterè¨­å®š"""
    
    # ã‚¤ãƒ³ã‚µã‚¤ãƒˆç”Ÿæˆè¨­å®š
    enable_trend_analysis: bool = True
    enable_comparative_analysis: bool = True
    enable_root_cause_analysis: bool = True
    min_confidence_for_insight: float = 0.6
    
    # ãƒ¬ãƒãƒ¼ãƒˆè¨­å®š
    include_executive_summary: bool = True
    include_detailed_analysis: bool = True
    include_action_items: bool = True
    report_format: str = "markdown"  # "markdown", "json", "html"


class InsightReporter(DocumentProcessor):
    """
    ã‚¤ãƒ³ã‚µã‚¤ãƒˆãƒ¬ãƒãƒ¼ã‚¿ãƒ¼
    
    è©•ä¾¡çµæœã‚’åˆ†æã—ã€é–¾å€¤ãƒ™ãƒ¼ã‚¹ã®è§£é‡ˆã¨ã‚¤ãƒ³ã‚µã‚¤ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚
    """
    
    def __init__(self, config=None, **kwargs):
        """Initialize InsightReporter processor
        
        Args:
            config: Optional InsightReporterConfig object (for backward compatibility)
            **kwargs: Configuration parameters, supports both individual parameters
                     and config dict, with environment variable fallback
        """
        # Handle backward compatibility with config object
        if config is not None and hasattr(config, 'enable_trend_analysis'):
            # Traditional config object passed
            super().__init__(config)
            self.enable_trend_analysis = config.enable_trend_analysis
            self.enable_comparative_analysis = config.enable_comparative_analysis
            self.enable_root_cause_analysis = config.enable_root_cause_analysis
            self.min_confidence_for_insight = config.min_confidence_for_insight
            self.include_executive_summary = config.include_executive_summary
            self.include_detailed_analysis = config.include_detailed_analysis
            self.include_action_items = config.include_action_items
            self.report_format = config.report_format
        else:
            # Extract config dict if provided
            config_dict = kwargs.get('config', {})
            
            # Environment variable fallback with priority: kwargs > config dict > env vars > defaults
            self.enable_trend_analysis = kwargs.get('enable_trend_analysis', 
                                                   config_dict.get('enable_trend_analysis', 
                                                                  os.getenv('REFINIRE_RAG_INSIGHT_TREND_ANALYSIS', 'true').lower() == 'true'))
            self.enable_comparative_analysis = kwargs.get('enable_comparative_analysis', 
                                                         config_dict.get('enable_comparative_analysis', 
                                                                        os.getenv('REFINIRE_RAG_INSIGHT_COMPARATIVE_ANALYSIS', 'true').lower() == 'true'))
            self.enable_root_cause_analysis = kwargs.get('enable_root_cause_analysis', 
                                                        config_dict.get('enable_root_cause_analysis', 
                                                                       os.getenv('REFINIRE_RAG_INSIGHT_ROOT_CAUSE', 'true').lower() == 'true'))
            self.min_confidence_for_insight = kwargs.get('min_confidence_for_insight', 
                                                        config_dict.get('min_confidence_for_insight', 
                                                                       float(os.getenv('REFINIRE_RAG_INSIGHT_MIN_CONFIDENCE', '0.6'))))
            self.include_executive_summary = kwargs.get('include_executive_summary', 
                                                       config_dict.get('include_executive_summary', 
                                                                      os.getenv('REFINIRE_RAG_INSIGHT_EXECUTIVE_SUMMARY', 'true').lower() == 'true'))
            self.include_detailed_analysis = kwargs.get('include_detailed_analysis', 
                                                       config_dict.get('include_detailed_analysis', 
                                                                      os.getenv('REFINIRE_RAG_INSIGHT_DETAILED_ANALYSIS', 'true').lower() == 'true'))
            self.include_action_items = kwargs.get('include_action_items', 
                                                  config_dict.get('include_action_items', 
                                                                 os.getenv('REFINIRE_RAG_INSIGHT_ACTION_ITEMS', 'true').lower() == 'true'))
            self.report_format = kwargs.get('report_format', 
                                           config_dict.get('report_format', 
                                                          os.getenv('REFINIRE_RAG_INSIGHT_REPORT_FORMAT', 'markdown')))
            
            # Create config object for backward compatibility
            config = InsightReporterConfig(
                enable_trend_analysis=self.enable_trend_analysis,
                enable_comparative_analysis=self.enable_comparative_analysis,
                enable_root_cause_analysis=self.enable_root_cause_analysis,
                min_confidence_for_insight=self.min_confidence_for_insight,
                include_executive_summary=self.include_executive_summary,
                include_detailed_analysis=self.include_detailed_analysis,
                include_action_items=self.include_action_items,
                report_format=self.report_format
            )
            
            super().__init__(config)
        
        self.generated_insights: List[Insight] = []
        self.all_thresholds = self._initialize_thresholds()
        self.historical_data: List[Dict[str, Any]] = []
    
    def get_config(self) -> Dict[str, Any]:
        """Get current configuration as dictionary
        ç¾åœ¨ã®è¨­å®šã‚’è¾æ›¸ã¨ã—ã¦å–å¾—
        
        Returns:
            Dict[str, Any]: Current configuration dictionary
        """
        return {
            'enable_trend_analysis': self.enable_trend_analysis,
            'enable_comparative_analysis': self.enable_comparative_analysis,
            'enable_root_cause_analysis': self.enable_root_cause_analysis,
            'min_confidence_for_insight': self.min_confidence_for_insight,
            'include_executive_summary': self.include_executive_summary,
            'include_detailed_analysis': self.include_detailed_analysis,
            'include_action_items': self.include_action_items,
            'report_format': self.report_format
        }
    
    @classmethod
    def get_config_class(cls) -> Type[InsightReporterConfig]:
        """Get the configuration class for this processor (backward compatibility)
        ã“ã®ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã®è¨­å®šã‚¯ãƒ©ã‚¹ã‚’å–å¾—ï¼ˆä¸‹ä½äº’æ›æ€§ï¼‰
        """
        return InsightReporterConfig
    
    def process(self, document: Document) -> List[Document]:
        """
        è©•ä¾¡çµæœãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ã‚¤ãƒ³ã‚µã‚¤ãƒˆã‚’ç”Ÿæˆ
        
        Args:
            document: è©•ä¾¡çµæœãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
            
        Returns:
            List[Document]: ã‚¤ãƒ³ã‚µã‚¤ãƒˆãƒ¬ãƒãƒ¼ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
        """
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè©•ä¾¡çµæœã‹ãƒã‚§ãƒƒã‚¯
        if not self._is_evaluation_document(document):
            return [document]  # è©•ä¾¡çµæœã§ãªã‘ã‚Œã°ãã®ã¾ã¾é€šã™
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡º
        metrics = self._extract_metrics(document)
        
        # å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ 
        self.historical_data.append({
            "timestamp": datetime.now().isoformat(),
            "document_id": document.id,
            "metrics": metrics
        })
        
        # ã‚¤ãƒ³ã‚µã‚¤ãƒˆã‚’ç”Ÿæˆ
        insights = self._generate_insights(metrics, document)
        self.generated_insights.extend(insights)
        
        # ã‚¤ãƒ³ã‚µã‚¤ãƒˆãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆ
        report_doc = Document(
            id=f"insight_report_{document.id}",
            content=self._format_insight_report(insights, metrics),
            metadata={
                "processing_stage": "insight_reporting",
                "source_document_id": document.id,
                "insights_generated": len(insights),
                "critical_insights": len([i for i in insights if i.severity == "critical"]),
                "overall_health_score": self._compute_health_score(metrics),
                "recommendation_count": sum(len(i.recommendations) for i in insights)
            }
        )
        
        return [report_doc]
    
    def _initialize_thresholds(self) -> List[Threshold]:
        """é–¾å€¤è¨­å®šã‚’åˆæœŸåŒ–"""
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®é–¾å€¤ã‚’ä½œæˆ
        thresholds = [
            Threshold(
                metric_name="accuracy",
                critical_threshold=0.5,
                warning_threshold=0.7,
                target_threshold=0.9,
                comparison_operator="greater_than"
            ),
            Threshold(
                metric_name="response_time",
                critical_threshold=5.0,
                warning_threshold=2.0,
                target_threshold=1.0,
                comparison_operator="less_than"
            ),
            Threshold(
                metric_name="confidence",
                critical_threshold=0.3,
                warning_threshold=0.5,
                target_threshold=0.8,
                comparison_operator="greater_than"
            )
        ]
        
        return thresholds
    
    def _is_evaluation_document(self, document: Document) -> bool:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè©•ä¾¡çµæœã‹ãƒã‚§ãƒƒã‚¯"""
        
        processing_stage = document.metadata.get("processing_stage", "")
        return processing_stage in ["evaluation", "test_results", "contradiction_detection"]
    
    def _extract_metrics(self, document: Document) -> Dict[str, float]:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡º"""
        
        metrics = {}
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡º
        metadata = document.metadata
        
        # ä¸€èˆ¬çš„ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹
        if "overall_score" in metadata:
            metrics["overall_score"] = float(metadata["overall_score"])
        
        if "success_rate" in metadata:
            metrics["accuracy"] = float(metadata["success_rate"])
        
        if "processing_time" in metadata:
            metrics["response_time"] = float(metadata["processing_time"])
        
        if "average_confidence" in metadata:
            metrics["confidence"] = float(metadata["average_confidence"])
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è§£æ
        content = document.content
        content_metrics = self._parse_metrics_from_content(content)
        metrics.update(content_metrics)
        
        return metrics
    
    def _parse_metrics_from_content(self, content: str) -> Dict[str, float]:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è§£æï¼ˆJSONå¯¾å¿œç‰ˆï¼‰"""
        
        metrics = {}
        
        # JSONå½¢å¼ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å‡¦ç†
        if content.strip().startswith('{'):
            try:
                import json
                data = json.loads(content)
                
                # evaluation_summaryã‹ã‚‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡º
                if "evaluation_summary" in data:
                    summary = data["evaluation_summary"]
                    
                    # æˆåŠŸç‡ â†’ ç²¾åº¦
                    if "success_rate" in summary:
                        metrics["accuracy"] = float(summary["success_rate"])
                    
                    # å¹³å‡ä¿¡é ¼åº¦ â†’ ä¿¡é ¼åº¦
                    if "average_confidence" in summary:
                        metrics["confidence"] = float(summary["average_confidence"])
                    
                    # å¹³å‡å‡¦ç†æ™‚é–“ â†’ å¿œç­”æ™‚é–“
                    if "average_processing_time" in summary:
                        metrics["response_time"] = float(summary["average_processing_time"])
                    
                    # ãã®ä»–ã®æŒ‡æ¨™
                    if "total_queries" in summary:
                        metrics["total_queries"] = float(summary["total_queries"])
                    
                    if "passed_queries" in summary:
                        metrics["passed_queries"] = float(summary["passed_queries"])
                
                # ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã®æŒ‡æ¨™ã‚‚ç¢ºèª
                for key, value in data.items():
                    if isinstance(value, (int, float)):
                        if key == "success_rate":
                            metrics["accuracy"] = float(value)
                        elif key == "average_confidence":
                            metrics["confidence"] = float(value) 
                        elif key == "average_processing_time":
                            metrics["response_time"] = float(value)
                        elif key in ["overall_score", "f1_score", "consistency"]:
                            metrics[key] = float(value)
                
                return metrics
                
            except json.JSONDecodeError:
                # JSONã§ãªã„å ´åˆã¯å¾“æ¥ã®ãƒ†ã‚­ã‚¹ãƒˆè§£æã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                pass
        
        # å¾“æ¥ã®ãƒ†ã‚­ã‚¹ãƒˆè§£æ
        lines = content.split('\n')
        
        for line in lines:
            line = line.strip()
            
            # ç²¾åº¦æƒ…å ±
            if "ç²¾åº¦" in line and ":" in line:
                try:
                    value_str = line.split(':')[1].replace('%', '').strip()
                    metrics["accuracy"] = float(value_str) / 100 if '%' in line else float(value_str)
                except:
                    pass
            
            # F1ã‚¹ã‚³ã‚¢
            elif "F1ã‚¹ã‚³ã‚¢" in line and ":" in line:
                try:
                    value_str = line.split(':')[1].strip()
                    metrics["f1_score"] = float(value_str)
                except:
                    pass
            
            # å¿œç­”æ™‚é–“
            elif "å¿œç­”æ™‚é–“" in line and ":" in line:
                try:
                    value_str = line.split(':')[1].replace('ç§’', '').strip()
                    metrics["response_time"] = float(value_str)
                except:
                    pass
            
            # ä¿¡é ¼åº¦
            elif "ä¿¡é ¼åº¦" in line and ":" in line:
                try:
                    value_str = line.split(':')[1].strip()
                    metrics["confidence"] = float(value_str)
                except:
                    pass
            
            # ä¸€è²«æ€§ã‚¹ã‚³ã‚¢
            elif "ä¸€è²«æ€§" in line and ":" in line:
                try:
                    value_str = line.split(':')[1].replace('%', '').strip()
                    metrics["consistency"] = float(value_str) / 100 if '%' in line else float(value_str)
                except:
                    pass
        
        return metrics
    
    def _generate_insights(self, metrics: Dict[str, float], document: Document) -> List[Insight]:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰ã‚¤ãƒ³ã‚µã‚¤ãƒˆã‚’ç”Ÿæˆ"""
        
        insights = []
        
        # é–¾å€¤ãƒ™ãƒ¼ã‚¹ã®ã‚¤ãƒ³ã‚µã‚¤ãƒˆç”Ÿæˆ
        threshold_insights = self._generate_threshold_insights(metrics)
        insights.extend(threshold_insights)
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã‚¤ãƒ³ã‚µã‚¤ãƒˆ
        if self.config.enable_trend_analysis and len(self.historical_data) > 1:
            trend_insights = self._generate_trend_insights(metrics)
            insights.extend(trend_insights)
        
        # æ¯”è¼ƒåˆ†æã‚¤ãƒ³ã‚µã‚¤ãƒˆ
        if self.config.enable_comparative_analysis:
            comparative_insights = self._generate_comparative_insights(metrics)
            insights.extend(comparative_insights)
        
        # æ ¹æœ¬åŸå› åˆ†æã‚¤ãƒ³ã‚µã‚¤ãƒˆ
        if self.config.enable_root_cause_analysis:
            root_cause_insights = self._generate_root_cause_insights(metrics, document)
            insights.extend(root_cause_insights)
        
        # ä¿¡é ¼åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        filtered_insights = [i for i in insights if i.confidence >= self.config.min_confidence_for_insight]
        
        return filtered_insights
    
    def _generate_threshold_insights(self, metrics: Dict[str, float]) -> List[Insight]:
        """é–¾å€¤ãƒ™ãƒ¼ã‚¹ã®ã‚¤ãƒ³ã‚µã‚¤ãƒˆã‚’ç”Ÿæˆ"""
        
        insights = []
        
        for threshold in self.all_thresholds:
            metric_name = threshold.metric_name
            if metric_name not in metrics:
                continue
            
            metric_value = metrics[metric_name]
            insight = self._evaluate_threshold(metric_name, metric_value, threshold)
            
            if insight:
                insights.append(insight)
        
        return insights
    
    def _evaluate_threshold(self, metric_name: str, value: float, threshold: Threshold) -> Optional[Insight]:
        """å€‹åˆ¥é–¾å€¤ã®è©•ä¾¡"""
        
        severity = "info"
        title = ""
        description = ""
        recommendations = []
        
        is_greater_than = threshold.comparison_operator == "greater_than"
        
        # é‡è¦åº¦åˆ¤å®š
        if threshold.critical_threshold is not None:
            if is_greater_than:
                if value < threshold.critical_threshold:
                    severity = "critical"
            else:
                if value > threshold.critical_threshold:
                    severity = "critical"
        
        if severity != "critical" and threshold.warning_threshold is not None:
            if is_greater_than:
                if value < threshold.warning_threshold:
                    severity = "high"
            else:
                if value > threshold.warning_threshold:
                    severity = "high"
        
        if severity not in ["critical", "high"] and threshold.target_threshold is not None:
            if is_greater_than:
                if value < threshold.target_threshold:
                    severity = "medium"
                else:
                    severity = "low"  # ç›®æ¨™é”æˆ
            else:
                if value > threshold.target_threshold:
                    severity = "medium"
                else:
                    severity = "low"  # ç›®æ¨™é”æˆ
        
        # ã‚¤ãƒ³ã‚µã‚¤ãƒˆå†…å®¹ã®ç”Ÿæˆ
        if severity == "critical":
            title = f"{metric_name}ãŒè‡¨ç•Œãƒ¬ãƒ™ãƒ«ã«é”ã—ã¦ã„ã¾ã™"
            description = f"{metric_name}ã®å€¤({value:.3f})ãŒè‡¨ç•Œé–¾å€¤ã‚’ä¸‹å›ã£ã¦ãŠã‚Šã€ç·Šæ€¥å¯¾å¿œãŒå¿…è¦ã§ã™ã€‚"
            recommendations = [
                "ç·Šæ€¥å¯¾å¿œãƒãƒ¼ãƒ ã‚’æ‹›é›†ã—ã¦ãã ã•ã„",
                "ã‚·ã‚¹ãƒ†ãƒ ã®æ ¹æœ¬çš„ãªè¦‹ç›´ã—ã‚’å®Ÿæ–½ã—ã¦ãã ã•ã„",
                "ä¸€æ™‚çš„ã«ã‚µãƒ¼ãƒ“ã‚¹ã‚’åœæ­¢ã™ã‚‹ã“ã¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„"
            ]
        elif severity == "high":
            title = f"{metric_name}ã«é‡è¦ãªå•é¡ŒãŒã‚ã‚Šã¾ã™"
            description = f"{metric_name}ã®å€¤({value:.3f})ãŒè­¦å‘Šãƒ¬ãƒ™ãƒ«ã«ã‚ã‚Šã€æ—©æ€¥ãªæ”¹å–„ãŒå¿…è¦ã§ã™ã€‚"
            recommendations = [
                "æ”¹å–„è¨ˆç”»ã‚’ç«‹æ¡ˆã—ã¦ãã ã•ã„",
                "é–¢é€£ã™ã‚‹è¨­å®šã‚„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¦‹ç›´ã—ã¦ãã ã•ã„",
                "ç›£è¦–ã‚’å¼·åŒ–ã—ã¦ãã ã•ã„"
            ]
        elif severity == "medium":
            title = f"{metric_name}ã®æ”¹å–„ä½™åœ°ãŒã‚ã‚Šã¾ã™"
            description = f"{metric_name}ã®å€¤({value:.3f})ã¯è¨±å®¹ç¯„å›²å†…ã§ã™ãŒã€ç›®æ¨™å€¤ã«åˆ°é”ã—ã¦ã„ã¾ã›ã‚“ã€‚"
            recommendations = [
                "æœ€é©åŒ–ã®æ©Ÿä¼šã‚’æ¢ã—ã¦ãã ã•ã„",
                "ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚’é©ç”¨ã—ã¦ãã ã•ã„"
            ]
        elif severity == "low":
            title = f"{metric_name}ã¯è‰¯å¥½ã§ã™"
            description = f"{metric_name}ã®å€¤({value:.3f})ã¯ç›®æ¨™ã‚’é”æˆã—ã¦ã„ã¾ã™ã€‚"
            recommendations = [
                "ç¾åœ¨ã®è¨­å®šã‚’ç¶­æŒã—ã¦ãã ã•ã„",
                "ä»–ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æ”¹å–„ã«é›†ä¸­ã—ã¦ãã ã•ã„"
            ]
        
        if severity in ["critical", "high", "medium"]:  # info ãƒ¬ãƒ™ãƒ«ã¯ã‚¹ã‚­ãƒƒãƒ—
            insight = Insight(
                id=f"threshold_{metric_name}_{severity}",
                insight_type=self._classify_insight_type(metric_name),
                title=title,
                description=description,
                severity=severity,
                confidence=0.9,  # é–¾å€¤ãƒ™ãƒ¼ã‚¹ã¯é«˜ä¿¡é ¼åº¦
                affected_metrics=[metric_name],
                recommendations=recommendations,
                supporting_data={
                    "current_value": value,
                    "threshold_config": threshold.dict(),
                    "target_met": severity == "low"
                }
            )
            return insight
        
        return None
    
    def _classify_insight_type(self, metric_name: str) -> InsightType:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹åã‹ã‚‰ã‚¤ãƒ³ã‚µã‚¤ãƒˆã‚¿ã‚¤ãƒ—ã‚’åˆ†é¡"""
        
        type_mapping = {
            "accuracy": InsightType.QUALITY,
            "precision": InsightType.QUALITY,
            "recall": InsightType.QUALITY,
            "f1_score": InsightType.QUALITY,
            "response_time": InsightType.PERFORMANCE,
            "processing_time": InsightType.EFFICIENCY,
            "confidence": InsightType.RELIABILITY,
            "consistency": InsightType.CONSISTENCY,
            "throughput": InsightType.SCALABILITY,
            "error_rate": InsightType.RELIABILITY
        }
        
        return type_mapping.get(metric_name, InsightType.PERFORMANCE)
    
    def _generate_trend_insights(self, current_metrics: Dict[str, float]) -> List[Insight]:
        """ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã‚¤ãƒ³ã‚µã‚¤ãƒˆã‚’ç”Ÿæˆ"""
        
        insights = []
        
        if len(self.historical_data) < 2:
            return insights
        
        # éå»ã®ãƒ‡ãƒ¼ã‚¿ã¨æ¯”è¼ƒ
        prev_metrics = self.historical_data[-2]["metrics"]
        
        for metric_name, current_value in current_metrics.items():
            if metric_name in prev_metrics:
                prev_value = prev_metrics[metric_name]
                change_rate = (current_value - prev_value) / prev_value if prev_value != 0 else 0
                
                # æœ‰æ„ãªå¤‰åŒ–ãŒã‚ã£ãŸå ´åˆã®ã¿ã‚¤ãƒ³ã‚µã‚¤ãƒˆç”Ÿæˆ
                if abs(change_rate) > 0.1:  # 10%ä»¥ä¸Šã®å¤‰åŒ–
                    insight = self._create_trend_insight(metric_name, current_value, prev_value, change_rate)
                    if insight:
                        insights.append(insight)
        
        return insights
    
    def _create_trend_insight(self, metric_name: str, current: float, previous: float, change_rate: float) -> Optional[Insight]:
        """ãƒˆãƒ¬ãƒ³ãƒ‰ã‚¤ãƒ³ã‚µã‚¤ãƒˆã‚’ä½œæˆ"""
        
        is_improvement = self._is_improvement(metric_name, change_rate)
        direction = "æ”¹å–„" if is_improvement else "æ‚ªåŒ–"
        severity = "low" if is_improvement else ("high" if abs(change_rate) > 0.3 else "medium")
        
        title = f"{metric_name}ãŒ{direction}ã—ã¦ã„ã¾ã™"
        description = f"{metric_name}ãŒå‰å›ã®{previous:.3f}ã‹ã‚‰{current:.3f}ã«å¤‰åŒ–ã—ã¾ã—ãŸï¼ˆ{change_rate:+.1%}ï¼‰ã€‚"
        
        recommendations = []
        if is_improvement:
            recommendations = [
                "è‰¯å¥½ãªå‚¾å‘ã‚’ç¶­æŒã—ã¦ãã ã•ã„",
                "æˆåŠŸè¦å› ã‚’åˆ†æã—ã¦ä»–ã®é ˜åŸŸã«å¿œç”¨ã—ã¦ãã ã•ã„"
            ]
        else:
            recommendations = [
                "æ‚ªåŒ–ã®åŸå› ã‚’ç‰¹å®šã—ã¦ãã ã•ã„",
                "å¯¾ç­–ã‚’è¬›ã˜ã¦æ”¹å–„ã‚’å›³ã£ã¦ãã ã•ã„",
                "ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚’å¼·åŒ–ã—ã¦ãã ã•ã„"
            ]
        
        insight = Insight(
            id=f"trend_{metric_name}_{direction}",
            insight_type=self._classify_insight_type(metric_name),
            title=title,
            description=description,
            severity=severity,
            confidence=0.8,
            affected_metrics=[metric_name],
            recommendations=recommendations,
            supporting_data={
                "current_value": current,
                "previous_value": previous,
                "change_rate": change_rate,
                "is_improvement": is_improvement
            }
        )
        
        return insight
    
    def _is_improvement(self, metric_name: str, change_rate: float) -> bool:
        """å¤‰åŒ–ãŒæ”¹å–„ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        
        # å€¤ãŒå¤§ãã„æ–¹ãŒè‰¯ã„ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        higher_is_better = ["accuracy", "precision", "recall", "f1_score", "confidence", "consistency", "throughput"]
        
        # å€¤ãŒå°ã•ã„æ–¹ãŒè‰¯ã„ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        lower_is_better = ["response_time", "processing_time", "error_rate"]
        
        if metric_name in higher_is_better:
            return change_rate > 0
        elif metric_name in lower_is_better:
            return change_rate < 0
        else:
            return change_rate > 0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯å¤§ãã„æ–¹ãŒè‰¯ã„
    
    def _generate_comparative_insights(self, metrics: Dict[str, float]) -> List[Insight]:
        """æ¯”è¼ƒåˆ†æã‚¤ãƒ³ã‚µã‚¤ãƒˆã‚’ç”Ÿæˆ"""
        
        insights = []
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹é–“ã®ç›¸é–¢åˆ†æ
        if "accuracy" in metrics and "confidence" in metrics:
            accuracy = metrics["accuracy"]
            confidence = metrics["confidence"]
            
            # ç²¾åº¦ã¨ä¿¡é ¼åº¦ã®ä¸ä¸€è‡´ã‚’æ¤œå‡º
            if abs(accuracy - confidence) > 0.3:
                insight = Insight(
                    id="accuracy_confidence_mismatch",
                    insight_type=InsightType.RELIABILITY,
                    title="ç²¾åº¦ã¨ä¿¡é ¼åº¦ã«å¤§ããªå·®ãŒã‚ã‚Šã¾ã™",
                    description=f"ç²¾åº¦({accuracy:.3f})ã¨ä¿¡é ¼åº¦({confidence:.3f})ã®é–“ã«å¤§ããªå·®ãŒã‚ã‚Šã¾ã™ã€‚",
                    severity="medium",
                    confidence=0.7,
                    affected_metrics=["accuracy", "confidence"],
                    recommendations=[
                        "ä¿¡é ¼åº¦æ ¡æ­£ã®è¦‹ç›´ã—ã‚’æ¤œè¨ã—ã¦ãã ã•ã„",
                        "ãƒ¢ãƒ‡ãƒ«ã®ä¸ç¢ºå®Ÿæ€§æ¨å®šã‚’æ”¹å–„ã—ã¦ãã ã•ã„"
                    ],
                    supporting_data={
                        "accuracy": accuracy,
                        "confidence": confidence,
                        "difference": abs(accuracy - confidence)
                    }
                )
                insights.append(insight)
        
        # æ€§èƒ½ã¨å“è³ªã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•åˆ†æ
        if "response_time" in metrics and "accuracy" in metrics:
            response_time = metrics["response_time"]
            accuracy = metrics["accuracy"]
            
            # é«˜ç²¾åº¦ã ãŒé…ã„å ´åˆ
            if accuracy > 0.8 and response_time > 2.0:
                insight = Insight(
                    id="accuracy_speed_tradeoff",
                    insight_type=InsightType.EFFICIENCY,
                    title="é«˜ç²¾åº¦ã§ã™ãŒå¿œç­”é€Ÿåº¦ãŒé…ã„ã§ã™",
                    description="é«˜ã„ç²¾åº¦ã‚’ç¶­æŒã—ã¦ã„ã¾ã™ãŒã€å¿œç­”æ™‚é–“ãŒç›®æ¨™ã‚’ä¸Šå›ã£ã¦ã„ã¾ã™ã€‚",
                    severity="medium",
                    confidence=0.8,
                    affected_metrics=["accuracy", "response_time"],
                    recommendations=[
                        "ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®å°å…¥ã‚’æ¤œè¨ã—ã¦ãã ã•ã„",
                        "ãƒ¢ãƒ‡ãƒ«ã®è»½é‡åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„",
                        "ä¸¦åˆ—å‡¦ç†ã®æœ€é©åŒ–ã‚’è¡Œã£ã¦ãã ã•ã„"
                    ]
                )
                insights.append(insight)
        
        return insights
    
    def _generate_root_cause_insights(self, metrics: Dict[str, float], document: Document) -> List[Insight]:
        """æ ¹æœ¬åŸå› åˆ†æã‚¤ãƒ³ã‚µã‚¤ãƒˆã‚’ç”Ÿæˆ"""
        
        insights = []
        
        # ä½æ€§èƒ½ã®æ ¹æœ¬åŸå› ã‚’åˆ†æ
        if metrics.get("accuracy", 1.0) < 0.6:
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰åŸå› ã‚’æ¨æ¸¬
            metadata = document.metadata
            
            potential_causes = []
            
            # ãƒ‡ãƒ¼ã‚¿å“è³ªå•é¡Œ
            if metadata.get("contradictions_found", 0) > 0:
                potential_causes.append("ãƒ‡ãƒ¼ã‚¿ã®çŸ›ç›¾ãŒæ¤œå‡ºã•ã‚Œã¦ã„ã¾ã™")
            
            # å‡¦ç†å•é¡Œ
            if metadata.get("errors_encountered", 0) > 0:
                potential_causes.append("å‡¦ç†ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã„ã¾ã™")
            
            # ã‚«ãƒãƒ¬ãƒƒã‚¸å•é¡Œ
            if metrics.get("coverage", 1.0) < 0.5:
                potential_causes.append("ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ãŒä¸ååˆ†ã§ã™")
            
            if potential_causes:
                insight = Insight(
                    id="low_performance_root_cause",
                    insight_type=InsightType.QUALITY,
                    title="ä½æ€§èƒ½ã®æ ¹æœ¬åŸå› ãŒç‰¹å®šã•ã‚Œã¾ã—ãŸ",
                    description="ã‚·ã‚¹ãƒ†ãƒ ã®ä½æ€§èƒ½ã«ã¤ã„ã¦ã€ä»¥ä¸‹ã®æ ¹æœ¬åŸå› ãŒè€ƒãˆã‚‰ã‚Œã¾ã™ã€‚",
                    severity="high",
                    confidence=0.7,
                    affected_metrics=["accuracy"],
                    recommendations=[
                        "ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿæ–½ã—ã¦ãã ã•ã„",
                        "å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’è¦‹ç›´ã—ã¦ãã ã•ã„",
                        "ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æ‹¡å……ã—ã¦ãã ã•ã„"
                    ],
                    supporting_data={
                        "potential_causes": potential_causes,
                        "evidence": metadata
                    }
                )
                insights.append(insight)
        
        return insights
    
    def _compute_health_score(self, metrics: Dict[str, float]) -> float:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ˜ãƒ«ã‚¹ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        
        if not metrics:
            return 0.0
        
        # é‡è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®é‡ã¿
        weights = {
            "accuracy": 0.3,
            "response_time": 0.2,
            "confidence": 0.2,
            "consistency": 0.15,
            "f1_score": 0.15
        }
        
        score = 0.0
        total_weight = 0.0
        
        for metric_name, weight in weights.items():
            if metric_name in metrics:
                value = metrics[metric_name]
                
                # æ­£è¦åŒ–ï¼ˆ0-1ã®ç¯„å›²ã«ï¼‰
                if metric_name == "response_time":
                    # å¿œç­”æ™‚é–“ã¯å°ã•ã„æ–¹ãŒè‰¯ã„
                    normalized_value = max(0.0, 1.0 - min(value / 5.0, 1.0))
                else:
                    # ãã®ä»–ã¯å¤§ãã„æ–¹ãŒè‰¯ã„
                    normalized_value = min(value, 1.0)
                
                score += normalized_value * weight
                total_weight += weight
        
        return score / total_weight if total_weight > 0 else 0.0
    
    def _format_insight_report(self, insights: List[Insight], metrics: Dict[str, float]) -> str:
        """ã‚¤ãƒ³ã‚µã‚¤ãƒˆãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        
        if self.config.report_format == "json":
            return self._format_json_report(insights, metrics)
        else:
            return self._format_markdown_report(insights, metrics)
    
    def _format_markdown_report(self, insights: List[Insight], metrics: Dict[str, float]) -> str:
        """Markdownå½¢å¼ã®ã‚¤ãƒ³ã‚µã‚¤ãƒˆãƒ¬ãƒãƒ¼ãƒˆ"""
        
        lines = ["# ã‚·ã‚¹ãƒ†ãƒ ã‚¤ãƒ³ã‚µã‚¤ãƒˆãƒ¬ãƒãƒ¼ãƒˆ\n"]
        
        # ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼
        if self.config.include_executive_summary:
            lines.append("## ğŸ“‹ ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼")
            
            health_score = self._compute_health_score(metrics)
            lines.append(f"**ã‚·ã‚¹ãƒ†ãƒ ãƒ˜ãƒ«ã‚¹ã‚¹ã‚³ã‚¢**: {health_score:.2f}/1.00")
            
            if health_score >= 0.8:
                lines.append("ğŸŸ¢ **çŠ¶æ…‹**: å¥å…¨ - ã‚·ã‚¹ãƒ†ãƒ ã¯è‰¯å¥½ã«å‹•ä½œã—ã¦ã„ã¾ã™")
            elif health_score >= 0.6:
                lines.append("ğŸŸ¡ **çŠ¶æ…‹**: æ³¨æ„ - ã„ãã¤ã‹ã®æ”¹å–„é ˜åŸŸãŒã‚ã‚Šã¾ã™")
            elif health_score >= 0.4:
                lines.append("ğŸŸ  **çŠ¶æ…‹**: è­¦å‘Š - é‡è¦ãªå•é¡ŒãŒã‚ã‚Šã¾ã™")
            else:
                lines.append("ğŸ”´ **çŠ¶æ…‹**: å±é™º - ç·Šæ€¥å¯¾å¿œãŒå¿…è¦ã§ã™")
            
            # é‡è¦ã‚¤ãƒ³ã‚µã‚¤ãƒˆã®ã‚µãƒãƒªãƒ¼
            critical_insights = [i for i in insights if i.severity == "critical"]
            high_insights = [i for i in insights if i.severity == "high"]
            
            if critical_insights:
                lines.append(f"âš ï¸ **ç·Šæ€¥å¯¾å¿œãŒå¿…è¦**: {len(critical_insights)}ä»¶ã®è‡¨ç•Œçš„å•é¡Œ")
            if high_insights:
                lines.append(f"ğŸ”¥ **æ—©æ€¥ãªå¯¾å¿œãŒå¿…è¦**: {len(high_insights)}ä»¶ã®é‡è¦å•é¡Œ")
            
            lines.append("")
        
        # ã‚­ãƒ¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        lines.append("## ğŸ“Š ã‚­ãƒ¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹")
        for metric_name, value in metrics.items():
            if metric_name == "response_time":
                lines.append(f"- **{metric_name}**: {value:.3f}ç§’")
            elif "rate" in metric_name or "accuracy" in metric_name:
                lines.append(f"- **{metric_name}**: {value:.1%}")
            else:
                lines.append(f"- **{metric_name}**: {value:.3f}")
        lines.append("")
        
        # ã‚¤ãƒ³ã‚µã‚¤ãƒˆåˆ¥ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        if insights:
            lines.append("## ğŸ” ã‚¤ãƒ³ã‚µã‚¤ãƒˆ")
            
            # é‡è¦åº¦åˆ¥ã«åˆ†é¡
            severity_order = ["critical", "high", "medium", "low"]
            severity_icons = {
                "critical": "ğŸš¨",
                "high": "âš ï¸",
                "medium": "ğŸ“ˆ",
                "low": "âœ…"
            }
            
            for severity in severity_order:
                severity_insights = [i for i in insights if i.severity == severity]
                
                if severity_insights:
                    lines.append(f"### {severity_icons[severity]} {severity.upper()} é‡è¦åº¦")
                    
                    for insight in severity_insights:
                        lines.append(f"#### {insight.title}")
                        lines.append(f"**ã‚«ãƒ†ã‚´ãƒª**: {insight.insight_type.value}")
                        lines.append(f"**ä¿¡é ¼åº¦**: {insight.confidence:.1%}")
                        lines.append(f"**èª¬æ˜**: {insight.description}")
                        
                        if insight.recommendations:
                            lines.append("**æ¨å¥¨äº‹é …**:")
                            for rec in insight.recommendations:
                                lines.append(f"- {rec}")
                        
                        lines.append("")
        
        # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ 
        if self.config.include_action_items and insights:
            lines.append("## ğŸ¯ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ")
            
            # å„ªå…ˆåº¦é †ã«ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ã‚’æ•´ç†
            critical_actions = []
            high_actions = []
            medium_actions = []
            
            for insight in insights:
                for rec in insight.recommendations:
                    action_item = f"{rec} (é–¢é€£: {insight.title})"
                    
                    if insight.severity == "critical":
                        critical_actions.append(action_item)
                    elif insight.severity == "high":
                        high_actions.append(action_item)
                    else:
                        medium_actions.append(action_item)
            
            if critical_actions:
                lines.append("### ğŸš¨ ç·Šæ€¥å¯¾å¿œï¼ˆ24æ™‚é–“ä»¥å†…ï¼‰")
                for action in critical_actions[:5]:  # ä¸Šä½5ä»¶
                    lines.append(f"1. {action}")
                lines.append("")
            
            if high_actions:
                lines.append("### âš ï¸ é‡è¦å¯¾å¿œï¼ˆ1é€±é–“ä»¥å†…ï¼‰")
                for action in high_actions[:5]:  # ä¸Šä½5ä»¶
                    lines.append(f"1. {action}")
                lines.append("")
            
            if medium_actions:
                lines.append("### ğŸ“ˆ æ”¹å–„æ©Ÿä¼šï¼ˆ1ã‹æœˆä»¥å†…ï¼‰")
                for action in medium_actions[:5]:  # ä¸Šä½5ä»¶
                    lines.append(f"1. {action}")
        
        return "\n".join(lines)
    
    def _format_json_report(self, insights: List[Insight], metrics: Dict[str, float]) -> str:
        """JSONå½¢å¼ã®ã‚¤ãƒ³ã‚µã‚¤ãƒˆãƒ¬ãƒãƒ¼ãƒˆ"""
        
        report = {
            "timestamp": datetime.now().isoformat(),
            "health_score": self._compute_health_score(metrics),
            "metrics": metrics,
            "insights": [insight.dict() for insight in insights],
            "summary": {
                "total_insights": len(insights),
                "critical_count": len([i for i in insights if i.severity == "critical"]),
                "high_count": len([i for i in insights if i.severity == "high"]),
                "medium_count": len([i for i in insights if i.severity == "medium"]),
                "low_count": len([i for i in insights if i.severity == "low"])
            }
        }
        
        return json.dumps(report, ensure_ascii=False, indent=2)
    
    def get_insight_summary(self) -> Dict[str, Any]:
        """ã‚¤ãƒ³ã‚µã‚¤ãƒˆã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
        
        if not self.generated_insights:
            return {"message": "No insights generated yet"}
        
        severity_counts = {}
        type_counts = {}
        
        for insight in self.generated_insights:
            # é‡è¦åº¦åˆ¥ã‚«ã‚¦ãƒ³ãƒˆ
            severity = insight.severity
            severity_counts[severity] = severity_counts.get(severity, 0) + 1
            
            # ã‚¿ã‚¤ãƒ—åˆ¥ã‚«ã‚¦ãƒ³ãƒˆ
            insight_type = insight.insight_type.value
            type_counts[insight_type] = type_counts.get(insight_type, 0) + 1
        
        avg_confidence = statistics.mean([i.confidence for i in self.generated_insights])
        
        return {
            "total_insights": len(self.generated_insights),
            "severity_distribution": severity_counts,
            "type_distribution": type_counts,
            "average_confidence": avg_confidence,
            "recommendations_count": sum(len(i.recommendations) for i in self.generated_insights)
        }