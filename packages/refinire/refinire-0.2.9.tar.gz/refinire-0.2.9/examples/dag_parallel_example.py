#!/usr/bin/env python3
"""
DAG Parallel Processing Example
DAGä¸¦åˆ—å‡¦ç†ã®ä¾‹

This example demonstrates how to use Refinire's DAG parallel processing capabilities
to execute multiple analysis tasks simultaneously and efficiently aggregate results.
ã“ã®ä¾‹ã§ã¯ã€Refinireã®DAGä¸¦åˆ—å‡¦ç†æ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¦è¤‡æ•°ã®åˆ†æã‚¿ã‚¹ã‚¯ã‚’åŒæ™‚å®Ÿè¡Œã—ã€
åŠ¹ç‡çš„ã«çµæœã‚’çµ±åˆã™ã‚‹æ–¹æ³•ã‚’ç¤ºã—ã¾ã™ã€‚
"""

import asyncio
import time
from refinire.flow import Flow, FunctionStep, Context


def create_text_analysis_flow():
    """
    Create a text analysis flow with parallel processing
    ä¸¦åˆ—å‡¦ç†ã‚’ä½¿ç”¨ã—ãŸãƒ†ã‚­ã‚¹ãƒˆåˆ†æãƒ•ãƒ­ãƒ¼ã‚’ä½œæˆ
    """
    
    def preprocess_text(input_data, ctx):
        """
        Preprocess input text
        å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’å‰å‡¦ç†
        """
        print(f"ğŸ“ å‰å‡¦ç†é–‹å§‹: {input_data}")
        processed_text = input_data.strip().lower()
        ctx.shared_state["preprocessed_text"] = processed_text
        print(f"âœ… å‰å‡¦ç†å®Œäº†: {processed_text}")
        return ctx
    
    def analyze_sentiment(input_data, ctx):
        """
        Analyze sentiment of the text
        ãƒ†ã‚­ã‚¹ãƒˆã®æ„Ÿæƒ…ã‚’åˆ†æ
        """
        print("ğŸ˜Š æ„Ÿæƒ…åˆ†æé–‹å§‹...")
        text = ctx.shared_state.get("preprocessed_text", input_data)
        
        # Simulate sentiment analysis
        # æ„Ÿæƒ…åˆ†æã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        time.sleep(0.5)  # Simulate processing time
        
        if "good" in text or "great" in text or "excellent" in text:
            sentiment = "ãƒã‚¸ãƒ†ã‚£ãƒ–"
        elif "bad" in text or "terrible" in text or "awful" in text:
            sentiment = "ãƒã‚¬ãƒ†ã‚£ãƒ–"
        else:
            sentiment = "ä¸­ç«‹"
        
        ctx.shared_state["sentiment"] = sentiment
        print(f"âœ… æ„Ÿæƒ…åˆ†æå®Œäº†: {sentiment}")
        return ctx
    
    def extract_keywords(input_data, ctx):
        """
        Extract keywords from the text
        ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        """
        print("ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºé–‹å§‹...")
        text = ctx.shared_state.get("preprocessed_text", input_data)
        
        # Simulate keyword extraction
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        time.sleep(0.3)
        
        # Simple keyword extraction (split and filter)
        # ç°¡å˜ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆåˆ†å‰²ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰
        words = text.split()
        keywords = [word for word in words if len(word) > 3][:5]
        
        ctx.shared_state["keywords"] = keywords
        print(f"âœ… ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºå®Œäº†: {keywords}")
        return ctx
    
    def classify_topic(input_data, ctx):
        """
        Classify the topic of the text
        ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ”ãƒƒã‚¯ã‚’åˆ†é¡
        """
        print("ğŸ“‚ ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡é–‹å§‹...")
        text = ctx.shared_state.get("preprocessed_text", input_data)
        
        # Simulate topic classification
        # ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        time.sleep(0.4)
        
        if any(tech_word in text for tech_word in ["ai", "technology", "computer", "software"]):
            topic = "æŠ€è¡“"
        elif any(business_word in text for business_word in ["business", "market", "finance"]):
            topic = "ãƒ“ã‚¸ãƒã‚¹"
        elif any(health_word in text for health_word in ["health", "medical", "doctor"]):
            topic = "å¥åº·"
        else:
            topic = "ä¸€èˆ¬"
        
        ctx.shared_state["topic"] = topic
        print(f"âœ… ãƒˆãƒ”ãƒƒã‚¯åˆ†é¡å®Œäº†: {topic}")
        return ctx
    
    def calculate_readability(input_data, ctx):
        """
        Calculate readability score
        èª­ã¿ã‚„ã™ã•ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        """
        print("ğŸ“Š èª­ã¿ã‚„ã™ã•åˆ†æé–‹å§‹...")
        text = ctx.shared_state.get("preprocessed_text", input_data)
        
        # Simulate readability calculation
        # èª­ã¿ã‚„ã™ã•è¨ˆç®—ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        time.sleep(0.2)
        
        # Simple readability score based on word count and average word length
        # å˜èªæ•°ã¨å¹³å‡å˜èªé•·ã«åŸºã¥ãç°¡å˜ãªèª­ã¿ã‚„ã™ã•ã‚¹ã‚³ã‚¢
        words = text.split()
        avg_word_length = sum(len(word) for word in words) / len(words) if words else 0
        
        if avg_word_length < 4:
            readability = "æ˜“ã—ã„"
        elif avg_word_length < 6:
            readability = "æ™®é€š"
        else:
            readability = "é›£ã—ã„"
        
        ctx.shared_state["readability"] = readability
        print(f"âœ… èª­ã¿ã‚„ã™ã•åˆ†æå®Œäº†: {readability}")
        return ctx
    
    def aggregate_analysis_results(input_data, ctx):
        """
        Aggregate all analysis results
        å…¨ã¦ã®åˆ†æçµæœã‚’çµ±åˆ
        """
        print("ğŸ“‹ çµæœçµ±åˆé–‹å§‹...")
        
        # Get all analysis results
        # å…¨åˆ†æçµæœã‚’å–å¾—
        sentiment = ctx.shared_state.get("sentiment", "ä¸æ˜")
        keywords = ctx.shared_state.get("keywords", [])
        topic = ctx.shared_state.get("topic", "ä¸æ˜")
        readability = ctx.shared_state.get("readability", "ä¸æ˜")
        original_text = ctx.shared_state.get("preprocessed_text", input_data)
        
        # Create comprehensive analysis report
        # åŒ…æ‹¬çš„ãªåˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆ
        analysis_report = {
            "original_text": original_text,
            "sentiment": sentiment,
            "keywords": keywords,
            "topic": topic,
            "readability": readability,
            "summary": f"ã“ã®ãƒ†ã‚­ã‚¹ãƒˆã¯{topic}åˆ†é‡ã®{readability}æ–‡ç« ã§ã€{sentiment}ãªæ„Ÿæƒ…ã‚’è¡¨ç¾ã—ã¦ã„ã¾ã™ã€‚"
        }
        
        ctx.shared_state["analysis_report"] = analysis_report
        ctx.finish()
        
        print("âœ… çµæœçµ±åˆå®Œäº†!")
        return ctx
    
    # Create DAG flow with parallel analysis
    # ä¸¦åˆ—åˆ†æã‚’å«ã‚€DAGãƒ•ãƒ­ãƒ¼ã‚’ä½œæˆ
    flow_definition = {
        "preprocess": FunctionStep("preprocess", preprocess_text),
        "parallel_analysis": {
            "parallel": [
                FunctionStep("sentiment", analyze_sentiment),
                FunctionStep("keywords", extract_keywords),
                FunctionStep("topic", classify_topic),
                FunctionStep("readability", calculate_readability)
            ],
            "next_step": "aggregate",
            "max_workers": 4  # Allow up to 4 concurrent analysis tasks
        },
        "aggregate": FunctionStep("aggregate", aggregate_analysis_results)
    }
    
    # Set up sequential flow connections
    # é †æ¬¡ãƒ•ãƒ­ãƒ¼æ¥ç¶šã‚’è¨­å®š
    flow_definition["preprocess"].next_step = "parallel_analysis"
    
    return Flow(start="preprocess", steps=flow_definition, name="TextAnalysisFlow")


async def run_basic_parallel_example():
    """
    Run basic parallel processing example
    åŸºæœ¬çš„ãªä¸¦åˆ—å‡¦ç†ã®ä¾‹ã‚’å®Ÿè¡Œ
    """
    print("=" * 60)
    print("ğŸš€ åŸºæœ¬DAGä¸¦åˆ—å‡¦ç†ãƒ‡ãƒ¢")
    print("=" * 60)
    
    # Create and run text analysis flow
    # ãƒ†ã‚­ã‚¹ãƒˆåˆ†æãƒ•ãƒ­ãƒ¼ã‚’ä½œæˆãƒ»å®Ÿè¡Œ
    flow = create_text_analysis_flow()
    
    sample_text = "This is a great example of AI technology in modern software development."
    
    print(f"ğŸ“„ åˆ†æãƒ†ã‚­ã‚¹ãƒˆ: {sample_text}")
    print("-" * 40)
    
    start_time = time.time()
    result = await flow.run(sample_text)
    end_time = time.time()
    
    print("-" * 40)
    print(f"â±ï¸ å®Ÿè¡Œæ™‚é–“: {end_time - start_time:.2f}ç§’")
    print()
    
    # Display results
    # çµæœã‚’è¡¨ç¤º
    if "analysis_report" in result.shared_state:
        report = result.shared_state["analysis_report"]
        print("ğŸ“Š åˆ†æçµæœ:")
        print(f"  æ„Ÿæƒ…: {report['sentiment']}")
        print(f"  ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(report['keywords'])}")
        print(f"  ãƒˆãƒ”ãƒƒã‚¯: {report['topic']}")
        print(f"  èª­ã¿ã‚„ã™ã•: {report['readability']}")
        print(f"  è¦ç´„: {report['summary']}")
    
    print()


async def compare_sequential_vs_parallel():
    """
    Compare sequential vs parallel execution performance
    é †æ¬¡å®Ÿè¡Œã¨ä¸¦åˆ—å®Ÿè¡Œã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æ¯”è¼ƒ
    """
    print("=" * 60)
    print("âš¡ é †æ¬¡ vs ä¸¦åˆ—å®Ÿè¡Œãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ")
    print("=" * 60)
    
    def simulate_heavy_task(task_name, duration):
        """Simulate a computationally heavy task"""
        async def task_func(input_data, ctx):
            print(f"  ğŸ”„ {task_name} é–‹å§‹...")
            await asyncio.sleep(duration)  # Use asyncio.sleep for proper parallel execution
            ctx.shared_state[f"{task_name}_result"] = f"{task_name} completed in {duration}s"
            print(f"  âœ… {task_name} å®Œäº†")
            return ctx
        return task_func
    
    # Sequential flow
    # é †æ¬¡ãƒ•ãƒ­ãƒ¼
    print("ğŸ“‹ é †æ¬¡å®Ÿè¡Œãƒ†ã‚¹ãƒˆ...")
    sequential_flow = Flow(steps=[
        FunctionStep("task1", simulate_heavy_task("Task1", 0.5)),
        FunctionStep("task2", simulate_heavy_task("Task2", 0.5)),
        FunctionStep("task3", simulate_heavy_task("Task3", 0.5)),
        FunctionStep("task4", simulate_heavy_task("Task4", 0.5))
    ])
    
    start_time = time.time()
    await sequential_flow.run("test")
    sequential_time = time.time() - start_time
    
    print(f"â±ï¸ é †æ¬¡å®Ÿè¡Œæ™‚é–“: {sequential_time:.2f}ç§’")
    print()
    
    # Parallel flow
    # ä¸¦åˆ—ãƒ•ãƒ­ãƒ¼
    print("ğŸš€ ä¸¦åˆ—å®Ÿè¡Œãƒ†ã‚¹ãƒˆ...")
    parallel_flow = Flow(start="parallel_tasks", steps={
        "parallel_tasks": {
            "parallel": [
                FunctionStep("ptask1", simulate_heavy_task("PTask1", 0.5)),
                FunctionStep("ptask2", simulate_heavy_task("PTask2", 0.5)),
                FunctionStep("ptask3", simulate_heavy_task("PTask3", 0.5)),
                FunctionStep("ptask4", simulate_heavy_task("PTask4", 0.5))
            ]
        }
    })
    
    start_time = time.time()
    await parallel_flow.run("test")
    parallel_time = time.time() - start_time
    
    print(f"â±ï¸ ä¸¦åˆ—å®Ÿè¡Œæ™‚é–“: {parallel_time:.2f}ç§’")
    print()
    
    # Performance comparison
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ
    speedup = sequential_time / parallel_time
    print(f"ğŸ¯ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Š: {speedup:.2f}å€é«˜é€ŸåŒ–")
    print(f"ğŸ“ˆ æ™‚é–“çŸ­ç¸®: {((sequential_time - parallel_time) / sequential_time * 100):.1f}%")


async def main():
    """
    Main function to run all examples
    å…¨ã¦ã®ä¾‹ã‚’å®Ÿè¡Œã™ã‚‹ãƒ¡ã‚¤ãƒ³é–¢æ•°
    """
    print("ğŸ‰ Refinire DAGä¸¦åˆ—å‡¦ç†ãƒ‡ãƒ¢")
    print()
    
    # Run all examples
    # å…¨ã¦ã®ä¾‹ã‚’å®Ÿè¡Œ
    await run_basic_parallel_example()
    await compare_sequential_vs_parallel()
    
    print("=" * 60)
    print("âœ¨ ãƒ‡ãƒ¢å®Œäº†ï¼")
    print("ğŸ’¡ è©³ç´°ãªæŠ€è¡“ä»•æ§˜ã«ã¤ã„ã¦ã¯ docs/composable-flow-architecture.md ã‚’ã”è¦§ãã ã•ã„")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())
