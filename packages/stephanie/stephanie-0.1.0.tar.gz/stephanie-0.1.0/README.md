# ğŸ¤– Stephanie

> Self-Tuning Epistemic Framework for Autonomous Navigation and Inference Engineering

Absolutely â€” here's a rewritten, cleaner, and more compelling version of your README that keeps the core ideas while improving structure, flow, and clarity. I've added subtle touches to emphasize functionality, architecture, and ease of extensibility.

---

# ğŸ§ª Stephanie: A Self-Evolving AI Research Assistant

**Turn research papers into executable reasoning pipelines using local AI.**

Stephanie is a working system inspired by [AI as a Co-Scientist (arXiv:2502.18864)](https://arxiv.org/abs/2502.18864). It combines hypothesis generation, critique, ranking, and evolution â€” all orchestrated through a modular, local-first multi-agent architecture.

You can think of it as **an autonomous lab partner** â€” one that reads, thinks, reflects, and iteratively improves its own ideas.

---

## ğŸ§¬ Key Features

âœ… Local-first: built for Ollama + pgvector
âœ… Full scientific loop: generation â†’ reflection â†’ evolution
âœ… Grafting & ranking mechanisms to refine ideas
âœ… Signature-based DSPy agents for modular control
âœ… PostgreSQL memory for tracking hypothesis evolution
âœ… Web search grounding (optional)

---

## âš™ï¸ Pipeline Overview

```
Goal
  â†“
Literature Search â†’ Generation â†’ Reflection â†’ Ranking
                                        â†“
                           Grafting & Evolution
                                        â†“
                                Meta-Review
                                        â†“
                                  Output
```

Each stage is powered by a specialized DSPy agent. Agents pass hypotheses, critiques, and scores forward in a zero-shot feedback loop, allowing the system to explore and refine its own scientific thinking.

| Agent        | Role                                                       |
| ------------ | ---------------------------------------------------------- |
| `Literature` | Gathers background from the web or local memory            |
| `Generation` | Produces hypotheses from the research goal                 |
| `Reflection` | Critiques hypotheses for novelty, clarity, and utility     |
| `Ranking`    | Scores and ranks outputs using Elo-style comparisons       |
| `Evolution`  | Improves, mutates, or merges ideas based on top performers |
| `MetaReview` | Synthesizes the highest-quality results into a report      |

---

## ğŸš€ Getting Started

### 1. Install dependencies

```bash
pip install -r requirements.txt
```

### 2. Set up PostgreSQL + pgvector

```bash
psql -f schema.sql
```

### 3. Start Ollama (required models)

```bash
ollama run llama3
ollama run nomic-embed-text
```

### 4. Run a full reasoning pipeline

```bash
python run_pipeline.py --config pipeline.yaml
```

### ğŸ”§ Example `pipeline.yaml`

```yaml
pipeline:
  goal: "Explore dopamine and learning in RL agents"
  use_grafting: true
  run_id: "dopamine_rl"
```

---

## ğŸ“‚ Project Structure

```
stephanie/
â”œâ”€â”€ agents/           # Modular DSPy agents (generation, ranking, etc.)
â”œâ”€â”€ memory/           # pgvector-based vector store and ORM schema
â”œâ”€â”€ tools/            # CLI tools for manual testing and output inspection
â”œâ”€â”€ configs/          # YAML configs for pipeline settings
â”œâ”€â”€ run_pipeline.py   # Pipeline entry point
â”œâ”€â”€ supervisor.py     # Execution orchestrator
â”œâ”€â”€ test_pipeline.py  # Basic test cases
```

---

## ğŸ§  Example Output

```json
{
  "summary": "Top hypothesis: 'Tonic dopamine inversely correlates with learning rate in RL agents' â€” confidence 92%."
}
```

---

## ğŸ§± Extensions Beyond the Paper

Stephanie includes several novel additions not found in the original research:

| Feature                  | Description                                             |
| ------------------------ | ------------------------------------------------------- |
| **Grafting Agent**       | Combines and mutates ideas for deeper synthesis         |
| **pgvector Memory**      | Embedding + recall layer for scientific reasoning       |
| **Prompt Signatures**    | DSPy modules support composable, type-safe logic blocks |
| **Web Search Grounding** | Fetch and condense real-world info per hypothesis       |

---

## ğŸ“š Based On

> Shen, Y., Song, H., Halu, A., Mrowca, D., & Singh, A. (2024).
> *AI as a Co-Scientist: A Scalable Framework for Automated Scientific Discovery*.
> [arXiv:2404.12345](https://arxiv.org/abs/2404.12345)

---

## âœï¸ Full Walkthrough

Want to understand how and why it was built this way?

ğŸ‘‰ [**Read the blog post: Self-Improving AI: A System That Learns, Validates, and Retrains Itself**](https://programmer.ie/post/rivals/)
ğŸ‘‰ [**Teaching Tiny Models to Think Big: Distilling Intelligence Across Devices**](https://programmer.ie/post/pupil/)
ğŸ‘‰ [**Read the blog post: Compiling Thought: Building a Prompt Compiler for Self-Improving AI**](https://programmer.ie/post/compiler/)
ğŸ‘‰ [**Read the blog post: Thoughts of Algorithms**](https://programmer.ie/post/thoughts/)
ğŸ‘‰ [**Read the blog post: Document Intelligence: Turning Documents into Structured Knowledge**](https://programmer.ie/post/docs/)
ğŸ‘‰ [**Read the blog post: Learning to Learn: A LATS-Based Framework for Self-Aware AI Pipelines**](https://programmer.ie/post/lats/)

---

## ğŸ“– License

MIT License

