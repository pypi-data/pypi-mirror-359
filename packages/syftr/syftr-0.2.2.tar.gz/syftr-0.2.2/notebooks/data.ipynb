{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Data\n",
    "You can use this notebook to load optimization raw data, not filtered for successful trials or any other criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core import ultratb\n",
    "\n",
    "ultratb.VerboseTB._tb_highlight = \"bg:#3e0054\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from syftr.configuration import cfg\n",
    "from syftr.optuna_helper import get_study_names\n",
    "\n",
    "INCLUDE_REGEX = [\n",
    "    \"silver1--.*\",\n",
    "]\n",
    "EXCLUDE_REGEX = []\n",
    "\n",
    "STORAGE = cfg.database.get_optuna_storage()\n",
    "\n",
    "study_names = get_study_names(\n",
    "    include_regex=INCLUDE_REGEX,\n",
    "    exclude_regex=EXCLUDE_REGEX,\n",
    ")\n",
    "\n",
    "study_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_data(study_names):\n",
    "    \n",
    "    import concurrent.futures\n",
    "\n",
    "    def load_study_trials(study_name):\n",
    "        study = optuna.load_study(study_name=study_name, storage=STORAGE)\n",
    "        df_trials: pd.DataFrame = study.trials_dataframe()\n",
    "        df_trials[\"study_name\"] = study_name\n",
    "        return df_trials\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        dfs = list(executor.map(load_study_trials, study_names))\n",
    "\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "df = get_data(study_names)\n",
    "\n",
    "print(f\"The given set of studies contains {len(df[\"user_attrs_flow\"].unique())} unique flows.\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_names_like(df: pd.DataFrame, names: str | list[str]) -> list[str]:\n",
    "    names = names if isinstance(names, list) else [names]\n",
    "    col_names = []\n",
    "    for name in names:\n",
    "        is_match = df.columns.str.contains(name, case=False, regex=True)\n",
    "        col_names.extend(list(df.columns[is_match]))\n",
    "    return col_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_column_names_like(df, [\"fail\", \"message\", \"state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_failed = df[df[\"user_attrs_metric_failed\"] == True]\n",
    "df_failed.dropna(axis=1, how='all', inplace=True)\n",
    "df_failed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "for _, row in df_failed.iterrows():\n",
    "    print(\"Exception: \", row[\"user_attrs_metric_exception_class\"])\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Message: \", row[\"user_attrs_metric_exception_message\"])\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Traceback: \", row[\"user_attrs_metric_exception_stacktrace\"])\n",
    "    print(\"-\" * 80)\n",
    "    flow_str = row[\"user_attrs_flow\"]\n",
    "    flow = json.loads(flow_str)\n",
    "    pprint(flow)\n",
    "\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_message_substring = \"Too few successful evaluations\"\n",
    "\n",
    "errored_configs = []\n",
    "for _, row in df_failed.iterrows():\n",
    "    if error_message_substring in row[\"user_attrs_metric_exception_message\"]:\n",
    "        flow_str = row[\"user_attrs_flow\"]\n",
    "        flow = json.loads(flow_str)\n",
    "        pprint(flow)\n",
    "        errored_configs.append(flow)\n",
    "\n",
    "df_errored_configs = pd.DataFrame(errored_configs)\n",
    "df_errored_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
