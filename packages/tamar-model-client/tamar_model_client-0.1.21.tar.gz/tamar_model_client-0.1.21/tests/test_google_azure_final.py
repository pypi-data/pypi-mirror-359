#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆçš„ Google/Azure åœºæ™¯æµ‹è¯•è„šæœ¬
åªä¿ç•™åŸºæœ¬è°ƒç”¨å’Œæ‰“å°åŠŸèƒ½
"""

import asyncio
import logging
import os
import sys

# é…ç½®æµ‹è¯•è„šæœ¬ä¸“ç”¨çš„æ—¥å¿—
# ä½¿ç”¨ç‰¹å®šçš„loggeråç§°ï¼Œé¿å…å½±å“å®¢æˆ·ç«¯æ—¥å¿—
test_logger = logging.getLogger('test_google_azure_final')
test_logger.setLevel(logging.INFO)
test_logger.propagate = False  # ä¸ä¼ æ’­åˆ°æ ¹logger

# åˆ›å»ºæµ‹è¯•è„šæœ¬ä¸“ç”¨çš„handler
test_handler = logging.StreamHandler()
test_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
test_logger.addHandler(test_handler)

logger = test_logger

os.environ['MODEL_MANAGER_SERVER_GRPC_USE_TLS'] = "false"
os.environ['MODEL_MANAGER_SERVER_ADDRESS'] = "localhost:50051"
os.environ['MODEL_MANAGER_SERVER_JWT_SECRET_KEY'] = "model-manager-server-jwt-key"

# å¯¼å…¥å®¢æˆ·ç«¯æ¨¡å—
try:
    from tamar_model_client import TamarModelClient, AsyncTamarModelClient
    from tamar_model_client.schemas import ModelRequest, UserContext
    from tamar_model_client.enums import ProviderType, InvokeType, Channel
except ImportError as e:
    logger.error(f"å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    sys.exit(1)


def test_google_ai_studio():
    """æµ‹è¯• Google AI Studio"""
    print("\nğŸ” æµ‹è¯• Google AI Studio...")
    
    try:
        client = TamarModelClient()
        
        request = ModelRequest(
            provider=ProviderType.GOOGLE,
            channel=Channel.AI_STUDIO,
            invoke_type=InvokeType.GENERATION,
            model="tamar-google-gemini-flash-lite",
            contents=[
                {"role": "user", "parts": [{"text": "Hello, how are you?"}]}
            ],
            user_context=UserContext(
                user_id="test_user",
                org_id="test_org",
                client_type="test_client"
            ),
            config={
                "temperature": 0.7,
                "maxOutputTokens": 100
            }
        )
        
        response = client.invoke(request)
        print(f"âœ… Google AI Studio æˆåŠŸ")
        print(f"   å“åº”ç±»å‹: {type(response)}")
        print(f"   å“åº”å†…å®¹: {str(response)[:200]}...")
        
    except Exception as e:
        print(f"âŒ Google AI Studio å¤±è´¥: {str(e)}")


def test_google_vertex_ai():
    """æµ‹è¯• Google Vertex AI"""
    print("\nğŸ” æµ‹è¯• Google Vertex AI...")
    
    try:
        client = TamarModelClient()
        
        request = ModelRequest(
            provider=ProviderType.GOOGLE,
            channel=Channel.VERTEXAI,
            invoke_type=InvokeType.GENERATION,
            model="tamar-google-gemini-flash-lite",
            contents=[
                {"role": "user", "parts": [{"text": "What is AI?"}]}
            ],
            user_context=UserContext(
                user_id="test_user",
                org_id="test_org",
                client_type="test_client"
            ),
            config={
                "temperature": 0.5
            }
        )
        
        response = client.invoke(request)
        print(f"âœ… Google Vertex AI æˆåŠŸ")
        print(f"   å“åº”ç±»å‹: {type(response)}")
        print(f"   å“åº”å†…å®¹: {str(response)[:200]}...")
        
    except Exception as e:
        print(f"âŒ Google Vertex AI å¤±è´¥: {str(e)}")


def test_azure_openai():
    """æµ‹è¯• Azure OpenAI"""
    print("\nâ˜ï¸  æµ‹è¯• Azure OpenAI...")
    
    try:
        client = TamarModelClient()
        
        request = ModelRequest(
            provider=ProviderType.AZURE,
            invoke_type=InvokeType.CHAT_COMPLETIONS,
            model="gpt-4o-mini",
            messages=[
                {"role": "user", "content": "Hello, how are you?"}
            ],
            user_context=UserContext(
                user_id="test_user",
                org_id="test_org",
                client_type="test_client"
            ),
        )
        
        response = client.invoke(request)
        print(f"âœ… Azure OpenAI æˆåŠŸ")
        print(f"   å“åº”å†…å®¹: {response.model_dump_json()}...")
        
    except Exception as e:
        print(f"âŒ Azure OpenAI å¤±è´¥: {str(e)}")


async def test_google_streaming():
    """æµ‹è¯• Google æµå¼å“åº”"""
    print("\nğŸ“¡ æµ‹è¯• Google æµå¼å“åº”...")
    
    try:
        async with AsyncTamarModelClient() as client:
            request = ModelRequest(
                provider=ProviderType.GOOGLE,
                channel=Channel.AI_STUDIO,
                invoke_type=InvokeType.GENERATION,
                model="tamar-google-gemini-flash-lite",
                contents=[
                    {"role": "user", "parts": [{"text": "Count 1 to 5"}]}
                ],
                user_context=UserContext(
                    user_id="test_user",
                    org_id="test_org",
                    client_type="test_client"
                ),
                stream=True,
                config={
                    "temperature": 0.1,
                    "maxOutputTokens": 50
                }
            )
            
            response_gen = await client.invoke(request)
            print(f"âœ… Google æµå¼è°ƒç”¨æˆåŠŸ")
            print(f"   å“åº”ç±»å‹: {type(response_gen)}")
            
            chunk_count = 0
            async for chunk in response_gen:
                chunk_count += 1
                print(f"   æ•°æ®å— {chunk_count}: {type(chunk)} - {chunk.model_dump_json()}...")
                if chunk_count >= 3:  # åªæ˜¾ç¤ºå‰3ä¸ªæ•°æ®å—
                    break
        
    except Exception as e:
        print(f"âŒ Google æµå¼å“åº”å¤±è´¥: {str(e)}")


async def test_azure_streaming():
    """æµ‹è¯• Azure æµå¼å“åº”"""
    print("\nğŸ“¡ æµ‹è¯• Azure æµå¼å“åº”...")
    
    try:
        async with AsyncTamarModelClient() as client:
            request = ModelRequest(
                provider=ProviderType.AZURE,
                channel=Channel.OPENAI,
                invoke_type=InvokeType.CHAT_COMPLETIONS,
                model="gpt-4o-mini",
                messages=[
                    {"role": "user", "content": "Count 1 to 5"}
                ],
                user_context=UserContext(
                    user_id="test_user",
                    org_id="test_org",
                    client_type="test_client"
                ),
                stream=True  # æ·»åŠ æµå¼å‚æ•°
            )
            
            response_gen = await client.invoke(request)
            print(f"âœ… Azure æµå¼è°ƒç”¨æˆåŠŸ")
            print(f"   å“åº”ç±»å‹: {type(response_gen)}")
            
            chunk_count = 0
            async for chunk in response_gen:
                chunk_count += 1
                print(f"   æ•°æ®å— {chunk_count}: {type(chunk)} - {chunk.model_dump_json()}...")
                if chunk_count >= 3:  # åªæ˜¾ç¤ºå‰3ä¸ªæ•°æ®å—
                    break
        
    except Exception as e:
        print(f"âŒ Azure æµå¼å“åº”å¤±è´¥: {str(e)}")


def test_sync_batch_requests():
    """æµ‹è¯•åŒæ­¥æ‰¹é‡è¯·æ±‚"""
    print("\nğŸ“¦ æµ‹è¯•åŒæ­¥æ‰¹é‡è¯·æ±‚...")
    
    try:
        from tamar_model_client.schemas import BatchModelRequest, BatchModelRequestItem
        
        with TamarModelClient() as client:
            # æ„å»ºæ‰¹é‡è¯·æ±‚ï¼ŒåŒ…å« Google å’Œ Azure çš„å¤šä¸ªè¯·æ±‚
            batch_request = BatchModelRequest(
                user_context=UserContext(
                    user_id="test_user",
                    org_id="test_org",
                    client_type="test_client"
                ),
                items=[
                    # Google AI Studio è¯·æ±‚
                    BatchModelRequestItem(
                        provider=ProviderType.GOOGLE,
                        invoke_type=InvokeType.GENERATION,
                        model="tamar-google-gemini-flash-lite",
                        contents=[
                            {"role": "user", "parts": [{"text": "Hello from sync batch - Google AI Studio"}]}
                        ],
                        custom_id="sync-google-ai-studio-1",
                    ),
                    # Azure OpenAI è¯·æ±‚
                    BatchModelRequestItem(
                        provider=ProviderType.AZURE,
                        invoke_type=InvokeType.CHAT_COMPLETIONS,
                        model="gpt-4o-mini",
                        messages=[
                            {"role": "user", "content": "Hello from sync batch - Azure OpenAI"}
                        ],
                        custom_id="sync-azure-openai-1",
                    ),
                    # å†æ·»åŠ ä¸€ä¸ª Azure è¯·æ±‚
                    BatchModelRequestItem(
                        provider=ProviderType.AZURE,
                        invoke_type=InvokeType.CHAT_COMPLETIONS,
                        model="gpt-4o-mini",
                        messages=[
                            {"role": "user", "content": "What is 2+2?"}
                        ],
                        custom_id="sync-azure-openai-2",
                    )
                ]
            )
            
            # æ‰§è¡Œæ‰¹é‡è¯·æ±‚
            batch_response = client.invoke_batch(batch_request)
            
            print(f"âœ… åŒæ­¥æ‰¹é‡è¯·æ±‚æˆåŠŸ")
            print(f"   è¯·æ±‚æ•°é‡: {len(batch_request.items)}")
            print(f"   å“åº”æ•°é‡: {len(batch_response.responses)}")
            print(f"   æ‰¹é‡è¯·æ±‚ID: {batch_response.request_id}")
            
            # æ˜¾ç¤ºæ¯ä¸ªå“åº”çš„ç»“æœ
            for i, response in enumerate(batch_response.responses):
                print(f"\n   å“åº” {i+1}:")
                print(f"   - custom_id: {response.custom_id}")
                print(f"   - å†…å®¹é•¿åº¦: {len(response.content) if response.content else 0}")
                print(f"   - æœ‰é”™è¯¯: {'æ˜¯' if response.error else 'å¦'}")
                if response.content:
                    print(f"   - å†…å®¹é¢„è§ˆ: {response.content[:100]}...")
                if response.error:
                    print(f"   - é”™è¯¯ä¿¡æ¯: {response.error}")
                
    except Exception as e:
        print(f"âŒ åŒæ­¥æ‰¹é‡è¯·æ±‚å¤±è´¥: {str(e)}")


async def test_batch_requests():
    """æµ‹è¯•å¼‚æ­¥æ‰¹é‡è¯·æ±‚"""
    print("\nğŸ“¦ æµ‹è¯•å¼‚æ­¥æ‰¹é‡è¯·æ±‚...")
    
    try:
        from tamar_model_client.schemas import BatchModelRequest, BatchModelRequestItem
        
        async with AsyncTamarModelClient() as client:
            # æ„å»ºæ‰¹é‡è¯·æ±‚ï¼ŒåŒ…å« Google å’Œ Azure çš„å¤šä¸ªè¯·æ±‚
            batch_request = BatchModelRequest(
                user_context=UserContext(
                    user_id="test_user",
                    org_id="test_org",
                    client_type="test_client"
                ),
                items=[
                    # Google AI Studio è¯·æ±‚
                    BatchModelRequestItem(
                        provider=ProviderType.GOOGLE,
                        invoke_type=InvokeType.GENERATION,
                        model="tamar-google-gemini-flash-lite",
                        contents=[
                            {"role": "user", "parts": [{"text": "Hello from Google AI Studio"}]}
                        ],
                        custom_id="google-ai-studio-1",
                    ),
                    # Google Vertex AI è¯·æ±‚
                    BatchModelRequestItem(
                        provider=ProviderType.GOOGLE,
                        channel=Channel.VERTEXAI,
                        invoke_type=InvokeType.GENERATION,
                        model="tamar-google-gemini-flash-lite",
                        contents=[
                            {"role": "user", "parts": [{"text": "Hello from Google Vertex AI"}]}
                        ],
                        custom_id="google-vertex-ai-1",
                    ),
                    # Azure OpenAI è¯·æ±‚
                    BatchModelRequestItem(
                        provider=ProviderType.AZURE,
                        invoke_type=InvokeType.CHAT_COMPLETIONS,
                        model="gpt-4o-mini",
                        messages=[
                            {"role": "user", "content": "Hello from Azure OpenAI"}
                        ],
                        custom_id="azure-openai-1",
                    ),
                    # å†æ·»åŠ ä¸€ä¸ª Azure è¯·æ±‚
                    BatchModelRequestItem(
                        provider=ProviderType.AZURE,
                        invoke_type=InvokeType.CHAT_COMPLETIONS,
                        model="gpt-4o-mini",
                        messages=[
                            {"role": "user", "content": "What is the capital of France?"}
                        ],
                        custom_id="azure-openai-2",
                    )
                ]
            )
            
            # æ‰§è¡Œæ‰¹é‡è¯·æ±‚
            batch_response = await client.invoke_batch(batch_request)
            
            print(f"âœ… æ‰¹é‡è¯·æ±‚æˆåŠŸ")
            print(f"   è¯·æ±‚æ•°é‡: {len(batch_request.items)}")
            print(f"   å“åº”æ•°é‡: {len(batch_response.responses)}")
            print(f"   æ‰¹é‡è¯·æ±‚ID: {batch_response.request_id}")
            
            # æ˜¾ç¤ºæ¯ä¸ªå“åº”çš„ç»“æœ
            for i, response in enumerate(batch_response.responses):
                print(f"\n   å“åº” {i+1}:")
                print(f"   - custom_id: {response.custom_id}")
                print(f"   - å†…å®¹é•¿åº¦: {len(response.content) if response.content else 0}")
                print(f"   - æœ‰é”™è¯¯: {'æ˜¯' if response.error else 'å¦'}")
                if response.content:
                    print(f"   - å†…å®¹é¢„è§ˆ: {response.content[:100]}...")
                if response.error:
                    print(f"   - é”™è¯¯ä¿¡æ¯: {response.error}")
                
    except Exception as e:
        print(f"âŒ æ‰¹é‡è¯·æ±‚å¤±è´¥: {str(e)}")


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ç®€åŒ–ç‰ˆ Google/Azure æµ‹è¯•")
    print("=" * 50)
    
    try:
        # åŒæ­¥æµ‹è¯•
        test_google_ai_studio()
        test_google_vertex_ai()
        test_azure_openai()
        
        # åŒæ­¥æ‰¹é‡æµ‹è¯•
        test_sync_batch_requests()
        
        # å¼‚æ­¥æµå¼æµ‹è¯•
        await asyncio.wait_for(test_google_streaming(), timeout=60.0)
        await asyncio.wait_for(test_azure_streaming(), timeout=60.0)
        
        # å¼‚æ­¥æ‰¹é‡æµ‹è¯•
        await asyncio.wait_for(test_batch_requests(), timeout=120.0)
        
        print("\nâœ… æµ‹è¯•å®Œæˆ")
        
    except asyncio.TimeoutError:
        print("\nâ° æµ‹è¯•è¶…æ—¶")
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•æ‰§è¡Œå‡ºé”™: {e}")
    finally:
        # ç®€å•ä¼˜é›…çš„ä»»åŠ¡æ¸…ç†
        print("ğŸ“ æ¸…ç†å¼‚æ­¥ä»»åŠ¡...")
        try:
            # çŸ­æš‚ç­‰å¾…è®©æ­£åœ¨å®Œæˆçš„ä»»åŠ¡è‡ªç„¶ç»“æŸ
            await asyncio.sleep(0.5)
            
            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æœªå®Œæˆçš„ä»»åŠ¡
            current_task = asyncio.current_task()
            tasks = [task for task in asyncio.all_tasks() 
                    if not task.done() and task != current_task]
            
            if tasks:
                print(f"   å‘ç° {len(tasks)} ä¸ªæœªå®Œæˆä»»åŠ¡ï¼Œç­‰å¾…è‡ªç„¶å®Œæˆ...")
                # ç®€å•ç­‰å¾…ï¼Œä¸å¼ºåˆ¶å–æ¶ˆ
                try:
                    await asyncio.wait_for(
                        asyncio.sleep(2.0),  # ç»™ä»»åŠ¡2ç§’æ—¶é—´è‡ªç„¶å®Œæˆ
                        timeout=2.0
                    )
                except asyncio.TimeoutError:
                    pass
            
            print("   ä»»åŠ¡æ¸…ç†å®Œæˆ")
                
        except Exception as e:
            print(f"   âš ï¸ ä»»åŠ¡æ¸…ç†æ—¶å‡ºç°å¼‚å¸¸: {e}")
        
        print("ğŸ”š ç¨‹åºå³å°†é€€å‡º")


if __name__ == "__main__":
    try:
        # ä¸´æ—¶é™ä½ asyncio æ—¥å¿—çº§åˆ«ï¼Œå‡å°‘ä»»åŠ¡å–æ¶ˆæ—¶çš„å™ªéŸ³
        asyncio_logger = logging.getLogger('asyncio')
        original_level = asyncio_logger.level
        asyncio_logger.setLevel(logging.ERROR)
        
        try:
            asyncio.run(main())
        finally:
            # æ¢å¤åŸå§‹æ—¥å¿—çº§åˆ«
            asyncio_logger.setLevel(original_level)
            
    except KeyboardInterrupt:
        print("\nâš ï¸ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
    finally:
        print("ğŸ ç¨‹åºå·²é€€å‡º")