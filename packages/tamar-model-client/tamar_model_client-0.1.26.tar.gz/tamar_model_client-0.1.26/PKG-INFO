Metadata-Version: 2.4
Name: tamar-model-client
Version: 0.1.26
Summary: A Python SDK for interacting with the Model Manager gRPC service
Home-page: http://gitlab.tamaredge.top/project-tap/AgentOS/model-manager-client
Author: Oscar Ou
Author-email: oscar.ou@tamaredge.ai
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: grpcio~=1.67.1
Requires-Dist: grpcio-tools~=1.67.1
Requires-Dist: pydantic
Requires-Dist: PyJWT
Requires-Dist: nest_asyncio
Requires-Dist: openai
Requires-Dist: google-genai
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# Tamar Model Client

**Tamar Model Client** æ˜¯ä¸€æ¬¾é«˜æ€§èƒ½çš„ Python SDKï¼Œé€šè¿‡ gRPC åè®®è¿æ¥ Model Manager æœåŠ¡ï¼Œä¸ºå¤šä¸ª AI æ¨¡å‹æœåŠ¡å•†æä¾›ç»Ÿä¸€çš„è°ƒç”¨æ¥å£ã€‚æ— è®ºæ‚¨ä½¿ç”¨ OpenAIã€Googleã€Azure è¿˜æ˜¯å…¶ä»– AI æœåŠ¡ï¼Œéƒ½å¯ä»¥é€šè¿‡ä¸€å¥— API è½»æ¾åˆ‡æ¢å’Œç®¡ç†ã€‚

## ğŸ¯ ä¸ºä»€ä¹ˆé€‰æ‹© Tamar Model Clientï¼Ÿ

### æ‚¨é‡åˆ°è¿‡è¿™äº›é—®é¢˜å—ï¼Ÿ

âŒ éœ€è¦åŒæ—¶é›†æˆ OpenAIã€Googleã€Azure ç­‰å¤šä¸ª AI æœåŠ¡ï¼Œæ¯ä¸ªéƒ½æœ‰ä¸åŒçš„ APIï¼Ÿ  
âŒ éš¾ä»¥ç»Ÿè®¡å’Œæ§åˆ¶ä¸åŒæœåŠ¡å•†çš„ä½¿ç”¨æˆæœ¬ï¼Ÿ  
âŒ åœ¨ä¸åŒ AI æä¾›å•†ä¹‹é—´åˆ‡æ¢éœ€è¦ä¿®æ”¹å¤§é‡ä»£ç ï¼Ÿ  
âŒ æ¯ä¸ªæœåŠ¡å•†éƒ½æœ‰è‡ªå·±çš„é”™è¯¯å¤„ç†å’Œé‡è¯•é€»è¾‘ï¼Ÿ  

### âœ… Tamar Model Client ä¸€ç«™å¼è§£å†³ï¼

ğŸ‰ **ä¸€ä¸ª SDKï¼Œè®¿é—®æ‰€æœ‰ AI æœåŠ¡**  
ğŸ“Š **ç»Ÿä¸€çš„ä½¿ç”¨é‡å’Œæˆæœ¬ç»Ÿè®¡**  
ğŸ”„ **æ— ç¼åˆ‡æ¢ï¼Œä¸€è¡Œä»£ç æå®š**  
ğŸ›¡ï¸ **ç”Ÿäº§çº§çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶**

## âœ¨ æ ¸å¿ƒç‰¹æ€§

### ğŸ”Œ å¤šæœåŠ¡å•†æ”¯æŒ
- **OpenAI** (GPT-3.5/4, DALL-E)
- **Google** (Gemini - AI Studio & Vertex AI)
- **Azure OpenAI** (ä¼ä¸šçº§éƒ¨ç½²)
- **Anthropic** (Claude)
- **DeepSeek** (æ·±åº¦æ±‚ç´¢)
- **Perplexity** (æœç´¢å¢å¼ºç”Ÿæˆ)

### âš¡ çµæ´»çš„è°ƒç”¨æ–¹å¼
- ğŸ§© **åŒæ­¥/å¼‚æ­¥** åŒæ¨¡å¼å®¢æˆ·ç«¯
- ğŸ“¡ **æµå¼/éæµå¼** å“åº”æ”¯æŒ
- ğŸ“¦ **æ‰¹é‡è¯·æ±‚** å¹¶è¡Œå¤„ç†
- ğŸ”„ **è‡ªåŠ¨é‡è¯•** æŒ‡æ•°é€€é¿ç­–ç•¥

### ğŸ›¡ï¸ ç”Ÿäº§çº§ç‰¹æ€§
- ğŸ” **JWT è®¤è¯** å®‰å…¨å¯é 
- ğŸ“Š **ä½¿ç”¨é‡è¿½è¸ª** Token ç»Ÿè®¡ä¸æˆæœ¬è®¡ç®—
- ğŸ†” **è¯·æ±‚è¿½è¸ª** å”¯ä¸€ request_id
- âš ï¸ **å®Œå–„é”™è¯¯å¤„ç†** è¯¦ç»†é”™è¯¯ä¿¡æ¯
- âœ… **ç±»å‹å®‰å…¨** Pydantic v2 éªŒè¯

### ğŸš€ é«˜æ€§èƒ½è®¾è®¡
- ğŸ”— **gRPC é€šä¿¡** HTTP/2 é•¿è¿æ¥
- â™»ï¸ **è¿æ¥å¤ç”¨** å‡å°‘æ¡æ‰‹å¼€é”€
- ğŸ¯ **æ™ºèƒ½è·¯ç”±** è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜é€šé“
- ğŸ“ˆ **æ€§èƒ½ç›‘æ§** å»¶è¿Ÿä¸ååé‡æŒ‡æ ‡

## ğŸ“‹ å®‰è£…

```bash
pip install tamar-model-client
```

### ç³»ç»Ÿè¦æ±‚

- Python â‰¥ 3.8
- æ”¯æŒ Windows / Linux / macOS
- ä¾èµ–é¡¹ä¼šè‡ªåŠ¨å®‰è£…ï¼ˆgrpcio, pydantic, python-dotenv ç­‰ï¼‰

## ğŸ—ï¸ é¡¹ç›®æ¶æ„

```
tamar_model_client/
â”œâ”€â”€ ğŸ“ generated/              # gRPC ç”Ÿæˆçš„ä»£ç 
â”‚   â”œâ”€â”€ model_service.proto    # Protocol Buffer å®šä¹‰
â”‚   â””â”€â”€ *_pb2*.py             # ç”Ÿæˆçš„ Python ä»£ç 
â”œâ”€â”€ ğŸ“ schemas/                # Pydantic æ•°æ®æ¨¡å‹
â”‚   â”œâ”€â”€ inputs.py             # è¯·æ±‚æ¨¡å‹ï¼ˆModelRequest, UserContextï¼‰
â”‚   â””â”€â”€ outputs.py            # å“åº”æ¨¡å‹ï¼ˆModelResponse, Usageï¼‰
â”œâ”€â”€ ğŸ“ enums/                  # æšä¸¾å®šä¹‰
â”‚   â”œâ”€â”€ providers.py          # AI æœåŠ¡å•†ï¼ˆOpenAI, Google, Azure...ï¼‰
â”‚   â”œâ”€â”€ invoke.py             # è°ƒç”¨ç±»å‹ï¼ˆgeneration, images...ï¼‰
â”‚   â””â”€â”€ channel.py            # æœåŠ¡é€šé“ï¼ˆopenai, vertexai...ï¼‰
â”œâ”€â”€ ğŸ“„ sync_client.py          # åŒæ­¥å®¢æˆ·ç«¯ TamarModelClient
â”œâ”€â”€ ğŸ“„ async_client.py         # å¼‚æ­¥å®¢æˆ·ç«¯ AsyncTamarModelClient
â”œâ”€â”€ ğŸ“„ exceptions.py           # å¼‚å¸¸å±‚çº§å®šä¹‰
â”œâ”€â”€ ğŸ“„ auth.py                 # JWT è®¤è¯ç®¡ç†
â””â”€â”€ ğŸ“„ utils.py                # å·¥å…·å‡½æ•°
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1ï¸âƒ£ å®¢æˆ·ç«¯åˆå§‹åŒ–

```python
from tamar_model_client import TamarModelClient, AsyncTamarModelClient

# æ–¹å¼ä¸€ï¼šä½¿ç”¨ç¯å¢ƒå˜é‡ï¼ˆæ¨èï¼‰
client = TamarModelClient()  # è‡ªåŠ¨è¯»å–ç¯å¢ƒå˜é‡é…ç½®

# æ–¹å¼äºŒï¼šä»£ç é…ç½®
client = TamarModelClient(
    server_address="localhost:50051",
    jwt_token="your-jwt-token"
)

# å¼‚æ­¥å®¢æˆ·ç«¯
async_client = AsyncTamarModelClient(
    server_address="localhost:50051",
    jwt_secret_key="your-jwt-secret-key"  # ä½¿ç”¨å¯†é’¥è‡ªåŠ¨ç”Ÿæˆ JWT
)
```

### 2ï¸âƒ£ åŸºç¡€ç¤ºä¾‹ - ä¸ AI å¯¹è¯

```python
from tamar_model_client import TamarModelClient
from tamar_model_client.schemas import ModelRequest, UserContext
from tamar_model_client.enums import ProviderType

# åˆ›å»ºå®¢æˆ·ç«¯
client = TamarModelClient()

# æ„å»ºè¯·æ±‚
request = ModelRequest(
    provider=ProviderType.OPENAI,
    model="gpt-3.5-turbo",
    messages=[
        {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}
    ],
    user_context=UserContext(
        user_id="test_user",
        org_id="test_org",
        client_type="python-sdk"
    )
)

# å‘é€è¯·æ±‚
response = client.invoke(request)
print(f"AI å›å¤: {response.content}")
```


## ğŸ“š è¯¦ç»†ä½¿ç”¨ç¤ºä¾‹

### OpenAI è°ƒç”¨ç¤ºä¾‹

```python
from tamar_model_client import TamarModelClient
from tamar_model_client.schemas import ModelRequest, UserContext
from tamar_model_client.enums import ProviderType, InvokeType, Channel

# åˆ›å»ºåŒæ­¥å®¢æˆ·ç«¯
client = TamarModelClient()

# OpenAI è°ƒç”¨ç¤ºä¾‹
request_data = ModelRequest(
    provider=ProviderType.OPENAI,  # é€‰æ‹© OpenAI ä½œä¸ºæä¾›å•†
    channel=Channel.OPENAI,  # ä½¿ç”¨ OpenAI æ¸ é“
    invoke_type=InvokeType.CHAT_COMPLETIONS,  # ä½¿ç”¨ chat completions è°ƒç”¨ç±»å‹
    model="gpt-4",  # æŒ‡å®šå…·ä½“æ¨¡å‹
    messages=[
        {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}
    ],
    user_context=UserContext(
        user_id="test_user",
        org_id="test_org",
        client_type="python-sdk"
    ),
    stream=False,  # éæµå¼è°ƒç”¨
    temperature=0.7,  # å¯é€‰å‚æ•°
    max_tokens=1000,  # å¯é€‰å‚æ•°
)

# å‘é€è¯·æ±‚å¹¶è·å–å“åº”
response = client.invoke(request_data)
if response.error:
    print(f"é”™è¯¯: {response.error}")
else:
    print(f"å“åº”: {response.content}")
    if response.usage:
        print(f"Token ä½¿ç”¨æƒ…å†µ: {response.usage}")
```

### Google è°ƒç”¨ç¤ºä¾‹ ï¼ˆAI Studio / Vertex AIï¼‰

```python
from tamar_model_client import TamarModelClient
from tamar_model_client.schemas import ModelRequest, UserContext
from tamar_model_client.enums import ProviderType, InvokeType, Channel

# åˆ›å»ºåŒæ­¥å®¢æˆ·ç«¯
client = TamarModelClient()

# Google AI Studio è°ƒç”¨ç¤ºä¾‹
request_data = ModelRequest(
    provider=ProviderType.GOOGLE,  # é€‰æ‹© Google ä½œä¸ºæä¾›å•†
    channel=Channel.AI_STUDIO,  # ä½¿ç”¨ AI Studio æ¸ é“
    invoke_type=InvokeType.GENERATION,  # ä½¿ç”¨ç”Ÿæˆè°ƒç”¨ç±»å‹
    model="gemini-pro",  # æŒ‡å®šå…·ä½“æ¨¡å‹
    contents=[
        {"role": "user", "parts": [{"text": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}]}
    ],
    user_context=UserContext(
        user_id="test_user",
        org_id="test_org",
        client_type="python-sdk"
    ),
    temperature=0.7,  # å¯é€‰å‚æ•°
)

# å‘é€è¯·æ±‚å¹¶è·å–å“åº”
response = client.invoke(request_data)
if response.error:
    print(f"é”™è¯¯: {response.error}")
else:
    print(f"å“åº”: {response.content}")
    if response.usage:
        print(f"Token ä½¿ç”¨æƒ…å†µ: {response.usage}")

# Google Vertex AI è°ƒç”¨ç¤ºä¾‹
vertex_request = ModelRequest(
    provider=ProviderType.GOOGLE,  # é€‰æ‹© Google ä½œä¸ºæä¾›å•†
    channel=Channel.VERTEXAI,  # ä½¿ç”¨ Vertex AI æ¸ é“
    invoke_type=InvokeType.GENERATION,  # ä½¿ç”¨ç”Ÿæˆè°ƒç”¨ç±»å‹
    model="gemini-pro",  # æŒ‡å®šå…·ä½“æ¨¡å‹
    contents=[
        {"role": "user", "parts": [{"text": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}]}
    ],
    user_context=UserContext(
        user_id="test_user",
        org_id="test_org",
        client_type="python-sdk"
    ),
    temperature=0.7,  # å¯é€‰å‚æ•°
)

# å‘é€è¯·æ±‚å¹¶è·å–å“åº”
vertex_response = client.invoke(vertex_request)
if vertex_response.error:
    print(f"é”™è¯¯: {vertex_response.error}")
else:
    print(f"å“åº”: {vertex_response.content}")
    if vertex_response.usage:
        print(f"Token ä½¿ç”¨æƒ…å†µ: {vertex_response.usage}")
```

### Azure OpenAI è°ƒç”¨ç¤ºä¾‹

```python
from tamar_model_client import TamarModelClient
from tamar_model_client.schemas import ModelRequest, UserContext
from tamar_model_client.enums import ProviderType, InvokeType, Channel

# åˆ›å»ºåŒæ­¥å®¢æˆ·ç«¯
client = TamarModelClient()

# Azure OpenAI è°ƒç”¨ç¤ºä¾‹
request_data = ModelRequest(
    provider=ProviderType.AZURE,  # é€‰æ‹© Azure ä½œä¸ºæä¾›å•†
    channel=Channel.OPENAI,  # ä½¿ç”¨ OpenAI æ¸ é“
    invoke_type=InvokeType.CHAT_COMPLETIONS,  # ä½¿ç”¨ chat completions è°ƒç”¨ç±»å‹
    model="gpt-4o-mini",  # æŒ‡å®šå…·ä½“æ¨¡å‹
    messages=[
        {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}
    ],
    user_context=UserContext(
        user_id="test_user",
        org_id="test_org",
        client_type="python-sdk"
    ),
    stream=False,  # éæµå¼è°ƒç”¨
    temperature=0.7,  # å¯é€‰å‚æ•°
    max_tokens=1000,  # å¯é€‰å‚æ•°
)

# å‘é€è¯·æ±‚å¹¶è·å–å“åº”
response = client.invoke(request_data)
if response.error:
    print(f"é”™è¯¯: {response.error}")
else:
    print(f"å“åº”: {response.content}")
    if response.usage:
        print(f"Token ä½¿ç”¨æƒ…å†µ: {response.usage}")
```

### å¼‚æ­¥è°ƒç”¨ç¤ºä¾‹

```python
import asyncio
from tamar_model_client import AsyncTamarModelClient
from tamar_model_client.schemas import ModelRequest, UserContext
from tamar_model_client.enums import ProviderType, InvokeType, Channel


async def main():
    # åˆ›å»ºå¼‚æ­¥å®¢æˆ·ç«¯
    client = AsyncTamarModelClient()

    # ç»„è£…è¯·æ±‚å‚æ•°
    request_data = ModelRequest(
        provider=ProviderType.OPENAI,
        channel=Channel.OPENAI,
        invoke_type=InvokeType.CHAT_COMPLETIONS,
        model="gpt-4o-mini",
        messages=[
            {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}
        ],
        user_context=UserContext(
            user_id="test_user",
            org_id="test_org",
            client_type="python-sdk"
        ),
        stream=False,
        temperature=0.7,
        max_tokens=1000,
    )

    # å‘é€è¯·æ±‚å¹¶è·å–å“åº”
    async for r in await client.invoke(model_request):
        if r.error:
            print(f"é”™è¯¯: {r.error}")
        else:
            print(f"å“åº”: {r.content}")
            if r.usage:
                print(f"Token ä½¿ç”¨æƒ…å†µ: {r.usage}")


# è¿è¡Œå¼‚æ­¥ç¤ºä¾‹
asyncio.run(main())
```

### æµå¼è°ƒç”¨ç¤ºä¾‹

```python
import asyncio
from tamar_model_client import AsyncTamarModelClient
from tamar_model_client.schemas import ModelRequest, UserContext
from tamar_model_client.enums import ProviderType, InvokeType, Channel


async def stream_example():
    # åˆ›å»ºå¼‚æ­¥å®¢æˆ·ç«¯
    client = AsyncTamarModelClient()

    # ç»„è£…è¯·æ±‚å‚æ•°
    request_data = ModelRequest(
        provider=ProviderType.OPENAI,
        channel=Channel.OPENAI,
        invoke_type=InvokeType.CHAT_COMPLETIONS,
        model="gpt-4",
        messages=[
            {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}
        ],
        user_context=UserContext(
            user_id="test_user",
            org_id="test_org",
            client_type="python-sdk"
        ),
        stream=True,  # å¯ç”¨æµå¼è¾“å‡º
        temperature=0.7,
    )

    # å‘é€è¯·æ±‚å¹¶è·å–æµå¼å“åº”
    async for response in client.invoke(request_data):
        if response.error:
            print(f"é”™è¯¯: {response.error}")
        else:
            print(f"å“åº”ç‰‡æ®µ: {response.content}", end="", flush=True)
            if response.usage:
                print(f"\nToken ä½¿ç”¨æƒ…å†µ: {response.usage}")


# è¿è¡Œæµå¼ç¤ºä¾‹
asyncio.run(stream_example())
```

### æ‰¹é‡è°ƒç”¨ç¤ºä¾‹

æ”¯æŒæ‰¹é‡å¤„ç†å¤šä¸ªæ¨¡å‹è¯·æ±‚ï¼š

```python
import asyncio
from tamar_model_client import AsyncTamarModelClient
from tamar_model_client.schemas import (
    BatchModelRequest, BatchModelRequestItem,
    UserContext
)
from tamar_model_client.enums import ProviderType, InvokeType, Channel


async def batch_example():
    # åˆ›å»ºå¼‚æ­¥å®¢æˆ·ç«¯
    client = AsyncTamarModelClient()

    # ç»„è£…æ‰¹é‡è¯·æ±‚å‚æ•°
    batch_request = BatchModelRequest(
        user_context=UserContext(
            user_id="test_user",
            org_id="test_org",
            client_type="python-sdk"
        ),
        items=[
            BatchModelRequestItem(
                provider=ProviderType.OPENAI,
                channel=Channel.OPENAI,
                invoke_type=InvokeType.CHAT_COMPLETIONS,
                model="gpt-4",
                messages=[
                    {"role": "user", "content": "ç¬¬ä¸€ä¸ªé—®é¢˜ï¼šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"}
                ],
                priority=1,
                custom_id="q1"
            ),
            BatchModelRequestItem(
                provider=ProviderType.GOOGLE,
                channel=Channel.AI_STUDIO,
                invoke_type=InvokeType.GENERATION,
                model="gemini-pro",
                contents=[
                    {"role": "user", "parts": [{"text": "ç¬¬äºŒä¸ªé—®é¢˜ï¼šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"}]}
                ],
                priority=2,
                custom_id="q2"
            )
        ]
    )

    # å‘é€æ‰¹é‡è¯·æ±‚å¹¶è·å–å“åº”
    response = await client.invoke_batch(batch_request)
    if response.responses:
        for resp in response.responses:
            print(f"\né—®é¢˜ {resp.custom_id} çš„å“åº”:")
            if resp.error:
                print(f"é”™è¯¯: {resp.error}")
            else:
                print(f"å†…å®¹: {resp.content}")
                if resp.usage:
                    print(f"Token ä½¿ç”¨æƒ…å†µ: {resp.usage}")


# è¿è¡Œæ‰¹é‡è°ƒç”¨ç¤ºä¾‹
asyncio.run(batch_example())
```

### æ–‡ä»¶è¾“å…¥ç¤ºä¾‹

æ”¯æŒå¤„ç†å›¾åƒç­‰æ–‡ä»¶è¾“å…¥ï¼ˆéœ€ä½¿ç”¨æ”¯æŒå¤šæ¨¡æ€çš„æ¨¡å‹ï¼Œå¦‚ gemini-2.0-flashï¼‰ï¼š

```python
from tamar_model_client import TamarModelClient
from tamar_model_client.schemas import ModelRequest, UserContext
from tamar_model_client.enums import ProviderType
from google.genai.types import Part
model_request = ModelRequest(
    provider=ProviderType.GOOGLE,  # é€‰æ‹© Googleä½œä¸ºæä¾›å•†
    model="gemini-2.0-flash",
    contents=[
        "What is shown in this image?",
        Part.from_uri( # è¿™ä¸ªæ˜¯Googleé‚£è¾¹çš„å‚æ•°æ”¯æŒ
            file_uri="https://images.pexels.com/photos/248797/pexels-photo-248797.jpeg",
            mime_type="image/jpeg",
        ),
    ],
    user_context=UserContext(
        org_id="testllm",
        user_id="testllm",
        client_type="conversation-service"
    ),
)
client = TamarModelClient("localhost:50051")
response = client.invoke(
    model_request=model_request
)
```

## ğŸ› ï¸ é«˜çº§åŠŸèƒ½

### ğŸ”¥ ä½¿ç”¨åœºæ™¯å’Œæœ€ä½³å®è·µ

#### ä½¿ç”¨åœºæ™¯
1. **å¤šæ¨¡å‹æ¯”è¾ƒ**ï¼šåŒæ—¶è°ƒç”¨å¤šä¸ªæœåŠ¡å•†çš„æ¨¡å‹ï¼Œæ¯”è¾ƒè¾“å‡ºè´¨é‡
2. **æˆæœ¬ä¼˜åŒ–**ï¼šæ ¹æ®ä»»åŠ¡ç±»å‹è‡ªåŠ¨é€‰æ‹©æ€§ä»·æ¯”æœ€é«˜çš„æ¨¡å‹
3. **é«˜å¯ç”¨æ¶æ„**ï¼šä¸»å¤‡æ¨¡å‹è‡ªåŠ¨åˆ‡æ¢ï¼Œç¡®ä¿æœåŠ¡ç¨³å®š
4. **ç»Ÿä¸€ç›‘æ§**ï¼šé›†ä¸­ç®¡ç†æ‰€æœ‰ AI æœåŠ¡çš„ä½¿ç”¨é‡å’Œæˆæœ¬

#### æœ€ä½³å®è·µ
1. **å®¢æˆ·ç«¯ç®¡ç†**
   ```python
   # âœ… æ¨èï¼šå•ä¾‹æ¨¡å¼ä½¿ç”¨
   client = TamarModelClient()
   # æ•´ä¸ªåº”ç”¨ç”Ÿå‘½å‘¨æœŸä½¿ç”¨åŒä¸€ä¸ªå®¢æˆ·ç«¯
   
   # âŒ é¿å…ï¼šé¢‘ç¹åˆ›å»ºå®¢æˆ·ç«¯
   for i in range(100):
       client = TamarModelClient()  # ä¸æ¨èï¼
   ```

2. **é”™è¯¯å¤„ç†**
   ```python
   try:
       response = client.invoke(request)
   except TamarModelException as e:
       logger.error(f"Model error: {e.message}, request_id: {e.request_id}")
       # å®æ–½é™çº§ç­–ç•¥æˆ–é‡è¯•
   ```

3. **æ€§èƒ½ä¼˜åŒ–**
   - ä½¿ç”¨æ‰¹é‡ API å¤„ç†å¤§é‡è¯·æ±‚
   - å¯ç”¨æµå¼å“åº”å‡å°‘é¦–å­—å»¶è¿Ÿ
   - åˆç†è®¾ç½® max_tokens é¿å…æµªè´¹

### ğŸ›¡ï¸ ç†”æ–­é™çº§åŠŸèƒ½ï¼ˆé«˜å¯ç”¨ä¿éšœï¼‰

SDK å†…ç½®äº†ç†”æ–­é™çº§æœºåˆ¶ï¼Œå½“ gRPC æœåŠ¡ä¸å¯ç”¨æ—¶è‡ªåŠ¨åˆ‡æ¢åˆ° HTTP æœåŠ¡ï¼Œç¡®ä¿ä¸šåŠ¡è¿ç»­æ€§ã€‚

#### å·¥ä½œåŸç†
1. **æ­£å¸¸çŠ¶æ€**ï¼šæ‰€æœ‰è¯·æ±‚é€šè¿‡é«˜æ€§èƒ½çš„ gRPC åè®®
2. **ç†”æ–­è§¦å‘**ï¼šå½“è¿ç»­å¤±è´¥è¾¾åˆ°é˜ˆå€¼æ—¶ï¼Œç†”æ–­å™¨æ‰“å¼€
3. **è‡ªåŠ¨é™çº§**ï¼šåˆ‡æ¢åˆ° HTTP åè®®ç»§ç»­æä¾›æœåŠ¡
4. **å®šæœŸæ¢å¤**ï¼šç†”æ–­å™¨ä¼šå®šæœŸå°è¯•æ¢å¤åˆ° gRPC

#### å¯ç”¨æ–¹å¼
```bash
# è®¾ç½®ç¯å¢ƒå˜é‡
export MODEL_CLIENT_RESILIENT_ENABLED=true
export MODEL_CLIENT_HTTP_FALLBACK_URL=http://localhost:8080
export MODEL_CLIENT_CIRCUIT_BREAKER_THRESHOLD=5
export MODEL_CLIENT_CIRCUIT_BREAKER_TIMEOUT=60
```

#### ä½¿ç”¨ç¤ºä¾‹
```python
from tamar_model_client import TamarModelClient

# å®¢æˆ·ç«¯ä¼šè‡ªåŠ¨å¤„ç†ç†”æ–­é™çº§ï¼Œå¯¹ä½¿ç”¨è€…é€æ˜
client = TamarModelClient()

# æ­£å¸¸ä½¿ç”¨ï¼Œæ— éœ€å…³å¿ƒåº•å±‚åè®®
response = client.invoke(request)

# è·å–ç†”æ–­å™¨çŠ¶æ€ï¼ˆå¯é€‰ï¼‰
metrics = client.get_resilient_metrics()
if metrics:
    print(f"ç†”æ–­å™¨çŠ¶æ€: {metrics['circuit_state']}")
    print(f"å¤±è´¥æ¬¡æ•°: {metrics['failure_count']}")
```

#### ç†”æ–­å™¨çŠ¶æ€
- **CLOSED**ï¼ˆå…³é—­ï¼‰ï¼šæ­£å¸¸å·¥ä½œçŠ¶æ€ï¼Œè¯·æ±‚æ­£å¸¸é€šè¿‡
- **OPEN**ï¼ˆæ‰“å¼€ï¼‰ï¼šç†”æ–­çŠ¶æ€ï¼Œæ‰€æœ‰è¯·æ±‚ç›´æ¥é™çº§åˆ° HTTP
- **HALF_OPEN**ï¼ˆåŠå¼€ï¼‰ï¼šæ¢å¤æµ‹è¯•çŠ¶æ€ï¼Œå…è®¸å°‘é‡è¯·æ±‚æµ‹è¯• gRPC æ˜¯å¦æ¢å¤

#### ç›‘æ§æŒ‡æ ‡
```python
# è·å–ç†”æ–­é™çº§æŒ‡æ ‡
metrics = client.get_resilient_metrics()
# è¿”å›ç¤ºä¾‹ï¼š
# {
#     "enabled": true,
#     "circuit_state": "closed",
#     "failure_count": 0,
#     "last_failure_time": null,
#     "http_fallback_url": "http://localhost:8080"
# }
```

### âš ï¸ æ³¨æ„äº‹é¡¹

1. **å‚æ•°è¯´æ˜**
   - **å¿…å¡«å‚æ•°**ï¼š`provider`, `model`, `user_context`
   - **å¯é€‰å‚æ•°**ï¼š`channel`, `invoke_type`ï¼ˆç³»ç»Ÿå¯è‡ªåŠ¨æ¨æ–­ï¼‰
   - **æµå¼æ§åˆ¶**ï¼šé€šè¿‡ `stream=True/False` å‚æ•°æ§åˆ¶

2. **è¿æ¥ç®¡ç†**
   - gRPC ä½¿ç”¨ HTTP/2 é•¿è¿æ¥ï¼Œå®¢æˆ·ç«¯åº”ä½œä¸ºå•ä¾‹ä½¿ç”¨
   - å¦‚éœ€å¤šå®ä¾‹ï¼ŒåŠ¡å¿…è°ƒç”¨ `client.close()` é‡Šæ”¾èµ„æº

3. **é”™è¯¯å¤„ç†**
   - æ‰€æœ‰é”™è¯¯åŒ…å« `request_id` ç”¨äºé—®é¢˜è¿½è¸ª
   - ç½‘ç»œé”™è¯¯ä¼šè‡ªåŠ¨é‡è¯•ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
   - æä¾›å•†é”™è¯¯ä¿ç•™åŸå§‹é”™è¯¯ä¿¡æ¯

## âš™ï¸ ç¯å¢ƒå˜é‡é…ç½®ï¼ˆæ¨èï¼‰

å¯ä»¥é€šè¿‡ .env æ–‡ä»¶æˆ–ç³»ç»Ÿç¯å¢ƒå˜é‡ï¼Œè‡ªåŠ¨é…ç½®è¿æ¥ä¿¡æ¯

```bash
export MODEL_MANAGER_SERVER_ADDRESS="localhost:50051"
export MODEL_MANAGER_SERVER_JWT_TOKEN="your-jwt-secret"
export MODEL_MANAGER_SERVER_GRPC_USE_TLS="false"
export MODEL_MANAGER_SERVER_GRPC_DEFAULT_AUTHORITY="localhost"
export MODEL_MANAGER_SERVER_GRPC_MAX_RETRIES="5"
export MODEL_MANAGER_SERVER_GRPC_RETRY_DELAY="1.5"
```

æˆ–è€…æœ¬åœ° `.env` æ–‡ä»¶

```
# ========================
# ğŸ”Œ gRPC é€šä¿¡é…ç½®
# ========================

# gRPC æœåŠ¡ç«¯åœ°å€ï¼ˆå¿…å¡«ï¼‰
MODEL_MANAGER_SERVER_ADDRESS=localhost:50051

# æ˜¯å¦å¯ç”¨ TLS åŠ å¯†é€šé“ï¼ˆtrue/falseï¼Œé»˜è®¤ trueï¼‰
MODEL_MANAGER_SERVER_GRPC_USE_TLS=true

# å½“ä½¿ç”¨ TLS æ—¶æŒ‡å®š authorityï¼ˆåŸŸåå¿…é¡»å’Œè¯ä¹¦åŒ¹é…æ‰éœ€è¦ï¼‰
MODEL_MANAGER_SERVER_GRPC_DEFAULT_AUTHORITY=localhost


# ========================
# ğŸ” é‰´æƒé…ç½®ï¼ˆJWTï¼‰
# ========================

# JWT ç­¾åå¯†é’¥ï¼ˆç”¨äºç”Ÿæˆ Tokenï¼‰
MODEL_MANAGER_SERVER_JWT_SECRET_KEY=your_jwt_secret_key


# ========================
# ğŸ” é‡è¯•é…ç½®ï¼ˆå¯é€‰ï¼‰
# ========================

# æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤ 3ï¼‰
MODEL_MANAGER_SERVER_GRPC_MAX_RETRIES=3

# åˆå§‹é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼Œé»˜è®¤ 1.0ï¼‰ï¼ŒæŒ‡æ•°é€€é¿
MODEL_MANAGER_SERVER_GRPC_RETRY_DELAY=1.0


# ========================
# ğŸ›¡ï¸ ç†”æ–­é™çº§é…ç½®ï¼ˆå¯é€‰ï¼‰
# ========================

# æ˜¯å¦å¯ç”¨ç†”æ–­é™çº§åŠŸèƒ½ï¼ˆé»˜è®¤ falseï¼‰
MODEL_CLIENT_RESILIENT_ENABLED=false

# HTTP é™çº§æœåŠ¡åœ°å€ï¼ˆå½“ gRPC ä¸å¯ç”¨æ—¶çš„å¤‡ç”¨åœ°å€ï¼‰
MODEL_CLIENT_HTTP_FALLBACK_URL=http://localhost:8080

# ç†”æ–­å™¨è§¦å‘é˜ˆå€¼ï¼ˆè¿ç»­å¤±è´¥å¤šå°‘æ¬¡åç†”æ–­ï¼Œé»˜è®¤ 5ï¼‰
MODEL_CLIENT_CIRCUIT_BREAKER_THRESHOLD=5

# ç†”æ–­å™¨æ¢å¤è¶…æ—¶ï¼ˆç§’ï¼Œç†”æ–­åå¤šä¹…å°è¯•æ¢å¤ï¼Œé»˜è®¤ 60ï¼‰
MODEL_CLIENT_CIRCUIT_BREAKER_TIMEOUT=60
```

åŠ è½½åï¼Œåˆå§‹åŒ–æ—¶æ— éœ€ä¼ å‚ï¼š

```python
from tamar_model_client import TamarModelClient

client = TamarModelClient()  # å°†ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„é…ç½®
```

## ğŸ”§ å¼€å‘æŒ‡å—

### ç¯å¢ƒè®¾ç½®

1. **å…‹éš†ä»“åº“**
```bash
git clone https://github.com/your-org/tamar-model-client.git
cd tamar-model-client
```

2. **åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ**
```bash
python -m venv .venv
source .venv/bin/activate  # Linux/macOS
# æˆ–
.venv\Scripts\activate  # Windows
```

3. **å®‰è£…å¼€å‘ä¾èµ–**
```bash
pip install -e .
pip install -r requirements-dev.txt  # å¦‚æœæœ‰å¼€å‘ä¾èµ–
```

### ä»£ç ç”Ÿæˆ

å¦‚æœéœ€è¦æ›´æ–° gRPC å®šä¹‰ï¼š
```bash
# ç”Ÿæˆ gRPC ä»£ç 
python make_grpc.py

# éªŒè¯ç”Ÿæˆçš„ä»£ç 
python -m pytest tests/
```

### å‘å¸ƒæµç¨‹

```bash
# 1. æ›´æ–°ç‰ˆæœ¬å· (setup.py)
# 2. æ„å»ºåŒ…
python setup.py sdist bdist_wheel

# 3. æ£€æŸ¥æ„å»º
twine check dist/*

# 4. ä¸Šä¼ åˆ° PyPI
twine upload dist/*
```

### è´¡çŒ®æŒ‡å—

1. Fork é¡¹ç›®
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯ (`git checkout -b feature/amazing-feature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add amazing feature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/amazing-feature`)
5. åˆ›å»º Pull Request

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

- **å“åº”å»¶è¿Ÿ**: å¹³å‡ < 10msï¼ˆgRPC å¼€é”€ï¼‰
- **å¹¶å‘æ”¯æŒ**: 1000+ å¹¶å‘è¯·æ±‚
- **è¿æ¥å¤ç”¨**: HTTP/2 å¤šè·¯å¤ç”¨
- **è‡ªåŠ¨é‡è¯•**: æŒ‡æ•°é€€é¿ï¼Œæœ€å¤š 5 æ¬¡

## ğŸ¤ æ”¯æŒä¸è´¡çŒ®

### è·å–å¸®åŠ©

- ğŸ“– [API æ–‡æ¡£](https://docs.tamar-model-client.com)
- ğŸ› [æäº¤ Issue](https://github.com/your-org/tamar-model-client/issues)
- ğŸ’¬ [è®¨è®ºåŒº](https://github.com/your-org/tamar-model-client/discussions)
- ğŸ“ [æ›´æ–°æ—¥å¿—](CHANGELOG.md)

### ç›¸å…³é¡¹ç›®

- [Model Manager Server](https://github.com/your-org/model-manager) - åç«¯ gRPC æœåŠ¡
- [Model Manager Dashboard](https://github.com/your-org/model-manager-dashboard) - ç®¡ç†æ§åˆ¶å°

## ğŸ“œ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶

## ğŸ‘¥ å›¢é˜Ÿ

- **Oscar Ou** - é¡¹ç›®è´Ÿè´£äºº - [oscar.ou@tamaredge.ai](mailto:oscar.ou@tamaredge.ai)
- [è´¡çŒ®è€…åˆ—è¡¨](https://github.com/your-org/tamar-model-client/graphs/contributors)

---

<div align="center">
  <p>
    <b>â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸ª Star æ”¯æŒæˆ‘ä»¬ï¼â­</b>
  </p>
  <p>
    Made with â¤ï¸ by Tamar Edge Team
  </p>
</div> 
