PySpark code

>>> from pyspark.ml.feature import StandardScaler, VectorAssembler
>>> df = VectorAssembler(inputCols=['feature2', 'feature3'], outputCol="features").transform(df) 
>>> scaler = StandardScaler(inputCol="features", outputCol="scaled_features", withMean=True)
>>> scaled_df = scaler.fit(df).transform(df)
>>> scaled_df.show()
+--------+--------+--------+----------+--------------------+
|feature1|feature2|feature3|  features|     scaled_features|
+--------+--------+--------+----------+--------------------+
|     1.0|     0.1|    -1.0|[0.1,-1.0]|[-0.6657502859356...|
|     2.0|     1.1|     1.0| [1.1,1.0]|[-0.4841820261350...|
|     3.0|    10.1|     3.0|[10.1,3.0]|[1.14993231207072...|
+--------+--------+--------+----------+--------------------+

teradatamlspk code

>>> from teradatamlspk.ml.feature import StandardScaler
>>> scaler = StandardScaler(inputCol=["feature2", "feature3"], outputCol="scaled_features", withMean=True)
>>> scaled_df = scaler.fit(df).transform(df)
>>> scaled_df.show()
+--------+------------------+--------+
|feature1|          feature2|feature3|
+--------+------------------+--------+
|     3.0| 6.333333333333332|     2.0|
|     2.0|-2.666666666666667|     0.0|
|     1.0|-3.666666666666667|    -2.0|
+--------+------------------+--------+
 