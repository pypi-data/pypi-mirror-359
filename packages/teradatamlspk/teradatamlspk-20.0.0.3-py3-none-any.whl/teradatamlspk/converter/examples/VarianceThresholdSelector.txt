PySpark code

# PySpark VarianceThresholdSelector function do not accept multiple columns so first create a Vector and then use VarianceThresholdSelector
>>> from pyspark.ml.feature import VarianceThresholdSelector, VectorAssembler
>>> df = VectorAssembler(inputCols=['feature2', 'feature3'], outputCol="features").transform(df)
>>> scaler = VarianceThresholdSelector(featuresCol="features", outputCol="scaled_features", varianceThreshold = 4.9)
>>> scaled_df = scaler.fit(df).transform(df)
>>> scaled_df.show()
+--------+--------+--------+----------+---------------+
|feature1|feature2|feature3|  features|scaled_features|
+--------+--------+--------+----------+---------------+
|     1.0|     0.1|    -1.0|[0.1,-1.0]|          [0.1]|
|     2.0|     1.1|     1.0| [1.1,1.0]|          [1.1]|
|     3.0|    10.1|     3.0|[10.1,3.0]|         [10.1]|
+--------+--------+--------+----------+---------------+


teradatamlspk code

>>> from teradatamlspk.ml.feature import VarianceThresholdSelector
>>> scaler = VarianceThresholdSelector(featuresCol=["feature2", "feature3"], outputCol="scaled_features", varianceThreshold = 4.9)
>>> scaled_df = scaler.fit(df).transform(df)
>>> scaled_df.show()
+--------+--------+
|feature1|feature2|
+--------+--------+
|     3.0|    10.1|
|     2.0|     1.1|
|     1.0|     0.1|
+--------+--------+
