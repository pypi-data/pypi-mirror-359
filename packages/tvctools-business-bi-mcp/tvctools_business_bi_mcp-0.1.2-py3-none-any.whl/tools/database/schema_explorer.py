"""
æ•°æ®åº“è¡¨ç»“æ„æ¢ç´¢æ¨¡å—
æä¾›æ•°æ®åº“è¡¨ç»“æ„æ¢ç´¢å’Œschemaä¿¡æ¯è·å–åŠŸèƒ½
"""

import json
import logging
import psycopg2
from psycopg2.extras import RealDictCursor
from typing import Dict, List, Any, Optional, Union
import sys
import os
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

# å¯¼å…¥å…¬å…±æ•°æ®åº“é…ç½®
try:
    from common.db_config import (
        POSTGRES_CONFIG, ECOMMERCE_TABLES,
        get_db_connection, execute_query, get_table_schema,
        get_database_summary
    )
    USE_COMMON_CONFIG = True
except ImportError:
    USE_COMMON_CONFIG = False
    # å¤‡ç”¨é…ç½®
    POSTGRES_CONFIG = {
        "host": "aws-0-us-east-2.pooler.supabase.com",
        "database": "postgres",
        "user": "postgres.lskfuytknzwktrrbkhsk",
        "password": "f$wFANpa+D.a3f_",
        "port": 6543
    }

def _get_default_connection_string() -> str:
    """è·å–é»˜è®¤è¿æ¥å­—ç¬¦ä¸²"""
    return f"postgresql://{POSTGRES_CONFIG['user']}:{POSTGRES_CONFIG['password']}@{POSTGRES_CONFIG['host']}:{POSTGRES_CONFIG['port']}/{POSTGRES_CONFIG['database']}"

def _get_connection_params(connection_string: Optional[str] = None) -> Dict[str, Any]:
    """ä»è¿æ¥å­—ç¬¦ä¸²æˆ–é»˜è®¤é…ç½®è·å–è¿æ¥å‚æ•°"""
    if connection_string:
        # è§£æè¿æ¥å­—ç¬¦ä¸²
        # postgresql://user:password@host:port/database
        import urllib.parse
        parsed = urllib.parse.urlparse(connection_string)
        return {
            "host": parsed.hostname,
            "port": parsed.port or 5432,
            "database": parsed.path.lstrip('/'),
            "user": parsed.username,
            "password": parsed.password
        }
    else:
        return POSTGRES_CONFIG

async def database_schema_explorer(
    database_type: str = "postgresql",
    connection_string: Optional[str] = None,
    table_pattern: Optional[str] = None,
    include_columns: bool = True
) -> Dict[str, Any]:
    """
    æ¢ç´¢æ•°æ®åº“è¡¨ç»“æ„å’Œschemaä¿¡æ¯
    
    Args:
        database_type: æ•°æ®åº“ç±»å‹ (postgresql, mysql, sqliteç­‰)
        connection_string: æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        table_pattern: è¡¨åè¿‡æ»¤æ¨¡å¼ (å¦‚ 'sales_%', '%_order%' ç­‰)
        include_columns: æ˜¯å¦åŒ…å«è¯¦ç»†çš„åˆ—ä¿¡æ¯
    
    Returns:
        åŒ…å«æ•°æ®åº“è¡¨ç»“æ„ä¿¡æ¯çš„å­—å…¸
    """
    
    try:
        # è·å–è¿æ¥å‚æ•°
        conn_params = _get_connection_params(connection_string)
            
        # ä½¿ç”¨çœŸå®çš„æ•°æ®åº“è¿æ¥
        schema_info = _query_real_schema(conn_params, table_pattern, include_columns)
        
        # ä¸ºè·¨å¢ƒç”µå•†ä¼˜åŒ–schemaå»ºè®®
        ecommerce_suggestions = _generate_ecommerce_schema_suggestions(schema_info)
        
        # ç”ŸæˆSQLç”Ÿæˆæç¤º
        sql_generation_hints = _generate_sql_hints(schema_info)
        
        return {
            "success": True,
            "data": {
                "æ•°æ®åº“ä¿¡æ¯": {
                    "æ•°æ®åº“ç±»å‹": database_type,
                    "è¡¨æ€»æ•°": len(schema_info.get("tables", [])),
                    "è¿æ¥çŠ¶æ€": "çœŸå®è¿æ¥",
                    "è¿æ¥é…ç½®": f"{conn_params['host']}:{conn_params['port']}"
                },
                "è¡¨ç»“æ„ä¿¡æ¯": schema_info,
                "è·¨å¢ƒç”µå•†å»ºè®®": ecommerce_suggestions,
                "SQLç”Ÿæˆæç¤º": sql_generation_hints
            },
            "suggested_next_steps": [
                "ä½¿ç”¨ sql_query_executor æ‰§è¡Œæ•°æ®æŸ¥è¯¢",
                "æ ¹æ®è¡¨ç»“æ„ç”Ÿæˆåˆ†æSQL",
                "é€‰æ‹©ç›¸å…³çš„ä¸šåŠ¡è¡¨è¿›è¡Œåˆ†æ"
            ]
        }
        
    except Exception as e:
        logging.error(f"æ•°æ®åº“schemaæ¢ç´¢å¤±è´¥: {str(e)}")
        return {
            "success": False,
            "error": f"schemaæ¢ç´¢å¤±è´¥: {str(e)}",
            "fallback_suggestions": [
                "æ£€æŸ¥æ•°æ®åº“è¿æ¥é…ç½®",
                "ç¡®è®¤æ•°æ®åº“è®¿é—®æƒé™",
                "ä½¿ç”¨ database_initializer åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„",
                "ä½¿ç”¨ data_collection_guide è·å–æ‰‹å·¥æ•°æ®æ”¶é›†æŒ‡å¯¼"
            ]
        }


async def sql_query_executor(
    query: str,
    query_purpose: str,
    database_type: str = "postgresql",
    connection_string: Optional[str] = None,
    limit_rows: int = 1000,
    explain_plan: bool = False
) -> Dict[str, Any]:
    """
    æ‰§è¡ŒSQLæŸ¥è¯¢å¹¶è¿”å›ç»“æœ
    
    Args:
        query: è¦æ‰§è¡Œçš„SQLæŸ¥è¯¢è¯­å¥
        query_purpose: æŸ¥è¯¢ç›®çš„è¯´æ˜ (ç”¨äºç»“æœè§£é‡Š)
        database_type: æ•°æ®åº“ç±»å‹
        connection_string: æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        limit_rows: æœ€å¤§è¿”å›è¡Œæ•°é™åˆ¶
        explain_plan: æ˜¯å¦æ˜¾ç¤ºæ‰§è¡Œè®¡åˆ’
    
    Returns:
        åŒ…å«æŸ¥è¯¢ç»“æœå’Œåˆ†æå»ºè®®çš„å­—å…¸
    """
    
    try:
        # å®‰å…¨æ£€æŸ¥SQLè¯­å¥
        safety_check = _validate_sql_safety(query)
        if not safety_check["is_safe"]:
            return {
                "success": False,
                "error": f"SQLå®‰å…¨æ£€æŸ¥å¤±è´¥: {safety_check['reason']}",
                "suggestions": [
                    "ä»…ä½¿ç”¨SELECTè¯­å¥è¿›è¡ŒæŸ¥è¯¢",
                    "é¿å…ä½¿ç”¨DELETEã€UPDATEã€DROPç­‰å±é™©æ“ä½œ",
                    "è”ç³»æ•°æ®åº“ç®¡ç†å‘˜è·å–å¸®åŠ©"
                ]
            }
        
        # æ·»åŠ LIMITé™åˆ¶ï¼ˆå¦‚æœæŸ¥è¯¢ä¸­æ²¡æœ‰ï¼‰
        modified_query = _add_limit_to_query(query, limit_rows)
        
        # è·å–è¿æ¥å‚æ•°
        conn_params = _get_connection_params(connection_string)
            
        # æ‰§è¡ŒçœŸå®æŸ¥è¯¢
        query_result = _execute_real_query(conn_params, modified_query, query_purpose)
        
        # ç”Ÿæˆæ•°æ®åˆ†æå»ºè®®
        analysis_suggestions = _generate_analysis_suggestions(query_result, query_purpose)
        
        # ç”Ÿæˆåç»­åˆ†æå·¥å…·å»ºè®®
        next_tools = _suggest_next_analysis_tools(query_result, query_purpose)
        
        return {
            "success": True,
            "data": {
                "æŸ¥è¯¢ä¿¡æ¯": {
                    "æŸ¥è¯¢ç›®çš„": query_purpose,
                    "æ‰§è¡ŒSQL": modified_query,
                    "è¿”å›è¡Œæ•°": query_result.get("row_count", 0),
                    "æ‰§è¡Œæ—¶é—´": query_result.get("execution_time", "< 1ç§’"),
                    "è¿æ¥ç±»å‹": "çœŸå®è¿æ¥",
                    "è¿æ¥é…ç½®": f"{conn_params['host']}:{conn_params['port']}"
                },
                "æŸ¥è¯¢ç»“æœ": query_result["data"],
                "æ•°æ®ç»Ÿè®¡": query_result.get("statistics", {}),
                "åˆ†æå»ºè®®": analysis_suggestions,
                "è´¨é‡è¯„ä¼°": _assess_data_quality(query_result)
            },
            "suggested_next_steps": next_tools
        }
        
    except Exception as e:
        logging.error(f"SQLæŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {str(e)}")
        return {
            "success": False,
            "error": f"æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {str(e)}",
            "troubleshooting": [
                "æ£€æŸ¥SQLè¯­æ³•æ˜¯å¦æ­£ç¡®",
                "ç¡®è®¤è¡¨åå’Œå­—æ®µåæ˜¯å¦å­˜åœ¨",
                "éªŒè¯æ•°æ®åº“è¿æ¥æ˜¯å¦æ­£å¸¸",
                "ä½¿ç”¨ database_schema_explorer é‡æ–°æŸ¥çœ‹è¡¨ç»“æ„",
                "å¦‚æœè¡¨ä¸å­˜åœ¨ï¼Œä½¿ç”¨ database_initializer åˆå§‹åŒ–æ•°æ®åº“"
            ]
        }


def _query_real_schema(conn_params: Dict[str, Any], table_pattern: Optional[str], include_columns: bool) -> Dict[str, Any]:
    """æŸ¥è¯¢çœŸå®æ•°æ®åº“çš„schemaä¿¡æ¯"""
    
    conn = None
    try:
        # ä½¿ç”¨psycopg2è¿æ¥æ•°æ®åº“
        conn = psycopg2.connect(**conn_params)
        cursor = conn.cursor(cursor_factory=RealDictCursor)
        
        # æŸ¥è¯¢æ‰€æœ‰è¡¨
        table_query = """
        SELECT table_name, table_type 
        FROM information_schema.tables 
        WHERE table_schema = 'public'
        """
        
        if table_pattern:
            # è½¬æ¢é€šé…ç¬¦æ¨¡å¼
            postgres_pattern = table_pattern.replace('*', '%').replace('?', '_')
            table_query += f" AND table_name LIKE %s"
            cursor.execute(table_query, (postgres_pattern,))
        else:
            cursor.execute(table_query)
        
        tables_result = cursor.fetchall()
        
        schema_info = {
            "database_type": "postgresql",
            "tables": {},
            "total_tables": len(tables_result),
            "schema_timestamp": "real-time"
        }
        
        for table_row in tables_result:
            table_name = table_row['table_name']
            table_info = {
                "description": f"è¡¨ç±»å‹: {table_row['table_type']}",
                "columns": {}
            }
            
            if include_columns:
                # æŸ¥è¯¢åˆ—ä¿¡æ¯
                columns_query = """
                SELECT column_name, data_type, is_nullable, column_default,
                       character_maximum_length, numeric_precision, numeric_scale
                FROM information_schema.columns 
                WHERE table_schema = 'public' AND table_name = %s
                ORDER BY ordinal_position
                """
                
                cursor.execute(columns_query, (table_name,))
                columns_result = cursor.fetchall()
                
                for col_row in columns_result:
                    col_name = col_row['column_name']
                    data_type = col_row['data_type']
                    
                    # æ„å»ºå®Œæ•´çš„ç±»å‹ä¿¡æ¯
                    if col_row['character_maximum_length']:
                        data_type += f"({col_row['character_maximum_length']})"
                    elif col_row['numeric_precision']:
                        data_type += f"({col_row['numeric_precision']}"
                        if col_row['numeric_scale']:
                            data_type += f",{col_row['numeric_scale']}"
                        data_type += ")"
                    
                    table_info["columns"][col_name] = {
                        "type": data_type.upper(),
                        "nullable": col_row['is_nullable'] == 'YES',
                        "default": col_row['column_default'],
                        "description": f"{data_type.upper()} å­—æ®µ"
                    }
            
            schema_info["tables"][table_name] = table_info
        
        return schema_info
        
    except Exception as e:
        logging.error(f"æŸ¥è¯¢çœŸå®schemaå¤±è´¥: {str(e)}")
        raise
    finally:
        if conn:
            conn.close()


def _execute_real_query(conn_params: Dict[str, Any], query: str, query_purpose: str) -> Dict[str, Any]:
    """æ‰§è¡ŒçœŸå®çš„SQLæŸ¥è¯¢"""
    
    conn = None
    try:
        # ä½¿ç”¨psycopg2è¿æ¥æ•°æ®åº“
        conn = psycopg2.connect(**conn_params)
        cursor = conn.cursor(cursor_factory=RealDictCursor)
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # æ‰§è¡ŒæŸ¥è¯¢
        cursor.execute(query)
        result = cursor.fetchall()
        
        # è®¡ç®—æ‰§è¡Œæ—¶é—´
        execution_time = round(time.time() - start_time, 3)
        
        # è½¬æ¢ç»“æœä¸ºå­—å…¸åˆ—è¡¨
        data = []
        for row in result:
            row_dict = {}
            for key, value in row.items():
                # å¤„ç†ç‰¹æ®Šæ•°æ®ç±»å‹
                if hasattr(value, 'isoformat'):  # datetimeå¯¹è±¡
                    row_dict[key] = value.isoformat()
                elif isinstance(value, (int, float, str, bool)) or value is None:
                    row_dict[key] = value
                else:
                    row_dict[key] = str(value)
            data.append(row_dict)
        
        # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        statistics = {}
        if data:
            numeric_columns = []
            for key, value in data[0].items():
                if isinstance(value, (int, float)):
                    numeric_columns.append(key)
            
            for col in numeric_columns:
                values = [row[col] for row in data if row[col] is not None]
                if values:
                    statistics[col] = {
                        "æ€»è®¡": sum(values),
                        "å¹³å‡å€¼": round(sum(values) / len(values), 2),
                        "æœ€å¤§å€¼": max(values),
                        "æœ€å°å€¼": min(values),
                        "è®°å½•æ•°": len(values)
                    }
        
        return {
            "data": data,
            "row_count": len(data),
            "execution_time": f"{execution_time}ç§’",
            "statistics": statistics
        }
        
    except Exception as e:
        logging.error(f"æ‰§è¡ŒçœŸå®æŸ¥è¯¢å¤±è´¥: {str(e)}")
        raise
    finally:
        if conn:
            conn.close()


def _add_limit_to_query(query: str, limit_rows: int) -> str:
    """ç»™æŸ¥è¯¢æ·»åŠ LIMITé™åˆ¶"""
    query_upper = query.upper().strip()
    
    # å¦‚æœæŸ¥è¯¢å·²ç»åŒ…å«LIMITï¼Œä¸é‡å¤æ·»åŠ 
    if 'LIMIT' in query_upper:
        return query
    
    # å¦‚æœæ˜¯SELECTæŸ¥è¯¢ï¼Œæ·»åŠ LIMIT
    if query_upper.startswith('SELECT'):
        return f"{query.rstrip(';')} LIMIT {limit_rows}"
    
    return query


def _simulate_schema_query(database_type: str, table_pattern: Optional[str], include_columns: bool) -> Dict[str, Any]:
    """æ¨¡æ‹Ÿæ•°æ®åº“schemaæŸ¥è¯¢"""
    
    # è·¨å¢ƒç”µå•†å…¸å‹è¡¨ç»“æ„
    ecommerce_tables = {
        "orders": {
            "description": "è®¢å•ä¸»è¡¨",
            "columns": {
                "order_id": {"type": "VARCHAR(50)", "description": "è®¢å•ID", "primary_key": True},
                "customer_id": {"type": "VARCHAR(50)", "description": "å®¢æˆ·ID"},
                "order_date": {"type": "DATETIME", "description": "è®¢å•æ—¥æœŸ"},
                "order_status": {"type": "VARCHAR(20)", "description": "è®¢å•çŠ¶æ€"},
                "total_amount": {"type": "DECIMAL(10,2)", "description": "è®¢å•æ€»é‡‘é¢"},
                "currency": {"type": "VARCHAR(3)", "description": "è´§å¸ç±»å‹"},
                "platform": {"type": "VARCHAR(20)", "description": "é”€å”®å¹³å°"},
                "market": {"type": "VARCHAR(20)", "description": "ç›®æ ‡å¸‚åœº"},
                "shipping_fee": {"type": "DECIMAL(8,2)", "description": "è¿è´¹"},
                "tax_amount": {"type": "DECIMAL(8,2)", "description": "ç¨è´¹"},
                "created_at": {"type": "TIMESTAMP", "description": "åˆ›å»ºæ—¶é—´"},
                "updated_at": {"type": "TIMESTAMP", "description": "æ›´æ–°æ—¶é—´"}
            } if include_columns else {}
        },
        "order_items": {
            "description": "è®¢å•å•†å“æ˜ç»†è¡¨",
            "columns": {
                "item_id": {"type": "BIGINT", "description": "æ˜ç»†ID", "primary_key": True},
                "order_id": {"type": "VARCHAR(50)", "description": "è®¢å•ID", "foreign_key": "orders.order_id"},
                "product_id": {"type": "VARCHAR(50)", "description": "å•†å“ID"},
                "sku": {"type": "VARCHAR(100)", "description": "å•†å“SKU"},
                "quantity": {"type": "INT", "description": "è´­ä¹°æ•°é‡"},
                "unit_price": {"type": "DECIMAL(8,2)", "description": "å•ä»·"},
                "discount_amount": {"type": "DECIMAL(8,2)", "description": "æŠ˜æ‰£é‡‘é¢"},
                "total_price": {"type": "DECIMAL(10,2)", "description": "å°è®¡é‡‘é¢"}
            } if include_columns else {}
        },
        "products": {
            "description": "å•†å“ä¿¡æ¯è¡¨",
            "columns": {
                "product_id": {"type": "VARCHAR(50)", "description": "å•†å“ID", "primary_key": True},
                "product_name": {"type": "VARCHAR(200)", "description": "å•†å“åç§°"},
                "category": {"type": "VARCHAR(50)", "description": "å•†å“åˆ†ç±»"},
                "brand": {"type": "VARCHAR(50)", "description": "å“ç‰Œ"},
                "cost_price": {"type": "DECIMAL(8,2)", "description": "æˆæœ¬ä»·"},
                "selling_price": {"type": "DECIMAL(8,2)", "description": "å”®ä»·"},
                "weight": {"type": "DECIMAL(8,3)", "description": "é‡é‡(kg)"},
                "dimensions": {"type": "VARCHAR(50)", "description": "å°ºå¯¸"},
                "status": {"type": "VARCHAR(20)", "description": "å•†å“çŠ¶æ€"}
            } if include_columns else {}
        },
        "customers": {
            "description": "å®¢æˆ·ä¿¡æ¯è¡¨",
            "columns": {
                "customer_id": {"type": "VARCHAR(50)", "description": "å®¢æˆ·ID", "primary_key": True},
                "customer_name": {"type": "VARCHAR(100)", "description": "å®¢æˆ·å§“å"},
                "email": {"type": "VARCHAR(100)", "description": "é‚®ç®±"},
                "country": {"type": "VARCHAR(50)", "description": "å›½å®¶"},
                "registration_date": {"type": "DATE", "description": "æ³¨å†Œæ—¥æœŸ"},
                "customer_type": {"type": "VARCHAR(20)", "description": "å®¢æˆ·ç±»å‹"},
                "total_orders": {"type": "INT", "description": "æ€»è®¢å•æ•°"},
                "total_spent": {"type": "DECIMAL(12,2)", "description": "æ€»æ¶ˆè´¹é‡‘é¢"}
            } if include_columns else {}
        },
        "inventory": {
            "description": "åº“å­˜ç®¡ç†è¡¨",
            "columns": {
                "sku": {"type": "VARCHAR(100)", "description": "å•†å“SKU", "primary_key": True},
                "warehouse": {"type": "VARCHAR(50)", "description": "ä»“åº“ä½ç½®"},
                "stock_quantity": {"type": "INT", "description": "åº“å­˜æ•°é‡"},
                "reserved_quantity": {"type": "INT", "description": "é¢„ç•™æ•°é‡"},
                "reorder_point": {"type": "INT", "description": "è¡¥è´§ç‚¹"},
                "last_updated": {"type": "TIMESTAMP", "description": "æœ€åæ›´æ–°æ—¶é—´"}
            } if include_columns else {}
        }
    }
    
    # æ ¹æ®table_patternè¿‡æ»¤è¡¨
    filtered_tables = {}
    if table_pattern:
        import fnmatch
        for table_name, table_info in ecommerce_tables.items():
            if fnmatch.fnmatch(table_name, table_pattern):
                filtered_tables[table_name] = table_info
    else:
        filtered_tables = ecommerce_tables
    
    return {
        "database_type": database_type,
        "tables": filtered_tables,
        "total_tables": len(filtered_tables),
        "schema_timestamp": "æ¨¡æ‹Ÿæ•°æ®"
    }


def _generate_ecommerce_schema_suggestions(schema_info: Dict[str, Any]) -> Dict[str, Any]:
    """ä¸ºè·¨å¢ƒç”µå•†ç”Ÿæˆschemaå»ºè®®"""
    
    tables = schema_info.get("tables", {})
    table_names = list(tables.keys())
    
    suggestions = {
        "æ•°æ®å®Œæ•´æ€§å»ºè®®": [],
        "æ€§èƒ½ä¼˜åŒ–å»ºè®®": [],
        "ä¸šåŠ¡åˆ†æå»ºè®®": []
    }
    
    # æ£€æŸ¥å…³é”®è¡¨æ˜¯å¦å­˜åœ¨
    key_tables = ["orders", "order_items", "products", "customers"]
    missing_tables = [table for table in key_tables if table not in table_names]
    
    if missing_tables:
        suggestions["æ•°æ®å®Œæ•´æ€§å»ºè®®"].append(f"å»ºè®®æ·»åŠ å…³é”®è¡¨: {', '.join(missing_tables)}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è®¢å•ç›¸å…³è¡¨
    if any("order" in name.lower() for name in table_names):
        suggestions["ä¸šåŠ¡åˆ†æå»ºè®®"].append("å¯ä»¥è¿›è¡Œè®¢å•è¶‹åŠ¿åˆ†æå’Œå®¢æˆ·è´­ä¹°è¡Œä¸ºåˆ†æ")
        suggestions["æ€§èƒ½ä¼˜åŒ–å»ºè®®"].append("ä¸ºè®¢å•è¡¨çš„æ—¥æœŸå­—æ®µåˆ›å»ºç´¢å¼•ä»¥ä¼˜åŒ–æ—¶é—´èŒƒå›´æŸ¥è¯¢")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰äº§å“ç›¸å…³è¡¨
    if any("product" in name.lower() for name in table_names):
        suggestions["ä¸šåŠ¡åˆ†æå»ºè®®"].append("å¯ä»¥è¿›è¡Œå•†å“é”€å”®åˆ†æå’Œåº“å­˜ç®¡ç†åˆ†æ")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å®¢æˆ·ç›¸å…³è¡¨
    if any("customer" in name.lower() or "user" in name.lower() for name in table_names):
        suggestions["ä¸šåŠ¡åˆ†æå»ºè®®"].append("å¯ä»¥è¿›è¡Œå®¢æˆ·ç»†åˆ†å’ŒRFMåˆ†æ")
        
    # é€šç”¨å»ºè®®
    suggestions["æ€§èƒ½ä¼˜åŒ–å»ºè®®"].extend([
        "ä¸ºå¤–é”®å­—æ®µåˆ›å»ºç´¢å¼•",
        "è€ƒè™‘å¯¹å¤§è¡¨è¿›è¡Œåˆ†åŒº",
        "å®šæœŸæ›´æ–°è¡¨ç»Ÿè®¡ä¿¡æ¯"
    ])
    
    return suggestions


def _generate_sql_hints(schema_info: Dict[str, Any]) -> List[Dict[str, str]]:
    """ç”ŸæˆSQLç¼–å†™æç¤º"""
    
    tables = schema_info.get("tables", {})
    hints = []
    
    # åŸºäºç°æœ‰è¡¨ç”ŸæˆæŸ¥è¯¢ç¤ºä¾‹
    table_names = list(tables.keys())
    
    if "orders" in table_names:
        hints.append({
            "æŸ¥è¯¢ç±»å‹": "é”€å”®è¶‹åŠ¿åˆ†æ",
            "SQLç¤ºä¾‹": "SELECT DATE(order_date) as date, COUNT(*) as order_count, SUM(total_amount) as revenue FROM orders WHERE order_date >= '2024-01-01' GROUP BY DATE(order_date) ORDER BY date",
            "è¯´æ˜": "æŒ‰æ—¥ç»Ÿè®¡è®¢å•æ•°é‡å’Œæ”¶å…¥"
        })
    
    if "order_items" in table_names and "products" in table_names:
        hints.append({
            "æŸ¥è¯¢ç±»å‹": "å•†å“é”€å”®æ’è¡Œ",
            "SQLç¤ºä¾‹": "SELECT p.product_name, SUM(oi.quantity) as total_sold, SUM(oi.total_price) as revenue FROM order_items oi JOIN products p ON oi.product_id = p.product_id GROUP BY p.product_id, p.product_name ORDER BY revenue DESC LIMIT 10",
            "è¯´æ˜": "è·å–é”€å”®é¢å‰10çš„å•†å“"
        })
    
    if "customers" in table_names:
        hints.append({
            "æŸ¥è¯¢ç±»å‹": "å®¢æˆ·åˆ†æ",
            "SQLç¤ºä¾‹": "SELECT country, COUNT(*) as customer_count, AVG(total_spent) as avg_spent FROM customers GROUP BY country ORDER BY customer_count DESC",
            "è¯´æ˜": "æŒ‰å›½å®¶ç»Ÿè®¡å®¢æˆ·æ•°é‡å’Œå¹³å‡æ¶ˆè´¹"
        })
    
    # é€šç”¨æŸ¥è¯¢æç¤º
    hints.extend([
        {
            "æŸ¥è¯¢ç±»å‹": "æ•°æ®è´¨é‡æ£€æŸ¥",
            "SQLç¤ºä¾‹": "SELECT COUNT(*) as total_rows, COUNT(DISTINCT order_id) as unique_orders FROM orders",
            "è¯´æ˜": "æ£€æŸ¥æ•°æ®é‡å¤æƒ…å†µ"
        },
        {
            "æŸ¥è¯¢ç±»å‹": "è¡¨ä¿¡æ¯æŸ¥è¯¢",
            "SQLç¤ºä¾‹": "SELECT table_name, column_name, data_type FROM information_schema.columns WHERE table_schema = 'public' ORDER BY table_name, ordinal_position",
            "è¯´æ˜": "æŸ¥çœ‹æ•°æ®åº“ç»“æ„ä¿¡æ¯"
        }
    ])
    
    return hints


def _simulate_query_execution(query: str, query_purpose: str, limit_rows: int) -> Dict[str, Any]:
    """æ¨¡æ‹ŸSQLæŸ¥è¯¢æ‰§è¡Œ"""
    
    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    mock_data = []
    
    if "orders" in query.lower():
        mock_data = [
            {"order_id": "ORD001", "platform": "Amazon", "total_amount": 156.78, "order_date": "2024-01-15"},
            {"order_id": "ORD002", "platform": "eBay", "total_amount": 89.50, "order_date": "2024-01-16"},
            {"order_id": "ORD003", "platform": "Shopify", "total_amount": 234.90, "order_date": "2024-01-16"},
            {"order_id": "ORD004", "platform": "Amazon", "total_amount": 67.20, "order_date": "2024-01-17"},
            {"order_id": "ORD005", "platform": "eBay", "total_amount": 145.60, "order_date": "2024-01-17"}
        ]
    elif "customers" in query.lower():
        mock_data = [
            {"customer_id": "CUST001", "country": "US", "total_spent": 456.78, "total_orders": 3},
            {"customer_id": "CUST002", "country": "UK", "total_spent": 234.50, "total_orders": 2},
            {"customer_id": "CUST003", "country": "DE", "total_spent": 567.90, "total_orders": 4},
            {"customer_id": "CUST004", "country": "CA", "total_spent": 123.40, "total_orders": 1},
            {"customer_id": "CUST005", "country": "AU", "total_spent": 789.10, "total_orders": 5}
        ]
    else:
        mock_data = [
            {"id": 1, "name": "ç¤ºä¾‹æ•°æ®1", "value": 100},
            {"id": 2, "name": "ç¤ºä¾‹æ•°æ®2", "value": 200},
            {"id": 3, "name": "ç¤ºä¾‹æ•°æ®3", "value": 150}
        ]
    
    # åº”ç”¨limité™åˆ¶
    limited_data = mock_data[:limit_rows]
    
    # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
    statistics = {}
    if limited_data and any(isinstance(v, (int, float)) for v in limited_data[0].values()):
        for key in limited_data[0].keys():
            values = [row[key] for row in limited_data if isinstance(row[key], (int, float))]
            if values:
                statistics[key] = {
                    "æ€»è®¡": sum(values),
                    "å¹³å‡å€¼": round(sum(values) / len(values), 2),
                    "æœ€å¤§å€¼": max(values),
                    "æœ€å°å€¼": min(values),
                    "è®°å½•æ•°": len(values)
                }
    
    return {
        "data": limited_data,
        "row_count": len(limited_data),
        "execution_time": "0.05ç§’",
        "statistics": statistics
    }


def _validate_sql_safety(query: str) -> Dict[str, Any]:
    """éªŒè¯SQLæŸ¥è¯¢çš„å®‰å…¨æ€§"""
    
    query_upper = query.upper().strip()
    
    # å±é™©å…³é”®è¯æ£€æŸ¥
    dangerous_keywords = [
        'DELETE', 'DROP', 'TRUNCATE', 'ALTER', 'CREATE', 'INSERT', 'UPDATE',
        'EXEC', 'EXECUTE', 'MERGE', 'REPLACE', 'GRANT', 'REVOKE'
    ]
    
    for keyword in dangerous_keywords:
        if keyword in query_upper:
            return {
                "is_safe": False,
                "reason": f"æŸ¥è¯¢åŒ…å«å±é™©å…³é”®è¯: {keyword}",
                "suggestion": "ä»…å…è®¸ä½¿ç”¨SELECTè¯­å¥è¿›è¡Œæ•°æ®æŸ¥è¯¢"
            }
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºSELECTæŸ¥è¯¢
    if not query_upper.startswith('SELECT') and not query_upper.startswith('WITH'):
        return {
            "is_safe": False,
            "reason": "ä»…æ”¯æŒSELECTæŸ¥è¯¢è¯­å¥",
            "suggestion": "è¯·ä½¿ç”¨SELECTè¯­å¥è¿›è¡Œæ•°æ®æŸ¥è¯¢"
        }
    
    return {
        "is_safe": True,
        "reason": "æŸ¥è¯¢å®‰å…¨æ£€æŸ¥é€šè¿‡"
    }


def _generate_analysis_suggestions(query_result: Dict[str, Any], query_purpose: str) -> List[str]:
    """æ ¹æ®æŸ¥è¯¢ç»“æœç”Ÿæˆåˆ†æå»ºè®®"""
    
    suggestions = []
    data = query_result.get("data", [])
    row_count = query_result.get("row_count", 0)
    
    # åŸºäºæ•°æ®é‡çš„å»ºè®®
    if row_count == 0:
        suggestions.append("âŒ æŸ¥è¯¢æœªè¿”å›æ•°æ®ï¼Œå»ºè®®æ£€æŸ¥æŸ¥è¯¢æ¡ä»¶æˆ–è¡¨ä¸­æ˜¯å¦æœ‰æ•°æ®")
    elif row_count < 10:
        suggestions.append("âš ï¸ æ•°æ®é‡è¾ƒå°‘ï¼Œå»ºè®®æ‰©å¤§æŸ¥è¯¢èŒƒå›´æˆ–æ£€æŸ¥æ•°æ®å®Œæ•´æ€§")
    elif row_count >= 1000:
        suggestions.append("ğŸ“Š æ•°æ®é‡è¾ƒå¤§ï¼Œå»ºè®®æ·»åŠ æ›´å¤šç­›é€‰æ¡ä»¶æˆ–ä½¿ç”¨åˆ†é¡µæŸ¥è¯¢")
    else:
        suggestions.append("âœ… æ•°æ®é‡é€‚ä¸­ï¼Œå¯ä»¥è¿›è¡Œè¿›ä¸€æ­¥åˆ†æ")
    
    # åŸºäºæŸ¥è¯¢ç›®çš„çš„å»ºè®®
    if "é”€å”®" in query_purpose or "è®¢å•" in query_purpose:
        suggestions.append("ğŸ’° å»ºè®®åˆ†æé”€å”®è¶‹åŠ¿ã€å¹³å°å¯¹æ¯”ã€å®¢æˆ·è¡Œä¸ºç­‰ç»´åº¦")
        suggestions.append("ğŸ“ˆ å¯ä»¥ä½¿ç”¨ chart_type_advisor é€‰æ‹©åˆé€‚çš„å›¾è¡¨å±•ç¤ºé”€å”®æ•°æ®")
    elif "å®¢æˆ·" in query_purpose:
        suggestions.append("ğŸ‘¥ å»ºè®®åˆ†æå®¢æˆ·åœ°åŸŸåˆ†å¸ƒã€æ¶ˆè´¹è¡Œä¸ºã€å¤è´­ç‡ç­‰æŒ‡æ ‡")
        suggestions.append("ğŸ¯ å¯ä»¥ä½¿ç”¨ business_problem_analyzer è·å–é—®é¢˜åˆ†ææŒ‡å¯¼")
    elif "åº“å­˜" in query_purpose:
        suggestions.append("ğŸ“¦ å»ºè®®åˆ†æåº“å­˜å‘¨è½¬ã€ç¼ºè´§é£é™©ã€æˆæœ¬æ§åˆ¶ç­‰æ–¹é¢")
    
    # åŸºäºæ•°æ®ç‰¹å¾çš„å»ºè®®
    if data:
        # æ£€æŸ¥æ˜¯å¦æœ‰æ—¶é—´å­—æ®µ
        time_fields = [k for k in data[0].keys() if 'date' in k.lower() or 'time' in k.lower()]
        if time_fields:
            suggestions.append(f"ğŸ“… å‘ç°æ—¶é—´å­—æ®µ {time_fields[0]}ï¼Œå»ºè®®è¿›è¡Œæ—¶é—´è¶‹åŠ¿åˆ†æ")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é‡‘é¢å­—æ®µ
        amount_fields = [k for k in data[0].keys() if any(word in k.lower() for word in ['amount', 'price', 'cost', 'spent'])]
        if amount_fields:
            suggestions.append(f"ğŸ’µ å‘ç°é‡‘é¢å­—æ®µ {amount_fields[0]}ï¼Œå»ºè®®è¿›è¡Œè´¢åŠ¡åˆ†æ")
    
    return suggestions


def _suggest_next_analysis_tools(query_result: Dict[str, Any], query_purpose: str) -> List[str]:
    """æ ¹æ®æŸ¥è¯¢ç»“æœæ¨èåç»­åˆ†æå·¥å…·"""
    
    tools = []
    
    # åŸºç¡€åˆ†æå·¥å…·
    tools.append("ğŸ“Š ä½¿ç”¨ insight_generator ä»æŸ¥è¯¢ç»“æœä¸­æå–ä¸šåŠ¡æ´å¯Ÿ")
    tools.append("ğŸ“ˆ ä½¿ç”¨ chart_type_advisor é€‰æ‹©åˆé€‚çš„å›¾è¡¨ç±»å‹å±•ç¤ºæ•°æ®")
    tools.append("ğŸ“ ä½¿ç”¨ insight_generator è§£è¯»åˆ†æç»“æœçš„ä¸šåŠ¡å«ä¹‰")
    
    # åŸºäºæŸ¥è¯¢ç›®çš„æ¨èå·¥å…·
    if "å¯¹æ¯”" in query_purpose or "æ¯”è¾ƒ" in query_purpose:
        tools.append("ğŸ” ä½¿ç”¨ sales_comparison_analyzer è¿›è¡Œæ·±åº¦å¯¹æ¯”åˆ†æ")
    
    if "è¶‹åŠ¿" in query_purpose or "å˜åŒ–" in query_purpose:
        tools.append("ğŸ“… ä½¿ç”¨ analysis_method_recommender è·å–è¶‹åŠ¿åˆ†ææ–¹æ³•")
    
    # åç»­è¡ŒåŠ¨å»ºè®®
    tools.append("ğŸ¯ ä½¿ç”¨ action_recommender è·å–åŸºäºæ•°æ®çš„è¡ŒåŠ¨å»ºè®®")
    tools.append("â“ ä½¿ç”¨ follow_up_questions ç”Ÿæˆè¿›ä¸€æ­¥åˆ†æçš„é—®é¢˜")
    
    return tools


def _assess_data_quality(query_result: Dict[str, Any]) -> Dict[str, str]:
    """è¯„ä¼°æŸ¥è¯¢ç»“æœçš„æ•°æ®è´¨é‡"""
    
    data = query_result.get("data", [])
    row_count = query_result.get("row_count", 0)
    
    assessment = {}
    
    # æ•°æ®å®Œæ•´æ€§è¯„ä¼°
    if row_count == 0:
        assessment["å®Œæ•´æ€§"] = "æ— æ•°æ® - éœ€è¦æ£€æŸ¥æ•°æ®æº"
    elif row_count < 10:
        assessment["å®Œæ•´æ€§"] = "æ•°æ®é‡å°‘ - å¯èƒ½å½±å“åˆ†æå‡†ç¡®æ€§"
    else:
        assessment["å®Œæ•´æ€§"] = "æ•°æ®é‡å……è¶³ - é€‚åˆè¿›è¡Œåˆ†æ"
    
    # æ•°æ®æ–°é²œåº¦è¯„ä¼°
    if data:
        time_fields = [k for k in data[0].keys() if 'date' in k.lower() or 'time' in k.lower()]
        if time_fields:
            assessment["æ—¶æ•ˆæ€§"] = "åŒ…å«æ—¶é—´ä¿¡æ¯ - å¯è¿›è¡Œæ—¶é—´åºåˆ—åˆ†æ"
        else:
            assessment["æ—¶æ•ˆæ€§"] = "ç¼ºå°‘æ—¶é—´å­—æ®µ - å»ºè®®æ£€æŸ¥æ•°æ®æ—¶æ•ˆæ€§"
    
    # æ•°æ®å¤šæ ·æ€§è¯„ä¼°
    if data and len(data) > 1:
        # æ£€æŸ¥æ˜¯å¦æœ‰åˆ†ç±»å­—æ®µ
        categorical_fields = []
        for key in data[0].keys():
            values = [row[key] for row in data[:10]]  # æ£€æŸ¥å‰10è¡Œ
            unique_values = len(set(str(v) for v in values))
            if unique_values > 1 and unique_values < len(values):
                categorical_fields.append(key)
        
        if categorical_fields:
            assessment["å¤šæ ·æ€§"] = f"å‘ç°åˆ†ç±»å­—æ®µ {categorical_fields[0]} - é€‚åˆåˆ†ç»„åˆ†æ"
        else:
            assessment["å¤šæ ·æ€§"] = "æ•°æ®ç›¸å¯¹å•ä¸€ - å»ºè®®æ‰©å¤§æŸ¥è¯¢èŒƒå›´"
    
    return assessment 