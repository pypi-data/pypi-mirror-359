{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌐 Extracting from: https://www.joanwestenberg.com/\n",
      "📄 Page: Westenberg\n",
      "🧠 Analyzing patterns with LLM...\n",
      "✅ Pattern identified: articles (confidence: 0.85)\n",
      "💾 Pattern cached to: pattern_cache/pattern_www.joanwestenberg.com_be32fae3.json\n",
      "✅ Found 9 articles items\n",
      "{\n",
      "  \"extraction_info\": {\n",
      "    \"base_url\": \"https://www.joanwestenberg.com/\",\n",
      "    \"total_items_found\": 9,\n",
      "    \"pattern_type\": \"articles\",\n",
      "    \"confidence_score\": 0.85\n",
      "  },\n",
      "  \"links\": [\n",
      "    {\n",
      "      \"url\": \"https://www.joanwestenberg.com/p/the-cannae-problem\",\n",
      "      \"title\": \"May 2, 2025\\u202216 min readThe Cannae Problem\",\n",
      "      \"publication_date\": \"2025-05-02\"\n",
      "    },\n",
      "    {\n",
      "      \"url\": \"https://www.joanwestenberg.com/p/the-cult-of-hard-mode-why-simplicity-offends-tech-elites\",\n",
      "      \"title\": \"Jun 12, 2025\\u20228 min readThe Cult of Hard Mode\",\n",
      "      \"publication_date\": \"2025-06-12\"\n",
      "    },\n",
      "    {\n",
      "      \"url\": \"https://www.joanwestenberg.com/p/i-miss-the-internet\",\n",
      "      \"title\": \"Nov 12, 2024\\u202210 min readI Miss the Internet\",\n",
      "      \"publication_date\": \"2024-11-12\"\n",
      "    },\n",
      "    {\n",
      "      \"url\": \"https://www.joanwestenberg.com/p/the-unbearable-lightness-of-cringe-ca057ebf7129989f\",\n",
      "      \"title\": \"Jul 1, 2025\\u20226 min readThe Unbearable Lightness of CringeEvery Great Creator Pays the Tax. The Rest Stay SpectatorsJoan Westenberg\",\n",
      "      \"publication_date\": \"2025-07-01\"\n",
      "    },\n",
      "    {\n",
      "      \"url\": \"https://www.joanwestenberg.com/p/members-only-q-a-731397f2ef21eac9\",\n",
      "      \"title\": \"Jun 30, 2025\\u20226 min readMembers-Only Q&AAsk Me (Almost) AnythingJoan Westenberg\",\n",
      "      \"publication_date\": \"2025-06-30\"\n",
      "    },\n",
      "    {\n",
      "      \"url\": \"https://www.joanwestenberg.com/p/the-good-life-is-a-moving-target-2e5a\",\n",
      "      \"title\": \"Jun 27, 2025\\u20227 min readThe Good Life Is a Moving TargetThe Last Remaining Acts of Moral AgencyJoan Westenberg\",\n",
      "      \"publication_date\": \"2025-06-27\"\n",
      "    },\n",
      "    {\n",
      "      \"url\": \"https://www.joanwestenberg.com/p/the-line-at-agincourt-a0a49160dae9da46\",\n",
      "      \"title\": \"Jun 26, 2025\\u20222 min readThe Line at AgincourtWhy Hierarchical Systems Collapse Under Mud and RainJoan Westenberg\",\n",
      "      \"publication_date\": \"2025-06-26\"\n",
      "    },\n",
      "    {\n",
      "      \"url\": \"https://www.joanwestenberg.com/p/the-spoken-word-is-the-hinge-of-history-8c741ecfd4eda719\",\n",
      "      \"title\": \"Jun 24, 2025\\u20226 min readThe Spoken Word Is the Hinge of HistoryGuest PostJoan Westenberg, +1\",\n",
      "      \"publication_date\": \"2025-06-24\"\n",
      "    },\n",
      "    {\n",
      "      \"url\": \"https://www.joanwestenberg.com/p/cognitive-offshoring-and-the-decline-of-personal-thought-26c6d43ac5089a9a\",\n",
      "      \"title\": \"Jun 23, 2025\\u20227 min readCognitive Offshoring and the Decline of Personal Thought\\u201cI would prefer not to\\u201d: Bartleby the Scrivener, and the Modern Knowledge WorkerJoan Westenberg\",\n",
      "      \"publication_date\": \"2025-06-23\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional\n",
    "from url2md4ai import Config, ContentExtractor\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "\n",
    "class PatternAnalysis(BaseModel):\n",
    "    pattern_type: str\n",
    "    primary_selectors: List[str]\n",
    "    fallback_selectors: List[str]\n",
    "    url_pattern_regex: str\n",
    "    content_indicators: List[str]\n",
    "    skip_patterns: List[str]\n",
    "    confidence_score: float\n",
    "    pattern_description: str\n",
    "    estimated_items: int\n",
    "    date_selectors: List[str]\n",
    "\n",
    "class ExtractedPattern(BaseModel):\n",
    "    links: List[Dict[str, Any]]\n",
    "    total_found: int\n",
    "    pattern_used: str\n",
    "    confidence: float\n",
    "    base_url: str\n",
    "    pattern_analysis: PatternAnalysis\n",
    "\n",
    "class UniversalPatternExtractor:\n",
    "    def __init__(self, openai_api_key: str = None, cache_dir: str = \"pattern_cache\"):\n",
    "        self.openai_api_key = openai_api_key or os.getenv(\"OPENAI_API_KEY\")\n",
    "        self.client = OpenAI(api_key=self.openai_api_key)\n",
    "        self.extractor = ContentExtractor(Config.from_env())\n",
    "        self.cache_dir = cache_dir\n",
    "        \n",
    "        if not os.path.exists(self.cache_dir):\n",
    "            os.makedirs(self.cache_dir)\n",
    "    \n",
    "    def _get_domain_hash(self, url: str) -> str:\n",
    "        \"\"\"Generate a hash for the domain to use as cache key.\"\"\"\n",
    "        domain = urlparse(url).netloc\n",
    "        return hashlib.md5(domain.encode()).hexdigest()[:8]\n",
    "    \n",
    "    def _get_cache_file_path(self, url: str) -> str:\n",
    "        \"\"\"Get the cache file path for a domain.\"\"\"\n",
    "        domain_hash = self._get_domain_hash(url)\n",
    "        domain = urlparse(url).netloc\n",
    "        return os.path.join(self.cache_dir, f\"pattern_{domain}_{domain_hash}.json\")\n",
    "    \n",
    "    def _load_cached_pattern(self, url: str) -> Optional[PatternAnalysis]:\n",
    "        \"\"\"Load cached pattern analysis for a domain.\"\"\"\n",
    "        cache_file = self._get_cache_file_path(url)\n",
    "        \n",
    "        if os.path.exists(cache_file):\n",
    "            try:\n",
    "                with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                    print(f\"📦 Using cached pattern from: {cache_file}\")\n",
    "                    return PatternAnalysis(**data)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Failed to load cache: {e}\")\n",
    "                return None\n",
    "        return None\n",
    "    \n",
    "    def _save_cached_pattern(self, url: str, pattern: PatternAnalysis):\n",
    "        \"\"\"Save pattern analysis to cache.\"\"\"\n",
    "        cache_file = self._get_cache_file_path(url)\n",
    "        try:\n",
    "            with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(pattern.model_dump(), f, indent=2, ensure_ascii=False)\n",
    "                print(f\"💾 Pattern cached to: {cache_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Failed to save cache: {e}\")\n",
    "    \n",
    "    def analyze_html_structure(self, soup: BeautifulSoup, base_url: str) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze HTML structure to identify patterns.\"\"\"\n",
    "        all_links = soup.find_all('a', href=True)\n",
    "        \n",
    "        link_contexts = []\n",
    "        for link in all_links:\n",
    "            href = link.get('href')\n",
    "            text = link.get_text(strip=True)\n",
    "            \n",
    "            if not href or not text or len(text) < 3:\n",
    "                continue\n",
    "                \n",
    "            parent = link.parent\n",
    "            parent_class = ' '.join(parent.get('class', [])) if parent else ''\n",
    "            \n",
    "            full_url = urljoin(base_url, href)\n",
    "            url_parts = href.strip('/').split('/')\n",
    "            \n",
    "            link_contexts.append({\n",
    "                'text': text,\n",
    "                'href': href,\n",
    "                'full_url': full_url,\n",
    "                'text_length': len(text),\n",
    "                'parent_tag': parent.name if parent else 'unknown',\n",
    "                'parent_class': parent_class,\n",
    "                'url_parts': url_parts,\n",
    "                'url_depth': len(url_parts)\n",
    "            })\n",
    "        \n",
    "        # Find common patterns\n",
    "        parent_patterns = Counter()\n",
    "        url_patterns = Counter()\n",
    "        \n",
    "        for ctx in link_contexts:\n",
    "            if ctx['parent_class']:\n",
    "                parent_patterns[f\"{ctx['parent_tag']}.{ctx['parent_class']}\"] += 1\n",
    "            else:\n",
    "                parent_patterns[ctx['parent_tag']] += 1\n",
    "            \n",
    "            if len(ctx['url_parts']) >= 2:\n",
    "                url_patterns[ctx['url_parts'][0]] += 1\n",
    "        \n",
    "        return {\n",
    "            'total_links': len(all_links),\n",
    "            'analyzed_links': len(link_contexts),\n",
    "            'link_contexts': link_contexts,\n",
    "            'parent_patterns': dict(parent_patterns.most_common(10)),\n",
    "            'url_patterns': dict(url_patterns.most_common(10)),\n",
    "            'avg_text_length': sum(ctx['text_length'] for ctx in link_contexts) / len(link_contexts) if link_contexts else 0\n",
    "        }\n",
    "    \n",
    "    async def analyze_patterns_with_llm(self, base_url: str, html_content: str, structure_analysis: Dict, force_regenerate: bool = False) -> PatternAnalysis:\n",
    "        \"\"\"Use LLM to identify content patterns.\"\"\"\n",
    "        \n",
    "        # Try cache first unless forced to regenerate\n",
    "        if not force_regenerate:\n",
    "            cached_pattern = self._load_cached_pattern(base_url)\n",
    "            if cached_pattern:\n",
    "                return cached_pattern\n",
    "        \n",
    "        print(\"🧠 Analyzing patterns with LLM...\")\n",
    "        \n",
    "        system_prompt = \"\"\"You are an expert at identifying repeating content patterns on web pages. Find lists of similar content items like blog posts, articles, job listings, products, news items, etc.\n",
    "\n",
    "Focus on identifying the PRIMARY pattern that represents the main content list on the page.\n",
    "\n",
    "Also identify CSS selectors for publication dates:\n",
    "- <time> elements with datetime attributes\n",
    "- Elements with classes containing 'date', 'time', 'published', 'created'\n",
    "- Date patterns like \"2024-01-15\", \"Jan 15, 2024\", \"15 January 2024\"\n",
    "- Date information in parent/sibling elements of the main content links\"\"\"\n",
    "\n",
    "        # Prepare context\n",
    "        context_summary = {\n",
    "            'url': base_url,\n",
    "            'total_links': structure_analysis['total_links'],\n",
    "            'top_parent_patterns': list(structure_analysis['parent_patterns'].keys())[:5],\n",
    "            'top_url_patterns': list(structure_analysis['url_patterns'].keys())[:5],\n",
    "            'sample_links': [\n",
    "                {\n",
    "                    'text': ctx['text'][:50],\n",
    "                    'href': ctx['href'],\n",
    "                    'parent': f\"{ctx['parent_tag']}.{ctx['parent_class']}\" if ctx['parent_class'] else ctx['parent_tag']\n",
    "                }\n",
    "                for ctx in structure_analysis['link_contexts'][:10]\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        user_prompt = f\"\"\"Analyze this webpage to identify the main repeating content pattern.\n",
    "\n",
    "URL: {base_url}\n",
    "\n",
    "STRUCTURE ANALYSIS:\n",
    "- Total links: {context_summary['total_links']}\n",
    "- Most common parent patterns: {context_summary['top_parent_patterns']}\n",
    "- Most common URL patterns: {context_summary['top_url_patterns']}\n",
    "\n",
    "SAMPLE LINKS:\n",
    "{json.dumps(context_summary['sample_links'], indent=2)}\n",
    "\n",
    "HTML CONTENT (first 6000 chars):\n",
    "{html_content[:6000]}\n",
    "\n",
    "Identify:\n",
    "1. Content pattern type (blog_posts, job_listings, articles, etc.)\n",
    "2. CSS selectors for main content links\n",
    "3. URL regex pattern for content\n",
    "4. Text indicators for content vs navigation\n",
    "5. CSS selectors for publication dates\n",
    "6. Confidence score (0.0-1.0)\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.parse(\n",
    "                model=\"gpt-4o-2024-08-06\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                response_format=PatternAnalysis\n",
    "            )\n",
    "            \n",
    "            result = response.choices[0].message.parsed\n",
    "            print(f\"✅ Pattern identified: {result.pattern_type} (confidence: {result.confidence_score})\")\n",
    "            \n",
    "            # Save to cache\n",
    "            self._save_cached_pattern(base_url, result)\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ LLM analysis failed: {e}\")\n",
    "            return PatternAnalysis(\n",
    "                pattern_type=\"unknown\",\n",
    "                primary_selectors=[\"a[href]\"],\n",
    "                fallback_selectors=[\"article a\", \".post a\", \".item a\"],\n",
    "                url_pattern_regex=\".*\",\n",
    "                content_indicators=[\".*\"],\n",
    "                skip_patterns=[\"about\", \"contact\", \"home\"],\n",
    "                confidence_score=0.1,\n",
    "                pattern_description=\"Fallback pattern - all links\",\n",
    "                estimated_items=0,\n",
    "                date_selectors=[\"time\", \".date\", \".published\", \"[datetime]\"]\n",
    "            )\n",
    "    \n",
    "    def extract_using_pattern(self, soup: BeautifulSoup, base_url: str, pattern: PatternAnalysis) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Extract links using the identified pattern.\"\"\"\n",
    "        extracted_links = []\n",
    "        selectors_to_try = pattern.primary_selectors + pattern.fallback_selectors\n",
    "        \n",
    "        for selector in selectors_to_try:\n",
    "            try:\n",
    "                links = soup.select(selector)\n",
    "                \n",
    "                for element in links:\n",
    "                    # Handle both direct links and elements containing links\n",
    "                    if element.name == 'a':\n",
    "                        link = element\n",
    "                        container = element.parent\n",
    "                    else:\n",
    "                        link = element.find('a', href=True)\n",
    "                        if not link:\n",
    "                            continue\n",
    "                        container = element\n",
    "                    \n",
    "                    href = link.get('href')\n",
    "                    text = link.get_text(strip=True)\n",
    "                    \n",
    "                    if not href or not text:\n",
    "                        continue\n",
    "                    \n",
    "                    full_url = urljoin(base_url, href)\n",
    "                    \n",
    "                    # Validate against pattern\n",
    "                    if self._matches_pattern(href, text, full_url, pattern):\n",
    "                        # Extract publication date\n",
    "                        pub_date = self._extract_publication_date(container, pattern.date_selectors)\n",
    "                        \n",
    "                        link_data = {\n",
    "                            'url': full_url,\n",
    "                            'title': text,\n",
    "                            'selector_used': selector\n",
    "                        }\n",
    "                        \n",
    "                        if pub_date:\n",
    "                            link_data['publication_date'] = pub_date\n",
    "                        \n",
    "                        extracted_links.append(link_data)\n",
    "                \n",
    "                # If primary selectors worked, use them\n",
    "                if extracted_links and selector in pattern.primary_selectors:\n",
    "                    break\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        # Remove duplicates\n",
    "        unique_links = []\n",
    "        seen_urls = set()\n",
    "        \n",
    "        for link in extracted_links:\n",
    "            if link['url'] not in seen_urls:\n",
    "                seen_urls.add(link['url'])\n",
    "                unique_links.append(link)\n",
    "        \n",
    "        return unique_links\n",
    "    \n",
    "    def _matches_pattern(self, href: str, text: str, full_url: str, pattern: PatternAnalysis) -> bool:\n",
    "        \"\"\"Check if a link matches the identified pattern.\"\"\"\n",
    "        \n",
    "        # Check skip patterns\n",
    "        for skip_pattern in pattern.skip_patterns:\n",
    "            if re.search(skip_pattern, href, re.IGNORECASE) or re.search(skip_pattern, text, re.IGNORECASE):\n",
    "                return False\n",
    "        \n",
    "        # Basic quality checks\n",
    "        if len(text.strip()) < 3:\n",
    "            return False\n",
    "            \n",
    "        # Skip obvious navigation\n",
    "        nav_patterns = [\n",
    "            r'^(home|about|contact|privacy|terms)$',\n",
    "            r'^(next|prev|previous|more)$',\n",
    "            r'^\\d+$',\n",
    "            r'^(←|→|>>|<<)$'\n",
    "        ]\n",
    "        \n",
    "        for nav_pattern in nav_patterns:\n",
    "            if re.search(nav_pattern, text, re.IGNORECASE):\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _extract_publication_date(self, container: BeautifulSoup, date_selectors: List[str]) -> Optional[str]:\n",
    "        \"\"\"Extract publication date from container element.\"\"\"\n",
    "        if not container or not date_selectors:\n",
    "            return None\n",
    "        \n",
    "        # Try each date selector\n",
    "        for selector in date_selectors:\n",
    "            try:\n",
    "                date_elements = container.select(selector)\n",
    "                \n",
    "                if not date_elements:\n",
    "                    # Try fallback selectors\n",
    "                    general_selectors = ['time', '[datetime]', '.date', '.published', '.created']\n",
    "                    for gen_sel in general_selectors:\n",
    "                        date_elements = container.select(gen_sel)\n",
    "                        if date_elements:\n",
    "                            break\n",
    "                \n",
    "                for date_element in date_elements:\n",
    "                    # Check datetime attribute\n",
    "                    datetime_attr = date_element.get('datetime')\n",
    "                    if datetime_attr:\n",
    "                        return self._standardize_date(datetime_attr)\n",
    "                    \n",
    "                    # Check element text\n",
    "                    date_text = date_element.get_text(strip=True)\n",
    "                    if date_text and self._is_valid_date_text(date_text):\n",
    "                        return self._standardize_date(date_text)\n",
    "                    \n",
    "                    # For time elements\n",
    "                    if date_element.name == 'time' and date_text:\n",
    "                        return self._standardize_date(date_text)\n",
    "                        \n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        # Fallback: search container text for date patterns\n",
    "        return self._extract_date_from_text(container.get_text())\n",
    "    \n",
    "    def _extract_date_from_text(self, text: str) -> Optional[str]:\n",
    "        \"\"\"Extract date from text using regex patterns.\"\"\"\n",
    "        if not text:\n",
    "            return None\n",
    "        \n",
    "        date_patterns = [\n",
    "            r'\\b\\d{4}-\\d{2}-\\d{2}\\b',  # 2024-01-15\n",
    "            r'\\b\\d{2}-\\d{2}\\b',  # 06-17\n",
    "            r'\\b\\d{1,2}-\\d{1,2}\\b',  # 6-17\n",
    "            r'\\b\\d{2}/\\d{2}/\\d{4}\\b',  # 01/15/2024\n",
    "            r'\\b\\d{1,2}/\\d{1,2}/\\d{4}\\b',  # 1/15/2024\n",
    "            r'\\b\\d{1,2}\\s+\\w+\\s+\\d{4}\\b',  # 15 January 2024\n",
    "            r'\\b\\w+\\s+\\d{1,2},\\s+\\d{4}\\b',  # January 15, 2024\n",
    "            r'\\b\\w{3}\\s+\\d{1,2},\\s+\\d{4}\\b',  # Jan 15, 2024\n",
    "        ]\n",
    "        \n",
    "        for pattern in date_patterns:\n",
    "            match = re.search(pattern, text)\n",
    "            if match:\n",
    "                found_date = match.group(0)\n",
    "                return self._standardize_date(found_date)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _is_valid_date_text(self, text: str) -> bool:\n",
    "        \"\"\"Check if text looks like a valid date.\"\"\"\n",
    "        if not text or len(text) < 2:\n",
    "            return False\n",
    "        \n",
    "        date_patterns = [\n",
    "            r'\\b\\d{4}-\\d{2}-\\d{2}\\b',\n",
    "            r'\\b\\d{2}-\\d{2}\\b',\n",
    "            r'\\b\\d{1,2}-\\d{1,2}\\b',\n",
    "            r'\\b\\d{2}/\\d{2}/\\d{4}\\b',\n",
    "            r'\\b\\d{1,2}/\\d{1,2}/\\d{4}\\b',\n",
    "            r'\\b\\d{1,2}\\s+\\w+\\s+\\d{4}\\b',\n",
    "            r'\\b\\w+\\s+\\d{1,2},\\s+\\d{4}\\b',\n",
    "            r'\\b\\w{3}\\s+\\d{1,2},\\s+\\d{4}\\b',\n",
    "            r'\\b\\d{4}\\b',\n",
    "        ]\n",
    "        \n",
    "        return any(re.search(pattern, text) for pattern in date_patterns)\n",
    "    \n",
    "    def _standardize_date(self, date_str: str) -> Optional[str]:\n",
    "        \"\"\"Standardize date to YYYY-MM-DD format.\"\"\"\n",
    "        if not date_str:\n",
    "            return None\n",
    "        \n",
    "        current_year = datetime.now().year\n",
    "        \n",
    "        try:\n",
    "            # Already ISO format\n",
    "            if re.match(r'\\b\\d{4}-\\d{2}-\\d{2}\\b', date_str):\n",
    "                return date_str\n",
    "            \n",
    "            # MM-DD format\n",
    "            if re.match(r'\\b\\d{2}-\\d{2}\\b', date_str):\n",
    "                month, day = date_str.split('-')\n",
    "                return f\"{current_year}-{month}-{day}\"\n",
    "            \n",
    "            # M-DD format\n",
    "            if re.match(r'\\b\\d{1,2}-\\d{1,2}\\b', date_str):\n",
    "                month, day = date_str.split('-')\n",
    "                return f\"{current_year}-{month.zfill(2)}-{day.zfill(2)}\"\n",
    "            \n",
    "            # MM/DD/YYYY format\n",
    "            if re.match(r'\\b\\d{1,2}/\\d{1,2}/\\d{4}\\b', date_str):\n",
    "                parts = date_str.split('/')\n",
    "                month, day, year = parts[0], parts[1], parts[2]\n",
    "                return f\"{year}-{month.zfill(2)}-{day.zfill(2)}\"\n",
    "            \n",
    "            # Month DD, YYYY format\n",
    "            month_match = re.match(r'\\b(\\w+)\\s+(\\d{1,2}),\\s+(\\d{4})\\b', date_str)\n",
    "            if month_match:\n",
    "                month_name, day, year = month_match.groups()\n",
    "                month_num = self._month_name_to_number(month_name)\n",
    "                if month_num:\n",
    "                    return f\"{year}-{month_num.zfill(2)}-{day.zfill(2)}\"\n",
    "            \n",
    "            # DD Month YYYY format\n",
    "            day_month_match = re.match(r'\\b(\\d{1,2})\\s+(\\w+)\\s+(\\d{4})\\b', date_str)\n",
    "            if day_month_match:\n",
    "                day, month_name, year = day_month_match.groups()\n",
    "                month_num = self._month_name_to_number(month_name)\n",
    "                if month_num:\n",
    "                    return f\"{year}-{month_num.zfill(2)}-{day.zfill(2)}\"\n",
    "            \n",
    "            # Mon DD, YYYY format\n",
    "            short_month_match = re.match(r'\\b(\\w{3})\\s+(\\d{1,2}),\\s+(\\d{4})\\b', date_str)\n",
    "            if short_month_match:\n",
    "                month_abbr, day, year = short_month_match.groups()\n",
    "                month_num = self._month_abbr_to_number(month_abbr)\n",
    "                if month_num:\n",
    "                    return f\"{year}-{month_num.zfill(2)}-{day.zfill(2)}\"\n",
    "            \n",
    "            return date_str\n",
    "            \n",
    "        except Exception:\n",
    "            return date_str\n",
    "    \n",
    "    def _month_name_to_number(self, month_name: str) -> Optional[str]:\n",
    "        \"\"\"Convert full month name to number.\"\"\"\n",
    "        months = {\n",
    "            'january': '01', 'february': '02', 'march': '03', 'april': '04',\n",
    "            'may': '05', 'june': '06', 'july': '07', 'august': '08',\n",
    "            'september': '09', 'october': '10', 'november': '11', 'december': '12'\n",
    "        }\n",
    "        return months.get(month_name.lower())\n",
    "    \n",
    "    def _month_abbr_to_number(self, month_abbr: str) -> Optional[str]:\n",
    "        \"\"\"Convert abbreviated month name to number.\"\"\"\n",
    "        months = {\n",
    "            'jan': '01', 'feb': '02', 'mar': '03', 'apr': '04',\n",
    "            'may': '05', 'jun': '06', 'jul': '07', 'aug': '08',\n",
    "            'sep': '09', 'oct': '10', 'nov': '11', 'dec': '12'\n",
    "        }\n",
    "        return months.get(month_abbr.lower())\n",
    "    \n",
    "    async def extract_pattern_links(self, url: str, force_regenerate: bool = False) -> ExtractedPattern:\n",
    "        \"\"\"Main method to extract patterned links from any webpage.\"\"\"\n",
    "        \n",
    "        print(f\"🌐 Extracting from: {url}\")\n",
    "        \n",
    "        # Extract HTML\n",
    "        html_result = await self.extractor.extract_html(url)\n",
    "        if not html_result:\n",
    "            return self._empty_result(url, \"HTML extraction failed\")\n",
    "        \n",
    "        soup = BeautifulSoup(html_result, 'html.parser')\n",
    "        title = soup.find('title')\n",
    "        print(f\"📄 Page: {title.get_text() if title else 'No title'}\")\n",
    "        \n",
    "        # Analyze structure\n",
    "        structure_analysis = self.analyze_html_structure(soup, url)\n",
    "        \n",
    "        # Get patterns (cached or LLM)\n",
    "        try:\n",
    "            pattern = await self.analyze_patterns_with_llm(url, html_result, structure_analysis, force_regenerate)\n",
    "        except Exception as e:\n",
    "            return self._empty_result(url, f\"Pattern analysis failed: {e}\")\n",
    "        \n",
    "        # Extract using pattern\n",
    "        try:\n",
    "            links = self.extract_using_pattern(soup, url, pattern)\n",
    "        except Exception as e:\n",
    "            return self._empty_result(url, f\"Extraction failed: {e}\")\n",
    "        \n",
    "        result = ExtractedPattern(\n",
    "            links=links,\n",
    "            total_found=len(links),\n",
    "            pattern_used=pattern.pattern_type,\n",
    "            confidence=pattern.confidence_score,\n",
    "            base_url=url,\n",
    "            pattern_analysis=pattern\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Found {len(links)} {pattern.pattern_type} items\")\n",
    "        return result\n",
    "    \n",
    "    def _empty_result(self, url: str, reason: str) -> ExtractedPattern:\n",
    "        \"\"\"Return empty result with error info.\"\"\"\n",
    "        return ExtractedPattern(\n",
    "            links=[],\n",
    "            total_found=0,\n",
    "            pattern_used=\"failed\",\n",
    "            confidence=0.0,\n",
    "            base_url=url,\n",
    "            pattern_analysis=PatternAnalysis(\n",
    "                pattern_type=\"error\",\n",
    "                primary_selectors=[],\n",
    "                fallback_selectors=[],\n",
    "                url_pattern_regex=\"\",\n",
    "                content_indicators=[],\n",
    "                skip_patterns=[],\n",
    "                confidence_score=0.0,\n",
    "                pattern_description=reason,\n",
    "                estimated_items=0,\n",
    "                date_selectors=[]\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def to_json(self, result: ExtractedPattern) -> Dict[str, Any]:\n",
    "        \"\"\"Convert result to JSON format with url, title, and publication_date.\"\"\"\n",
    "        clean_items = []\n",
    "        for item in result.links:\n",
    "            clean_item = {\n",
    "                \"url\": item[\"url\"],\n",
    "                \"title\": item[\"title\"],\n",
    "                \"publication_date\": item.get(\"publication_date\", \"\")\n",
    "            }\n",
    "            clean_items.append(clean_item)\n",
    "        \n",
    "        return {\n",
    "            \"extraction_info\": {\n",
    "                \"base_url\": result.base_url,\n",
    "                \"total_items_found\": result.total_found,\n",
    "                \"pattern_type\": result.pattern_used,\n",
    "                \"confidence_score\": result.confidence\n",
    "            },\n",
    "            \"links\": clean_items\n",
    "        }\n",
    "\n",
    "# Simple extraction function\n",
    "async def extract_pattern_links(url: str, force_regenerate: bool = False) -> Dict[str, Any]:\n",
    "    \"\"\"Extract patterned links and return JSON directly.\"\"\"\n",
    "    extractor = UniversalPatternExtractor()\n",
    "    result = await extractor.extract_pattern_links(url, force_regenerate)\n",
    "    return extractor.to_json(result)\n",
    "\n",
    "# Example usage\n",
    "url1 = \"https://www.ssp.sh/posts/\"\n",
    "url2 = \"https://www.joanwestenberg.com/\"\n",
    "\n",
    "json_result1 = await extract_pattern_links(url2, force_regenerate=True)\n",
    "\n",
    "print(json.dumps(json_result1, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
