Metadata-Version: 2.1
Name: vllmctl
Version: 0.2.1
Summary: CLI tool for launching and managing vllm model servers via SSH and tmux
License: MIT
Author: vllmctl
Author-email: adefful46@gmail.com
Requires-Python: >=3.8,<4.0
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Requires-Dist: psutil (>=5.9.0)
Requires-Dist: requests (>=2.28.0)
Requires-Dist: rich (>=13.0.0)
Requires-Dist: typer (>=0.9.0)
Description-Content-Type: text/markdown

# vllmctl

A powerful CLI for launching, managing, and monitoring vLLM model servers on remote machines via SSH and tmux.

---

## âš ï¸ SSH Configuration Required

Many commands in `vllmctl` rely on your SSH configuration (`~/.ssh/config`).
- Make sure all your remote servers are properly listed in your SSH config.
- The tool will automatically discover and use these hosts for remote operations, port forwarding, and GPU monitoring.

Example SSH config entry:
```
Host myserver
    HostName myserver.example.com
    User myuser
    IdentityFile ~/.ssh/id_rsa
```

---

## ğŸš€ Features
- Launch vLLM servers on remote hosts in isolated tmux sessions
- Automatic SSH tunneling for secure local API access
- Real-time health checks and queue monitoring
- List, attach, and kill tmux sessions for full process control
- GPU utilization dashboard across your cluster
- Flexible model/port/env selection per launch
- Safe for production: no processes die on SSH disconnect

---

## ğŸ“¦ Installation

```bash
pip install -r requirements.txt
```
- Requires Python 3.8+
- Ensure `tmux` is installed on both local and remote machines
- Passwordless SSH access is recommended

## ğŸš Shell Autocompletion

To enable shell autocompletion for vllmctl, run:

```bash
vllmctl --install-completion
```

Or, to see the completion script for your shell:

```bash
vllmctl --show-completion
```

You can add the output to your shell profile (e.g., `.bashrc`, `.zshrc`) for persistent autocompletion.

---

## ğŸ› ï¸ Commands Overview

### 1. `list_local`
Show all local vLLM models (including forwarded ports).

```bash
vllmctl list-local
```

**Sample Output:**
```
â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Server   â”ƒ Remote portâ”ƒ Local portâ”ƒ Status       â”ƒ Model                                â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ -        â”‚ -          â”‚ 8000      â”‚ Local launch â”‚ Qwen/Qwen2.5-Coder-32B-Instruct      â”‚
â”‚ server1  â”‚ 8000       â”‚ 16100     â”‚ Forwarded    â”‚ Llama-2-13B-chat                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 2. `list_remote`
Show vLLM models running on all servers from your SSH config.

```bash
vllmctl list-remote [--host-regex <pattern>] [--remote-port <port>] [--debug]
```

---

### 3. `auto_forward`
Automatically forward ports with running models to your local machine.

```bash
vllmctl auto-forward [--host-regex <pattern>] [--remote-port <port>] [--local-range <start-end>] [--no-kill] [--debug]
```

---

### 4. `tmux_forwards`
Show all tmux-based SSH forwards and their status.

```bash
vllmctl tmux-forwards
```

---

### 5. `vllm_queue_top`
Real-time dashboard for vLLM queue status on all local ports (like `nvtop` for vLLM).

```bash
vllmctl vllm-queue-top
```

**Sample Output:**
```
Scanning ports for vLLM models... â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:00:00
                                         \ vLLM Queue Status (refreshes every 1.0s)                                          
â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”“
â”ƒ Local Port â”ƒ Model          â”ƒ Waiting â”ƒ Running â”ƒ Wait graph             â”ƒ Run graph               â”ƒ Prompt TPT â”ƒ Gen TPT â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”©
â”‚ 16004      â”‚ Qwen/Qwen3-32B â”‚ 774     â”‚ 226     â”‚ â–â–â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–ƒâ–„â–„â–„â–…â–…â–…â–…â€¦ â”‚ â–ˆâ–‡â–†â–†â–†â–†â–…â–†â–†â–…â–…â–„â–„â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â€¦ â”‚ -          â”‚ -       â”‚
â”‚ 16101      â”‚ Qwen/Qwen3-32B â”‚ 774     â”‚ 226     â”‚ â–â–â–‚â–‚â–â–‚â–ƒâ–â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â€¦ â”‚ â–ˆâ–‡â–†â–†â–‡â–†â–…â–‡â–†â–…â–…â–…â–„â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â€¦ â”‚ -          â”‚ -       â”‚
â”‚ 16102      â”‚ Qwen/Qwen3-4B  â”‚ 663     â”‚ 113     â”‚ â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–„â–…â–…â–„â–…â–…â€¦ â”‚ â–ˆâ–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â€¦ â”‚ -          â”‚ -       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 6. `gpu_idle_top`
Live GPU utilization and memory dashboard for all servers in your SSH config.

```bash
vllmctl gpu-idle-top --host-regex <pattern>
```

**Sample Output:**
```
Scanning GPU utilization on hosts... â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:00:27
                            | GPU Idle Top (refreshes every 0.5s)                            
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Host              â”ƒ Util (%) â”ƒ Util Graph                     â”ƒ Mem (%) â”ƒ Mem Graph â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ host-a            â”‚ 0.0      â”‚ â–â–                             â”‚ 5.1     â”‚ â–â–        â”‚
â”‚ host-b            â”‚ 0.0      â”‚ â–â–                             â”‚ 94.4    â”‚ â–â–        â”‚
â”‚ host-c            â”‚ 0.0      â”‚ â–â–                             â”‚ 0.0     â”‚ â–â–        â”‚
â”‚ host-d            â”‚ 86.5     â”‚                             â–ˆâ– â”‚ 90.9    â”‚ â–â–        â”‚
â”‚ host-e            â”‚ 89.0     â”‚                             â–ˆâ– â”‚ 59.3    â”‚ â–â–        â”‚
â”‚ host-f            â”‚ 91.9     â”‚                             â–â–ˆ â”‚ 92.9    â”‚ â–â–        â”‚
â”‚ host-g            â”‚ 95.0     â”‚                             â–ˆâ– â”‚ 93.3    â”‚ â–â–        â”‚
â”‚ host-h            â”‚ 97.4     â”‚                             â–â–ˆ â”‚ 52.3    â”‚ â–â–        â”‚
â”‚ host-i            â”‚ 100.0    â”‚                             â–â–ˆ â”‚ 91.9    â”‚ â–â–        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 7. `serve` (recommended)
Launch a vLLM server on a remote host and set up a local SSH tunnel.

```bash
vllmctl serve --server <host> [OPTIONS] <model_name> [EXTRA_ARGS]
```

**Key options:**
- `--conda-env <env>`: Conda environment to use on the remote server (default: vllm_env)
- `--local-range <start-end>`: Range of local ports for forwarding (default: 16100-16199)
- `--timeout <seconds>`: Maximum waiting time for vLLM API to become available (default: 600)
- `--lifetime <duration>`: Maximum lifetime for the vLLM process. Supports formats like `10m` (minutes), `2h` (hours), `1d` (days), `30s` (seconds)
- `--tensor-parallel-size <N>`: Number of GPUs to use (passed to vllm serve)
- `--remote-port <port>`: Port to use on the remote server (default: 8000)
- Any additional arguments after the model name are passed directly to `vllm serve` (e.g. `--reasoning-parser ...`)

**Examples:**
```bash
vllmctl serve --server myserver Qwen/Qwen3-4B --tensor-parallel-size 2 --remote-port 8001
vllmctl serve --server myserver --lifetime 2h Qwen/Qwen3-4B --tensor-parallel-size 2 --port 8001
vllmctl serve --server myserver Qwen/Qwen3-4B --reasoning-parser deepseek_r1 --tensor-parallel-size 8
```

- After the specified lifetime, the vLLM server will be automatically stopped on the remote server.
- Both the vLLM process and the SSH tunnel run in tmux sessions for reliability.
- You can view logs with:
  ```bash
  ssh <host> tmux attach -t vllmctl_server_<port>
  ```

#### âš ï¸ `launch` is deprecated
The `launch` command is now deprecated and will be removed in a future release. Please use `serve` instead. If you call `launch`, it will redirect to `serve` and print a warning.

---

### 8. Other Utilities

- **Attach to tmux session:**
  ```bash
  vllmctl attach-tmux <session_name>
  ```
- **Kill a tmux session:**
  ```bash
  vllmctl kill-tmux <session_name>
  ```
- **Clean up dead/unused tmux sessions:**
  ```bash
  vllmctl clean-tmux-forwards
  ```

---

## ğŸ“ Best Practices
- Always use tmux for remote process management
- Use SSH keys for authentication
- Monitor endpoints with health checks and logs
- Clean up unused sessions regularly
- For production, consider systemd for static deployments

---

## â„¹ï¸ Help
All commands support `--help` for detailed usage:

```bash
vllmctl <command> --help
```

---

## License
MIT



