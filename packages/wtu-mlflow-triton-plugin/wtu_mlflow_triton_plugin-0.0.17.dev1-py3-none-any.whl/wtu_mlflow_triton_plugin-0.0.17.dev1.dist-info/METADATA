Metadata-Version: 2.4
Name: wtu-mlflow-triton-plugin
Version: 0.0.17.dev1
Summary: MLflow plugin for Triton Inference Server with secure Python function execution
Project-URL: Homepage, https://github.com/hbjs/w-train-utils-mlflow-triton-plugin
Project-URL: Bug Tracker, https://github.com/hbjs/w-train-utils-mlflow-triton-plugin/issues
Project-URL: Documentation, https://github.com/hbjs/w-train-utils-mlflow-triton-plugin#readme
Author-email: hbjs <hbjs97@naver.com>
License: MIT
License-File: LICENSE
Keywords: deployment,inference,machine-learning,mlflow,triton
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Requires-Python: >=3.9
Requires-Dist: boto3>=1.24.0
Requires-Dist: cryptography>=45.0.4
Requires-Dist: mlflow==1.30.1
Requires-Dist: numpy>=1.26.4
Requires-Dist: requests==2.32.4
Requires-Dist: tqdm==4.67.1
Requires-Dist: tritonclient[grpc,http]>=2.36.0
Description-Content-Type: text/markdown

# w-train-utils-mlflow-triton-plugin

MLflow plugin for Triton Inference Server with secure Python function execution

## ğŸ“š ì£¼ìš” ê¸°ëŠ¥

### ğŸ”Œ MLflow-Triton í†µí•©

- MLflow ëª¨ë¸ì„ Triton Inference Serverë¡œ ìë™ ë°°í¬
- Python í•¨ìˆ˜ë¥¼ Triton ëª¨ë¸ë¡œ ë³€í™˜
- S3/MinIO ê¸°ë°˜ ëª¨ë¸ ì €ì¥ì†Œ ì§€ì›

### ğŸ› ï¸ Keynet CLI

- Python í•¨ìˆ˜ ê²€ì¦ ë° í…ŒìŠ¤íŠ¸
- í•¨ìˆ˜ ë°°í¬ ë° ê´€ë¦¬
- ì¸ì¦ ë° ìê²© ì¦ëª… ê´€ë¦¬
- ë‹¤ì–‘í•œ Python ë²„ì „ ì§€ì› (3.9, 3.10, 3.11, 3.12)

## ğŸš€ ì„¤ì¹˜

```bash
pip install wtu-mlflow-triton-plugin
```

## ğŸ“– ì‚¬ìš©ë²•

### 1. Python í•¨ìˆ˜ ì‘ì„±

```python
# my_function.py
from wtu_mlflow_triton_plugin.function import keynet_function

@keynet_function("my_model")
def main(args):
    """ì…ë ¥ì„ ë°›ì•„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜"""
    # argsëŠ” ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ ì…ë ¥
    text = args.get("text", "")
    result = text.upper()

    return {
        "result": result,
        "length": len(text)
    }
```

### 2. Keynet CLIë¡œ í•¨ìˆ˜ ê²€ì¦

```bash
# í•¨ìˆ˜ ê²€ì¦
keynet validate my_function.py

# ì˜ì¡´ì„±ê³¼ í•¨ê»˜ ê²€ì¦
keynet validate my_function.py -r requirements.txt

# í…ŒìŠ¤íŠ¸ íŒŒë¼ë¯¸í„°ë¡œ ì‹¤í–‰
keynet test my_function.py --params '{"text": "hello world"}'
```

### 3. í•¨ìˆ˜ ë°°í¬

```bash
# ë¡œì»¬ í™˜ê²½ì— ë°°í¬
keynet deploy my_function.py --name my_model

# ì›ê²© ì„œë²„ì— ë°°í¬
keynet deploy my_function.py --name my_model --server https://api.example.com
```

### 4. MLflowì™€ í†µí•©

```python
import mlflow
from wtu_mlflow_triton_plugin import TritonPlugin

# MLflow ì„¤ì •
mlflow.set_tracking_uri("http://localhost:5001")

# Triton í”ŒëŸ¬ê·¸ì¸ í™œì„±í™”
with mlflow.start_run():
    # ëª¨ë¸ ë¡œê¹…
    mlflow.pyfunc.log_model(
        artifact_path="model",
        python_model=my_model,
        deployment_flavor="triton"
    )

    # Tritonìœ¼ë¡œ ë°°í¬
    mlflow.deployments.create_deployment(
        name="my-deployment",
        model_uri=f"runs:/{run_id}/model",
        flavor="triton"
    )
```

## ğŸ”§ í™˜ê²½ ì„¤ì •

### í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜

| í™˜ê²½ë³€ìˆ˜               | ì„¤ëª…                   | ì˜ˆì‹œ                    |
| ---------------------- | ---------------------- | ----------------------- |
| MLFLOW_S3_ENDPOINT_URL | MinIO ì—”ë“œí¬ì¸íŠ¸ URL   | `http://localhost:9000` |
| MLFLOW_TRACKING_URI    | MLflow íŠ¸ë˜í‚¹ ì„œë²„ URI | `http://localhost:5001` |
| AWS_ACCESS_KEY_ID      | AWS/MinIO ì•¡ì„¸ìŠ¤ í‚¤    | `minio`                 |
| AWS_SECRET_ACCESS_KEY  | AWS/MinIO ì‹œí¬ë¦¿ í‚¤    | `miniostorage`          |
| TRITON_URL             | Triton gRPC ì—”ë“œí¬ì¸íŠ¸ | `http://localhost:8001` |
| TRITON_MODEL_REPO      | Triton ëª¨ë¸ ì €ì¥ì†Œ URL | `s3://bucket/models`    |

### Triton Inference Server ì‹¤í–‰

```bash
docker run --rm -p8000:8000 -p8001:8001 -p8002:8002 \
    -e AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \
    -e AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \
    nvcr.io/nvidia/tritonserver:24.01-py3 \
    tritonserver --model-repository=$TRITON_MODEL_REPO \
    --model-control-mode=explicit \
    --log-verbose=1
```

## ğŸ“‹ Keynet CLI ëª…ë ¹ì–´

### ê¸°ë³¸ ëª…ë ¹ì–´

```bash
# í•¨ìˆ˜ ê²€ì¦
keynet validate <python_file> [-r requirements.txt]

# í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
keynet test <python_file> [--params JSON] [-r requirements.txt]

# í•¨ìˆ˜ ë°°í¬
keynet deploy <python_file> --name <model_name> [--server URL]
```

### ì¸ì¦ ê´€ë¦¬

```bash
# ìê²© ì¦ëª… ë“±ë¡
keynet login https://api.example.com

# íŠ¹ì • ì„œë²„ì—ì„œ ë¡œê·¸ì•„ì›ƒ
keynet logout --server api.example.com

# ëª¨ë“  ì„œë²„ì—ì„œ ë¡œê·¸ì•„ì›ƒ
keynet logout --all
```

### ê³ ê¸‰ ì˜µì…˜

```bash
# Python ë²„ì „ ì§€ì •
keynet validate my_function.py --python 3.11

# íƒ€ì„ì•„ì›ƒ ì„¤ì •
keynet test my_function.py --import-timeout 60 --execution-timeout 120

# ìƒì„¸ ë¡œê·¸ ì¶œë ¥
keynet validate my_function.py -v

# ìºì‹œ ì •ë¦¬
keynet cache clear
```

## ğŸ—ï¸ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
wtu_mlflow_triton_plugin/
â”œâ”€â”€ auth/               # ì¸ì¦ ë° ìê²© ì¦ëª… ê´€ë¦¬
â”œâ”€â”€ function/           # í•¨ìˆ˜ ë¹Œë” ë° CLI
â”‚   â”œâ”€â”€ cli.py         # Keynet CLI êµ¬í˜„
â”‚   â”œâ”€â”€ client.py      # API í´ë¼ì´ì–¸íŠ¸
â”‚   â”œâ”€â”€ models.py      # ë°ì´í„° ëª¨ë¸
â”‚   â”œâ”€â”€ validator.py   # í•¨ìˆ˜ ê²€ì¦ê¸°
â”‚   â””â”€â”€ venv_manager.py # ê°€ìƒí™˜ê²½ ê´€ë¦¬
â”œâ”€â”€ plugin.py          # MLflow í”ŒëŸ¬ê·¸ì¸
â”œâ”€â”€ storage.py         # S3/MinIO ì €ì¥ì†Œ ê´€ë¦¬
â””â”€â”€ config.py          # ì„¤ì • ê´€ë¦¬
```

## ğŸ”’ ë³´ì•ˆ ê¸°ëŠ¥ ìƒì„¸

### ì½”ë“œ ê²€ì¦

- AST ê¸°ë°˜ êµ¬ë¬¸ ë¶„ì„
- í•„ìˆ˜ í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜ ê²€ì‚¬
- ë°ì½”ë ˆì´í„° ê²€ì¦

### ì‹¤í–‰ ê²©ë¦¬

- ë…ë¦½ëœ ê°€ìƒí™˜ê²½ì—ì„œ ì‹¤í–‰
- ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì ‘ê·¼ ì œí•œ
- íƒ€ì„ì•„ì›ƒ ê¸°ë°˜ ì‹¤í–‰ ì œì–´

### ë©”ëª¨ë¦¬ ë³´í˜¸

- ìµœëŒ€ 512MB ë©”ëª¨ë¦¬ ì œí•œ
- ë©”ëª¨ë¦¬ í­íƒ„ ë°©ì§€
- ë¦¬ì†ŒìŠ¤ ê³ ê°ˆ ë°©ì§€

## ğŸ¤ ê¸°ì—¬í•˜ê¸°

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ ë¼ì´ì„ ìŠ¤

MIT License - ìì„¸í•œ ë‚´ìš©ì€ [LICENSE](LICENSE) íŒŒì¼ì„ ì°¸ì¡°í•˜ì„¸ìš”.

## ğŸ”— ì°¸ê³  ìë£Œ

- [MLflow Documentation](https://www.mlflow.org/docs/latest/index.html)
- [Triton Inference Server](https://github.com/triton-inference-server/server)
- [ë²„ì „ ê´€ë¦¬ ê°€ì´ë“œ](VERSIONING.md)
- [ê°œë°œ ê°€ì´ë“œ](DEVELOPMENT.md)

## ğŸ“ ì§€ì›

- ì´ìŠˆ íŠ¸ë˜ì»¤: [GitHub Issues](https://github.com/hbjs/w-train-utils-mlflow-triton-plugin/issues)
- ì´ë©”ì¼: hbjs97@naver.com
